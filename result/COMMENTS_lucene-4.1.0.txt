#CANONICAL_CLASS_NAME "COMMENTS"
org.apache.lucene.index.IndexWriterConfig "holds all the configuration that is used to create an {@link indexwriter}. once {@link indexwriter} has been specifies the open mode for {@link indexwriter}. creates a new index or overwrites an existing one. opens an existing index. creates a new index if one does not exist, otherwise it opens the index and documents will be appended. default value is 32. change using {@link #settermindexinterval(int)}. denotes a flush trigger is disabled. disabled by default (because indexwriter flushes by ram usage by default). disabled by default (because indexwriter flushes by ram usage by default). default value is 16 mb (which means flush when buffered docs consume approximately 16 mb ram). default value for the write lock timeout (1,000 ms). @see #setdefaultwritelocktimeout(long) default setting for {@link #setreaderpooling}. default value is 1. change using {@link #setreadertermsindexdivisor(int)}. default value is 1945. change using {@link #setramperthreadhardlimitmb(int)} the maximum number of simultaneous threads that may be indexing documents at once in indexwriter; if more than this many threads arrive they will wait for others to finish. default value is 8. sets the default (for any instance) maximum time to wait for a write lock (in milliseconds). returns the default write lock timeout for newly instantiated indexwriterconfigs. @see #setdefaultwritelocktimeout(long) creates a new config that with defaults that match the specified {@link version} as well as the default {@link analyzer}. if matchversion is >= {@link version#lucene_32}, {@link tieredmergepolicy} is used for merging; else {@link logbytesizemergepolicy}. note that {@link tieredmergepolicy} is free to select non-contiguous merges, which means docids may not remain monotonic over time. if this is a problem you should switch to {@link logbytesizemergepolicy} or {@link logdocmergepolicy}. specifies {@link openmode} of the index. only takes effect when indexwriter is first expert: allows an optional {@link indexdeletionpolicy} implementation to be specified. you can use this to control when prior commits are deleted from the index. the default policy is {@link keeponlylastcommitdeletionpolicy} which removes all prior commits as soon as a new commit is done (this matches behavior before 2.2). creating your own policy can allow you to explicitly keep previous "point in time" commits alive in the index for some time, to allow readers to refresh to the new commit without having the old commit deleted out from under them. this is necessary on filesystems like nfs that do not support "delete on last close" semantics, which lucene's "point in time" search normally relies on.  note: the deletion policy cannot be null. if null is passed, the deletion policy will be set to the default. only takes effect when indexwriter is first expert: allows to open a certain commit point. the default is null which opens the latest commit point. only takes effect when indexwriter is first expert: set the {@link similarity} implementation used by this indexwriter.  note: the similarity cannot be null. if null is passed, the similarity will be set to the default implementation (unspecified). only takes effect when indexwriter is first expert: sets the merge scheduler used by this writer. the default is {@link concurrentmergescheduler}.  note: the merge scheduler cannot be null. if null is passed, the merge scheduler will be set to the default. only takes effect when indexwriter is first sets the maximum time to wait for a write lock (in milliseconds) for this instance. you can change the default value for all instances by calling {@link #setdefaultwritelocktimeout(long)}. only takes effect when indexwriter is first expert: {@link mergepolicy} is invoked whenever there are changes to the segments in the index. its role is to select which merges to do, if any, and return a {@link mergepolicy.mergespecification} describing the merges. it also selects merges to do for forcemerge. (the default is {@link logbytesizemergepolicy}. only takes effect when indexwriter is first set the {@link codec}.  only takes effect when indexwriter is first expert: sets the {@link documentswriterperthreadpool} instance used by the indexwriter to assign thread-states to incoming indexing threads. if no {@link documentswriterperthreadpool} is set {@link indexwriter} will use {@link threadaffinitydocumentswriterthreadpool} with max number of thread-states set to {@link #default_max_thread_states} (see {@link #default_max_thread_states}).   note: the given {@link documentswriterperthreadpool} instance must not be used with other {@link indexwriter} instances once it has been initialized / associated with an {@link indexwriter}.   note: this only takes effect when indexwriter is first sets the max number of simultaneous threads that may be indexing documents at once in indexwriter. values &lt; 1 are invalid and if passed maxthreadstates will be set to {@link #default_max_thread_states}. only takes effect when indexwriter is first by default, indexwriter does not pool the segmentreaders it must open for deletions and merging, unless a near-real-time reader has been obtained by calling {@link directoryreader#open(indexwriter, boolean)}. this method lets you enable pooling without getting a near-real-time reader. note: if you set this to false, indexwriter will still pool readers once {@link directoryreader#open(indexwriter, boolean)} is called. only takes effect when indexwriter is first expert: sets the {@link docconsumer} chain to be used to process documents. only takes effect when indexwriter is first expert: controls when segments are flushed to disk during indexing. the {@link flushpolicy} initialized during {@link indexwriter} instantiation and once initialized the given instance is bound to this {@link indexwriter} and should not be used with another writer. @see #setmaxbuffereddeleteterms(int) @see #setmaxbuffereddocs(int) @see #setrambuffersizemb(double) expert: sets the maximum memory consumption per thread triggering a forced flush if exceeded. a {@link documentswriterperthread} is forcefully flushed once it exceeds this limit even if the {@link #getrambuffersizemb()} has not been exceeded. this is a safety limit to prevent a {@link documentswriterperthread} from address space exhaustion due to its internal 32 bit signed integer based memory addressing. the given value must be less that 2gb (2048mb) @see #default_ram_per_thread_hard_limit_mb if non-null, information about merges, deletes and a message when maxfieldlength is reached will be printed to this. convenience method that uses {@link printstreaminfostream} todo: this should be private to the codec, not settable here mostly shallow clone, but do a deepish clone of certain objects that have state that cannot be shared across iw instances:"
org.apache.lucene.index.DocInverter "this is a docfieldconsumer that inverts each field, separately, from a document, and accepts a invertedtermsconsumer to process those terms. todo: allow endconsumer.finishdocument to also return a docwriter"
org.apache.lucene.index.DocumentsWriterDeleteQueue "{@link documentswriterdeletequeue} is a non-blocking linked pending deletes queue. in contrast to other queue implementation we only maintain the tail of the queue. a delete queue is always used in a context of a set of dwpts and a global delete pool. each of the dwpt and the global pool need to maintain their 'own' head of the queue (as a deleteslice instance per dwpt). the difference between the dwpt and the global pool is that the dwpt starts maintaining a head once it has added its first document since for its segments private deletes only the deletes after that document are relevant. the global pool instead starts maintaining the head once this instance is only acquired to update the global deletes we use a sentinel instance as our initial tail. no slice will ever try to apply this tail since the head is always omitted. invariant for document update this is an update request where the term is the updated documents delterm. in that case we need to guarantee that this insert is atomic with regards to the given delete slice. this means if two threads try to update the same document with in turn the same delterm one of them must win. by taking the node we have this non-blocking / 'wait-free' linked list add was inspired by apache harmony's concurrentlinkedqueue implementation. we are in intermediate state here. the tails next pointer has been advanced but the tail itself might not be updated yet. help to advance the tail and try again updating it. we are in quiescent state and can try to insert the item to the current tail if we fail to insert we just retry the operation since somebody else has already added its item now that we are done we need to advance the tail while another thread could have advanced it already so we can ignore the return type of this cas call check if all items in the global slice were applied and if the global slice is up-to-date and if globalbuffereddeletes has changes the global buffer must be locked but we don't need to update them if there is an update going on right now. it is sufficient to apply the deletes that have been added after the current in-flight global slices tail the next time we can get the lock! here we freeze the global buffer so we need to lock it, apply all deletes in the queue and reset the global slice to let the gc prune the queue. initially this is a 0 length slice pointing to the 'current' tail of the queue. once we update the slice we only need to assign the tail and have a new slice when we apply a slice we take the head and get its next as our first item to apply and continue until we applied the tail. if the head and tail in this slice are not equal then there will be at least one more non-null node in the slice! returns true iff the given item is identical to the item hold by the slices tail, otherwise false. sentinel system.out.println(thread.currentthread().getname() + ": push " + termnode + " this=" + this); todo doing this each time is not necessary maybe we can do it just every n times or so? can fail system.out.println(thread.currentthread() + ": apply globalslice"); take the current tail make this local any changes after this call are applied later and not relevant here update the callers slices so we are on the same page system.out.println(thread.currentthread().getname() + ": now freeze global buffer " + globalbuffereddeletes); if we are the same just no need to be volatile, slices are thread captive (only accessed by one thread)! we don't apply this one 0 length slice system.out.println(thread.currentthread().getname() + ": pull " + current + " docidupto=" + docidupto); reset to a 0 length slice"
org.apache.lucene.index.TieredMergePolicy "merges segments of approximately equal size, subject to an allowed number of segments per tier. this is similar to {@link logbytesizemergepolicy}, except this merge policy is able to merge non-adjacent segment, and separates how many segments are merged at once ({@link #setmaxmergeatonce}) from how many segments are allowed per tier ({@link #setsegmentspertier}). this merge policy also does not over-merge (i.e. cascade merges). for normal merging, this policy first computes a "budget" of how many segments are allowed to be in the index. if the index is over-budget, then the policy sorts segments by decreasing size (pro-rating by percent deletes), and then finds the least-cost merge. merge cost is measured by a combination of the "skew" of the merge (size of largest segment divided by smallest segment), total merge size and percent deletes reclaimed, so that merges with lower skew, smaller size and those reclaiming more deletes, are favored. if a merge will produce a segment that's larger than {@link #setmaxmergedsegmentmb}, then the policy will merge fewer segments (down to 1 at once, if that one has deletions) to keep the segment size under budget. note: this policy freely merges non-adjacent segments; if this is a problem, use {@link logmergepolicy}. note: this policy always merges by byte size of the segments, always pro-rates by percent deletes, and does not apply any maximum segment size during forcemerge (unlike {@link logbytesizemergepolicy}). @lucene.experimental sole constructor, setting all settings to their defaults. maximum number of segments to be merged at a time during "normal" merging. for explicit merging (eg, forcemerge or forcemergedeletes was called), see {@link #setmaxmergeatonceexplicit}. default is 10. returns the current maxmergeatonce setting. @see #setmaxmergeatonce maximum number of segments to be merged at a time, during forcemerge or forcemergedeletes. default is 30. returns the current maxmergeatonceexplicit setting. @see #setmaxmergeatonceexplicit maximum sized segment to produce during normal merging. this setting is approximate: the estimate of the merged segment size is made by summing sizes of to-be-merged segments (compensating for percent deleted docs). default is 5 gb. returns the current maxmergedsegmentmb setting. @see #getmaxmergedsegmentmb controls how aggressively merges that reclaim more deletions are favored. higher values favor selecting merges that reclaim deletions. a value of 0.0 means deletions don't impact merge selection. see {@link #setreclaimdeletesweight}. segments smaller than this are "rounded up" to this size, ie treated as equal (floor) size for merge selection. this is to prevent frequent flushing of tiny segments from allowing a long tail in the index. default is 2 mb. returns the current floorsegmentmb. @see #setfloorsegmentmb when forcemergedeletes is called, we only merge away a segment if its delete percentage is over this threshold. default is 10%. returns the current forcemergedeletespctallowed setting. @see #setforcemergedeletespctallowed sets the allowed number of segments per tier. smaller values mean more merging but fewer segments. note: this value should be >= the {@link #setmaxmergeatonce} otherwise you'll force too much merging to occur. default is 10.0. returns the current segmentspertier setting. @see #setsegmentspertier sets whether compound file format should be used for newly flushed and newly merged segments. default true. returns the current usecompoundfile setting. @see #setusecompoundfile if a merged segment will be more than this percentage of the total size of the index, leave the segment as non-compound file even if compound file is enabled. set to 1.0 to always use cfs regardless of merge size. default is 0.1. returns the current nocfsratio setting. @see #setnocfsratio holds score and explanation for a single candidate merge. sole constructor. (for invocation by subclass constructors, typically implicit.) expert: scores one merge; subclasses can override. returns the largest size allowed for a compound file segment if a merged segment will be more than this value, leave the segment as non-compound file even if compound file is enabled. set this to double.positive_infinity (default) and nocfsratio to 1.0 to always use cfs regardless of merge size. todo - we could try to take into account whether a large merge is already running (under cms) and then bias ourselves towards picking smaller merges if so (or, maybe cms should do so) todo: should addindexes do explicit merging, too? and, if compute total index bytes & print details about the index accum total byte size if we have too-large segments, grace them out of the maxsegmentcount: compute max allowed segs in the index cycle to possibly select more than one merge: gather eligible segments for merging, ie segments not already being merged and not already picked (by prior iteration of this loop) for merging: ok we are over budget -- find best merge! consider all merge starts: note: we continue, so that we can try "packing" smaller segments into this merge to see if we can get closer to the max size; this in general is not perfect since this is really "bin packing" and we'd have to try different permutations. if we are already running a max sized merge (maxmergeisrunning), don't allow another max sized merge to kick off: measure "skew" of the merge, which can range from 1.0/numsegsbeingmerged (good) to 1.0 (poor): pretend the merge has perfect skew; skew doesn't matter in this case because this merge will not "cascade" and so it cannot lead to n^2 merge cost over time: strongly favor merges with less skew (smaller mergescore is better): gently favor smaller merges over bigger ones. we don't want to make this exponent too large else we can end up doing poor merges of small segments in order to avoid the large merges: strongly favor merges that reclaim deletes: do full merges, first, backwards: do final merge don't enforce max merged size here: app is explicitly calling forcemergedeletes, and knows this may take a long time / produce big segments (like forcemerge): segment size in bytes, pro-rated by % deleted"
org.apache.lucene.index.DocFieldProcessor "this is a docconsumer that gathers all fields under the same name, and calls per-field consumers to process field by field. this class doesn't doesn't do any "real" work of its own: it just forwards the fields to a docfieldconsumer. in flush we reset the fieldhash to not maintain per-field state across segments holds all fields seen in current doc hash table for all fields ever seen close perdocconsumer during flush to ensure all files are flushed due to percodec cfs important to save after asking consumer to flush so consumer can alter the fieldinfo if necessary. eg, freqproxtermswriter does this with fieldinfo.storepayload. todo add abort to perdocconsumer! if any errors occured, throw it. defensive code - we should not hit unchecked exceptions rehash absorb any new fields first seen in this document. also absorb any changes to fields we had already seen before (eg suddenly turning on norms or vectors, etc.): make sure we have a perfield allocated todo fi: we need to genericize the "flags" that a field holds, and, how these flags are merged; it needs to be more "pluggable" such that if i want to have a new "thing" my fields can do, i can easily add it first time we're seeing this field for this doc if we are writing vectors then we must visit fields in sorted order so they are written in sorted order. todo: we actually only need to sort the subset of fields that have vectors enabled; we could save [small amount of] cpu here. only used to enforce that same dv field name is never added more than once per doc:"
org.apache.lucene.index.LogByteSizeMergePolicy "this is a {@link logmergepolicy} that measures size of a segment as the total byte size of the segment's files. default minimum segment size. @see setminmergemb default maximum segment size. a segment of this size or larger will never be merged. @see setmaxmergemb default maximum segment size. a segment of this size or larger will never be merged during forcemerge. @see setmaxmergembforforcemerge sole constructor, setting all settings to their defaults. determines the largest segment (measured by total byte size of the segment's files, in mb) that may be merged with other segments. small values (e.g., less than 50 mb) are best for interactive indexing, as this limits the length of pauses while indexing to a few seconds. larger values are best for batched indexing and speedier searches. note that {@link #setmaxmergedocs} is also used to check whether a segment is too large for merging (it's either or). returns the largest segment (measured by total byte size of the segment's files, in mb) that may be merged with other segments. @see #setmaxmergemb determines the largest segment (measured by total byte size of the segment's files, in mb) that may be merged with other segments during forcemerge. setting it low will leave the index with more than 1 segment, even if {@link indexwriter#forcemerge} is called. returns the largest segment (measured by total byte size of the segment's files, in mb) that may be merged with other segments during forcemerge. @see #setmaxmergembforforcedmerge sets the minimum size for the lowest level segments. any segments below this size are considered to be on the same level (even if they vary drastically in size) and will be merged whenever there are mergefactor of them. this effectively truncates the "long tail" of small segments that would otherwise be get the minimum size for a segment to remain un-merged. @see #setminmergemb"
org.apache.lucene.index.SegmentReadState "holder class for common parameters used during read. @lucene.experimental {@link directory} where this segment is read from. {@link segmentinfo} describing this segment. {@link fieldinfos} describing all fields in this segment. {@link iocontext} to pass to {@link directory#openinput(string,iocontext)}. the {@code terminfosindexdivisor} to use, if appropriate (not all {@link postingsformat}s support it; in particular the current default does not).  note: if this is &lt; 0, that means "defer terms index load until needed". but if the codec must load the terms index on init (preflex is the only once currently that must do so), then it should negate this value to get the app's terms divisor unique suffix for any postings files read for this segment. {@link perfieldpostingsformat} sets this for each of the postings formats it wraps. if you create a new {@link postingsformat} then any files you write/read must be derived using this suffix (use {@link indexfilenames#segmentfilename(string,string,string)}). create a {@code segmentreadstate}. create a {@code segmentreadstate}. create a {@code segmentreadstate}. javadocs javadocs"
org.apache.lucene.index.SnapshotDeletionPolicy "an {@link indexdeletionpolicy} that wraps around any other {@link indexdeletionpolicy} and adds the ability to hold and later release snapshots of an index. while a snapshot is held, the {@link indexwriter} will not remove any files associated with it even if the index is otherwise being actively, arbitrarily changed. because we wrap another arbitrary {@link indexdeletionpolicy}, this gives you the freedom to continue using whatever {@link indexdeletionpolicy} you would normally want to use with your index.  this class maintains all snapshots in-memory, and so the information is not persisted and not protected against system failures. if persistency is important, you can use {@link persistentsnapshotdeletionpolicy} (or your own extension) and when creating a new instance of this deletion policy, pass the persistent snapshots information to {@link #snapshotdeletionpolicy(indexdeletionpolicy, map)}. @lucene.experimental holds a snapshot's information. wraps a provided {@link indexcommit} and prevents it from being deleted. the {@link indexcommit} we are preventing from deletion. creates a {@code snapshotcommitpoint} wrapping the provided {@link indexcommit}. returns true if this segment can be deleted. the default implementation returns false if this segment is currently held as snapshot. snapshots info most recently committed {@link indexcommit}. sole constructor, taking the incoming {@link indexdeletionpolicy} to wrap. {@link snapshotdeletionpolicy} wraps another {@link indexdeletionpolicy} to enable flexible snapshotting. the {@link indexdeletionpolicy} that is used on non-snapshotted commits. snapshotted commits, are not deleted until explicitly released via {@link #release(string)} a mapping of snapshot id to the segments filename that is being snapshotted. the expected input would be the output of {@link #getsnapshots()}. a null value signals that there are no initial snapshots to maintain. checks if the given id is already used by another snapshot, and throws {@link illegalstateexception} if it is. registers the given snapshot information. wraps each {@link indexcommit} as a {@link snapshotcommitpoint}. get a snapshotted indexcommit by id. the indexcommit can then be used to open an indexreader on a specific commit point, or rollback the index by opening an indexwriter with the indexcommit specified in its {@link indexwriterconfig}. a unique identifier of the commit that was snapshotted. @throws illegalstateexception if no snapshot exists by the specified id. {@link indexcommit} for this particular snapshot. get all the snapshots in a map of snapshot ids to the segments they 'cover.' this can be passed to {@link #snapshotdeletionpolicy(indexdeletionpolicy, map)} in order to initialize snapshots at construction. returns true if the given id is already used by a snapshot. you can call this method before {@link #snapshot(string)} if you are not sure whether the id is already used or not. assign snapshotted indexcommits to their correct snapshot ids as specified in the constructor. second, see if there are any instances where a snapshot id was specified in the constructor but an indexcommit doesn't exist. in this case, the id should be removed. note: this code is protective for extreme cases where ids point to non-existent segments. as the constructor should have received its information via a call to getsnapshots(), the data should be well-formed. release a snapshotted commit by id. a unique identifier of the commit that is un-snapshotted. @throws illegalstateexception if no snapshot exists by this id. snapshots the last commit. once a commit is 'snapshotted,' it is protected from deletion (as long as this {@link indexdeletionpolicy} is used). the commit can be removed by calling {@link #release(string)} using the same id parameter followed by a call to {@link indexwriter#deleteunusedfiles()}.  note: id must be unique in the system. if the same id is used twice, an {@link illegalstateexception} is thrown.  note: while the snapshot is held, the files it references will not be deleted, which will consume additional disk space in your index. if you take a snapshot at a particularly bad time (say just before you call forcemerge) then in the worst case this could consume an extra 1x of your total index size, until you release the snapshot. a unique identifier of the commit that is being snapshotted. @throws illegalstateexception if either there is no 'last commit' to snapshot, or if the parameter 'id' refers to an already snapshotted commit. {@link indexcommit} that was snapshotted. suppress the delete request if this commit point is currently snapshotted. multiple ids could point to the same commit point (segments file name) add the id->segmentids here - the actual indexcommits will be reconciled on the call to oninit() find lost snapshots finally, remove those 'lost' snapshots. no commit exists. really shouldn't happen, but might be if sdp is accessed before oninit or oncommit were called. can't use the same snapshot id twice..."
org.apache.lucene.index.BufferedDeletesStream "tracks the stream of {@link buffereddeletes}. when documentswriterperthread flushes, its buffered deletes are appended to this stream. we later apply these deletes (resolve them to the actual docids, per segment) when a merge is started (only to the to-be-merged segments). we also apply to all segments when nrt reader is pulled, commit/close is called, or when too many deletes are buffered and must be flushed (by ram usage or by count). each packet is assigned a generation, and each flushed or merged segment is also assigned a generation, so we can track which buffereddeletes packets to apply to any given segment. the insert operation must be atomic. if we let threads increment the gen and push the packet afterwards we risk that packets are out of order. with dwpt this is possible if two or more flushes are racing for pushing updates. if the pushed packets get our of order would loose documents since deletes are applied to the wrong segments. resolves the buffered deleted term/query/docids, into actual deleted docids in the livedocs mutablebits for each segmentreader. only coalesce if we are not on a segment private del packet: the segment private del packet must only applied to segments with the same delgen. yet, if a segment is already deleted from the si since it had no more documents remaining after some del packets younger than its segprivate packet (higher delgen) have been applied, the segprivate packet has not been removed. since we are on a segment private del packet we must not update the coalesceddeletes here! we can simply advance to the next packet and seginfo. removes any buffereddeletes that we no longer need to store because all segments in the index have had the deletes applied. todo: maybe linked list? starts at 1 so that segmentinfos that have never had deletes applied (whose buffereddelgen defaults to 0) will be correct: used only by assert appends a new packet of buffered deletes to the stream, setting its generation: true if any actual deletes took place: current gen, for the merged segment: if non-null, contains segments that are 100% deleted sorts segmentinfos from smallest to biggest buffereddelgen: system.out.println("bd: cycle delidx=" + delidx + " infoidx=" + infosidx); system.out.println(" coalesce"); system.out.println(" eq"); lock order: iw -> bd -> rp system.out.println(" del coalesced"); system.out.println(" del exact"); don't delete by term here; documentswriterperthread already did that on flush: system.out.println(" gt"); lock order: iw -> bd -> rp assert infos != segmentinfos || !any() : "infos=" + infos + " segmentinfos=" + segmentinfos + " any=" + any; lock order iw -> bd all deletes pruned delete by term this reader has no postings system.out.println(thread.currentthread().getname() + " del terms reader=" + reader); since we visit terms sorted, we gain performance by re-using the same termsenum and seeking only forwards system.out.println(" term=" + term); we don't need term frequencies for this system.out.println("bds: got docsenum=" + docsenum); system.out.println(thread.currentthread().getname() + " del term=" + term + " doc=" + docid); note: there is no limit check on the docid when deleting by term (unlike by query) because on flush we apply all term deletes to each segment. so all term deleting here is against prior segments: delete by query used only by assert todo: we re-use term now in our merged iterable, but we shouldn't clone, instead copy for this assert only for assert"
org.apache.lucene.index.FlushPolicy "{@link flushpolicy} controls when segments are flushed from a ram resident internal data-structure to the {@link indexwriter}s {@link directory}.  segments are traditionally flushed by:  ram consumption - configured via {@link indexwriterconfig#setrambuffersizemb(double)} number of ram resident documents - configured via {@link indexwriterconfig#setmaxbuffereddocs(int)} number of buffered delete terms/queries - configured via {@link indexwriterconfig#setmaxbuffereddeleteterms(int)}  the {@link indexwriter} consults a provided {@link flushpolicy} to control the flushing process. the policy is informed for each added or updated document as well as for each delete term. based on the {@link flushpolicy}, the information provided via {@link threadstate} and {@link documentswriterflushcontrol}, the {@link flushpolicy} decides if a {@link documentswriterperthread} needs flushing and mark it as flush-pending via {@link documentswriterflushcontrol#setflushpending(documentswriterperthreadpool.threadstate)}. @see threadstate @see documentswriterflushcontrol @see documentswriterperthread @see indexwriterconfig#setflushpolicy(flushpolicy) called for each delete term. if this is a delete triggered due to an update the given {@link threadstate} is non-null.  note: this method is called synchronized on the given {@link documentswriterflushcontrol} and it is guaranteed that the calling thread holds the lock on the given {@link threadstate} called for each document update on the given {@link threadstate}'s {@link documentswriterperthread}.  note: this method is called synchronized on the given {@link documentswriterflushcontrol} and it is guaranteed that the calling thread holds the lock on the given {@link threadstate} called for each document addition on the given {@link threadstate}s {@link documentswriterperthread}.  note: this method is synchronized by the given {@link documentswriterflushcontrol} and it is guaranteed that the calling thread holds the lock on the given {@link threadstate} called by documentswriter to initialize the flushpolicy returns the current most ram consuming non-pending {@link threadstate} with at least one indexed document.  this method will never return null the dwpt which needs to be flushed eventually should not happen"
org.apache.lucene.index.SortedBytesMergeUtils "utility class for merging sortedbytes docvalues instances. @lucene.internal creates the {@link mergecontext} necessary for merging the ordinals. encapsulates contextual information about the merge. this class holds document id to ordinal mappings, offsets for variable length values and the comparator to sort the merged bytes. @lucene.internal how many bytes each value occupies, or -1 if it varies. maps each document to the ordinal for its value. file-offset for each document; will be null if it's not needed (eg fixed-size values). sole constructor. returns number of documents merged. creates the {@link sortedsourceslice}s for merging. in order to merge we need to map the ords used in each segment to the new global ords in the new segment. additionally we need to drop values that are not referenced anymore due to deleted documents. this method walks all live documents and fetches their current ordinal. we store this ordinal per slice and (sortedsourceslice#ordmapping) and remember the doc to ord mapping in docidtorelativeord. after the merge sortedsourceslice#ordmapping contains the new global ordinals for the relative index. does the "real work" of merging the slices and computing the ord mapping. implementation of this interface consume the merged bytes with their corresponding ordinal and byte offset. the offset is the byte offset in target sorted source where the currently merged {@link bytesref} instance should be stored at. consumes a single {@link bytesref}. the provided {@link bytesref} instances are strictly increasing with respect to the used {@link comparator} used for merging the {@link bytesref} to consume the ordinal of the given {@link bytesref} in the merge target the byte offset of the given {@link bytesref} in the merge target @throws ioexception if an {@link ioexception} occurs a simple {@link bytesrefconsumer} that writes the merged {@link bytesref} instances sequentially to an {@link indexoutput}. sole constructor. {@link recordmerger} merges a list of {@link sortedsourceslice} lazily by consuming the sorted source records one by one and de-duplicates records that are shared across slices. the algorithm is based on a lazy priority queue that prevents reading merge sources into heap memory. @lucene.internal {@link sortedsourceslice} represents a single {@link sortedsource} merge candidate. it encapsulates ordinal and pre-calculated target doc id to ordinal mappings. this class also holds state private to the merge process. @lucene.internal global array indexed by docid containg the relative ord for the doc maps relative ords to merged global ords - index is relative ord value new global ord this map gets updates as we merge ords. later we use the docidtorelativeord to get the previous relative ord to get the new ord from the relative ord map. start index into docidtorelativeord end index into docidtorelativeord the currently merged relative ordinal fills in the absolute ords for this slice. provided {@code doctoord} writes ords for this slice. if a segment has no values at all we use this source to fill in the missing value in the right place (depending on the comparator used) merge queue no instance -1 if var length if non-null #mergerecords collects byte offsets here we have deletes not deleted collect ords strictly increasing use ord + 1 to identify unreferenced values (ie. == 0) no deletes use ord + 1 to identify unreferenced values (ie. == 0) extract all subs from the queue that have the same top record use ord + 1 to identify unreferenced values (ie. == 0) call next() on each top, and put back into queue skip ords that are not referenced anymore just a tie-breaker"
org.apache.lucene.index.MultiDocValues "a wrapper for compositeindexreader providing access to per segment {@link docvalues} note: for multi readers, you'll get better performance by gathering the sub readers using {@link indexreader#getcontext()} to get the atomic leaves and then operate per-atomicreader, instead of using this class. @lucene.experimental @lucene.internal returns a single {@link docvalues} instance for this field, merging their values on the fly.  note: this is a slow way to access docvalues. it's better to get the sub-readers and iterate through them yourself. returns a single {@link docvalues} instance for this norms field, merging their values on the fly.  note: this is a slow way to access docvalues. it's better to get the sub-readers and iterate through them yourself. empty source marks a gap in the array skip if we encounter one for norms we drop all norms if one leaf reader has no norms and the field is present already an atomic reader no fields already an atomic reader / reader with one leave gather all docvalues fields, accumulating a promoted type across potentially incompatible types return null if no docvalues encountered anywhere populate starts and fill gaps with empty docvalues cached array if supported todo how should we handle this emptysource is skipped - marks a gap in the array todo: this is dup of docvalues.getdefaultsource()?"
org.apache.lucene.index.UpgradeIndexMergePolicy "this {@link mergepolicy} is used for upgrading all existing segments of an index when calling {@link indexwriter#forcemerge(int)}. all other methods delegate to the base {@code mergepolicy} given to the constructor. this allows for an as-cheap-as possible upgrade of an older index by only upgrading segments that are wrapped {@link mergepolicy}. wrap the given {@link mergepolicy} and intercept forcemerge requests to only upgrade segments written with previous lucene versions. returns if the given segment should be upgraded. the default implementation will return {@code !constants.lucene_main_version.equals(si.getversion())}, so all segments first find all old segments remove all segments that are in merge specification from oldsegments, the resulting set contains all segments that are left over and will be merged to one additional segment: add the final merge"
org.apache.lucene.index.Fields "flex api for access to fields and terms @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns an iterator that will step through all fields names. this will not return null. get the {@link terms} for this field. this will return null if the field does not exist. returns the number of fields or -1 if the number of distinct field names is unknown. if &gt;= 0, {@link #iterator} will return as many field names. returns the number of terms for all fields, or -1 if this measure isn't stored by the codec. note that, just like other term measures, this measure does not take deleted documents into account. @deprecated iterate fields and add their size() instead. this method is only provided as a transition mechanism to access this statistic for 3.x indexes, which do not have this statistic per-field. @see terms#size() zero-length {@code fields} array."
org.apache.lucene.index.StoredFieldVisitor "expert: provides a low-level means of accessing the stored field values in an index. see {@link indexreader#document(int, storedfieldvisitor)}. see {@link documentstoredfieldvisitor}, which is a storedfieldvisitor that builds the {@link document} containing all stored fields. this is used by {@link indexreader#document(int)}. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) process a binary field. newly allocated byte array with the binary contents. process a string field process a int numeric field. process a long numeric field. process a float numeric field. process a double numeric field. hook before processing a field. before a field is processed, this method is invoked so that subclasses can return a {@link status} representing whether they need that particular field or not, or to stop processing entirely. enumeration of possible return values for {@link #needsfield}. yes: the field should be visited. no: don't visit this field, but continue processing fields for this document. stop: don't visit this field and stop processing any other fields for this document."
org.apache.lucene.index.InvertedDocEndConsumer ""
org.apache.lucene.index.FlushByRamOrCountsPolicy "default {@link flushpolicy} implementation that flushes based on ram used, document count and number of buffered deletes depending on the indexwriter's {@link indexwriterconfig}.  {@link #ondelete(documentswriterflushcontrol, documentswriterperthreadpool.threadstate)} - flushes based on the global number of buffered delete terms iff {@link indexwriterconfig#getmaxbuffereddeleteterms()} is enabled {@link #oninsert(documentswriterflushcontrol, documentswriterperthreadpool.threadstate)} - flushes either on the number of documents per {@link documentswriterperthread} ( {@link documentswriterperthread#getnumdocsinram()}) or on the global active memory consumption in the current indexing session iff {@link indexwriterconfig#getmaxbuffereddocs()} or {@link indexwriterconfig#getrambuffersizemb()} is enabled respectively {@link #onupdate(documentswriterflushcontrol, documentswriterperthreadpool.threadstate)} - calls {@link #oninsert(documentswriterflushcontrol, documentswriterperthreadpool.threadstate)} and {@link #ondelete(documentswriterflushcontrol, documentswriterperthreadpool.threadstate)} in order  all {@link indexwriterconfig} settings are used to mark {@link documentswriterperthread} as flush pending during indexing with respect to their live updates.  if {@link indexwriterconfig#setrambuffersizemb(double)} is enabled, the largest ram consuming {@link documentswriterperthread} will be marked as pending iff the global active ram consumption is >= the configured max ram buffer. marks the most ram consuming active {@link documentswriterperthread} flush pending returns true if this {@link flushpolicy} flushes on {@link indexwriterconfig#getmaxbuffereddocs()}, otherwise false. returns true if this {@link flushpolicy} flushes on {@link indexwriterconfig#getmaxbuffereddeleteterms()}, otherwise false. returns true if this {@link flushpolicy} flushes on {@link indexwriterconfig#getrambuffersizemb()}, otherwise false. flush this state by num del terms flush this state by num docs"
org.apache.lucene.index.Term "a term represents a word from text. this is the unit of search. it is composed of two elements, the text of the word, as a string, and the name of the field that the text occurred in. note that terms may represent more than words from text fields, but also things like dates, email addresses, urls, etc. constructs a term with the given field and bytes. note that a null field or null bytes value results in undefined behavior for most lucene apis that accept a term parameter. warning: the provided bytesref is not copied, but used directly. therefore the bytes should not be modified after construction, for example, you should clone a copy by {@link bytesref#deepcopyof} rather than pass reused bytes from a termsenum. constructs a term with the given field and text. note that a null field or null text value results in undefined behavior for most lucene apis that accept a term parameter. constructs a term with the given field and empty text. this serves two purposes: 1) reuse of a term with the same field. 2) pattern for a query. field's name returns the field of this term. the field indicates the part of a document which this term came from. returns the text of this term. in the case of words, this is simply the text of the word. in the case of dates and other types, this is an encoding of the object as a string. returns the bytes of this term. compares two terms, returning a negative integer if this term belongs before the argument, zero if this term is equal to the argument, and a positive integer if this term belongs after the argument. the ordering of terms is first by field, then by text. resets the field and text of a term. warning: the provided bytesref is not copied, but used directly. therefore the bytes should not be modified after construction, for example, you should clone a copy rather than pass reused bytes from a termsenum."
org.apache.lucene.index.LogMergePolicy "this class implements a {@link mergepolicy} that tries to merge segments into levels of exponentially increasing size, where each level has fewer segments than the value of the merge factor. whenever extra segments (beyond the merge factor upper bound) are encountered, all segments within the level are merged. you can get or set the merge factor using {@link #getmergefactor()} and {@link #setmergefactor(int)} respectively. this class is abstract and requires a subclass to define the {@link #size} method which specifies how a segment's size is determined. {@link logdocmergepolicy} is one subclass that measures size by document count in the segment. {@link logbytesizemergepolicy} is another subclass that measures size as the total byte size of the file(s) for the segment. defines the allowed range of log(size) for each level. a level is computed by taking the max segment log size, minus level_log_span, and finding all segments falling within that range. default merge factor, which is how many segments are merged at a time default maximum segment size. a segment of this size or larger will never be merged. @see setmaxmergedocs default nocfsratio. if a merge's size is >= 10% of the index, then we disable compound file for it. @see #setnocfsratio default maxcfssegmentsize value allows compound file for a segment of any size. the actual file format is still subject to nocfsratio. @see #setmaxcfssegmentsizemb(double) how many segments to merge at a time. any segments whose size is smaller than this value will be rounded up to this value. this ensures that tiny segments are aggressively merged. if the size of a segment exceeds this value then it will never be merged. if the size of a segment exceeds this value then it will never be merged during {@link indexwriter#forcemerge}. if a segment has more than this many documents then it will never be merged. if the size of the merge segment exceesd this ratio of the total index size then it will remain in non-compound format even if {@link #setusecompoundfile} is {@code true}. if the size of the merged segment exceeds this value then it will not use compound file format. if true, we pro-rate a segment's size by the percentage of non-deleted documents. true if new segments (flushed or merged) should use the compound file format. note that large segments may sometimes still use non-compound format (see {@link #setnocfsratio}. sole constructor. (for invocation by subclass constructors, typically implicit.) returns true if {@code lmp} is enabled in {@link indexwriter}'s {@code infostream}. returns current {@code nocfsratio}. @see #setnocfsratio if a merged segment will be more than this percentage of the total size of the index, leave the segment as non-compound file even if compound file is enabled. set to 1.0 to always use cfs regardless of merge size. print a debug message to {@link indexwriter}'s {@code infostream}. returns the number of segments that are merged at once and also controls the total number of segments allowed to accumulate in the index. determines how often segment indices are merged by adddocument(). with smaller values, less ram is used while indexing, and searches are faster, but indexing speed is slower. with larger values, more ram is used during indexing, and while searches is slower, indexing is faster. thus larger values (> 10) are best for batch index creation, and smaller values (determines the largest segment (measured by document count) that may be merged with other segments. small values (e.g., less than 10,000) are best for interactive indexing, as this limits the length of pauses while indexing to a few seconds. larger values are best for batched indexing and speedier searches. the default value is {@link integer#max_value}. the default merge policy ({@link logbytesizemergepolicy}) also allows you to set this limit by net size (in mb) of the segment, using {@link logbytesizemergepolicy#setmaxmergemb}. returns the largest segment (measured by document count) that may be merged with other segments. @see #setmaxmergedocs returns the largest size allowed for a compound file segment if a merged segment will be more than this value, leave the segment as non-compound file even if compound file is enabled. set this to double.positive_infinity (default) and nocfsratio to 1.0 to always use cfs regardless of merge size. although the core mps set it explicitly, we must default in case someone out there wrote his own lmp ... javadoc inherited need to skip that segment + add a merge for the 'right' segments, unless there is only 1 which is merged. there is more than 1 segment to the right of this one, or a mergeable single segment. mergefactor eligible segments were found, add them as a merge. add any left-over segments, unless there is just 1 already fully merged first, enroll all "full" merges (size mergefactor) to potentially be run concurrently: only if there are no full merges pending do we add a final partial (< mergefactor segments) merge: since we must merge down to 1 segment, the choice is simple: take care to pick a partial merge that is least cost, but does not make the index too lopsided. if we always just picked the partial tail then we could produce a highly lopsided index over time: we must merge this many segments to leave maxnumsegments in the index (from when forcemerge was first kicked off): consider all possible starting points: if the segments are already merged (e.g. there's only 1 segment), or there are <maxnumsegements:. find the newest (rightmost) segment that needs to be merged (other segments may have been flushed since merging started): there is only one segment already, and it is merged check if there are any segments above the threshold we've seen mergefactor segments in a row with deletions, so force a merge now: end of a sequence of segments with deletions, so, merge those past segments even if it's fewer than mergefactor segments sorts largest to smallest compute levels, which is just log (base mergefactor) of the size of each segment floor tiny segments now, we quantize the log values into levels. the first level is any segment whose log size is within level_log_span of the max size, or, who has such as segment "to the right". then, we find the max of all other segments and use that to define the next level segment, etc. find max level of all segments not already quantized. now search backwards for the rightmost segment that falls into this level: all remaining segments fall into the min level force a boundary at the level floor finally, record all merges that are viable at this level: skip"
org.apache.lucene.index.DirectoryReader "directoryreader is an implementation of {@link compositereader} that can read indexes in a {@link directory}. directoryreader instances are usually constructed with a call to one of the static open() methods, e.g. {@link #open(directory)}.  for efficiency, in this api documents are often referred to via document numbers, non-negative integers which each name a unique document in the index. these document numbers are ephemeral -- they may change as documents are added to and deleted from an index. clients should thus not rely on a given document having the same number between sessions.  note: {@link indexreader} instances are completely thread safe, meaning multiple threads can call any of its methods, concurrently. if your application requires external synchronization, you should not synchronize on the indexreader instance; use your own (non-lucene) objects instead. default terminfosindexdivisor. the index directory. returns a indexreader reading the index in the given directory the index directory @throws ioexception if there is a low-level io error expert: returns a indexreader reading the index in the given directory with the given terminfosindexdivisor. the index directory subsamples which indexed terms are loaded into ram. this has the same effect as {@link indexwriterconfig#settermindexinterval} except that setting must be done at indexing time while this setting can be set per reader. when set to n, then one in every ntermindexinterval terms in the index is loaded into memory. by setting this to a value > 1 you can reduce memory usage, at the expense of higher latency when loading a terminfo. the default value is 1. set this to -1 to skip loading the terms index entirely. note: divisor settings &gt; 1 do not apply to all postingsformat implementations, including the default one in this release. it only makes sense for terms indexes that can efficiently re-sample terms at load time. @throws ioexception if there is a low-level io error open a near real time indexreader from the {@link org.apache.lucene.index.indexwriter}. the indexwriter to open from if true, all buffered deletes will be applied (made visible) in the returned reader. if false, the deletes are not applied but remain buffered (in indexwriter) so that they will be applied in the future. applying deletes can be costly, so if your app can tolerate deleted documents being returned you might gain some performance by passing false. new indexreader @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error @see #openifchanged(directoryreader,indexwriter,boolean) @lucene.experimental expert: returns an indexreader reading the index in the given {@link indexcommit}. the commit point to open @throws ioexception if there is a low-level io error expert: returns an indexreader reading the index in the given {@link indexcommit} and terminfosindexdivisor. the commit point to open subsamples which indexed terms are loaded into ram. this has the same effect as {@link indexwriterconfig#settermindexinterval} except that setting must be done at indexing time while this setting can be set per reader. when set to n, then one in every ntermindexinterval terms in the index is loaded into memory. by setting this to a value > 1 you can reduce memory usage, at the expense of higher latency when loading a terminfo. the default value is 1. set this to -1 to skip loading the terms index entirely. note: divisor settings &gt; 1 do not apply to all postingsformat implementations, including the default one in this release. it only makes sense for terms indexes that can efficiently re-sample terms at load time. @throws ioexception if there is a low-level io error if the index has changed since the provided reader was opened, open and return a new reader; else, return null. the new reader, if not null, will be the same type of reader as the previous one, ie an nrt reader will open a new nrt reader, a multireader will open a new multireader, etc. this method is typically far less costly than opening a fully new directoryreader as it shares resources (for example sub-readers) with the provided directoryreader, when possible. the provided reader is not closed (you are responsible for doing so); if a new reader is returned you also must eventually close it. be sure to never close a reader while other threads are still using it; see {@link searchermanager} to simplify managing this. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error if there are no changes; else, a new directoryreader instance which you must eventually close if the indexcommit differs from what the provided reader is searching, open and return a new reader; else, return null. @see #openifchanged(directoryreader) expert: if there changes (committed or not) in the {@link indexwriter} versus what the provided reader is searching, then open and return a new indexreader searching both committed and uncommitted changes from the writer; else, return null (though, the current implementation never returns null). this provides "near real-time" searching, in that changes made during an {@link indexwriter} session can be quickly made available for searching without closing the writer nor calling {@link indexwriter#commit}. it's near real-time because there is no hard guarantee on how quickly you can get a new reader after making changes with indexwriter. you'll have to experiment in your situation to determine if it's fast enough. as this is a new and experimental feature, please report back on your findings so we can learn, improve and iterate. the very first time this method is called, this writer instance will make every effort to pool the readers that it opens for doing merges, applying deletes, etc. this means additional resources (ram, file descriptors, cpu time) will be consumed. for lower latency on reopening a reader, you should call {@link indexwriterconfig#setmergedsegmentwarmer} to pre-warm a newly merged segment before it's committed to the index. this is important for minimizing index-to-search delay after a large merge.  if an addindexes call is running in another thread, then this reader will only search those segments from the foreign index that have been successfully copied over, so far. note: once the writer is closed, any outstanding readers may continue to be used. however, if you attempt to reopen any of those readers, you'll hit an {@link org.apache.lucene.store.alreadyclosedexception}. that covers entire index plus all changes made so far by this indexwriter instance, or null if there are no new changes the indexwriter to open from if true, all buffered deletes will be applied (made visible) in the returned reader. if false, the deletes are not applied but remain buffered (in indexwriter) so that they will be applied in the future. applying deletes can be costly, so if your app can tolerate deleted documents being returned you might gain some performance by passing false. @throws ioexception if there is a low-level io error @lucene.experimental returns all commit points that exist in the directory. normally, because the default is {@link keeponlylastcommitdeletionpolicy}, there would be only one commit point. but if you're using a custom {@link indexdeletionpolicy} then there could be many commits. once you have a given commit, you can open a reader on it by calling {@link directoryreader#open(indexcommit)} there must be at least one commit in the directory, else this method throws {@link indexnotfoundexception}. note that if a commit is in progress while this method is running, that commit may or may not be returned. sorted list of {@link indexcommit}s, from oldest to latest. returns true if an index exists at the specified directory. directory the directory to check for an index true if an index exists; false otherwise expert: constructs a {@code directoryreader} on the given subreaders. the wrapped atomic index segment readers. this array is returned by {@link #getsequentialsubreaders} and used to resolve the correct subreader for docid-based methods. please note: this array is not cloned and not protected for modification outside of this reader. subclasses of {@code directoryreader} should take care to not allow modification of this internal array, e.g. {@link #doopenifchanged()}. returns the directory this index resides in. implement this method to support {@link #openifchanged(directoryreader)}. if this reader does not support reopen, return {@code null}, so client code is happy. this should be consistent with {@link #iscurrent} (should always return {@code true}) if reopen is not supported. @throws ioexception if there is a low-level io error if there are no changes; else, a new directoryreader instance. implement this method to support {@link #openifchanged(directoryreader,indexcommit)}. if this reader does not support reopen from a specific {@link indexcommit}, throw {@link unsupportedoperationexception}. @throws ioexception if there is a low-level io error if there are no changes; else, a new directoryreader instance. implement this method to support {@link #openifchanged(directoryreader,indexwriter,boolean)}. if this reader does not support reopen from {@link indexwriter}, throw {@link unsupportedoperationexception}. @throws ioexception if there is a low-level io error if there are no changes; else, a new directoryreader instance. version number when this indexreader was opened. this method returns the version recorded in the commit that the reader opened. this version is advanced every time a change is made with {@link indexwriter}. check whether any new changes have occurred to the index since this reader was opened. if this reader was expert: return the indexcommit that this reader has opened.  @lucene.experimental javadocs ioexception allowed to throw there, in case segments_n is corrupt lucene-948: on nfs (and maybe others), if you have writers switching back and forth between machines, it's very likely that the dir listing will be stale and will claim a file segments_x exists when in fact it doesn't. so, we catch this and handle it as if the file does not exist ensure that the commit points are sorted in ascending order. don't ensureopen here -- in certain cases, when a cloned/reopened reader needs to commit, it may call this method on the closed original reader"
org.apache.lucene.index.PersistentSnapshotDeletionPolicy "a {@link snapshotdeletionpolicy} which adds a persistence layer so that snapshots can be maintained across the life of an application. the snapshots are persisted in a {@link directory} and are committed as soon as {@link #snapshot(string)} or {@link #release(string)} is called.  note: this class receives a {@link directory} to persist the data into a lucene index. it is highly recommended to use a dedicated directory (and on stable storage as well) for persisting the snapshots' information, and not reuse the content index directory, or otherwise conflicts and index corruption will occur.  note: you should call {@link #close()} when you're done using this class for safety (it will close the {@link indexwriter} instance used). reads the snapshots information from the given {@link directory}. this method can be used if the snapshots information is needed, however you cannot instantiate the deletion policy (because e.g., some other process keeps a lock on the snapshots directory). {@link persistentsnapshotdeletionpolicy} wraps another {@link indexdeletionpolicy} to enable flexible snapshotting. the {@link indexdeletionpolicy} that is used on non-snapshotted commits. snapshotted commits, by definition, are not deleted until explicitly released via {@link #release(string)}. the {@link directory} which will be used to persist the snapshots information. specifies whether a new index should be snapshots the last commit using the given id. once this method returns, the snapshot information is persisted in the directory. @see snapshotdeletionpolicy#snapshot(string) deletes a snapshotted commit by id. once this method returns, the snapshot information is committed to the directory. @see snapshotdeletionpolicy#release(string) closes the index which writes the snapshots to the directory. persists all snapshots information. if the given id and segment are not null, it persists their information as well. used to validate that the given directory includes just one document w/ the given id field. otherwise, it's not a valid directory for snapshotting. the index writer which maintains the snapshots metadata index is allowed to have exactly one document or 0. initialize the index writer over the snapshot directory. indexwriter no longer creates a first commit on an empty directory. so if we were asked to create, call commit() just to be sure. if the index contains information and mode is create_or_append, it's a no-op. initializes the snapshots information. this code should basically run only if mode != create, but if it is, it's no harm as we only open the reader once and immediately close it. don't leave any open file handles don't leave any open file handles super.oninit() needs to be called first to ensure that initialization behaves as expected. the superclass, snapshotdeletionpolicy, ensures that any snapshot ids with empty indexcommits are released. since this happens, this class needs to persist these changes."
org.apache.lucene.index.PerDocWriteState "encapsulates all necessary state to initiate a {@link perdocconsumer} and create all necessary files in order to consume and merge per-document values. @lucene.experimental infostream used for debugging. {@link directory} to write all files to. {@link segmentinfo} describing this segment. number of bytes allocated in ram to hold this state. segment suffix to pass to {@link indexfilenames#segmentfilename(string,string,string)}. {@link iocontext} to use for all file writing. creates a {@code perdocwritestate}. creates a {@code perdocwritestate}, copying fields from another and allocating a new {@link #bytesused}. creates a {@code perdocwritestate}, copying fields from another (copy constructor) but setting a new {@link #segmentsuffix}."
org.apache.lucene.index.SegmentWriteState "holder class for common parameters used during write. @lucene.experimental {@link infostream} used for debugging messages. {@link directory} where this segment will be written to. {@link segmentinfo} describing this segment. {@link fieldinfos} describing all fields in this segment. number of deleted documents set while flushing the segment. deletes to apply while we are flushing the segment. a term is enrolled in here if it was deleted at one point, and it's mapped to the docidupto, meaning any docid &lt; docidupto containing this term should be deleted. {@link mutablebits} recording live documents; this is only set if there is one or more deleted documents. unique suffix for any postings files written for this segment. {@link perfieldpostingsformat} sets this for each of the postings formats it wraps. if you create a new {@link postingsformat} then any files you write/read must be derived using this suffix (use {@link indexfilenames#segmentfilename(string,string,string)}). expert: the fraction of terms in the "dictionary" which should be stored in ram. smaller values use more memory, but make searching slightly faster, while larger values use less memory and make searching slightly slower. searching is typically not dominated by dictionary lookup, so tweaking this is rarely useful. {@link iocontext} for all writes; you should pass this to {@link directory#createoutput(string,iocontext)}. sole constructor. create a shallow {@link segmentwritestate} copy final a format id javadocs javadocs todo: this should be private to the codec, not settable here or in iwc"
org.apache.lucene.index.DocsEnum "iterates through the documents and term freqs. note: you must first call {@link #nextdoc} before using any of the per-doc methods. flag to pass to {@link termsenum#docs(bits,docsenum,int)} if you don't require term frequencies in the returned enum. when passed to {@link termsenum#docsandpositions(bits,docsandpositionsenum,int)} means that no offsets and payloads will be returned. flag to pass to {@link termsenum#docs(bits,docsenum,int)} if you require term frequencies in the returned enum. sole constructor. (for invocation by subclass constructors, typically implicit.) returns term frequency in the current document, or 1 if the field was indexed with {@link indexoptions#docs_only}. do not call this before {@link #nextdoc} is first called, nor after {@link #nextdoc} returns {@link docidsetiterator#no_more_docs}.  note: if the {@link docsenum} was obtain with {@link #flag_none}, the result of this method is undefined. returns the related attributes. javadocs"
org.apache.lucene.index.MultiDocsEnum "exposes {@link docsenum}, merged from {@link docsenum} api of sub-segments. @lucene.experimental sole constructor the {@link multitermsenum} that returns {@code true} if this instance can be reused by the provided {@link multitermsenum}. how many sub-readers we are merging. @see #getsubs returns sub-readers we are merging. holds a {@link docsenum} along with the corresponding {@link readerslice}. {@link docsenum} of this sub-reader. {@link readerslice} describing how this sub-reader fits into the composite reader. todo: implement bulk read more efficiently than super"
org.apache.lucene.index.IndexUpgrader "this is an easy-to-use tool that upgrades all segments of an index from previous lucene versions to the current segment file format. it can be used from command line:  java -cp lucene-core.jar org.apache.lucene.index.indexupgrader [-delete-prior-commits] [-verbose] indexdir  alternatively this class can be instantiated and {@link #upgrade} invoked. it uses {@link upgradeindexmergepolicy} and triggers the upgrade via an forcemerge request to {@link indexwriter}. this tool keeps only the last commit in an index; for this reason, if the incoming index has more than one commit, the tool refuses to run by default. specify {@code -delete-prior-commits} to override this, allowing the tool to delete all but the last commit. from java code this can be enabled by passing {@code true} to {@link #indexupgrader(directory,version,printstream,boolean)}. warning: this tool may reorder documents if the index was partially upgraded before execution (e.g., documents were added). if your application relies on &quot;monotonicity&quot; of doc ids (which means that the order in which the documents were added to the index is preserved), do a full forcemerge instead. the {@link mergepolicy} set by {@link indexwriterconfig} may also reorder documents. main method to run {code indexupgrader} from the command-line. creates index upgrader on the given directory, using an {@link indexwriter} using the given {@code matchversion}. the tool refuses to upgrade indexes with multiple commit points. creates index upgrader on the given directory, using an {@link indexwriter} using the given {@code matchversion}. you have the possibility to upgrade indexes with multiple commit points by removing all older ones. if {@code infostream} is not {@code null}, all logging output will be sent to this stream. creates index upgrader on the given directory, using an {@link indexwriter} using the given config. you have the possibility to upgrade indexes with multiple commit points by removing all older ones. perform the upgrade."
org.apache.lucene.index.DocValues "{@link docvalues} provides a dense per-document typed storage for fast value access based on the lucene internal document id. {@link docvalues} exposes two distinct apis:  via {@link #getsource()} providing ram resident random access via {@link #getdirectsource()} providing on disk random access  {@link docvalues} are exposed via {@link atomicreader#docvalues(string)} on a per-segment basis. for best performance {@link docvalues} should be consumed per-segment just like indexreader.  {@link docvalues} are fully integrated into the {@link docvaluesformat} api.  note: docvalues is a strongly typed per-field api. type changes within an indexing session can result in exceptions if the type has changed in a way that the previously give type for a field can't promote the value without losing information. for instance a field initially indexed with {@link type#fixed_ints_32} can promote a value with {@link type#fixed_ints_8} but can't promote {@link type#fixed_ints_64}. during segment merging type-promotion exceptions are suppressed. fields will be promoted to their common denominator or automatically transformed into a 3rd type like {@link type#bytes_var_straight} to prevent data loss and merge exceptions. this behavior is considered best-effort might change in future releases.   docvalues are exposed via the {@link field} api with type safe specializations for each type variant:   {@link bytedocvaluesfield} - for adding byte values to the index  {@link shortdocvaluesfield} - for adding short values to the index  {@link intdocvaluesfield} - for adding int values to the index  {@link longdocvaluesfield} - for adding long values to the index  {@link floatdocvaluesfield} - for adding float values to the index  {@link doubledocvaluesfield} - for adding double values to the index  {@link packedlongdocvaluesfield} - for adding packed long values to the index  {@link sortedbytesdocvaluesfield} - for adding sorted {@link bytesref} values to the index  {@link straightbytesdocvaluesfield} - for adding straight {@link bytesref} values to the index  {@link derefbytesdocvaluesfield} - for adding deref {@link bytesref} values to the index  see {@link type} for limitations of each type variant.   @see docvaluesformat#docsconsumer(org.apache.lucene.index.perdocwritestate) @lucene.experimental zero length docvalues array. sole constructor. (for invocation by subclass constructors, typically implicit.) loads a new {@link source} instance for this {@link docvalues} field instance. source instances returned from this method are not cached. it is the callers responsibility to maintain the instance and release its resources once the source is not needed anymore.  for managed {@link source} instances see {@link #getsource()}. @see #getsource() @see #setcache(sourcecache) returns a {@link source} instance through the current {@link sourcecache}. iff no {@link source} has been loaded into the cache so far the source will be loaded through {@link #loadsource()} and passed to the {@link sourcecache}. the caller of this method should not close the obtained {@link source} instance unless it is not needed for the rest of its life time.  {@link source} instances obtained from this method are closed / released from the cache once this {@link docvalues} instance is closed by the {@link indexreader}, {@link fields} or the {@link docvalues} was returns a disk resident {@link source} instance through the current {@link sourcecache}. direct sources are cached per thread in the {@link sourcecache}. the obtained instance should not be shared with other threads. loads a new {@link source direct source} instance from this {@link docvalues} field instance. source instances returned from this method are not cached. it is the callers responsibility to maintain the instance and release its resources once the source is not needed anymore.  for managed {@link source direct source} instances see {@link #getdirectsource()}. @see #getdirectsource() @see #setcache(sourcecache) returns the {@link type} of this {@link docvalues} instance closes this {@link docvalues} instance. this method should only be called by the creator of this {@link docvalues} instance. api returns the size per value in bytes or -1 iff size per value is variable. size per value in bytes or -1 iff size per value is variable. sets the {@link sourcecache} used by this {@link docvalues} instance. this method should be called before {@link #loadsource()} is called. all {@link source} instances in the currently used cache will be closed before the new cache is installed.  note: all instances previously obtained from {@link #loadsource()} will be lost. @throws illegalargumentexception if the given cache is null returns the currently used cache instance; @see #setcache(sourcecache) source of per document values like long, double or {@link bytesref} depending on the {@link docvalues} fields {@link type}. source implementations provide random access semantics similar to array lookups  @see docvalues#getsource() @see docvalues#getdirectsource() {@link type} of this {@code source}. sole constructor. (for invocation by subclass constructors, typically implicit.) returns a long for the given document id or throws an {@link unsupportedoperationexception} if this source doesn't support long values. @throws unsupportedoperationexception if this source doesn't support long values. returns a double for the given document id or throws an {@link unsupportedoperationexception} if this source doesn't support double values. @throws unsupportedoperationexception if this source doesn't support double values. returns a {@link bytesref} for the given document id or throws an {@link unsupportedoperationexception} if this source doesn't support byte[] values. @throws unsupportedoperationexception if this source doesn't support byte[] values. returns the {@link type} of this source. {@link type} of this source. returns true iff this {@link source} exposes an array via {@link #getarray()} otherwise false. true iff this {@link source} exposes an array via {@link #getarray()} otherwise false. returns the internal array representation iff this {@link source} uses an array as its inner representation, otherwise uoe. if this {@link source} is sorted this method will return an instance of {@link sortedsource} otherwise uoe a sorted variant of {@link source} for byte[] values per document.  sole constructor. (for invocation by subclass constructors, typically implicit.) returns ord for specified docid. ord is dense, ie, starts at 0, then increments by 1 for the next (as defined by {@link comparator} value. returns value for specified ord. return true if it's safe to call {@link #getdoctoord}. returns the packedints.reader impl that maps document to ord. returns the comparator used to order the bytesrefs. lookup ord by value. the value to look up a spare {@link bytesref} instance used to compare internal values to the given value. must not be null given values ordinal if found or otherwise (-(ord)-1), defined as the ordinal of the first element that is greater than the given value (the insertion point). this guarantees that the return value will always be &gt;= 0 if the given value is found. returns the number of unique values in this sorted source returns a source that always returns default (missing) values for all documents. returns a sortedsource that always returns default (missing) values for all documents. type specifies the {@link docvalues} type for a certain field. a type only defines the data type for a field while the actual implementation used to encode and decode the values depends on the the {@link docvaluesformat#docsconsumer} and {@link docvaluesformat#docsproducer} methods. @lucene.experimental a variable bit signed integer value. by default this type uses {@link packedints} to compress the values, as an offset from the minimum value, as long as the value range fits into 263-1. otherwise, the default implementation falls back to fixed size 64bit integers ({@link #fixed_ints_64}).  note: this type uses 0 as the default value without any distinction between provided 0 values during indexing. all documents without an explicit value will use 0 instead. custom default values must be assigned explicitly.  a 8 bit signed integer value. {@link source} instances of this type return a byte array from {@link source#getarray()}  note: this type uses 0 as the default value without any distinction between provided 0 values during indexing. all documents without an explicit value will use 0 instead. custom default values must be assigned explicitly.  a 16 bit signed integer value. {@link source} instances of this type return a short array from {@link source#getarray()}  note: this type uses 0 as the default value without any distinction between provided 0 values during indexing. all documents without an explicit value will use 0 instead. custom default values must be assigned explicitly.  a 32 bit signed integer value. {@link source} instances of this type return a int array from {@link source#getarray()}  note: this type uses 0 as the default value without any distinction between provided 0 values during indexing. all documents without an explicit value will use 0 instead. custom default values must be assigned explicitly.  a 64 bit signed integer value. {@link source} instances of this type return a long array from {@link source#getarray()}  note: this type uses 0 as the default value without any distinction between provided 0 values during indexing. all documents without an explicit value will use 0 instead. custom default values must be assigned explicitly.  a 32 bit floating point value. by default there is no compression applied. to fit custom float values into less than 32bit either a custom implementation is needed or values must be encoded into a {@link #bytes_fixed_straight} type. {@link source} instances of this type return a float array from {@link source#getarray()}  note: this type uses 0.0f as the default value without any distinction between provided 0.0f values during indexing. all documents without an explicit value will use 0.0f instead. custom default values must be assigned explicitly.  a 64 bit floating point value. by default there is no compression applied. to fit custom float values into less than 64bit either a custom implementation is needed or values must be encoded into a {@link #bytes_fixed_straight} type. {@link source} instances of this type return a double array from {@link source#getarray()}  note: this type uses 0.0d as the default value without any distinction between provided 0.0d values during indexing. all documents without an explicit value will use 0.0d instead. custom default values must be assigned explicitly.  a fixed length straight byte[]. all values added to such a field must be of the same length. all bytes are stored sequentially for fast offset access.  note: this type uses 0 byte filled byte[] based on the length of the first seen value as the default value without any distinction between explicitly provided values during indexing. all documents without an explicit value will use the default instead.custom default values must be assigned explicitly.  a fixed length dereferenced byte[] variant. fields with this type only store distinct byte values and store an additional offset pointer per document to dereference the shared byte[]. use this type if your documents may share the same byte[].  note: fields of this type will not store values for documents without an explicitly provided value. if a documents value is accessed while no explicit value is stored the returned {@link bytesref} will be a 0-length reference. custom default values must be assigned explicitly.  variable length straight stored byte[] variant. all bytes are stored sequentially for compactness. usage of this type via the disk-resident api might yield performance degradation since no additional index is used to advance by more than one document value at a time.  note: fields of this type will not store values for documents without an explicitly provided value. if a documents value is accessed while no explicit value is stored the returned {@link bytesref} will be a 0-length byte[] reference. custom default values must be assigned explicitly.  a variable length dereferenced byte[]. just like {@link #bytes_fixed_deref}, but allowing each document's value to be a different length.  note: fields of this type will not store values for documents without an explicitly provided value. if a documents value is accessed while no explicit value is stored the returned {@link bytesref} will be a 0-length reference. custom default values must be assigned explicitly.  a variable length pre-sorted byte[] variant. just like {@link #bytes_fixed_sorted}, but allowing each document's value to be a different length.  note: fields of this type will not store values for documents without an explicitly provided value. if a documents value is accessed while no explicit value is stored the returned {@link bytesref} will be a 0-length reference.custom default values must be assigned explicitly.  @see sortedsource a fixed length pre-sorted byte[] variant. fields with this type only store distinct byte values and store an additional offset pointer per document to dereference the shared byte[]. the stored byte[] is presorted, by default by unsigned byte order, and allows access via document id, ordinal and by-value. use this type if your documents may share the same byte[].  note: fields of this type will not store values for documents without an explicitly provided value. if a documents value is accessed while no explicit value is stored the returned {@link bytesref} will be a 0-length reference. custom default values must be assigned explicitly.  @see sortedsource abstract base class for {@link docvalues} {@link source} cache.  {@link source} instances loaded via {@link docvalues#loadsource()} are entirely memory resident and need to be maintained by the caller. each call to {@link docvalues#loadsource()} will cause an entire reload of the underlying data. source instances obtained from {@link docvalues#getsource()} and {@link docvalues#getsource()} respectively are maintained by a {@link sourcecache} that is closed ( {@link #close(docvalues)}) once the {@link indexreader} that sole constructor. (for invocation by subclass constructors, typically implicit.) atomically loads a {@link source} into the cache from the given {@link docvalues} and returns it iff no other {@link source} has already been cached. otherwise the cached source is returned.  this method will not return null atomically loads a {@link source direct source} into the per-thread cache from the given {@link docvalues} and returns it iff no other {@link source direct source} has already been cached. otherwise the cached source is returned.  this method will not return null atomically invalidates the cached {@link source} instances if any and empties the cache. atomically closes the cache and frees all resources. simple per {@link docvalues} instance cache implementation that holds a {@link source} a member variable.  if a {@link directsourcecache} instance is closed or invalidated the cached reference are simply set to null sole constructor. javadocs javadocs javadocs javadocs javadocs javadocs javadocs javadocs javadocs javadocs javadocs for tests negative ord means doc was missing? todo(simonw): -- shouldn't lucene decide/detect straight vs deref, as well fixed vs var?"
org.apache.lucene.index.CompositeReaderContext "{@link indexreadercontext} for {@link compositereader} instance. creates a {@link compositereadercontext} for intermediate readers that aren't not top-level readers in the current context creates a {@link compositereadercontext} for top-level readers with parent set to null"
org.apache.lucene.index.TwoPhaseCommit "an interface for implementations that support 2-phase commit. you can use {@link twophasecommittool} to execute a 2-phase commit algorithm over several {@link twophasecommit}s. @lucene.experimental the first stage of a 2-phase commit. implementations should do as much work as possible in this method, but avoid actual committing changes. if the 2-phase commit fails, {@link #rollback()} is called to discard all changes since last successful commit. the second phase of a 2-phase commit. implementations should ideally do very little work in this method (following {@link #preparecommit()}, and after it returns, the caller can assume that the changes were successfully committed to the underlying storage. discards any changes that have occurred since the last commit. in a 2-phase commit algorithm, where one of the objects failed to {@link #commit()} or {@link #preparecommit()}, this method is used to roll all other objects back to their previous state."
org.apache.lucene.index.FieldInfos "collection of {@link fieldinfo}s (accessible by number or by name). @lucene.experimental constructs a new fieldinfos from an array of fieldinfo objects returns true if any fields have freqs returns true if any fields have positions returns true if any fields have payloads returns true if any fields have offsets returns true if any fields have vectors returns true if any fields have norms returns true if any fields have docvalues returns the number of fields returns an iterator over all the fieldinfo objects present, ordered by ascending field number return the fieldinfo object referenced by the field name fieldinfo object or null when the given fieldname doesn't exist. return the fieldinfo object referenced by the fieldnumber. field's number. if this is negative, this method always returns null. fieldinfo object or null when the given fieldnumber doesn't exist. returns the global field number for the given field name. if the name does not exist yet it tries to add it with the given preferred field number assigned if possible otherwise the first unassigned field number is used as the field number. sets the given field number and name if not yet set. creates a new instance with the given {@link fieldnumbers}. adds the given field to this fieldinfos name / number mapping. the given fi must be present in the global field number mapping before this method it called if the field is not yet known, adds it. if it is known, checks to make sure that the isindexed flag is the same as was given previously for this field. if not - marks it as being indexed. same goes for the termvector parameters. the name of the field true if the field is indexed true if the term vector should be stored true if the norms for the indexed field should be omitted true if payloads should be stored for this field if term freqs should be omitted for this field note: this method does not carry over termvector booleans nor docvaluestype; the indexer chain (termvectorsconsumerperfield, docfieldprocessor) must set these fields when they succeed in consuming the document for an unmodifiable iterator todo: what happens if in fact a different order is used? todo: fix this negative behavior, this was something related to lucene3x? if the field name is empty, i think it writes the fieldnumber as -1 cool - we can use this number globally find a new fieldnumber might not be up to date - lets do the work once needed used by assert todo: fix testcodecs to do this another way, its the only todo: really, indexer shouldn't even call this method (it's only called from docfieldprocessor); rather, each component in the chain should update what it "owns". eg fieldtype.indexoptions() should be updated by maybe freqproxtermswriterperfield: get a global number for this field important - reuse the field number if possible for consistent field numbers across segments"
org.apache.lucene.index.DocumentsWriterPerThreadPool "{@link documentswriterperthreadpool} controls {@link threadstate} instances and their thread assignments during indexing. each {@link threadstate} holds a reference to a {@link documentswriterperthread} that is once a {@link threadstate} is obtained from the pool exclusively used for indexing a single document by the obtaining thread. each indexing thread must obtain such a {@link threadstate} to make progress. depending on the {@link documentswriterperthreadpool} implementation {@link threadstate} assignments might differ from document to document.  once a {@link documentswriterperthread} is selected for flush the thread pool is reusing the flushing {@link documentswriterperthread}s threadstate with a new {@link documentswriterperthread} instance.  {@link threadstate} references and guards a {@link documentswriterperthread} instance that is used during indexing to build a in-memory index segment. {@link threadstate} also holds all flush related per-thread data controlled by {@link documentswriterflushcontrol}.  a {@link threadstate}, its methods and members should only accessed by one thread a time. resets the internal {@link documentswriterperthread} with the given one. if the given dwpt is null this threadstate is marked as inactive and should not be used for indexing anymore. @see #isactive() returns true if this threadstate is still open. this will only return false iff the dw has been closed and this threadstate is already checked out for flush. returns the number of currently active bytes in this threadstate's {@link documentswriterperthread} returns this {@link threadstate}s {@link documentswriterperthread} returns true iff this {@link threadstate} is marked as flush pending otherwise false creates a new {@link documentswriterperthreadpool} with a given maximum of {@link threadstate}s. returns the max number of {@link threadstate} instances available in this {@link documentswriterperthreadpool} returns the active number of {@link threadstate} instances. returns a new {@link threadstate} iff any new state is available otherwise null.  note: the returned {@link threadstate} is already locked iff non- null. new {@link threadstate} iff any new state is available otherwise null deactivate all unreleased threadstates returns the ith active {@link threadstate} where i is the given ord. the ordinal of the {@link threadstate} ith active {@link threadstate} where i is the given ord. returns the threadstate with the minimum estimated number of threads waiting to acquire its lock or null if no {@link threadstate} is yet visible to the calling thread. returns the number of currently deactivated {@link threadstate} instances. a deactivated {@link threadstate} should not be used for indexing anymore. number of currently deactivated {@link threadstate} instances. deactivates an active {@link threadstate}. inactive {@link threadstate} can not be used for indexing anymore once they are deactivated. this method should only be used if the parent {@link documentswriter} is closed or aborted. the state to deactivate reinitialized an active {@link threadstate}. a {@link threadstate} should only be reinitialized if it is active without any pending documents. the state to reinitialize todo this should really be part of documentswriterflushcontrol write access guarded by documentswriterflushcontrol todo this should really be part of documentswriterflushcontrol write access guarded by documentswriterflushcontrol guarded by reentrant lock public for flushpolicy public for flushpolicy thread pool is bound to dw we should only be cloned before being used: should not happen lock so nobody else will get this threadstate unreleased thread states are deactivated during dw#close() increment will publish the threadstate unlock since the threadstate is not active anymore - we are closed! in any case make sure we unlock if we fail don't recycle dwpt by default you cannot subclass this without being in o.a.l.index package anyway, so the class is already pkg-private... fix me: see lucene-4013"
org.apache.lucene.index.NoMergePolicy "a {@link mergepolicy} which never returns merges to execute (hence it's name). it is also a singleton and can be accessed through {@link nomergepolicy#no_compound_files} if you want to indicate the index does not use compound files, or through {@link nomergepolicy#compound_files} otherwise. use it if you want to prevent an {@link indexwriter} from ever executing merges, without going through the hassle of tweaking a merge policy's settings to achieve that, such as changing its merge factor. a singleton {@link nomergepolicy} which indicates the index does not use compound files. a singleton {@link nomergepolicy} which indicates the index uses compound files. prevent instantiation"
org.apache.lucene.index.ReadersAndLiveDocs "returns a ref to a clone. note: this clone is not enrolled in the pool, so you should simply close() it when you're done (ie, do not call release()). used by indexwriter to hold open segmentreaders (for searching or merging), plus pending deletes, for a given segment not final because we replace (clone) when we need to change it and it's been shared: tracks how many consumers are using this instance: set once (null, and then maybe set, and never set again): todo: it's sometimes wasteful that we hold open two separate srs (one for merging one for reading)... maybe just use a single sr? the gains of not loading the terms index (for merging in the non-nrt case) are far less now... and if the app has any deletes it'll open real readers anyway. set once (null, and then maybe set, and never set again): holds the current shared (readable and writable livedocs). this is null when there are no deleted docs, and it's copy-on-write (cloned whenever we need to change it but it's been shared to an external nrt reader). how many further deletions we've done against livedocs vs when we loaded it or last wrote it: true if the current livedocs is referenced by an external nrt reader: call only from assert! get reader for searching/deleting system.out.println(" livedocs=" + rld.livedocs); we steal returned ref: system.out.println("add seg=" + rld.info + " ismerge=" + ismerge + " " + readermap.size() + " in pool"); system.out.println(thread.currentthread().getname() + ": getreader seg=" + info.name); ref for caller get reader for merging (does not load the terms index): system.out.println(" livedocs=" + rld.livedocs); just use the already opened non-merge reader for merging. in the nrt case this saves us pointless double-open: system.out.println("promote non-merge reader seg=" + rld.info); ref for us: system.out.println(thread.currentthread().getname() + ": getmergereader share seg=" + info.name); system.out.println(thread.currentthread().getname() + ": getmergereader seg=" + info.name); we steal returned ref: ref for caller system.out.println(" new del seg=" + info + " docid=" + docid + " pendingdelcount=" + pendingdeletecount + " totdelcount=" + (info.doccount-livedocs.count())); note: removes callers ref todo: can we somehow use ioutils here...? problem is we are calling .decref not .close)... system.out.println(" pool.drop info=" + info + " rc=" + reader.getrefcount()); system.out.println(" pool.drop info=" + info + " merge rc=" + mergereader.getrefcount()); system.out.println("initwritablelivedocs seg=" + info + " livedocs=" + livedocs + " shared=" + shared); copy on write: this means we've cloned a segmentreader sharing the current livedocs instance; must now make a private clone so we can change it: system.out.println("create bv seg=" + info); system.out.println("getrolivedocs seg=" + info); if (livedocs != null) { system.out.println(" livecount=" + livedocs.count()); } discard (don't save) changes when we are dropping the reader; this is used only on the sub-readers after a successful merge. if deletes had accumulated on those sub-readers while the merge is running, by now we have carried forward those deletes onto the newly merged segment, so we can discard them on the sub-readers: commit live docs to the directory (writes new _x_n.del files); returns true if it wrote the file and false if there were no new deletes to write: system.out.println("rld.writelivedocs seg=" + info + " pendingdelcount=" + pendingdeletecount); we have new deletes do this so we can delete any exception; this saves all codecs from having to do it: we can write directly to the actual name (vs to a .tmp & renaming it) because the file is not live until segments file is written: advance only the nextwritedelgen so that a 2nd attempt to write will write to a new file delete any partially ignore so we throw only the first exc if we hit an exc in the line above (eg disk full) then info's delgen remains pointing to the previous (successfully written) del docs:"
org.apache.lucene.index.TermsEnum "iterator to seek ({@link #seekceil(bytesref)}, {@link #seekexact(bytesref,boolean)}) or step through ({@link #next} terms to obtain frequency information ({@link #docfreq}), {@link docsenum} or {@link docsandpositionsenum} for the current term ({@link #docs}. term enumerations are always ordered by {@link #getcomparator}. each term in the enumeration is greater than the one before it. the termsenum is unpositioned when you first obtain it and you must first successfully call {@link #next} or one of the seek methods. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns the related attributes. represents returned result from {@link #seekceil}. the term was not found, and the end of iteration was hit. the precise term was found. a different term was found after the requested term attempts to seek to the exact term, returning true if the term is found. if this returns false, the enum is unpositioned. for some codecs, seekexact may be substantially faster than {@link #seekceil}. expert: just like {@link #seekceil(bytesref)} but allows you to control whether the implementation should attempt to use its term cache (if it uses one). seeks to the specified term, if it exists, or to the next (ceiling) term. returns seekstatus to indicate whether exact term was found, a different term was found, or eof was hit. the target term may be before or after the current term. if this returns seekstatus.end, the enum is unpositioned. seeks to the specified term by ordinal (position) as previously returned by {@link #ord}. the target ord may be before or after the current ord, and must be within bounds. expert: seeks a specific position by {@link termstate} previously obtained from {@link #termstate()}. callers should maintain the {@link termstate} to use this method. low-level implementations may position the termsenum without re-seeking the term dictionary.  seeking by {@link termstate} should only be used iff the enum the state was obtained from and the enum the state is used for seeking are obtained from the same {@link indexreader}.  note: using this method with an incompatible {@link termstate} might leave this {@link termsenum} in undefined state. on a segment level {@link termstate} instances are compatible only iff the source and the target {@link termsenum} operate on the same field. if operating on segment level, termstate instances must not be used across segments.  note: a seek by {@link termstate} might not restore the {@link attributesource}'s state. {@link attributesource} states must be maintained separately if this method is used. the term the termstate corresponds to the {@link termstate} returns current term. do not call this when the enum is unpositioned. returns ordinal position for current term. this is an optional method (the codec may throw {@link unsupportedoperationexception}). do not call this when the enum is unpositioned. returns the number of documents containing the current term. do not call this when the enum is unpositioned. {@link seekstatus#end}. returns the total number of occurrences of this term across all documents (the sum of the freq() for each doc that has this term). this will be -1 if the codec doesn't support this measure. note that, like other term measures, this measure does not take deleted documents into account. get {@link docsenum} for the current term. do not call this when the enum is unpositioned. this method will not return null. unset bits are documents that should not be returned pass a prior docsenum for possible reuse get {@link docsenum} for the current term, with control over whether freqs are required. do not call this when the enum is unpositioned. this method will not return null. unset bits are documents that should not be returned pass a prior docsenum for possible reuse specifies which optional per-document values you require; see {@link docsenum#flag_freqs} @see #docs(bits, docsenum, int) get {@link docsandpositionsenum} for the current term. do not call this when the enum is unpositioned. this method will return null if positions were not indexed. unset bits are documents that should not be returned pass a prior docsandpositionsenum for possible reuse @see #docsandpositions(bits, docsandpositionsenum, int) get {@link docsandpositionsenum} for the current term, with control over whether offsets and payloads are required. some codecs may be able to optimize their implementation when offsets and/or payloads are not required. do not call this when the enum is unpositioned. this will return null if positions were not indexed. unset bits are documents that should not be returned pass a prior docsandpositionsenum for possible reuse specifies which optional per-position values you require; see {@link docsandpositionsenum#flag_offsets} and {@link docsandpositionsenum#flag_payloads}. expert: returns the termsenums internal state to position the termsenum without re-seeking the term dictionary.  note: a seek by {@link termstate} might not capture the {@link attributesource}'s state. callers must maintain the {@link attributesource} states separately @see termstate @see #seekexact(bytesref, termstate) an empty termsenum for quickly returning an empty instance e.g. in {@link org.apache.lucene.search.multitermquery} please note: this enum should be unmodifiable, but it is currently possible to add attributes to it. this should not be a problem, as the enum is always empty and the existence of unused attributes does not matter. make it synchronized here, to prevent double lazy init"
org.apache.lucene.index.TermState "encapsulates all required internal state to position the associated {@link termsenum} without re-seeking. @see termsenum#seekexact(org.apache.lucene.util.bytesref, termstate) @see termsenum#termstate() @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) copies the content of the given {@link termstate} to this instance the termstate to copy should not happen"
org.apache.lucene.index.MergePolicy "expert: a mergepolicy determines the sequence of primitive merge operations. whenever the segments in an index have been altered by {@link indexwriter}, either the addition of a newly flushed segment, addition of many segments from addindexes calls, or a previous merge that may now need to cascade, {@link indexwriter} invokes {@link #findmerges} to give the mergepolicy a chance to pick merges that are now required. this method returns a {@link mergespecification} instance describing the set of merges that should be done, or null if no merges are necessary. when indexwriter.forcemerge is called, it calls {@link #findforcedmerges(segmentinfos,int,map)} and the mergepolicy should then return the necessary merges. note that the policy can return more than one merge at a time. in this case, if the writer is using {@link serialmergescheduler}, the merges will be run sequentially but if it is using {@link concurrentmergescheduler} they will be run concurrently. the default mergepolicy is {@link tieredmergepolicy}. @lucene.experimental onemerge provides the information necessary to perform an individual primitive merge operation, resulting in a single new segment. the merge spec includes the subset of segments to be merged as well as whether the new segment should use the compound file format. estimated size in bytes of the merged segment. segments to be merged. number of documents in the merged segment. sole constructor. list of {@link segmentinfopercommit}s to be merged. record that an exception occurred while executing this merge retrieve previous exception set by {@link #setexception}. mark this merge as aborted. if this is called before the merge is committed then the merge will not be committed. returns true if this merge was aborted. called periodically by {@link indexwriter} while merging to see if the merge is aborted. set or clear whether this merge is paused paused (for example {@link concurrentmergescheduler} will pause merges if too many are running). returns true if this merge is paused. @see #setpause(boolean) returns a readable description of the current merge state. returns the total size in bytes of this merge. note that this does not indicate the size of the merged segment, but the input total size. returns the total number of documents that are included with this merge. note that this does not indicate the number of documents after the merge. return {@link mergeinfo} describing this merge. a mergespecification instance provides the information necessary to perform multiple merges. it simply contains a list of {@link onemerge} instances. the subset of segments to be included in the primitive merge. sole constructor. use {@link #add(mergepolicy.onemerge)} to add merges. adds the provided {@link onemerge} to this specification. returns a description of the merges in this specification. exception thrown if there are any problems while executing a merge. create a {@code mergeexception}. create a {@code mergeexception}. returns the {@link directory} of the index that hit the exception. thrown when a merge was explicity aborted because {@link indexwriter#close(boolean)} was called with false. normally this exception is privately caught and suppresed by {@link indexwriter}. create a {@link mergeabortedexception}. create a {@link mergeabortedexception} with a specified message. {@link indexwriter} that contains this instance. creates a new merge policy instance. note that if you intend to use it without passing it to {@link indexwriter}, you should call {@link #setindexwriter(indexwriter)}. sets the {@link indexwriter} to use by this merge policy. this method is allowed to be called only once, and is usually set by indexwriter. if it is called more than once, {@link alreadysetexception} is thrown. @see setonce determine what set of merge operations are now necessary on the index. {@link indexwriter} calls this whenever there is a change to the segments. this call is always synchronized on the {@link indexwriter} instance so only one thread at a time will call this method. the event that triggered the merge the total set of segments in the index determine what set of merge operations is necessary in order to merge to <= the specified segment count. {@link indexwriter} calls this when its {@link indexwriter#forcemerge} method is called. this call is always synchronized on the {@link indexwriter} instance so only one thread at a time will call this method. the total set of segments in the index requested maximum number of segments in the index (currently this is always 1) contains the specific segmentinfo instances that must be merged away. this may be a subset of all segmentinfos. if the value is true for a given segmentinfo, that means this segment was an original segment present in the to-be-merged index; else, it was a segment produced by a cascaded merge. determine what set of merge operations is necessary in order to expunge all deletes from the index. the total set of segments in the index release all resources for the policy. returns true if a new segment (regardless of its origin) should use the compound file format. mergetrigger is passed to {@link mergepolicy#findmerges(mergetrigger, segmentinfos)} to indicate the event that triggered the merge. merge was triggered by a segment flush. merge was triggered by a full flush. full flushes can be caused by a commit, nrt reader reopen or a close call on the index writer. merge has been triggered explicitly by the merge was triggered by a successfully finished merge. used by indexwriter used by indexwriter used by indexwriter used by indexwriter used by indexwriter used by indexwriter used by indexwriter clone the list, as the in list may be based off original segmentinfos and may be modified in theory we could wait() indefinitely, but we do 1000 msec, defensively wakeup merge thread, if it's waiting should not happen"
org.apache.lucene.index.InvertedDocConsumerPerField "called once per field, and is given all indexablefield occurrences for this field in the document. return true if you wish to see inverted tokens for these fields: called before a field instance is being processed called once per inverted token called once per field per document, after all indexablefields are inverted called on hitting an aborting exception"
org.apache.lucene.index.ConcurrentMergeScheduler "a {@link mergescheduler} that runs each merge using a separate thread. specify the max number of threads that may run at once with {@link #setmaxthreadcount}. separately specify the maximum number of simultaneous merges with {@link #setmaxmergecount}. if the number of merges exceeds the max number of threads then the largest merges are paused until one of the smaller merges completes. if more than {@link #getmaxmergecount} merges are requested then this class will forcefully throttle the incoming threads by pausing until one more more merges complete. list of currently active {@link mergethread}s. {@link directory} that holds the index. {@link indexwriter} that owns this instance. how many {@link mergethread}s have kicked off (this is use to name them). sole constructor, with all settings set to default values. sets the max # simultaneous merge threads that should be running at once. this must be  if (verbose()) { message(&quot;your message&quot;); }  outputs the given message - this method assumes {@link #verbose()} was called and returned true. wait for any running merge threads to finish. this call is not interruptible as used by {@link #close()}. returns the number of merge threads that are alive. note that this number is &le; {@link #mergethreads} size. does the actual merge, by calling {@link indexwriter#merge} create and return a new mergethread runs a merge thread, which may run one or more merges in sequence. sole constructor. record the currently running merge. return the currently running merge. return the current merge, or null if this {@code mergethread} is done. set the priority of this thread. called when an exception is hit in a background merge thread used for testing used for testing max number of merge threads allowed to be running at once. when there are more merges then this, we forcefully pause the larger ones, letting the smaller ones run, up until maxmergecount merges at which point we forcefully pause incoming threads (that presumably are the ones causing so much merging). we default to 1 here: tests on spinning-magnet drives showed slower indexing perf if more than one merge thread runs at once (though on an ssd it was faster): max number of merges we accept before forcefully throttling the incoming threads only look at threads that are alive & not in the process of stopping (ie have an active merge): prune any dead threads sort the merge threads in descending order. pause the thread if maxthreadcount is smaller than the number of merge threads. default to slightly higher priority than our calling thread ignore this exception, we will retry until all threads are dead finally, restore interrupt status: first, quickly run through the newly proposed merges and add any orthogonal merges (ie a merge not involving segments already pending to be merged) to the queue. if we are way behind on merging, many of these newly proposed merges will likely already be registered. iterate, pulling from the indexwriter's queue of pending merges, until it's empty: this means merging has fallen too far behind: we have already now there's at least one more merge pending. note that only maxthreadcount of those running; the rest will be paused (see updatemergethreads). we stall this producer thread to prevent creation of new segments, until merging has caught up: ok to spawn a new merge thread to handle this merge: must call this after starting the thread else the new thread is removed from mergethreads (since it's not alive yet): strangely, sun's jdk 1.5 on linux sometimes throws npe out of here... ignore this because we will still run fine with normal thread priority first time through the while loop we do the merge that we were started with: subsequent times through the loop we do any new merge that writer says is necessary: notify here in case any threads were stalled; they will notice that the pending merge has been pulled and possibly resume: ignore the exception if it was due to abort: system.out.println(thread.currentthread().getname() + ": cms: exc"); exc.printstacktrace(system.out); suppressexceptions is normally only set during testing. when an exception is hit during merge, indexwriter removes any partial files and then allows another merge to run. if whatever caused the error is not transient then the exception will keep happening, so, we sleep here to avoid saturating cpu in such cases:"
org.apache.lucene.index.IndexFormatTooOldException "this exception is thrown when lucene detects an index that is too old for this lucene version creates an {@code indexformattoooldexception}. describes the file that was too old the version of the file that was too old @lucene.internal creates an {@code indexformattoooldexception}. the open file that's too old the version of the file that was too old @lucene.internal creates an {@code indexformattoooldexception}. describes the file that was too old the version of the file that was too old the minimum version accepted the maxium version accepted @lucene.internal creates an {@code indexformattoooldexception}. the open file that's too old the version of the file that was too old the minimum version accepted the maxium version accepted @lucene.internal"
org.apache.lucene.index.MultiDocsAndPositionsEnum "exposes flex api, merged from flex api of sub-segments. @lucene.experimental sole constructor. returns {@code true} if this instance can be reused by the provided {@link multitermsenum}. rre-use and reset this instance on the provided slices. how many sub-readers we are merging. @see #getsubs returns sub-readers we are merging. holds a {@link docsandpositionsenum} along with the corresponding {@link readerslice}. {@link docsandpositionsenum} for this sub-reader. {@link readerslice} describing how this sub-reader fits into the composite reader. todo: implement bulk read more efficiently than super"
org.apache.lucene.index.TermContext "maintains a {@link indexreader} {@link termstate} view over {@link indexreader} instances containing a single term. the {@link termcontext} doesn't track if the given {@link termstate} objects are valid, neither if the {@link termstate} instances refer to the same terms in the associated readers. @lucene.experimental holds the {@link indexreadercontext} of the top-level {@link indexreader}, used internally only for asserting. @lucene.internal creates an empty {@link termcontext} from a {@link indexreadercontext} creates a {@link termcontext} with an initial {@link termstate}, {@link indexreader} pair. creates a {@link termcontext} from a top-level {@link indexreadercontext} and the given {@link term}. this method will lookup the given term in all context's leaf readers and register each of the readers containing the term in the returned {@link termcontext} using the leaf reader's ordinal.  note: the given context must be a top-level context. clears the {@link termcontext} internal state and removes all registered {@link termstate}s registers and associates a {@link termstate} with an leaf ordinal. the leaf ordinal should be derived from a {@link indexreadercontext}'s leaf ord. returns the {@link termstate} for an leaf ordinal or null if no {@link termstate} for the ordinal was registered. the readers leaf ordinal to get the {@link termstate} for. {@link termstate} for the given readers ord or null if no {@link termstate} for the reader was registered returns the accumulated document frequency of all {@link termstate} instances passed to {@link #register(termstate, int, int, long)}. accumulated document frequency of all {@link termstate} instances passed to {@link #register(termstate, int, int, long)}. returns the accumulated term frequency of all {@link termstate} instances passed to {@link #register(termstate, int, int, long)}. accumulated term frequency of all {@link termstate} instances passed to {@link #register(termstate, int, int, long)}. expert: only available for queries that want to lie about docfreq @lucene.internal public static boolean debug = blocktreetermswriter.debug; if (debug) system.out.println("prts.build term=" + term); if (debug) system.out.println(" r=" + leaves[i].reader); if (debug) system.out.println(" found");"
org.apache.lucene.index.StoredFieldsConsumer "this is a docfieldconsumer that writes stored fields. fills in any hole in the docids it's possible that all documents seen in this segment hit non-aborting exceptions, in which case we will not have yet init'd the fieldswriter: we must "catch up" for all docs before us that had no stored fields:"
org.apache.lucene.index.SerialMergeScheduler "a {@link mergescheduler} that simply does each merge sequentially, using the current thread. sole constructor. just do the merges in sequence. we do this "synchronized" so that even if the application is using multiple threads, only one merge may run at a time."
org.apache.lucene.index.OrdTermState "an ordinal based {@link termstate} @lucene.experimental term ordinal, i.e. it's position in the full list of sorted terms. sole constructor."
org.apache.lucene.index.IndexFormatTooNewException "this exception is thrown when lucene detects an index that is newer than this lucene version. creates an {@code indexformattoonewexception} describes the file that was too old the version of the file that was too old the minimum version accepted the maxium version accepted @lucene.internal creates an {@code indexformattoonewexception} the open file that's too old the version of the file that was too old the minimum version accepted the maxium version accepted @lucene.internal"
org.apache.lucene.index.AtomicReader "{@code atomicreader} is an abstract class, providing an interface for accessing an index. search of an index is done entirely through this abstract interface, so that any subclass which implements it is searchable. indexreaders implemented by this subclass do not consist of several sub-readers, they are atomic. they support retrieval of stored fields, doc values, terms, and postings. for efficiency, in this api documents are often referred to via document numbers, non-negative integers which each name a unique document in the index. these document numbers are ephemeral -- they may change as documents are added to and deleted from an index. clients should thus not rely on a given document having the same number between sessions.  note: {@link indexreader} instances are completely thread safe, meaning multiple threads can call any of its methods, concurrently. if your application requires external synchronization, you should not synchronize on the indexreader instance; use your own (non-lucene) objects instead. sole constructor. (for invocation by subclass constructors, typically implicit.) returns true if there are norms stored for this field. @deprecated (4.0) use {@link #getfieldinfos()} and check {@link fieldinfo#hasnorms()} for the field instead. returns {@link fields} for this reader. this method may return null if the reader has no postings. returns the number of documents containing the term t. this method returns 0 if the term or field does not exists. this method does not take into account deleted documents that have not yet been merged away. this may return null if the field does not exist. returns {@link docsenum} for the specified term. this will return null if either the field or term does not exist. @see termsenum#docs(bits, docsenum) returns {@link docsandpositionsenum} for the specified term. this will return null if the field or term does not exist or positions weren't indexed. @see termsenum#docsandpositions(bits, docsandpositionsenum) returns {@link docvalues} for this field. this method may return null if the reader has no per-document values stored. returns {@link docvalues} for this field's normalization values. this method may return null if the field has no norms. get the {@link fieldinfos} describing all fields in this reader. @lucene.experimental returns the {@link bits} representing live (not deleted) docs. a set bit indicates the doc id has not been deleted. if this method returns null it means there are no deleted documents (all documents are live). the returned instance has been safely published for use by multiple threads without additional synchronization. javadocs note: using normvalues(field) != null would potentially cause i/o"
org.apache.lucene.index.SegmentReader "indexreader implementation over a single segment.  instances pointing to the same segment (but with different deletes, etc) may share the same core data. @lucene.experimental constructs a new segmentreader with a new core. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error create new segmentreader sharing core from a previous segmentreader and loading new live docs from a new deletes file. used by openifchanged. create new segmentreader sharing core from a previous segmentreader and using the provided in-memory livedocs. used by indexwriter to provide a new nrt reader expert: retrieve thread-private {@link storedfieldsreader} @lucene.internal expert: retrieve thread-private {@link termvectorsreader} @lucene.internal return the name of the segment this reader is reading. return the segmentinfopercommit of the segment this reader is reading. returns the directory this index resides in. returns term infos index divisor originally passed to {@link #segmentreader(segmentinfopercommit, int, iocontext)}. called when the shared core for this segmentreader is closed.  this listener is called only once all segmentreaders sharing the same core are closed. at this point it is safe for apps to evict this reader from any caches keyed on {@link #getcorecachekey}. this is the same interface that {@link fieldcache} uses, internally, to evict entries. @lucene.experimental invoked when the shared core of the provided {@link segmentreader} has closed. expert: adds a coreclosedlistener to this reader's shared core expert: removes a coreclosedlistener from this reader's shared core javadocs normally set to si.doccount - si.deldoccount, unless we were tells us the doccount: todo: why is this public? note: the bitvector is stored using the regular directory, not cfs with lock-less commits, it's entirely possible (and fine) to hit a filenotfound exception above. in this case, we want to explicitly close any subset of things that were opened so that we don't have to wait for a gc to do so. system.out.println("sr.close seg=" + si); don't call ensureopen() here (it could affect performance) don't call ensureopen() here (it could affect performance) don't call ensureopen() here (it could affect performance) segmentinfo.tostring takes dir and number of pending deletions; so we reverse compute that here: don't ensureopen here -- in certain cases, when a cloned/reopened reader needs to commit, it may call this method on the closed original reader this is necessary so that cloned segmentreaders (which share the underlying postings data) will map to the same entry in the fieldcache. see lucene-1579."
org.apache.lucene.index.TermVectorsConsumerPerField "called once per field per document if term vectors are enabled, to write the vectors to ramoutputstream, which is then quickly flushed to the real term vectors files in the directory. if enabled, and we actually saw any for this field todo: move this check somewhere else, and impl the other missing ones only necessary if previous doc hit a non-aborting exception while writing vectors in this field: todo: only if needed for performance perthread.postingscount = 0; this is called once, after inverting all occurrences of a given field in the doc. at this point we flush our hash into the docwriter. get bytesref commit the termvectors once successful - fi will otherwise reset them how many times this term occurred in the current doc last offset we saw last position where this term occurred"
org.apache.lucene.index.IndexFileNames "this class contains useful constants representing filenames and extensions used by lucene, as well as convenience methods for querying whether a file name matches an extension ({@link #matchesextension(string, string) matchesextension}), as well as generating file names from a segment name, generation and extension ( {@link #filenamefromgeneration(string, string, long) filenamefromgeneration}, {@link #segmentfilename(string, string, string) segmentfilename}). note: extensions used by codecs are not listed here. you must interact with the {@link codec} directly. @lucene.internal no instance name of the index segment file extension of gen file name of the generation reference file name extension of compound file extension of compound file entries this array contains all filename extensions used by lucene's index files, with one exception, namely the extension made up from .s + a number. also note that lucene's segments_n files do not have any filename extension. computes the full file name from base, extension and generation. if the generation is -1, the file name is null. if it's 0, the file name is &lt;base&gt;.&lt;ext&gt;. if it's > 0, the file name is &lt;base&gt;_&lt;gen&gt;.&lt;ext&gt;. note: .&lt;ext&gt; is added to the name only if ext is not an empty string. main part of the file name extension of the filename generation returns a file name that includes the given segment name, your own custom name and extension. the format of the filename is: &lt;segmentname&gt;(_&lt;name&gt;)(.&lt;ext&gt;).  note: .&lt;ext&gt; is added to the result file name only if ext is not empty.  note: _&lt;segmentsuffix&gt; is added to the result file name only if it's not the empty string  note: all custom files should be named using this method, or otherwise some structures may fail to handle them properly (such as if they are added to compound files). returns true if the given filename ends with the given extension. one should provide a pure extension, without '.'. locates the boundary of the segment name, or -1 strips the segment name out of the given file name. if you used {@link #segmentfilename} or {@link #filenamefromgeneration} to create your files, then this method simply removes whatever comes before the first '.', or the second '_' (excluding both). filename with the segment name removed, or the given filename if it does not contain a '.' and '_'. parses the segment name out of the given file name. segment name only, or filename if it does not contain a '.' and '_'. removes the extension (anything after the first '.'), otherwise returns the original filename. todo: put all files under codec and remove all the static extensions here the '6' part in the length is: 1 for '.', 1 for '_' and 4 as estimate to the gen length as string (hopefully an upper limit so sb won't expand in the middle. it doesn't make a difference whether we allocate a stringbuilder ourself or not, since there's only 1 '+' operator. if it is a .del file, there's an '_' after the first character if it's not, strip everything that's before the '.' all files check this in segmentinfo.java):"
org.apache.lucene.index.FreqProxTermsWriter "current writer chain: fieldsconsumer -> impl: formatpostingstermsdictwriter -> termsconsumer -> impl: formatpostingstermsdictwriter.termswriter -> docsconsumer -> impl: formatpostingsdocswriter -> positionsconsumer -> impl: formatpostingspositionswriter todo: would be nice to factor out more of this, eg the freqproxfieldmergestate, and code to visit all fields under the same fieldinfo together, up into termshash. other writers would presumably share alot of this... gather all fielddata's that have postings, across all threadstates sort by field name if this field has postings then add them to the segment"
org.apache.lucene.index.FilteredTermsEnum "abstract class for enumerating a subset of all terms. term enumerations are always ordered by {@link #getcomparator}. each term in the enumeration is greater than all that precede it. please note: consumers of this enum cannot call {@code seek()}, it is forward only; it throws {@link unsupportedoperationexception} when a seeking method is called. return value, if term should be accepted or the iteration should {@code end}. the {@code _seek} values denote, that after handling the current term the enum should call {@link #nextseekterm} and step forward. @see #accept(bytesref) accept the term and position the enum at the next term. accept the term and advance ({@link filteredtermsenum#nextseekterm(bytesref)}) to the next term. reject the term and position the enum at the next term. reject the term and advance ({@link filteredtermsenum#nextseekterm(bytesref)}) to the next term. reject the term and stop enumerating. return if term is accepted, not accepted or the iteration should ended (and possibly seek). creates a filtered {@link termsenum} on a terms enum. the terms enumeration to filter. creates a filtered {@link termsenum} on a terms enum. the terms enumeration to filter. use this method to set the initial {@link bytesref} to seek before iterating. this is a convenience method for subclasses that do not override {@link #nextseekterm}. if the initial seek term is {@code null} (default), the enum is empty. you can only use this method, if you keep the default implementation of {@link #nextseekterm}. on the first call to {@link #next} or if {@link #accept} returns {@link acceptstatus#yes_and_seek} or {@link acceptstatus#no_and_seek}, this method will be called to eventually seek the underlying termsenum to a new position. on the first call, {@code currentterm} will be {@code null}, later calls will provide the term the underlying enum is positioned at. this method returns per default only one time the initial seek term and then {@code null}, so no repositioning is ever done. override this method, if you want a more sophisticated termsenum, that repositions the iterator during enumeration. if this method always returns {@code null} the enum is empty. please note: this method should always provide a greater term than the last enumerated term, else the behaviour of this enum violates the contract for termsenums. returns the related attributes, the returned {@link attributesource} is shared with the delegate {@code termsenum}. this enum does not support seeking! @throws unsupportedoperationexception in general, subclasses do not support seeking. this enum does not support seeking! @throws unsupportedoperationexception in general, subclasses do not support seeking. this enum does not support seeking! @throws unsupportedoperationexception in general, subclasses do not support seeking. this enum does not support seeking! @throws unsupportedoperationexception in general, subclasses do not support seeking. returns the filtered enums term state system.out.println("fte.next doseek=" + doseek); new throwable().printstacktrace(system.out); seek or forward the iterator system.out.println(" seek to t=" + (t == null ? "null" : t.utf8tostring()) + " tenum=" + tenum); make sure we always seek forward: no more terms to seek to or enum exhausted system.out.println(" return null"); system.out.println(" got term=" + actualterm.utf8tostring()); enum exhausted check if term is accepted term accepted, but we need to seek so fall-through term accepted invalid term, seek next time we are supposed to end the enum"
org.apache.lucene.index.ParallelCompositeReader "an {@link compositereader} which reads multiple, parallel indexes. each index added must have the same number of documents, and exactly the same hierarchical subreader structure, but typically each contains different fields. deletions are taken from the first reader. each document contains the union of the fields of all documents with the same document number. when searching, matches for a query term are from the first index added that has the field. this is useful, e.g., with collections that have large fields which change rarely and small fields that change more frequently. the smaller fields may be re-indexed in a new index and both indexes may be searched together. warning: it is up to you to make sure all indexes are create a parallelcompositereader based on the provided readers; auto-closes the given readers on {@link #close()}. create a parallelcompositereader based on the provided readers. expert: create a parallelcompositereader based on the provided readers and storedfieldreaders; when a document is loaded, only storedfieldsreaders will be used. do this finally so any exceptions occurred before don't affect refcounts: check compatibility: hierarchically build the same subreader structure as the first compositereader with parallelreaders: we simply enable closing of subreaders, to prevent increfs on subreaders -> for synthetic subreaders, close() is never called by our doclose() we simply enable closing of subreaders, to prevent increfs on subreaders -> for synthetic subreaders, close() is never called by our doclose() throw the first exception"
org.apache.lucene.index.ReaderSlice "subreader slice from a parent composite reader. @lucene.internal zero-length {@code readerslice} array. document id this slice starts from. number of documents in this slice. sub-reader index for this slice. sole constructor."
org.apache.lucene.index.ByteSliceWriter "class to write byte streams into slices of shared byte[]. this is used by documentswriter to hold the posting list for many terms in ram. set up the writer to write at address. write byte into byte slice stream end marker"
org.apache.lucene.index.TwoPhaseCommitTool "a utility for executing 2-phase commit on several objects. @see twophasecommit @lucene.experimental no instance thrown by {@link twophasecommittool#execute(twophasecommit...)} when an object fails to preparecommit(). sole constructor. thrown by {@link twophasecommittool#execute(twophasecommit...)} when an object fails to commit(). sole constructor. rollback all objects, discarding any exceptions that occur. executes a 2-phase commit algorithm by first {@link twophasecommit#preparecommit()} all objects and only if all succeed, it proceeds with {@link twophasecommit#commit()}. if any of the objects fail on either the preparation or actual commit, it terminates and {@link twophasecommit#rollback()} all of them.  note: it may happen that an object fails to commit, after few have already successfully committed. this tool will still issue a rollback instruction on them as well, but depending on the implementation, it may not have any effect.  note: if any of the objects are {@code null}, this method simply skips over them. @throws preparecommitfailexception if any of the objects fail to {@link twophasecommit#preparecommit()} @throws commitfailexception if any of the objects fail to {@link twophasecommit#commit()} ignore any exception that occurs during rollback - we want to ensure all objects are rolled-back. first, all should successfully preparecommit() first object that fails results in rollback all of them and throwing an exception. if all successfully preparecommit(), attempt the actual commit() first object that fails results in rollback all of them and throwing an exception."
org.apache.lucene.index.FrozenBufferedDeletes "holds buffered deletes by term or query, once pushed. pushed deletes are write-once, so we shift to more memory efficient data structure to hold them. we don't hold docids because these are applied on flush. query we often undercount (say 24 bytes), plus int. terms, in sorted order: just for debugging parallel array of deleted query, and the docidupto for each assigned by buffereddeletesstream once pushed set to true iff this frozen packet represents a segment private deletes. in that case is should only have queries"
org.apache.lucene.index.IndexWriter "an indexwriter creates and maintains an index. the {@link openmode} option on {@link indexwriterconfig#setopenmode(openmode)} determines whether a new index is clarification: check points (and commits) indexwriter writes new index files to the directory without writing a new segments_n file which references these new files. it also means that the state of the in memory segmentinfos object is different than the most recent segments_n file written to the directory. each time the segmentinfos is changed, and matches the (possibly modified) directory files, we have a new "check point". if the modified/new segmentinfos is written to disk - as a new (generation of) segments_n file - this check point is also an indexcommit. a new checkpoint always replaces the previous checkpoint and becomes the new "front" of the index. this allows the indexfiledeleter to delete files that are referenced only by stale checkpoints. (files that were name of the write lock in the index. absolute hard maximum length for a term, in bytes once encoded as utf8. if a term arrives from the analyzer longer than this length, it is skipped and a message is printed to infostream, if set (see {@link indexwriterconfig#setinfostream(infostream)}). expert: returns a readonly reader, covering all committed as well as un-committed changes to the index. this provides "near real-time" searching, in that changes made during an indexwriter session can be quickly made available for searching without closing the writer nor calling {@link #commit}. note that this is functionally equivalent to calling {#flush} and then opening a new reader. but the turnaround time of this method should be faster since it avoids the potentially costly {@link #commit}. you must close the {@link indexreader} returned by this method once you are done using it. it's near real-time because there is no hard guarantee on how quickly you can get a new reader after making changes with indexwriter. you'll have to experiment in your situation to determine if it's fast enough. as this is a new and experimental feature, please report back on your findings so we can learn, improve and iterate. the resulting reader supports {@link directoryreader#openifchanged}, but that call will simply forward back to this method (though this may change in the future). the very first time this method is called, this writer instance will make every effort to pool the readers that it opens for doing merges, applying deletes, etc. this means additional resources (ram, file descriptors, cpu time) will be consumed. for lower latency on reopening a reader, you should call {@link indexwriterconfig#setmergedsegmentwarmer} to pre-warm a newly merged segment before it's committed to the index. this is important for minimizing index-to-search delay after a large merge.  if an addindexes call is running in another thread, then this reader will only search those segments from the foreign index that have been successfully copied over, so far. note: once the writer is closed, any outstanding readers may continue to be used. however, if you attempt to reopen any of those readers, you'll hit an {@link alreadyclosedexception}. @lucene.experimental that covers entire index plus all changes made so far by this indexwriter instance @throws ioexception if there is a low-level i/o error for releasing a nrt reader we must ensure that dw doesn't add any segments or deletes until we are done with creating the nrt directoryreader. we release the two stage full flush after we are done opening the directory reader! holds shared segmentreader instances. indexwriter uses segmentreaders for 1) applying deletes, 2) doing merges, 3) handing out a real-time reader. this pool reuses instances of the segmentreaders in all these places if it is in "near real-time mode" (getreader() has been called on this instance). remove all our references to readers, and commits any pending changes. commit live docs changes for the segment readers for the provided infos. @throws ioexception if there is a low-level i/o error obtain a readersandlivedocs instance from the readerpool. if create is true, you must later call {@link #release(readersandlivedocs)}. obtain the number of deleted docs for a pooled reader. if the reader isn't being pooled, the segmentinfo's delcount is returned. used internally to throw an {@link alreadyclosedexception} if this indexwriter has been closed or is in the process of closing. if true, also fail when {@code indexwriter} is in the process of closing ({@code closing=true}) but not yet done closing ( {@code closed=false}) @throws alreadyclosedexception if this indexwriter is closed or in the process of closing used internally to throw an {@link alreadyclosedexception} if this indexwriter has been closed ({@code closed=true}) or is in the process of closing ({@code closing=true}).  calls {@link #ensureopen(boolean) ensureopen(true)}. @throws alreadyclosedexception if this indexwriter is closed constructs a new indexwriter per the settings given in conf. note that the passed in {@link indexwriterconfig} is privately cloned; if you need to make subsequent "live" changes to the configuration use {@link #getconfig}.  the index directory. the index is either loads or returns the already loaded the global field number map for this {@link segmentinfos}. if this {@link segmentinfos} has no global field number map the returned instance is empty returns a {@link liveindexwriterconfig}, which can be used to query the indexwriter current settings, as well as modify "live" ones. commits all changes to an index, waits for pending merges to complete, and closes all associated files.  this is a "slow graceful shutdown" which may take a long time especially if a big merge is pending: if you only want to close resources use {@link #rollback()}. if you only want to commit pending changes and close resources see {@link #close(boolean)}.  note that this may be a costly operation, so, try to re-use a single writer instead of closing and opening a new one. see {@link #commit()} for caveats about write caching done by some io devices.  if an exception is hit during close, eg due to disk full or some other reason, then both the on-disk index and the internal state of the indexwriter instance will be consistent. however, the close will not be complete even though part of it (flushing buffered documents) may have succeeded, so the write lock will still be held.  if you can correct the underlying cause (eg free up some disk space) then you can call close() again. failing that, if you want to force the write lock to be released (dangerous, because you may then lose buffered docs in the indexwriter instance) then you can do something like this:  try { writer.close(); } finally { if (indexwriter.islocked(directory)) { indexwriter.unlock(directory); } }  after which, you must be certain not to use the writer instance anymore. note: if this method hits an outofmemoryerror you should immediately close the writer, again. see above for details. @throws ioexception if there is a low-level io error closes the index with or without waiting for currently running merges to finish. this is only meaningful when using a mergescheduler that runs merges in background threads. note: if this method hits an outofmemoryerror you should immediately close the writer, again. see above for details. note: it is dangerous to always call close(false), especially when indexwriter is not open for very long, because this can result in "merge starvation" whereby long merges will never have a chance to finish. this will cause too many segments in your index over time. if true, this call will block until all merges complete; else, it will ask all running merges to abort, wait until those merges have finished (which should be at most a few seconds), and then return. returns the directory used by this index. returns the analyzer used by this index. returns total number of docs in this index, including docs not yet flushed (still in the ram buffer), not counting deletions. @see #numdocs returns total number of docs in this index, including docs not yet flushed (still in the ram buffer), and including deletions. note: buffered deletions are not counted. if you really need these to be counted you should call {@link #commit()} first. @see #numdocs returns true if this index has deletions (including buffered deletions). adds a document to this index.  note that if an exception is hit (for example disk full) then the index will be consistent, but this document may not have been added. furthermore, it's possible the index will have one segment in non-compound format even when using compound files (when a merge has partially succeeded).  this method periodically flushes pending documents to the directory (see above), and also periodically triggers segment merges in the index according to the {@link mergepolicy} in use. merges temporarily consume space in the directory. the amount of space required is up to 1x the size of all segments being merged, when no readers/searchers are open against the index, and up to 2x the size of all segments being merged when readers/searchers are open against the index (see {@link #forcemerge(int)} for details). the sequence of primitive merge operations performed is governed by the merge policy. note that each term in the document can be no longer than 16383 characters, otherwise an illegalargumentexception will be thrown. note that it's possible to create an invalid unicode string in java if a utf16 surrogate pair is malformed. in this case, the invalid characters are silently replaced with the unicode replacement character u+fffd. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error adds a document to this index, using the provided analyzer instead of the value of {@link #getanalyzer()}. see {@link #adddocument(iterable)} for details on index and indexwriter state after an exception, and flushing/merging temporary free space requirements. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error atomically adds a block of documents with sequentially assigned document ids, such that an external reader will see all or none of the documents. warning: the index does not currently record which documents were added as a block. today this is fine, because merging will preserve a block. the order of documents within a segment will be preserved, even when child documents within a block are deleted. most search features (like result grouping and block joining) require you to mark documents; when these documents are deleted these search features will not work as expected. obviously adding documents to an existing block will require you the reindex the entire block. however it's possible that in the future lucene may merge more aggressively re-order documents (for example, perhaps to obtain better index compression), in which case you may need to fully re-index your documents at that time. see {@link #adddocument(iterable)} for details on index and indexwriter state after an exception, and flushing/merging temporary free space requirements. note: tools that do offline splitting of an index (for example, indexsplitter in contrib) or re-sorting of documents (for example, indexsorter in contrib) are not aware of these atomically added documents and will likely break them up. use such tools at your own risk! note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error @lucene.experimental atomically adds a block of documents, analyzed using the provided analyzer, with sequentially assigned document ids, such that an external reader will see all or none of the documents. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error @lucene.experimental atomically deletes documents matching the provided delterm and adds a block of documents with sequentially assigned document ids, such that an external reader will see all or none of the documents. see {@link #adddocuments(iterable)}. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error @lucene.experimental atomically deletes documents matching the provided delterm and adds a block of documents, analyzed using the provided analyzer, with sequentially assigned document ids, such that an external reader will see all or none of the documents. see {@link #adddocuments(iterable)}. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error @lucene.experimental deletes the document(s) containing term. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. the term to identify the documents to be deleted @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error expert: attempts to delete by document id, as long as the provided reader is a near-real-time reader (from {@link directoryreader#open(indexwriter,boolean)}). if the provided reader is an nrt reader obtained from this writer, and its segment has not been merged away, then the delete succeeds and this method returns true; else, it returns false the caller must then separately delete by term or query. note: this method can only delete documents visible to the currently open nrt reader. if you need to delete documents indexed after opening the nrt reader you must use the other deletedocument methods (e.g., {@link #deletedocuments(term)}). deletes the document(s) containing any of the terms. all given deletes are applied and flushed atomically at the same time. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. array of terms to identify the documents to be deleted @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error deletes the document(s) matching the provided query. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. the query to identify the documents to be deleted @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error deletes the document(s) matching any of the provided queries. all given deletes are applied and flushed atomically at the same time. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. array of queries to identify the documents to be deleted @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error updates a document by first deleting the document(s) containing term and then adding the new document. the delete and then add are atomic as seen by a reader on the same index (flush may happen only after the add). note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. the term to identify the document(s) to be deleted the document to be added @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error updates a document by first deleting the document(s) containing term and then adding the new document. the delete and then add are atomic as seen by a reader on the same index (flush may happen only after the add). note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. the term to identify the document(s) to be deleted the document to be added the analyzer to use when analyzing the document @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error if non-null, information about merges will be printed to this. forces merge policy to merge segments until there are this is a horribly costly operation, especially when you pass a small {@code maxnumsegments}; usually you should only call this if the index is static (will no longer be changed). note that this requires up to 2x the index size free space in your directory (3x if you're using compound file format). for example, if your index size is 10 mb then you need up to 20 mb free for this to complete (30 mb if you're using compound file format). also, it's best to call {@link #commit()} afterwards, to allow indexwriter to free up disk space. if some but not all readers re-open while merging is underway, this will cause > 2x temporary space to be consumed as those new readers will then hold open the temporary segments at that time. it is best not to re-open readers while merging is running. the actual temporary usage could be much less than these figures (it depends on many factors). in general, once this completes, the total size of the index will be less than the size of the starting index. it could be quite a bit smaller (if there were many pending deletes) or just slightly smaller. if an exception is hit, for example due to disk full, the index will not be corrupted and no documents will be lost. however, it may have been partially merged (some segments were merged but not all), and it's possible that one of the segments in the index will be in non-compound format even when using compound file format. this will occur when the exception is hit during conversion of the segment into compound format. this call will merge those segments present in the index when the call started. if other threads are still adding documents and flushing segments, those newly just like {@link #forcemerge(int)}, except you can specify whether the call should block until all merging completes. this is only meaningful with a {@link mergescheduler} that is able to run merges in background threads. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. returns true if any merges in pendingmerges or runningmerges are maxnumsegments merges. just like {@link #forcemergedeletes()}, except you can specify whether the call should block until the operation completes. this is only meaningful with a {@link mergescheduler} that is able to run merges in background threads. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. note: if you call {@link #close(boolean)} with false, which aborts all running merges, then any thread still running this method might hit a {@link mergepolicy.mergeabortedexception}. forces merging of all segments that have deleted documents. the actual merges to be executed are determined by the {@link mergepolicy}. for example, the default {@link tieredmergepolicy} will only pick a segment if the percentage of deleted docs is over 10%. this is often a horribly costly operation; rarely is it warranted. to see how many deletions you have pending in your index, call {@link indexreader#numdeleteddocs}. note: this method first flushes a new segment (if there are indexed documents), and applies all buffered deletes. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. expert: asks the mergepolicy whether any merges are necessary now and if so, runs the requested merges and then iterate (test again if merges are needed) until no more merges are returned by the mergepolicy. explicit calls to maybemerge() are usually not necessary. the most common case is when merge policy parameters have changed. this method will call the {@link mergepolicy} with {@link mergetrigger#explicit}. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. expert: to be used by a {@link mergepolicy} to avoid selecting merges for segments already being merged. the returned collection is not cloned, and thus is only safe to access if you hold indexwriter's lock (which you do when indexwriter invokes the mergepolicy). do not alter the returned collection! expert: the {@link mergescheduler} calls this method to retrieve the next merge requested by the mergepolicy @lucene.experimental expert: returns true if there are merges waiting to be scheduled. @lucene.experimental close the indexwriter without committing any changes that have occurred since the last commit (or since it was opened, if commit hasn't been called). this removes any temporary files that had been delete all documents in the index. this method will drop all buffered documents and will remove all segments from the index. this change will not be visible until a {@link #commit()} has been called. this method can be rolled back using {@link #rollback()}. note: this method is much faster than using deletedocuments( new matchalldocsquery() ). note: this method will forcefully abort all merges in progress. if other threads are running {@link #forcemerge}, {@link #addindexes(indexreader[])} or {@link #forcemergedeletes} methods, they may receive {@link mergepolicy.mergeabortedexception}s. wait for any currently outstanding merges to finish. it is guaranteed that any merges started prior to calling this method will have completed once this method completes. called whenever the segmentinfos has been updated and the index files referenced exist (correctly) in the index directory. atomically adds the segment private delete packet and publishes the flushed segments segmentinfo to the index writer. adds all segments from an array of indexes into this index. this may be used to parallelize batch indexing. a large document collection can be broken into sub-collections. each sub-collection can be indexed in parallel, on a different thread, process or machine. the complete index can then be merges the provided indexes into this index.  the provided indexreaders are not closed.  see {@link #addindexes} for details on transactional semantics, temporary free space required in the directory, and non-cfs segments on an exception.  note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details.  note: this method merges all given {@link indexreader}s in one merge. if you intend to merge a large number of readers, it may be better to call this method multiple times, each time with a small set of readers. in principle, if you use a merge policy with a {@code mergefactor} or {@code maxmergeatonce} parameter, you should pass that many readers in one call. also, if the given readers are {@link directoryreader}s, they can be opened with {@code termindexinterval=-1} to save ram, since during merge the in-memory structure is not used. see {@link directoryreader#open(directory, int)}.  note: if you call {@link #close(boolean)} with false, which aborts all running merges, then any thread still running this method might hit a {@link mergepolicy.mergeabortedexception}. @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error copies the segment files as-is into the indexwriter's directory. a hook for extending classes to execute operations after pending added and deleted documents have been flushed to the directory but before the change is committed (new segments_n file written). a hook for extending classes to execute operations before pending added and deleted documents are flushed to the directory. expert: prepare for commit. this does the first phase of 2-phase commit. this method does all steps necessary to commit changes since this writer was opened: flushes pending added and deleted docs, syncs the index files, writes most of next segments_n file. after calling this you must call either {@link #commit()} to finish the commit, or {@link #rollback()} to revert the commit and undo all changes done since the writer was opened. you can also just call {@link #commit()} directly without preparecommit first in which case that method will internally call preparecommit. note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. sets the commit returns the commit commits all pending changes (added & deleted documents, segment merges, added indexes, etc.) to the index, and syncs all referenced index files, such that a reader will see the changes and the index updates will survive an os or machine crash or power loss. note that this does not wait for any running background merges to finish. this may be a costly operation, so you should test the cost in your application and do it only when really necessary.  note that this operation calls directory.sync on the index files. that call should not return until the file contents & metadata are on stable storage. for fsdirectory, this calls the os's fsync. but, beware: some hardware devices may in fact cache writes even during fsync, and return before the bits are actually on stable storage, to give the appearance of faster performance. if you have such a device, and it does not have a battery backup (for example) then on power loss it may still lose data. lucene cannot guarantee consistency on such devices.  note: if this method hits an outofmemoryerror you should immediately close the writer. see above for details. @see #preparecommit flush all in-memory buffered updates (adds and deletes) to the directory. if true, we may merge segments (if deletes or docs were flushed) if necessary whether pending deletes should also expert: return the total size of all index files currently cached in memory. useful for size management with flushramdocs() expert: return the number of documents currently buffered in ram. carefully merges deletes for the segments we just merged. this is tricky because, although merging will clear all deletes (compacts the documents), new deletes may have been flushed to the segments since the merge was started. this method "carries over" such new deletes onto the newly merged segment, and saves the resulting deletes file (incrementing the delete generation for merge.info). if no deletes were flushed, no new deletes file is saved. merges the indicated segments, replacing them in the stack with a single segment. @lucene.experimental hook that's called when the specified merge is complete. checks whether this merge involves any segments already participating in a merge. if not, this merge is "registered", meaning we record that its segments are now participating in a merge, and true is returned. else (the merge conflicts) false is returned. does initial setup for a merge, which is fast but holds the synchronized lock on indexwriter instance. does fininishing for a merge, which is fast but holds the synchronized lock on indexwriter instance. does the actual (time-consuming) work of the merge, but without holding synchronized lock on indexwriter instance returns a string description of all segments, for debugging. @lucene.internal returns a string description of the specified segments, for debugging. @lucene.internal returns a string description of the specified segment, for debugging. @lucene.internal only for testing. @lucene.internal walk through all files referenced by the current segmentinfos and ask the directory to sync each file, if it wasn't already. if that succeeds, then we prepare a new segments_n file but do not fully commit it. returns true iff the index in the named directory is currently locked. the directory to check for a lock @throws ioexception if there is a low-level io error forcibly unlocks the index in the named directory.  caution: this should only be used by failure recovery code, when it is known that no other process nor thread is in fact currently accessing this index. if {@link directoryreader#open(indexwriter,boolean)} has been called (ie, this writer is in near real-time mode), then after a merge completes, this class can be invoked to warm the reader on the newly merged segment, before the merge commits. this is not required for near real-time search, but will reduce search latency on opening a new near real-time reader after a merge completes. @lucene.experimental note: warm is called before any deletes have been carried over to the merged segment. sole constructor. (for invocation by subclass constructors, typically implicit.) invoked on the {@link atomicreader} for the newly merged segment, before that segment is made visible to near-real-time readers. expert: remove any index files that are no longer used.  indexwriter normally deletes unused files itself, during indexing. however, on windows, which disallows deletion of open files, if there is a reader open on the index then those files cannot be deleted. this is fine, because indexwriter will periodically retry the deletion.  however, indexwriter doesn't try that often: only on open, close, flushing a new segment, and finishing a merge. if you don't do any of these actions with your indexwriter, you'll see the unused files linger. if that's a problem, call this method to delete them (once you've closed the open readers that were preventing their deletion).  in addition, you can call this method to delete unreferenced index commits. this might be useful if you are using an {@link indexdeletionpolicy} which holds onto index commits until some criteria are met, but those commits are no longer needed. otherwise, those commits will be deleted the next time commit() is called. note: this method creates a compound file for all files returned by info.files(). while, generally, this may include separate norms and deletion files, this segmentinfo must not reference such files when this method is called, because they are not allowed within a compound file. tries to delete the given files if unreferenced the files to delete @throws ioexception if an {@link ioexception} occurs @see indexfiledeleter#deletenewfiles(collection) cleans up residuals from a segment that could not be entirely flushed due to an error @see indexfiledeleter#refresh(string) where this index resides how to analyze text increments every time a change is completed last changecount that was committed list of segmentinfo we will fallback to if the commit fails set when a commit is pending (after preparecommit() & before commit()) the segments used by forcemerge to note those needing merging holds all segmentinfo instances currently involved in merges this is a "write once" variable (like the organic dye on a dvd-r that may or may not be heated by a laser and then cooled to permanently record the event): it's false, until getreader() is called for the first time, at which point it's switched to true and never changes back to false. once this is true, we hold open and reuse segmentreader instances internally for applying deletes, doing merges, and reopening near real-time readers. the instance that was passed to the constructor. it is saved only in order to allow do this up front before flushing so that the readers obtained during this flush are pooled, the first time this method is called: prevent double increment since docwriter#doflush increments the flushcount if we flushed anything. prevent segmentinfos from changing while opening the reader; in theory we could do similar retry logic, just like we do when loading segments_n never reached but javac disagrees: done: finish the full flush! used only by asserts matches incref in get: pool still holds a ref: this is the last ref to this rld, and we're not pooling, so remove it: make sure we only write del docs for a live segment: must checkpoint w/ deleter, because we just  make sure we only write del docs for a live segment: must checkpoint w/ deleter, because we just  important to remove as-we-go, not with .clear() in the end, in case we hit an exception; otherwise we could over-decref if close() is called again: note: it is allowed that these decrefs do not actually close the srs; this happens when a near real-time reader is kept open after the indexwriter instance is closed: make sure we only write del docs for a live segment: must checkpoint w/ deleter, because we just  steal initial reference: return ref to caller: for writing new segments obtain write lock create_or_append - create only if an index does not exist if index is too old, reading the segments will throw indexformattoooldexception. try to read first. this is to allow create against an index that's currently open for searching. in this case we write the next segments_n file with no segments: likely this means it's a fresh directory record that we have a change (zero out all segments) pending: swap out all segments, but, keep metadata in segmentinfos, like version & generation, to preserve write-once. this is important if readers are open against the future commit points. start with previous field numbers, but new fieldinfos default deleter (for backwards compatibility) is keeponlylastcommitdeleter: deletion policy deleted the "head" commit point. we have to mark ourself as changed so that if we are closed w/o any further changes we write a new segments_n file. don't mask the original exception todo: we could also pull dv type of each field here, and use that to make sure new segment(s) don't change the type... ensure that only one thread actually gets to do the closing, and make sure no commit is also in progress: if any methods have hit outofmemoryerror, then abort on close, in case the internal state of indexwriter or documentswriter is corrupt returns true if this thread should attempt to close, or false if indexwriter is now closed; else, waits until another thread finishes closing another thread is presently trying to close; wait until it finishes one way (closes successfully) or another (fails to close) only allow a new merge to be triggered if we are going to wait for merges: already closed clean up merge scheduler in all cases, although flushing may have failed: give merge scheduler last chance to run, in case any pending merges are waiting: ignore any interruption, does not matter by setting the interrupted status, the next call to finishmerges will pass false, so it will not wait shutdown policy, scheduler and all threads (this call is not interruptible): used by assert below release write lock finally, restore interrupt status: reader is already atomic: use the incoming docid: composite reader: lookup sub-reader and re-base docid: todo: this is a slow linear search, but, number of segments should be contained unless something is seriously wrong w/ the index, so it should be a minor cost: if a merge has already registered for this segment, we leave it in the readerpool; the merge will skip merging it and will then drop it once it's done: must bump changecount so if no other changes happened, we still commit this change: system.out.println(" yes " + info.info.name + " " + docid); system.out.println(" no rld " + info.info.name + " " + docid); system.out.println(" no seg " + info.info.name + " " + docid); for test purpose for test purpose for test purpose for test purpose for test purpose for test purpose cannot synchronize on indexwriter because that causes deadlock important to increment changecount so that the segmentinfos is written on close. otherwise we could close, re-open and re-return the same segment name that was previously returned which can cause problems at least with concurrentmergescheduler. now mark all pending & running merges for forced merge: forward any exceptions in background merge threads to the current thread: if close is called while we are still running, throw an exception so the calling thread will know merging did not complete note: in the concurrentmergescheduler case, when dowait is false, we can return immediately while background threads accomplish the merging check each merge that mergepolicy asked us to do, to see if any of them are still running and if any of them have hit an exception. if any of our merges are still running, wait: note: in the concurrentmergescheduler case, when dowait is false, we can return immediately while background threads accomplish the merging do not start new merges if we've hit oome advance the merge from pending to running ensure that only one thread actually gets to do the closing, and make sure no commit is also in progress: must pre-close these two, in case they increment changecount so that we can then set it to false before calling closeinternal mark it as closed first to prevent subsequent indexing actions/flushes don't bother saving any changes in our segmentinfos keep the same segmentinfos instance but replace all of its segmentinfo instances. this is so the next attempt to commit using this instance of indexwriter will always write to a new generation ("write once"). ask deleter to locate unreferenced files & remove them: abort any running merges remove any buffered docs remove all segments ask deleter to locate unreferenced files & remove them: don't bother saving any changes in our segmentinfos mark that the index has changed abort all pending & running merges: these merges periodically check whether they have been aborted, and stop if so. we wait here to make sure they all stop. it should not take very long because the merge threads periodically check if they are aborted. waitformerges() will ensure any running addindexes finishes. it's fine if a new one attempts to start because from our caller above the call will see that we are in the process of closing, and will throw an alreadyclosedexception. sanity check lock order iw -> bds publishing the segment must be synched on iw -> bds to make the sure that no merge prunes away the seg. private delete packet since we don't have a delete packet to apply we can get a new generation right away read infos from dir todo: somehow we should fix this merge so it's abortable so that iw.close(false) is able to stop it add new indexes merge 'em guard segmentinfos now create the compound file if needed delete new non cfs files directly: they were never registered with ifd have codec write segmentinfo. must do this after creating cfs so that 1) .si isn't slurped into cfs, and 2) .si reflects usecompoundfile=true change above: register the new segment determine if the doc store of this segment needs to be copied. it's only relevant for segments that share doc store with others, because the ds might have been copied already, in which case we just want to update the ds name of this segmentinfo. note: we don't really need this fis (its copied), but we load it up so we don't pass a null value to the si writer copy the attributes map, we might modify it below. also we need to ensure its read-write, since we will invoke the siwriter (which might want to set something). only violate the codec this way if it's preflex & shares doc stores change docstoresegment to newdsname system.out.println("copy seg=" + info.info.name + " version=" + info.info.getversion()); same si as before but we change directory, name and docstoresegment: build up new segment's file names. must do this before writing segmentinfo: we must rewrite the si file because it references segment name (its own name, if its 3.x, and doc store segment name): ok: 3x codec cannot write a new si file; segmentinfos will write this on commit copy the segment's files we already rewrote this above this is copied from doflush, except it's modified to clone & incref the flushed segmentinfos inside the sync block: prevent double increment since docwriter#doflush increments the flushcount if we flushed anything. must clone the segmentinfos while we still hold fullflushlock and while sync'd so that no partial changes (eg a delete w/o corresponding add from an updatedocument) can sneak into the commit point: this protects the segmentinfos we are now going to commit. this is important in case, eg, while we are trying to sync all referenced files, a merge completes which would otherwise have removed the files we are now syncing. done: finish the full flush! used only by commit and preparecommit, below; lock order is commitlock -> iw matches the incref done in preparecommit: ensures only one flush() is actually flushing segments at a time: note: this method cannot be sync'd because maybemerge() in turn calls mergescheduler.merge which in turn can take a long time to run and we don't want to hold the lock for that. in the case of concurrentmergescheduler this can lead to deadlock when it stalls due to too many running merges. we can be called during close, when closing==true, so we must pass false to ensureopen: flushcount is incremented in flushallthreads never hit if a merge has already registered for this segment, we leave it in the readerpool; the merge will skip merging it and will then drop it once it's done: for testing only carefully merge deletes that occurred after we started merging: lazy init (only when we find a delete to carry over): we hold a ref so it should still be in the pool: if we had deletions on starting the merge we must still have deletions now: there were deletes on this segment when the merge started. the merge has collapsed away those deletes, but, if new deletes were flushed since the merge started, we must now carefully keep any newly flushed deletes but mapping them to the new docids. since we copy-on-write, if any new deletes were applied after merging has started, we can just check if the before/after livedocs have changed. if so, we must carefully merge the livedocs one doc at a time: this means this segment received new deletes since we started the merge, so we must merge them: this segment had no deletes before but now it does: no deletes before or after if merge was explicitly aborted, or, if rollback() or rollbacktransaction() had been called since our merge started (which results in an unqualified deleter.refresh() call that will remove any index file that current segments does not reference), we abort this merge if the doc store we are using has been closed and is in now compound format (but wasn't when we started), then we will switch to the compound format as well: if we merged no segments then we better be dropping the new segment: must close before checkpoint, otherwise ifd won't be able to delete the held-open files from the merge readers: must note the change to segmentinfos so any commits in-flight don't lose it (ifd will incref/protect the new files we ignore so we keep throwing original exception. cascade the forcemerge: set the exception on the merge, so if forcemerge is waiting on us it sees the root cause exception: we can ignore this exception (it happens when close(false) or rollback is called), unless the merge involves segments from external directories, in which case we must throw it so, for example, the rollbacktransaction code in addindexes is executed. should not get here if (merge.info != null) { system.out.println("merge: " + merge.info.info.name); } this merge (and, generally, any change to the segments) may now enable new merges, so we call merge policy & update pending merges. ok it does not conflict; now record that this merge is running (while synchronized) to avoid race condition where two conflicting merges from different threads, start don't call mergingsegments.tostring() could lead to concurrentmodexception since merge updates the segments fieldinfos merge is now registered mergeinit already done todo: in the non-pool'd case this is somewhat wasteful, because we open these readers, close them, and then open them again for merging. maybe we could pre-pool them somehow in that case... lock order: iw -> bd bind a new segment name here so even with concurrentmergepolicy we keep deterministic segment names. lock order: iw -> bd forcemerge, addindexes or finishmerges may be waiting on merges to finish. it's possible we are called twice, eg if there was an exception inside mergeinit we still hold a ref so it should not have been removed: if any error occured, throw it. this is try/finally to make sure merger's readers are closed: hold onto the "live" reader; we will use this to commit merged deletes carefully pull the most recent live docs: must sync to ensure buffereddeletesstream cannot change livedocs/pendingdeletecount while we pull a copy: deletes might have happened after we pulled the merge reader and before we got a read-only copy of the segment's actual live docs (taking pending deletes into account). in that case we need to make a new reader with updated live docs and del count. fix the reader's live docs and del count beware of zombies this is where all the work happens: record which codec was used to write the segment very important to do this before opening the reader because codec must know if prox was written for this segment: system.out.println("merger set hasprox=" + merger.hasprox() + " seg=" + merge.info.name); guard segmentinfos this can happen if rollback or close(false) is called -- fall through to logic below to remove the partially so that, if we hit exc in deletenewfiles (next) or in commitmerge (later), we close the per-segment readers in the finally clause below: delete new non cfs files directly: they were never registered with ifd so that, if we hit exc in commitmerge (later), we close the per-segment readers in the finally clause below: have codec write segmentinfo. must do this after creating cfs so that 1) .si isn't slurped into cfs, and 2) .si reflects usecompoundfile=true change above: todo: ideally we would freeze merge.info here!! because any changes after writing the .si will be lost... force read context because we merge deletes onto this reader: commitmerge will return false if this merge was aborted readers are already closed in commitmerge if we didn't hit an exc: for test purposes. for test purposes. utility routines for tests note: the callers of this method should in theory be able to do simply wait(), but, as a defense against thread timing hazards where notifyall() fails to be called, we wait for at most 1 second and then return so caller can check if wait conditions are satisfied: called only from assert if this trips it means we are missing a call to .checkpoint somewhere, because by the time we are called, deleter should know about every file referenced by the current head segmentinfos: for infostream output exception here means nothing is prepared (this method unwinds everything it did on an exception) system.out.println("done preparecommit"); this call can take a long time -- 10s of seconds or more. we do it without syncing on this: have our master segmentinfos record the generations we just prepared. we do this on error or success so we don't double-write a segments_n file. hit exception used only by assert for testing. current points: startdoflush startcommitmerge startstartcommit midstartcommit midstartcommit2 midstartcommitsuccess finishstartcommit startcommitmergedeletes startmergeinit documentswriter.threadstate.init start system.out.println("iw.nrtiscurrent " + (infos.version == segmentinfos.version && !docwriter.anychanges() && !buffereddeletesstream.any())); called by directoryreader.doclose now merge all added files replace all previous files with the cfs/cfe files:"
org.apache.lucene.index.BitsSlice "exposes a slice of an existing bits as a new bits. @lucene.internal start is inclusive; end is exclusive (length = end-start)"
org.apache.lucene.index.IndexableFieldType "describes the properties of a field. @lucene.experimental true if this field should be indexed (inverted) true if the field's value should be stored true if this field's value should be analyzed by the {@link analyzer}.  this has no effect if {@link #indexed()} returns false. true if this field's indexed form should be also stored into term vectors.  this builds a miniature inverted-index for this field which can be accessed in a document-oriented way from {@link indexreader#gettermvector(int,string)}.  this option is illegal if {@link #indexed()} returns false. true if this field's token character offsets should also be stored into term vectors.  this option is illegal if term vectors are not enabled for the field ({@link #storetermvectors()} is false) true if this field's token positions should also be stored into the term vectors.  this option is illegal if term vectors are not enabled for the field ({@link #storetermvectors()} is false). true if this field's token payloads should also be stored into the term vectors.  this option is illegal if term vector positions are not enabled for the field ({@link #storetermvectors()} is false). true if normalization values should be omitted for the field.  this saves memory, but at the expense of scoring quality (length normalization will be disabled), and if you omit norms, you cannot use index-time boosts. {@link indexoptions}, describing what should be recorded into the inverted index docvalues {@link docvalues.type}: if non-null then the field's value will be indexed into docvalues. javadocs"
org.apache.lucene.index.DocumentsWriterFlushControl "this class controls {@link documentswriterperthread} flushing during indexing. it tracks the memory consumption per {@link documentswriterperthread} and uses a configured {@link flushpolicy} to decide if a {@link documentswriterperthread} must flush.  in addition to the {@link flushpolicy} the flush control might set certain {@link documentswriterperthread} as flush pending iff a {@link documentswriterperthread} exceeds the {@link indexwriterconfig#getramperthreadhardlimitmb()} to prevent address space exhaustion. if we are indexing with very low maxrambuffer like 0.1mb memory can easily overflow if we check out some dwpt based on doccount and have several dwpt in flight indexing large documents (compared to the ram buffer). this means that those dwpt and their threads will not hit the stall control before asserting the memory which would in turn fail. to prevent this we only assert if the the largest document seen is smaller than the 1/2 of the maxrambuffermb we need to differentiate here if we are pending since setflushpending moves the perthread memory to the flushbytes and we could be set to pending during a delete updates the number of documents "finished" while we are in a stalled state. this is important for asserting memory upper bounds since it corresponds to the number of threads that are in-flight and crossed the stall control check before we actually stalled. see #assertmemory() we block indexing threads if net byte grows due to slow flushes yet, for small ram buffers and large documents we can easily reach the limit without any ongoing flushes. we need to ensure that we don't stall/block if an ongoing or pending flush can not free up enough memory to release the stall lock. sets flush pending state on the given {@link threadstate}. the {@link threadstate} must have indexed at least on document and must not be already pending. returns an iterator that provides access to all currently active {@link threadstate}s returns the number of delete terms in the global pool make sure we move all dwpt that are where concurrently marked as pending and moved to blocked are moved over to the flushqueue. there is a chance that this happens since we marking dwpt for full flush without blocking indexing. prunes the blockedqueue by removing all dwpt that are associated with the given flush queue. returns true if a full flush is currently running returns the number of flushes that are already checked out but not yet actively flushing returns the number of flushes that are checked out but not yet available for flushing. this only applies during a full flush if a dwpt needs flushing but must not be flushed until the full flush has finished. this method will block if too many dwpt are currently flushing and no checked out dwpt are available returns true iff stalled only with assert only for safety reasons if a dwpt is close to the ram limit only with assert for this assert we must be tolerant to ram buffer changes! take peakdelta into account - worst case is that all flushing, pending and blocked dwpt had maxmem and the last doc had the peakdelta 2 rambufferbytes -> before we stall we need to cross the 2xram buffer border this is still a valid limit (numpending + numflushingdwpt() + numblockedflushes()) peakdelta) -> those are the total number of dwpt that are not active but not yet fully fluhsed all of them could theoretically be taken out of the loop once they crossed the ram buffer and the last document was the peak delta (numdocssincestalled peakdelta) -> at any given time there could be n threads in flight that crossed the stall control before we reached the limit and each of them could hold a peak document the expected ram consumption is an upper bound at this point and not really the expected consumption only for asserts safety check to prevent a single dwpt exceeding its ram limit. this is super important since we can not address more than 2048 mb per dwpt write access synced write access synced don't assert on numdocs since we could hit an abort excp. while selecting that dwpt for flushing take it out of the loop this dwpt is stale we are pending so all memory is already moved to flushbytes do that before replace! record the flushing dwpt to reduce flushbytes in doafterflush write access synced don't check if we are doing a full flush set by dw to signal that we should not release new dwpt after close pass null this is a global delete no update there is a flush-all in process and this dwpt is now stale -- enroll it for flush and try for another dwpt: simply return the threadstate even in a flush all case sine we already hold the lock make sure we unlock if this fails set a new delete queue - all subsequent dwpt will use this queue until we do another full flush this one is already a new dwpt make this state inactive record the flushing dwpt to reduce flushbytes in doafterflush don't decr pending here - its already done when dwpt is blocked ignore - keep on aborting the flush queue ignore - keep on aborting the blocked queue"
org.apache.lucene.index.AutomatonTermsEnum "a filteredtermsenum that enumerates terms based upon what is accepted by a dfa.  the algorithm is such:  as long as matches are successful, keep reading sequentially. when a match fails, skip to the next string in lexicographic order that does not enter a reject state.   the algorithm does not attempt to actually skip to the next string that is completely accepted. this is not possible when the language accepted by the fsm is not finite (i.e. operator).  @lucene.experimental construct an enumerator based upon an automaton, enumerating the specified field, working on a supplied termsenum  @lucene.experimental  compiledautomaton returns true if the term matches the automaton. also stashes away the term to assist with smart enumeration. sets the enum to operate in linear fashion, as we have found a looping transition at position: we set an upper bound and act like a termrangequery for this portion of the term space. position + maxtransition increments the byte buffer to the next string in binary order after s that will not put the machine into a reject state. if such a string does not exist, returns false. the correctness of this method depends upon the automaton being deterministic, and having no transitions to dead states. if more possible solutions exist for the dfa no more solutions exist from this useful portion, backtrack no more solutions at all string is good to go as-is else advance further returns the next string in lexicographic order that will not put the machine into a reject state. this method traverses the dfa from the given position in the string, starting at the given state. if this cannot satisfy the machine, returns false. this method will walk the minimal path, in lexicographic order, as long as possible. if this method returns false, then there might still be more solutions, it is necessary to backtrack to find out. current non-reject state useful portion of the string if more possible solutions exist for the dfa from this position the next lexicographic character must be greater than the existing character, if it exists. as long as is possible, continue down the minimal path in lexicographic order. if a loop or accept state is encountered, stop. note: we work with a dfa with no transitions to dead states. so the below is ok, if it is not an accept state, then there must be at least one transition. attempts to backtrack thru the string after encountering a dead end at some given position. returns false if no more possible strings can match. current position in the input string >=0 if more possible solutions exist for the dfa all solutions exhausted a tableized array-based form of the dfa common suffix of the automaton true if the automaton accepts a finite language array of sorted transitions for each state, indexed by state number for path tracking: each long records gen when we last visited the state; we use gens to avoid having to clear the reference used for seeking forwards through the term dictionary true if we are enumerating an infinite portion of the dfa. in this case it is faster to drive the query based on the terms dictionary. when this is true, linearupperbound indicate the end of range of terms where we should simply do sequential reads instead. used for path tracking, where each bit is a numbered state. system.out.println("ate.nextseekterm term=" + term); return the empty term, as its valid seek to the next possible string; reposition no more possible strings can match 0xff terms don't get the optimization... not worth the trouble. walk the automaton until a character is rejected. we found a loop, record it for faster enumeration take the useful portion, and the last non-reject state, and attempt to append characters that will match. todo: paranoia? if we backtrack thru an infinite dfa, the loop detection is important! for now, restart from scratch for all infinite dfas if the next byte is 0xff and is not part of the useful portion, then by definition it puts us in a reject state, and therefore this path is dead. there cannot be any higher transitions. backtrack. find the minimal path (lexicographic order) that is >= c append either the next sequential char, or the minimum transition append the minimum transition we found a loop, record it for faster enumeration if a character is 0xff its a dead-end too, because there is no higher character in binary sort order."
org.apache.lucene.index.CorruptIndexException "this exception is thrown when lucene detects an inconsistency in the index. sole constructor."
org.apache.lucene.index.MergeState "holds common state used during segment merging. @lucene.experimental remaps docids around deletes during merge sole constructor. (for invocation by subclass constructors, typically implicit.) creates a {@link docmap} instance appropriate for this reader. returns the mapped docid corresponding to the provided one. returns the mapped docid corresponding to the provided one. returns the total number of documents, ignoring deletions. returns the number of not-deleted documents. returns the number of deleted documents. returns true if there are any deletions. {@link segmentinfo} of the newly merged segment. {@link fieldinfos} of the newly merged segment. readers being merged. maps docids around deletions. new docid base per reader. holds the checkabort instance, which is invoked periodically to see if the merge has been aborted. infostream for debugging messages. current field being merged. {@link segmentreader}s that have identical field name/number mapping, so their stored fields and term vectors may be bulk merged. how many {@link #matchingsegmentreaders} are set. sole constructor. class for recording units of work when merging segments. creates a #checkabort instance. records the fact that roughly units amount of work have been done since this method was last called. when adding time-consuming code into segmentmerger, you should test different values for units to ensure that the time in between calls to merge.checkaborted is up to ~ 1 second. if you use this: iw.close(false) cannot abort your merge! @lucene.internal todo: get rid of this? it tells you which segments are 'aligned' (e.g. for bulk merging) but is this really so expensive to compute again in different components, versus once in sm? do nothing"
org.apache.lucene.index.CoalescedDeletes "note: we could add/collect more debugging information"
org.apache.lucene.index.IndexReader "indexreader is an abstract class, providing an interface for accessing an index. search of an index is done entirely through this abstract interface, so that any subclass which implements it is searchable. there are two different types of indexreaders:  {@link atomicreader}: these indexes do not consist of several sub-readers, they are atomic. they support retrieval of stored fields, doc values, terms, and postings. {@link compositereader}: instances (like {@link directoryreader}) of this reader can only be used to get stored fields from the underlying atomicreaders, but it is not possible to directly retrieve postings. to do that, get the sub-readers via {@link compositereader#getsequentialsubreaders}. alternatively, you can mimic an {@link atomicreader} (with a serious slowdown), by wrapping composite readers with {@link slowcompositereaderwrapper}.  indexreader instances for indexes on disk are usually constructed with a call to one of the static directoryreader.open() methods, e.g. {@link directoryreader#open(org.apache.lucene.store.directory)}. {@link directoryreader} implements the {@link compositereader} interface, it is not possible to directly get postings.  for efficiency, in this api documents are often referred to via document numbers, non-negative integers which each name a unique document in the index. these document numbers are ephemeral -- they may change as documents are added to and deleted from an index. clients should thus not rely on a given document having the same number between sessions.  note: {@link indexreader} instances are completely thread safe, meaning multiple threads can call any of its methods, concurrently. if your application requires external synchronization, you should not synchronize on the indexreader instance; use your own (non-lucene) objects instead. a custom listener that's invoked when the indexreader is closed. @lucene.experimental invoked when the {@link indexreader} is closed. expert: adds a {@link readerclosedlistener}. the provided listener will be invoked when this reader is closed. @lucene.experimental expert: remove a previously added {@link readerclosedlistener}. @lucene.experimental expert: this method is called by {@code indexreader}s which wrap other readers (e.g. {@link compositereader} or {@link filteratomicreader}) to register the parent at the child (this reader) on construction of the parent. when this reader is closed, it will mark all registered parents as closed, too. the references to parent readers are weak only, so they can be gced once they are no longer in use. @lucene.experimental expert: returns the current refcount for this reader expert: increments the refcount of this indexreader instance. refcounts are used to determine when a reader can be closed safely, i.e. as soon as there are no more references. be sure to always call a corresponding {@link #decref}, in a finally clause; otherwise the reader may never be closed. note that {@link #close} simply calls decref(), which means that the indexreader will not really be closed until {@link #decref} has been called for all outstanding references. @see #decref @see #tryincref expert: increments the refcount of this indexreader instance only if the indexreader has not been closed yet and returns true iff the refcount was successfully incremented, otherwise false. if this method returns false the reader is either already closed or is currently been closed. either way this reader instance shouldn't be used by an application unless true is returned.  refcounts are used to determine when a reader can be closed safely, i.e. as soon as there are no more references. be sure to always call a corresponding {@link #decref}, in a finally clause; otherwise the reader may never be closed. note that {@link #close} simply calls decref(), which means that the indexreader will not really be closed until {@link #decref} has been called for all outstanding references. @see #decref @see #incref expert: decreases the refcount of this indexreader instance. if the refcount drops to 0, then this reader is closed. if an exception is hit, the refcount is unchanged. @throws ioexception in case an ioexception occurs in doclose() @see #incref throws alreadyclosedexception if this indexreader or any of its child readers is closed, otherwise returns. {@inheritdoc} for caching purposes, {@code indexreader} subclasses are not allowed to implement equals/hashcode, so methods are declared final. to lookup instances from caches use {@link #getcorecachekey} and {@link #getcombinedcoreanddeleteskey}. {@inheritdoc} for caching purposes, {@code indexreader} subclasses are not allowed to implement equals/hashcode, so methods are declared final. to lookup instances from caches use {@link #getcorecachekey} and {@link #getcombinedcoreanddeleteskey}. returns a indexreader reading the index in the given directory the index directory @throws ioexception if there is a low-level io error @deprecated use {@link directoryreader#open(directory)} expert: returns a indexreader reading the index in the given directory with the given terminfosindexdivisor. the index directory subsamples which indexed terms are loaded into ram. this has the same effect as {@link indexwriterconfig#settermindexinterval} except that setting must be done at indexing time while this setting can be set per reader. when set to n, then one in every ntermindexinterval terms in the index is loaded into memory. by setting this to a value > 1 you can reduce memory usage, at the expense of higher latency when loading a terminfo. the default value is 1. set this to -1 to skip loading the terms index entirely. @throws ioexception if there is a low-level io error @deprecated use {@link directoryreader#open(directory,int)} open a near real time indexreader from the {@link org.apache.lucene.index.indexwriter}. the indexwriter to open from if true, all buffered deletes will be applied (made visible) in the returned reader. if false, the deletes are not applied but remain buffered (in indexwriter) so that they will be applied in the future. applying deletes can be costly, so if your app can tolerate deleted documents being returned you might gain some performance by passing false. new indexreader @throws ioexception if there is a low-level io error @see directoryreader#openifchanged(directoryreader,indexwriter,boolean) @lucene.experimental @deprecated use {@link directoryreader#open(indexwriter,boolean)} expert: returns an indexreader reading the index in the given {@link indexcommit}. the commit point to open @throws ioexception if there is a low-level io error @deprecated use {@link directoryreader#open(indexcommit)} expert: returns an indexreader reading the index in the given {@link indexcommit} and terminfosindexdivisor. the commit point to open subsamples which indexed terms are loaded into ram. this has the same effect as {@link indexwriterconfig#settermindexinterval} except that setting must be done at indexing time while this setting can be set per reader. when set to n, then one in every ntermindexinterval terms in the index is loaded into memory. by setting this to a value > 1 you can reduce memory usage, at the expense of higher latency when loading a terminfo. the default value is 1. set this to -1 to skip loading the terms index entirely. @throws ioexception if there is a low-level io error @deprecated use {@link directoryreader#open(indexcommit,int)} retrieve term vectors for this document, or null if term vectors were not indexed. the returned fields instance acts like a single-document inverted index (the docid will be 0). retrieve term vector for this document and field, or null if term vectors were not indexed. the returned fields instance acts like a single-document inverted index (the docid will be 0). returns the number of documents in this index. returns one greater than the largest possible document number. this may be used to, e.g., determine how big to allocate an array which will have an element for every document number in an index. returns the number of deleted documents. expert: visits the fields of a stored document, for custom processing/loading of each field. if you simply want to load all fields, use {@link #document(int)}. if you want to load a subset, use {@link documentstoredfieldvisitor}. returns the stored fields of the nth document in this index. this is just sugar for using {@link documentstoredfieldvisitor}.  note: for performance reasons, this method does not check if the requested document is deleted, and therefore asking for a deleted document may yield unspecified results. usually this is not required, however you can test if the doc is deleted by checking the {@link bits} returned from {@link multifields#getlivedocs}. note: only the content of a field is returned, if that field was stored during indexing. metadata like boost, omitnorm, indexoptions, tokenized, etc., are not preserved. @throws ioexception if there is a low-level io error like {@link #document(int)} but only loads the specified fields. note that this is simply sugar for {@link documentstoredfieldvisitor#documentstoredfieldvisitor(set)}. returns true if any documents have been deleted closes files associated with this index. also saves any new deletions to disk. no other methods should be called after this has been called. @throws ioexception if there is a low-level io error implements close. expert: returns the root {@link indexreadercontext} for this {@link indexreader}'s sub-reader tree.  iff this reader is composed of sub readers, i.e. this reader being a composite reader, this method returns a {@link compositereadercontext} holding the reader's direct children as well as a view of the reader tree's atomic leaf contexts. all sub- {@link indexreadercontext} instances referenced from this readers top-level context are private to this reader and are not shared with another context tree. for example, indexsearcher uses this api to drive searching by one atomic leaf reader at a time. if this reader is not composed of child readers, this method returns an {@link atomicreadercontext}.  note: any of the sub-{@link compositereadercontext} instances referenced from this top-level context do not support {@link compositereadercontext#leaves()}. only the top-level context maintains the convenience leaf-view for performance reasons. returns the reader's leaves, or itself if this reader is atomic. this is a convenience method calling {@code this.getcontext().leaves()}. @see indexreadercontext#leaves() expert: returns a key for this indexreader, so fieldcache/cachingwrapperfilter can find it again. this key must not have equals()/hashcode() methods, so &quot;equals&quot; means &quot;identical&quot;. expert: returns a key for this indexreader that also includes deletions, so fieldcache/cachingwrapperfilter can find it again. this key must not have equals()/hashcode() methods, so &quot;equals&quot; means &quot;identical&quot;. returns the number of documents containing the term. this method returns 0 if the term or field does not exists. this method does not take into account deleted documents that have not yet been merged away. @see termsenum#docfreq() returns the number of documents containing the term term. this method returns 0 if the term or field does not exists, or -1 if the codec does not support the measure. this method does not take into account deleted documents that have not yet been merged away. @see termsenum#totaltermfreq() javadocs cross memory barrier by a fake write: recurse: note: don't ensureopen, so that callers can see refcount is 0 (reader is closed) only check refcount here (don't call ensureopen()), so we can still close the reader if it was made invalid by a child: put reference back on failure the happens before rule on reading the refcount, which must be after the fake write, ensures that we see the value: todo: we need a separate storedfield, so that the document returned here contains that class not indexablefield don't can ensureopen since fc calls this (to evict) on close don't can ensureopen since fc calls this (to evict) on close"
org.apache.lucene.index.BufferedDeletes "holds buffered deletes, by docid, term or query for a single segment. this is used to hold buffered pending deletes against the to-be-flushed segment. once the deletes are pushed (on flush in documentswriter), these deletes are converted to a frozendeletes instance. rough logic: hashmap has an array[entry] w/ varying load factor (say 2 pointer). entry is object w/ term key, integer val, int hash, entry next (obj_header + 3pointer + int). term is object w/ string field and string text (obj_header + 2pointer). term's field is string (obj_header + 4int + pointer + obj_header + string.lengthchar). term's text is string (obj_header + 4int + pointer + obj_header + string.lengthchar). integer is obj_header + int. rough logic: del docids are list. say list allocates ~2x size (2pointer). integer is obj_header + int rough logic: hashmap has an array[entry] w/ varying load factor (say 2 pointer). entry is object w/ query key, integer val, int hash, entry next (obj_header + 3pointer + int). query we often undercount (say 24 bytes). integer is obj_header + int. note: we are sync'd by buffereddeletes, ie, all access to instances of this class is via sync'd methods on buffereddeletes increment bytes used only if the query wasn't added so far. only record the new number if it's greater than the current one. this is important because if multiple threads are replacing the same doc at nearly the same time, it's possible that one thread that got a higher docid is scheduled before the other threads. if we blindly replace than we can incorrectly get both docs indexed."
org.apache.lucene.index.SlowCompositeReaderWrapper "this class forces a composite reader (eg a {@link multireader} or {@link directoryreader}) to emulate an atomic reader. this requires implementing the postings apis on-the-fly, using the static methods in {@link multifields}, {@link multidocvalues}, by stepping through the sub-readers to merge fields/terms, appending docs, etc. note: this class almost always results in a performance hit. if this is important to your use case, you'll get better performance by gathering the sub readers using {@link indexreader#getcontext()} to get the atomic leaves and then operate per-atomicreader, instead of using this class. this method is sugar for getting an {@link atomicreader} from an {@link indexreader} of any kind. if the reader is already atomic, it is returned unchanged, otherwise wrapped by this class. sole constructor, wrapping the provided {@link compositereader}. javadoc javadoc don't call ensureopen() here (it could affect performance) don't call ensureopen() here (it could affect performance) todo: as this is a wrapper, should we really close the delegate?"
org.apache.lucene.index.MultiTermsEnum "exposes {@link termsenum} api, merged from {@link termsenum} api of sub-segments. this does a merge sort, by term text, of the sub-readers. @lucene.experimental returns how many sub-reader slices contain the current term. @see #getmatcharray returns sub-reader slices positioned to the current term. sole constructor. which sub-reader slices we should merge. the terms array must be newly all of our subs (one per sub-reader) current subs that have at least one term for this field init our term comp we cannot merge sub-readers that have different termcomps field has no terms lucene-2130: if we had just seek'd already, prior to this seek, and the new seek term is after the previous one, don't try to re-seek this sub if its current term is already beyond this new seek term. doing so is a waste because this sub will simply seek to the same spot. if at least one sub had exact match to the requested term then we found match lucene-2130: if we had just seek'd already, prior to this seek, and the new seek term is after the previous one, don't try to re-seek this sub if its current term is already beyond this new seek term. doing so is a waste because this sub will simply seek to the same spot. enum exhausted at least one sub had exact match to the requested term no sub had exact match, but at least one sub found a term after the requested term -- advance to that next term: extract all subs from the queue that have the same top term call next() on each top, and put back into queue no more fields in this reader must seekceil at this point, so those subs that didn't have the term can find the following term. note: we could save some cpu by only seekceil the subs that didn't match the last exact seek... but most impls short-circuit if you seekceil to term they are already on. restore queue gather equal top fields can only reuse if incoming enum is also a multidocsenum ... and was previously optimize for common case: requested skip docs is a congruent sub-slice of multibits: in this case, we just pull the livedocs from the sub reader, rather than making the inefficient slice(multi(sub-readers)): custom case: requested skip docs is foreign: must slice it on every access no deletions should this be an error? can only reuse if incoming enum is also a multidocsandpositionsenum ... and was previously optimize for common case: requested skip docs is a congruent sub-slice of multibits: in this case, we just pull the livedocs from the sub reader, rather than making the inefficient slice(multi(sub-readers)): custom case: requested skip docs is foreign: must slice it on every access (very inefficient) no deletions at least one of our subs does not store offsets or positions -- we can't correctly produce a multidocsandpositions enum"
org.apache.lucene.index.MultiTerms "exposes flex api, merged from flex api of sub-segments. @lucene.experimental sole constructor. the {@link terms} instances of all sub-readers. a parallel array (matching {@code subs}) describing the sub-reader slices. we cannot merge sub-readers that have different termcomps if all subs have pos, and at least one has payloads."
org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy "this {@link indexdeletionpolicy} implementation that keeps only the most recent commit and immediately removes all prior commits after a new commit is done. this is the default deletion policy. sole constructor. deletes all commits except the most recent one. deletes all commits except the most recent one. note that commits.size() should normally be 1: note that commits.size() should normally be 2 (if not called by oninit above):"
org.apache.lucene.index.CheckIndex "basic tool and api to check the health of an index and write a new segments file that removes reference to problematic segments. as this tool checks every byte in the index, on a large index it can take quite a long time to run. @lucene.experimental please make a complete backup of your index before using this to fix your index! returned from {@link #checkindex()} detailing the health and status of the index. @lucene.experimental true if no problems were found with the index. true if we were unable to locate and load the segments_n file. true if we were unable to open the segments_n file. true if we were unable to read the version number from segments_n file. name of latest segments_n file in the index. number of segments in the index. empty unless you passed specific segments list to check as optional 3rd argument. @see checkindex#checkindex(list) true if the index was list of {@link segmentinfostatus} instances, detailing status of each segment. directory index is in. segmentinfos instance containing only segments that had no problems (this is used with the {@link checkindex#fixindex} method to repair the index. how many documents will be lost to bad segments. how many bad segments were found. true if we checked only specific segments ({@link #checkindex(list)}) was called with non-null argument). the greatest segment name. whether the segmentinfos.counter is greater than any of the segments' names. holds the holds the status of each segment in the index. see {@link #segmentinfos}. @lucene.experimental name of the segment. codec used to read this segment. document count (does not take deletions into account). true if segment is compound file format. number of files referenced by this segment. net size (mb) of the files referenced by this segment. doc store offset, if this segment shares the doc store files (stored fields and term vectors) with other segments. this is -1 if it does not share. string of the shared doc store segment, or null if this segment does not share the doc store files. true if the shared doc store files are compound file format. true if this segment has pending deletions. current deletions generation. number of deleted documents. true if we were able to open a segmentreader on this segment. number of fields in this segment. map that includes certain debugging details that indexwriter records into each segment it creates status for testing of field norms (null if field norms could not be tested). status for testing of indexed terms (null if indexed terms could not be tested). status for testing of stored fields (null if stored fields could not be tested). status for testing of term vectors (null if term vectors could not be tested). status for testing of docvalues (null if docvalues could not be tested). status from testing field norms. number of fields successfully tested exception thrown during term index test (null on success) status from testing term index. number of terms with at least one live doc. number of terms with zero live docs docs. total frequency across all terms. total number of positions. exception thrown during term index test (null on success) holds details of block allocations in the block tree terms dictionary (this is only set if the {@link postingsformat} for this segment uses block tree. status from testing stored fields. number of documents tested. total number of stored fields tested. exception thrown during stored fields test (null on success) status from testing stored fields. number of documents tested. total number of term vectors tested. exception thrown during term vector test (null on success) status from testing docvalues number of documents tested. total number of docvalues tested. exception thrown during doc values test (null on success) create a new checkindex on the directory. if true, term vectors are compared against postings to make sure they are the same. this will likely drastically increase time it takes to run checkindex! see {@link #setcrosschecktermvectors}. set infostream where messages should go. if null, no messages are printed. if verbose is true then more details are printed. set infostream where messages should go. see {@link #setinfostream(printstream,boolean)}. returns a {@link status} instance detailing the state of the index. as this method checks every byte in the index, on a large index it can take quite a long time to run. warning: make sure you only call this when the index is not opened by any writer. returns a {@link status} instance detailing the state of the index. list of specific segment names to check as this method checks every byte in the specified segments, on a large index it can take quite a long time to run. warning: make sure you only call this when the index is not opened by any writer. test field norms. checks fields api is consistent with itself. searcher is optional, to verify with queries. can be null. test the term index. test stored fields for a segment. helper method to verify values (either docvalues or norms), also checking type and size against fieldinfos/segmentinfo test term vectors for a segment. if (endoffset  endoffset=" + endoffset); } if (startoffset  run it like this:  java -ea:org.apache.lucene... org.apache.lucene.index.checkindex pathtoindex [-fix] [-verbose] [-segment x] [-segment y]   -fix: actually write a new segments_n file, removing any problematic segments -segment x: only check the specified segment(s). this can be specified multiple times, to check more than one segment, eg -segment _2 -segment _a. you can't use this with the -fix option.  warning: -fix should only be used on an emergency basis as it will cause documents (perhaps many) to be permanently removed from the index. always make a backup copy of your index before running this! do not run this tool on an index that is actively being written to. you have been warned!  run without -fix, this tool will open the index, report version information and report any exceptions it hits and what action it would take if -fix were specified. with -fix, this tool will remove any segments that have issues and write a new segments_n file. this means all documents contained in the affected segments will be removed.  this tool exits with exit code 1 if the index cannot be opened or has any corruption, else 0. javadocs for javadocs find the oldest and newest segment versions pre-3.1 segment note: we only read the format byte (required preamble) here! don't print size in bytes if its a 3.0 segment with shared docstores todo: we could append the info attributes() to the msg? its ok for it to be non-null here, as long as none are set right? test getfieldinfos() test field norms test the term index test stored fields test term vectors rethrow the first exception we encountered this will cause stats for failed segments to be incremented properly keeper test field norms deprecated path deprecated path todo: we should probably return our own stats thing...?! multifieldsenum relies upon this order... check that the field is in fieldinfos, and is indexed. todo: add a separate test to check this for different reader impls todo: really the codec should not return a field from fieldsenum if it has no terms... but we do this today: assert fields.terms(field) != null; term vectors cannot omit tf make sure terms arrive in order according to the comp note: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before? but for offsets in the postings lists these checks are fine: they were always enforced by indexwriter re-count if there are deleted docs: test skipping note: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before? but for offsets in the postings lists these checks are fine: they were always enforced by indexwriter unusual: the fieldsenum returned a field but the terms for that field is null; this should only happen if it's a ghost field (field with no terms, eg there used to be terms but all docs got deleted and then merged away): test seek to last term: check unique term count test seeking by ord seek by ord seek by term for most implementations, this is boring (just the sum across all fields) but codecs that don't work per-field like preflex actually implement this, but don't implement it on terms, so the check isn't redundant. this means something is seriously screwed, e.g. we are somehow getting enclosed in pfcw!!!!!! todo: we should go and verify term vectors match, if crosschecktermvectors is on... scan stored fields for all documents intentionally pull even deleted documents to make sure they too are not corrupt: validate doccount check sorted bytes only used if crosschecktermvectors is true: todo: testtermsindex intentionally pull/visit (but don't count in stats) deleted documents to make sure they too are not corrupt: todo: can we make a is(fir) that searches just this term vector... to pass for searcher? first run with no deletions: again, with the one doc deleted: only agg stats if the doc is live: make sure fieldinfo thinks this field is vector'd: term vectors were indexed w/ pos but postings were not call the methods to at least make sure they don't throw exc: todo: these are too anal...? we have payloads, but not at this position. postings has payloads too, it should not have one at this position we have payloads, and one at this position postings should also have one at this position, with the same bytes. only used when fixing"
org.apache.lucene.index.IndexableField "represents a single field for indexing. indexwriter consumes iterable&lt;indexablefield&gt; as a document. @lucene.experimental field name {@link indexablefieldtype} describing the properties of this field. returns the field's index-time boost.  only fields can have an index-time boost, if you want to simulate a "document boost", then you must pre-multiply it across all the relevant fields yourself. the boost is used to compute the norm factor for the field. by default, in the {@link similarity#computenorm(fieldinvertstate, norm)} method, the boost value is multiplied by the length normalization factor and then rounded by {@link defaultsimilarity#encodenormvalue(float)} before it is stored in the index. one should attempt to ensure that this product does not overflow the range of that encoding.  it is illegal to return a boost other than 1.0f for a field that is not indexed ({@link indexablefieldtype#indexed()} is false) or omits normalization values ({@link indexablefieldtype#omitnorms()} returns true). @see similarity#computenorm(fieldinvertstate, norm) @see defaultsimilarity#encodenormvalue(float) non-null if this field has a binary value non-null if this field has a string value non-null if this field has a reader value non-null if this field has a numeric value creates the tokenstream used for indexing this field. if appropriate, implementations should use the given analyzer to create the tokenstreams. analyzer that should be used to create the tokenstreams from value for indexing the document. should always return a non-null value if the field is to be indexed @throws ioexception can be thrown while creating the tokenstream javadocs javadocs todo: how to handle versioning here...? todo: we need to break out separate storedfield..."
org.apache.lucene.index.IndexFileDeleter "this class keeps track of each segmentinfos instance that is still "live", either because it corresponds to a segments_n file in the directory (a "commit", i.e. a committed segmentinfos) or because it's an in-memory segmentinfos that a writer is actively updating but has not yet committed. this class uses simple reference counting to map the live segmentinfos instances to individual files in the directory. the same directory file may be referenced by more than one indexcommit, i.e. more than one segmentinfos. therefore we count how many commits reference each file. when all the commits referencing a certain file have been deleted, the refcount for that file becomes zero, and the file is deleted. a separate deletion policy interface (indexdeletionpolicy) is consulted on creation (oninit) and once per commit (oncommit), to decide when a commit should be removed. it is the business of the indexdeletionpolicy to choose when to delete commit points. the actual mechanics of file deletion, retrying, etc, derived from the deletion of commit points is the business of the indexfiledeleter. the current default deletion policy is {@link keeponlylastcommitdeletionpolicy}, which removes all prior commits when a new commit has completed. this matches the behavior before 2.2. note that you must hold the write.lock before instantiating this class. it opens segments_n file(s) directly with no retry logic. files that we tried to delete but failed (likely because they are open and we are running on windows), so we will retry them again later: reference count for all files in the index. counts how many existing commits reference a file. holds all commits (segments_n) currently in the index. this will have just 1 commit if you are using the default delete policy (keeponlylastcommitdeletionpolicy). other policies may leave commit points live for longer in which case this list would be longer than 1: holds files we had incref'd from the previous non-commit checkpoint: commits that the indexdeletionpolicy have decided to delete: change to true to see details of reference counts when infostream is enabled initialize the deleter: find all previous commits in the directory, incref the files they reference, call the policy to let it delete commits. this will remove any files not referenced by any of the commits. @throws ioexception if there is a low-level io error remove the commitpoints in the commitstodelete list by decref'ing all files from each segmentinfos. writer calls this when it has hit an error and had to roll back, to tell us that there may now be unreferenced files in the filesystem. so we re-list the filesystem and delete such files. if segmentname is non-null, we will only delete files corresponding to that segment. revisits the {@link indexdeletionpolicy} by calling its {@link indexdeletionpolicy#oncommit(list)} again with the known commits. this is useful in cases where a deletion policy which holds onto index commits is used. the application may know that some commits are not held by the deletion policy anymore and call {@link indexwriter#deleteunusedfiles()}, which will attempt to delete the unused commits again. for definition of "check point" see indexwriter comments: "clarification: check points (and commits)". writer calls this when it has made a "consistent change" to the index, meaning new files are written to the index and the in-memory segmentinfos have been modified to point to those files. this may or may not be a commit (segments_n may or may not have been written). we simply incref the files referenced by the new segmentinfos and decref the files we had previously seen (if any). if this is a commit, we also call the policy to give it a chance to remove other commits. if any commits are removed, we decref their files as well. deletes the specified files, but only if they are new (have not yet been incref'd). tracks the reference count for a single index file: holds details for each commit point. this class is also passed to the deletion policy. note: this class has a natural ordering that is inconsistent with equals. called only be the deletion policy, to remove this commit point from the index. used only for assert called only from assert first pass: walk the files and initialize our ref counts: it means the directory is empty, so ignore it. add this file to refcounts with initial count 0: this is a commit (segments or segments_n), and it's valid (<= the max gen). load it, then incref all files it refers to: lucene-948: on nfs (and maybe others), if you have writers switching back and forth between machines, it's very likely that the dir listing will be stale and will claim a file segments_x exists when in fact it doesn't. so, we catch this and handle it as if the file does not exist most likely we are opening an index that has an aborted "future" commit, so suppress exc in this case we did not in fact see the segments_n file corresponding to the segmentinfos that was passed in. yet, it must exist, because our caller holds the write lock. this can happen when the directory listing was stale (eg when index accessed via nfs client with stale directory listing cache). so we try now to explicitly open this commit point: we keep commits list in sorted order (oldest to newest): now delete anything with ref count at 0. these are presumably abandoned files eg due to crash of indexwriter. finally, give policy a chance to remove things on startup: always protect the incoming segmentinfos since sometime it may not be the most recent commit first decref all files that had been referred to by the now-deleted commits: now compact commits to remove deleted ones (preserving the sort): unreferenced file, so remove it set to null so that we regenerate the list of pending files; else we can accumulate same file more than once decref old files from the last checkpoint, if any: try again now to delete any previously un-deletable files (because they were in use, on windows): incref the files: append to our commits list: tell policy so it can remove commits: decref files for commits that were deleted by the policy: decref old files from the last checkpoint, if any: save files so we can decr on next checkpoint/commit: if this is a commit point, also incref the segments_n file: this file is no longer referenced by any past commit points nor by the in-memory segmentinfos: note: it's very unusual yet possible for the refcount to be present and 0: it can happen if you open iw on a crashed index, and it removes a bunch of unref'd files, and then you add new docs / do merging, and it reuses that segment name. testcrash.testcrashafterreopen can hit this: if delete fails some operating systems (e.g. windows) don't permit a file to be deleted while it is opened for read (e.g. by another process or thread). so we assume that when a delete fails it is because the file is open in another process, and queue the file for subsequent deletion. add to deletable filename used only for better assert error messages"
org.apache.lucene.index.InvertedDocEndConsumerPerField ""
org.apache.lucene.index.TermVectorsConsumer "fills in no-term-vectors for all docs we haven't seen since the last doc that had term vectors. used by perfield when serializing the term vectors at least one doc in this run had term vectors enabled append term vectors to the real outputs: called only by assert called only by assert"
org.apache.lucene.index.TermsHash "this class implements {@link inverteddocconsumer}, which is passed each token produced by the analyzer on each field. it stores these tokens in a hash table, and allocates separate byte streams per token. consumers of this class, eg {@link freqproxtermswriter} and {@link termvectorsconsumer}, write their own byte streams under each term. used when comparing postings via termrefcomp, in termshashperfield used by perfield to obtain terms from the analysis chain we are primary clear all state we don't reuse so we drop everything and don't fill with 0"
org.apache.lucene.index.TermsHashPerField "collapse the hash table & sort in-place. copied from our perthread fully free the byteshash on each flush but keep the pool untouched byteshash.clear will clear the bytestartarray and in turn the parallelpostingsarray too secondary entry point (for 2nd & subsequent termshash), because token text has already been "interned" into textstart, so we hash by textstart new posting first time we are seeing this token since we last flushed the hash. init stream slices primary entry point (for first termshash) we are first in the chain so we must "intern" the term text into textstart address get the text & hash of this term. not enough room in current block just skip this term, to remain as robust as possible during indexing. a tokenfilter can be inserted into the analyzer chain if other behavior is wanted (pruning the term to a prefix, throwing an exception, etc). init stream slices end of slice; allocate a new one todo: optimize"
org.apache.lucene.index.CompositeReader "instances of this reader type can only be used to get stored fields from the underlying atomicreaders, but it is not possible to directly retrieve postings. to do that, get the {@link atomicreadercontext} for all sub-readers via {@link #leaves()}. alternatively, you can mimic an {@link atomicreader} (with a serious slowdown), by wrapping composite readers with {@link slowcompositereaderwrapper}. indexreader instances for indexes on disk are usually constructed with a call to one of the static directoryreader.open() methods, e.g. {@link directoryreader#open(directory)}. {@link directoryreader} implements the {@code compositereader} interface, it is not possible to directly get postings.  concrete subclasses of indexreader are usually constructed with a call to one of the static open() methods, e.g. {@link directoryreader#open(directory)}.  for efficiency, in this api documents are often referred to via document numbers, non-negative integers which each name a unique document in the index. these document numbers are ephemeral -- they may change as documents are added to and deleted from an index. clients should thus not rely on a given document having the same number between sessions.  note: {@link indexreader} instances are completely thread safe, meaning multiple threads can call any of its methods, concurrently. if your application requires external synchronization, you should not synchronize on the indexreader instance; use your own (non-lucene) objects instead. sole constructor. (for invocation by subclass constructors, typically implicit.) expert: returns the sequential sub readers that this reader is logically composed of. this method may not return {@code null}. note: in contrast to previous lucene versions this method is no longer public, code that wants to get all {@link atomicreader}s this composite is composed of should use {@link indexreader#leaves()}. @see indexreader#leaves() javadocs lazy init lazy init without thread safety for perf reasons: building the readercontext twice does not hurt!"
org.apache.lucene.index.LogDocMergePolicy "this is a {@link logmergepolicy} that measures size of a segment as the number of documents (not taking deletions into account). default minimum segment size. @see setminmergedocs sole constructor, setting all settings to their defaults. sets the minimum size for the lowest level segments. any segments below this size are considered to be on the same level (even if they vary drastically in size) and will be merged whenever there are mergefactor of them. this effectively truncates the "long tail" of small segments that would otherwise be get the minimum size for a segment to remain un-merged. @see #setminmergedocs maxmergesize(forforcedmerge) are never used by logdocmergepolicy; set it to long.max_value to disable it"
org.apache.lucene.index.SegmentInfos "a collection of segmentinfo objects with methods for operating on those segments in relation to the file system.  the active segments in the index are stored in the segment info file, segments_n. there may be one or more segments_n files in the index; however, the one with the largest generation is the active one (when older segments_n files are present it's because they temporarily cannot be deleted, or, a writer is in the process of committing, or a custom {@link org.apache.lucene.index.indexdeletionpolicy indexdeletionpolicy} is in use). this file lists each segment by name and has details about the codec and generation of deletes.  there is also a file segments.gen. this file contains the current generation (the _n in segments_n) of the index. this is used only as a fallback in case the current generation cannot be accurately determined by directory listing alone (as is the case for some nfs clients with time-based directory cache expiration). this file simply contains an {@link dataoutput#writeint int32} version header ({@link #format_segments_gen_current}), followed by the generation recorded as {@link dataoutput#writelong int64}, written twice.  files:  segments.gen: genheader, generation, generation segments_n: header, version, namecounter, segcount, &lt;segname, segcodec, delgen, deletioncount&gt;segcount, commit the file format version for the segments_n codec header used for the segments.gen file only! whenever you add a new format, make it 1 smaller (negative version logic)! used to name new segments. counts how often the index has been changed. opaque map&lt;string, string&gt; that if non-null, information about loading segments_n files will be printed here. @see #setinfostream. sole constructor. typically you call this and then use {@link #read(directory) or #read(directory,string)} to populate each {@link segmentinfopercommit}. alternatively, you can add/remove your own {@link segmentinfopercommit}s. returns {@link segmentinfopercommit} at the provided index. get the generation of the most recent commit to the list of index files (n in the segments_n file). -- array of file names to check get the generation of the most recent commit to the index in this directory (n in the segments_n file). -- directory to search for the latest segments_n file get the filename of the segments_n file for the most recent commit in the list of index files. -- array of file names to check get the filename of the segments_n file for the most recent commit to the index in this directory. -- directory to search for the latest segments_n file get the segments_n filename in use by this segment infos. parse the generation off the segments file name and return it. get the next segments_n filename that will be written. read a particular segmentfilename. note that this may throw an ioexception if a commit is in process. -- directory containing the segments file -- segment file to load @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error find the latest commit ({@code segments_n file}) and load all {@link segmentinfopercommit}s. returns a copy of this instance, also copying each segmentinfo. version number when this segmentinfos was generated. returns current generation. returns last succesfully read or written generation. if non-null, information about retries when loading the segments file will be printed to this. advanced configuration of retry logic in loading segments_n file advanced: set how many times to try incrementing the gen when loading the segments file. this only runs if the primary (listing directory) and secondary (opening segments.gen file) methods fail to find the segments file. @lucene.experimental returns the {@code defaultgenlookaheadcount}. @see #setdefaultgenlookaheadcount @lucene.experimental returns {@code infostream}. @see #setinfostream prints the given message to the infostream. note, this method does not check for null infostream. it assumes this check has been performed by the caller, which is recommended to avoid the (usually) expensive message creation. utility class for executing code that needs to do something with the current segments file. this is necessary with lock-less commits because from the time you locate the current segments file name, until you actually open it, read its contents, or check modified time, etc., it could have been deleted due to a writer commit finishing. sole constructor. locate the most recent {@code segments} file and run {@link #dobody} on it. run {@link #dobody} on the provided commit. subclass must implement this. the assumption is an ioexception will be thrown if something goes wrong during the processing that could have been caused by a writer committing. call this to start a commit. this writes the new segments file, but writes an invalid checksum at the end, so that it is not visible to readers. once this is called you must call {@link #finishcommit} to complete the commit or {@link #rollbackcommit} to abort it.  note: {@link #changed()} should be called prior to this method if changes have been made to this {@link segmentinfos} instance  returns all file names referenced by segmentinfo instances matching the provided directory (ie files associated with any "external" segments are skipped). the returned collection is recomputed on each invocation. todo: if lastgen == -1 we get might get null here it seems wrong to add null to the files set writes & syncs to the directory dir, taking care to remove the segments file on exception  note: {@link #changed()} should be called prior to this method if changes have been made to this {@link segmentinfos} instance  returns readable description of this segment. return {@code replaces all segments in this instance, but keeps generation, version, counter so that future commits remain write once. returns sum of all segment's doccounts. note that this does not include deletions call this before committing if changes have been made to the segments. applies all changes caused by committing a merge to this segmentinfos returns an unmodifiable {@link iterator} of contained segments in order. returns all contained segments as an unmodifiable {@link list} view. returns number of {@link segmentinfopercommit}s. appends the provided {@link segmentinfopercommit}. appends the provided {@link segmentinfopercommit}s. clear all {@link segmentinfopercommit}s. remove the provided {@link segmentinfopercommit}. warning: o(n) cost remove the {@link segmentinfopercommit} at the provided index. warning: o(n) cost return true if the provided {@link segmentinfopercommit} is contained. warning: o(n) cost returns index of the provided {@link segmentinfopercommit}. warning: o(n) cost javadocs generation of the "segments_n" for the next commit generation of the "segments_n" file we last successfully read or wrote; this is normally the same as generation except if there was an ioexception that had interrupted a commit clear any previous segments: 4.0+ system.out.println("sis.read seg=" + seg + " codec=" + codec); clear any segment infos we had loaded so we have a clean slate on retry: only non-null after preparecommit has been called and before finishcommit is called always advance the generation on write: write counter write infos if this segment is pre-4.x, perform a one-time "ugprade" to write the .si file for it: write separate marker file indicating upgrade is completed. this way, if there is a jvm kill/crash, os crash, power loss, etc. while writing the upgraded file, the marker file will be missing: we hit an exception above; try to close the file but suppress any exception: suppress so we keep throwing the original exception try not to leave a truncated segments_n file in the index: suppress so we keep throwing the original exception check marker file: ignore: if something is wrong w/ the marker file, we will just upgrade again note: this is not how 3.x is really written... system.out.println("upgrade write " + filename); we are about to write this si in 3.x format, dropping all codec information, etc. so it had better be a 3.x segment or you will get very confusing errors later. write the lucene version that suppress so we keep throwing the original exception deep clone, first recreate all collections: dont directly access segments, use add method!!! loop until we succeed in calling dobody() without hitting an ioexception. an ioexception most likely means a commit was in process and has finished, in the time it took us to load the now-old infos files (and segments files). it's also possible it's a true error (corrupt index). to distinguish these, on each retry we must see "forward progress" on which generation we are trying to load. if we don't, then the original error is real and we throw it. we have three methods for determining the current generation. we try the first two in parallel (when usefirstmethod is true), and fall back to the third when necessary. list the directory and use the highest segments_n file. this method works well as long as there is no stale caching on the directory contents (note: nfs clients often have such stale caching): also open segments.gen and read its contents. then we take the larger of the two gens. this way, if either approach is hitting a stale cache (nfs) we have a better chance of getting the right generation. the file is consistent. rethrow any format exception pick the larger of the two gen's: neither approach found a generation give up on first method -- this is 3rd cycle on listing directory and checking gen file to attempt to locate the segments file. second method: since both directory cache and file contents cache seem to be stale, just advance the generation. all attempts have failed -- throw first exc: this means we're about to try the same segments_n last tried. segment file has advanced since our last loop (we made "progress"), so reset retrycount: save the original root cause: this is our second time trying this same segments file (because retrycount is 1), and, there is possibly a segments_(n-1) (because gen > 1). so, check if the segments_(n-1) exists and try it if so: carry over generation numbers from another segmentinfos suppress so we keep throwing the original exception in our caller must carefully compute filename from "generation" since lastgeneration isn't incremented: suppress so we keep throwing the original exception in our caller closes pendingsegnoutput & deletes partial segments_n: closes pendingsegnoutput & deletes partial segments_n: note: if we crash here, we have left a segments_n file in the directory in a possibly corrupt state (if some bytes made it to stable storage and others didn't). but, the segments_n file includes checksum at the end, which should catch this case. so when a reader tries to read it, it will throw a corruptindexexception, which should cause the retry logic in segmentinfos to kick in and load the last good (previous) segments_n-1 file. suppress so we keep throwing the original exception it's ok if we fail to write this file since it's used only as one of the retry fallbacks. ignore; this file is only used in a retry fallback on init. the rest of the segments in list are duplicates, so don't remove from map, only list! either we found place to insert segment, or, we did not, but only because all segments we merged becamee deleted while we are merging, in which case it should be the case that the new segment is also all deleted, we insert it at the beginning if it should not be dropped: @override (comment out until java 6)"
org.apache.lucene.index.MultiBits "concatenates multiple bits together, on every lookup. note: this is very costly, as every lookup must do a binary search to locate the right sub-reader. @lucene.experimental represents a sub-bits from {@link multibits#getmatchingsub(org.apache.lucene.index.readerslice) getmatchingsub()}. returns a sub-bits matching the provided slice  because null usually has a special meaning for bits (e.g. no deleted documents), you must check {@link subresult#matches} instead to ensure the sub was actually found. length is 1+subs.length (the last entry has the maxdoc):"
org.apache.lucene.index.DocumentsWriter "this class accepts multiple added documents and directly writes segment files. each added document is passed to the {@link docconsumer}, which in turn processes the document and interacts with other consumers in the indexing chain. certain consumers, like {@link storedfieldsconsumer} and {@link termvectorsconsumer}, digest a document and immediately write bytes to the "doc store" files (ie, they do not consume ram per document, except while they are processing the document). other consumers, eg {@link freqproxtermswriter} and {@link normsconsumer}, buffer bytes in ram and flush only when a new segment is produced. once we have used our allowed ram buffer, or the number of added docs is large enough (in the case we are flushing by doc count instead of ram usage), we create a real segment and flush it to the directory. threads: multiple threads are allowed into adddocument at once. there is an initial synchronized call to getthreadstate which allocates a threadstate for this thread. the same thread will get the same threadstate over time (thread affinity) so that if there are consistent patterns (for example each thread is indexing a different content source) then we make better use of ram. then processdocument is called on that threadstate without synchronization (most of the "heavy lifting" is in this call). finally the synchronized "finishdocument" is called to flush changes to the directory. when flush is called by indexwriter we forcefully idle all threads and flush only once they are all idle. this means you can call flush with a given thread even while other threads are actively adding/deleting documents. exceptions: because this class directly updates in-memory posting lists, and flushes stored fields and term vectors directly to files in the directory, there are certain limited times when an exception can corrupt this state. for example, a disk full while flushing stored fields leaves this file in a corrupt state. or, an oom exception while appending to the in-memory posting lists can corrupt that posting list. we call such exceptions "aborting exceptions". in these cases we must call abort() to discard all docs added since the last flush. all other exceptions ("non-aborting exceptions") can still partially update the index structures. these updates are consistent, but, they represent only a part of the document seen up until the exception was hit. when this happens, we immediately mark the document as deleted so that the document is always atomically ("all or none") added to the index. we preserve changes during a full flush since iw might not checkout before we release all changes. nrt readers otherwise suddenly return true from iscurrent while there are actually changes currently committed. see also #anychanges() & #flushallthreads returns how many docs are currently buffered in ram. called if we hit an exception at a bad time (when updating the index files) and must discard all currently buffered docs. this resets our state, discarding any docs added since last flush. changes are either in a dwpt or in the deletequeue. yet if we currently flush deletes and / or dwpt there could be a window where all changes are in the ticket queue before they are published to the iw. ie we need to check if the ticket queue has any tickets. since with dwpt the flush process is concurrent and several dwpt could flush at the same time we must maintain the order of the flushes before we can apply the flushed segment and the frozen global deletes it is buffering. the reason for this is that the global deletes mark a certain point in time where we took a dwpt out of rotation and freeze the global deletes. example: a flush 'a' starts and freezes the global deletes, then flush 'b' starts and freezes all deletes occurred since 'a' has started. if 'b' finishes before 'a' we need to wait until 'a' is done otherwise the deletes frozen by 'b' are not applied to 'a' and we might miss to deletes documents in 'a'. now we are done and try to flush the ticket queue if the head of the queue has already finished the flush. publishes the flushed segment, segment private deletes (if any) and its associated global delete (if present) to indexwriter. the actual publishing operation is synced on iw -> bds so that the {@link segmentinfo}'s delete generation is always globalpacket_deletegeneration + 1 flushallthreads is synced by iw fullflushlock. flushing all threads is a two stage operation; the caller must ensure (in try/finally) that finishflush is called after this method, to release the flush lock in dwflushcontrol cutover to a new delete queue. this must be synced on the flush control otherwise a new dwpt could sneak into the loop with an already flushing delete queue todo: cut over to bytesrefhash in buffereddeletes list of files that were written before last abort() todo: we could check w/ freqproxtermswriter: if the term doesn't exist, don't bother buffering into the per-dwpt map (but still must go into the global map) we might be closed for testing help out flushing any queued dwpts so we can un-stall: try pick up pending threads here if possible don't push the delete here since the update could fail! block if stalled still queued dwpts try help flushing each flush is assigned a ticket in the order they acquire the ticketqueue lock flush concurrently without locking flush was successful once we reached this point - new seg. has been assigned to the ticket! in the case of a failure make sure we are making progress and apply all the deletes since the segment flush failed since the flush ticket could hold global deletes see flushticket#canpublish() this means there is a backlog: the one thread in innerpurge can't keep up with all other threads flushing segments. in this case we forcefully stall the producers. if deletes alone are consuming > 1/2 our ram buffer, force them all to apply now. this is to prevent too-frequent flushing of a long tail of tiny segments: finish the flushed segment and publish it to indexwriter system.out.println("flush: " + newsegment.segmentinfo.info.name); now publish! for asserts for asserts swaps the delqueue synced on flushcontrol help out with flushing: if a concurrent flush is still in flight wait for it apply deletes if we did not flush any document release the flush lock"
org.apache.lucene.index.DocumentsWriterFlushQueue "@lucene.internal if we block on publish -> lock iw -> lock buffereddeletes we don't block concurrent segment flushes just because they want to append to the queue. the downside is that we need to force a purge on fullflush since ther could be a ticket still in the queue. we track tickets separately since count must be present even before the ticket is constructed ie. queue.size would not reflect it. a window for #anychanges to fail don't hold the lock on the flushqueue when forcing the purge - this blocks and deadlocks if we hold the lock. each flush is assigned a ticket in the order they acquire the ticketqueue lock prepare flush freezes the global deletes - do in synced block! the actual flush is done asynchronously and once done the flushedsegment is passed to the flush ticket to free the queue we mark tickets as failed just to clean up the queue. do this synced finally remove the published ticket from the queue its a global ticket - no segment to publish"
org.apache.lucene.index.IndexNotFoundException "signals that no index was found in the directory. possibly because the directory is empty, however can also indicate an index corruption. creates indexfilenotfoundexception with the description message."
org.apache.lucene.index.MultiReader "a {@link compositereader} which reads multiple indexes, appending their content. it can be used to create a view on several sub-readers (like {@link directoryreader}) and execute searches on it.  for efficiency, in this api documents are often referred to via document numbers, non-negative integers which each name a unique document in the index. these document numbers are ephemeral -- they may change as documents are added to and deleted from an index. clients should thus not rely on a given document having the same number between sessions. note: {@link indexreader} instances are completely thread safe, meaning multiple threads can call any of its methods, concurrently. if your application requires external synchronization, you should not synchronize on the indexreader instance; use your own (non-lucene) objects instead. construct a multireader aggregating the named set of (sub)readers. note that all subreaders are closed if this multireader is closed. set of (sub)readers construct a multireader aggregating the named set of (sub)readers. set of (sub)readers; this array will be cloned. indicates whether the subreaders should be closed when this multireader is closed throw the first exception"
org.apache.lucene.index.NoMergeScheduler "a {@link mergescheduler} which never executes any merges. it is also a singleton and can be accessed through {@link nomergescheduler#instance}. use it if you want to prevent an {@link indexwriter} from ever executing merges, regardless of the {@link mergepolicy} used. note that you can achieve the same thing by using {@link nomergepolicy}, however with {@link nomergescheduler} you also ensure that no unnecessary code of any {@link mergescheduler} implementation is ever executed. hence it is recommended to use both if you want to disable merges from ever happening. the single instance of {@link nomergescheduler} prevent instantiation"
org.apache.lucene.index.ByteSliceReader "indexinput that knows how to read the byte slices written by posting and postingvector. we read the bytes in each slice until we hit the end of that slice at which point we read the forwarding address of the next slice and then jump to it. there is only this one slice to read skip to our next slice we are advancing to the final slice this is not the final slice (subtract 4 for the forwarding address at the end of this new slice) read entire slice this slice is the last one"
org.apache.lucene.index.NoDeletionPolicy "an {@link indexdeletionpolicy} which keeps all index commits around, never deleting them. this class is a singleton and can be accessed by referencing {@link #instance}. the single instance of this class. keep private to avoid instantiation"
org.apache.lucene.index.FieldInvertState "this class tracks the number and position / offset parameters of terms being added to the index. the information collected in this class is also used to calculate the normalization factor for a field. @lucene.experimental creates {code fieldinvertstate} for the specified field name. creates {code fieldinvertstate} for the specified field name and values for all fields. re-initialize the state get the last processed term position. position get total number of terms in this field. length set length value. get the number of terms with positionincrement == 0. numoverlap set number of terms with {@code positionincrement == 0}. get end offset of the last processed term. offset get boost value. this is the cumulative product of document boost and field boost for all field instances sharing the same field name. boost set boost value. get the maximum term-frequency encountered for any term in the field. a field containing "the quick brown fox jumps over the lazy dog" would have a value of 2, because "the" appears twice. return the number of unique terms encountered in this field. returns the {@link attributesource} from the {@link tokenstream} that provided the indexed tokens for this field. return the field's name javadocs"
org.apache.lucene.index.SegmentInfo "information about a segment such as it's name, directory, and files related to the segment. @lucene.experimental used by some member fields to mean not present (e.g., norms, deletions). used by some member fields to mean present (e.g., norms, deletions). unique segment name in the directory. where this segment resides. returns diagnostics saved into the segment when it was written. construct a new complete segmentinfo instance from input. note: this is public only to allow access from the codecs package. returns total size in bytes of all of files used by this segment. note that this will not include any live docs for the segment; to include that use {@link segmentinfopercommit#sizeinbytes()} instead. note: this value is not correct for 3.0 segments that have shared docstores. to get the correct value, upgrade! @deprecated separate norms are not supported in >= 4.0 mark whether this segment is stored as a compound file. true if this is a compound file; else, false returns true if this segment is stored as a compound file; else, false. can only be called once. return {@link codec} that wrote this segment. returns number of documents in this segment (deletions are not taken into account). return all files referenced by this segmentinfo. used for debugging. format may suddenly change. current format looks like _a(3.1):c45/4, which means the segment's name is _a; it was we consider another segmentinfo instance equal if it has the same dir and same name. used by defaultsegmentinfosreader to upgrade a 3.0 segment to record its version is "3.0". this method can be removed when we're not required to support 3x indexes anymore, e.g. in 5.0.  note: this method is used for internal purposes only - you should not modify the version of a segmentinfo, or it may result in unexpected exceptions thrown when you attempt to open the index. @lucene.internal returns the version of the code which wrote the segment. sets the files written for this segment. add these files to the set of files written for this segment. add this file to the set of files written for this segment. get a codec attribute value, or null if it does not exist puts a codec attribute value.  this is a key-value mapping for the field that the codec can use to store additional metadata, and will be available to the codec when reading the segment via {@link #getattribute(string)}  if a value already exists for the field, it will be replaced with the new value. returns the internal codec attributes map. codec attributes map. may be null if no mappings exist. todo: remove these from this class, for now this is the representation e.g. no norms; no deletes; e.g. have norms; have deletes; number of docs in seg total byte size of all files (computed on demand) tracks the lucene version this segment was indicates an older than 3.0 index, and it's used to detect a too old index. the format expected is "x.y" - "2.x" for pre-3.0 indexes (or null), and specific versions afterwards ("3.0", "3.1" etc.). see constants.lucene_main_version. note: leave package private todo: we could append tostring of attributes() here?"
org.apache.lucene.index.TermsHashConsumer ""
org.apache.lucene.index.PrefixCodedTerms "prefix codes term instances (prefixes are shared) @lucene.experimental in bytes over the bytes builds a prefixcodedterms: call add repeatedly, then finish. add a term return finalized form new field"
org.apache.lucene.index.SingleTermsEnum "subclass of filteredtermsenum for enumerating a single term.  for example, this can be used by {@link multitermquery}s that need only visit one term, but want to preserve multitermquery semantics such as {@link multitermquery#getrewritemethod}. creates a new singletermsenum.  after calling the constructor the enumeration is already pointing to the term, if it exists. javadocs"
org.apache.lucene.index.FieldInfo "access to the field info file that describes document fields and whether or not they are indexed. each segment has a separate field info file. objects of this class are thread-safe for multiple readers, but only one thread can be adding documents at a time, with no other reader or writer threads accessing this object. field's name internal field number controls how much information is stored in the postings lists. @lucene.experimental only documents are indexed: term frequencies and positions are omitted. phrase and other positional queries on the field will throw an exception, and scoring will behave as if any term in the document appears only once. only documents and term frequencies are indexed: positions are omitted. this enables normal scoring, except phrase and other positional queries will throw an exception. indexes documents, frequencies and positions. this is a typical default for full-text search: full scoring is enabled and positional queries are supported. indexes documents, frequencies, positions and offsets. character offsets are encoded alongside the positions. sole constructor. @lucene.experimental returns indexoptions for the field, or null if the field is not indexed returns true if this field has any docvalues. returns {@link docvalues.type} of the docvalues. this may be null if the field has no docvalues. returns {@link docvalues.type} of the norm. this may be null if the field has no norms. returns true if norms are explicitly omitted for this field returns true if this field actually has any norms. returns true if this field is indexed. returns true if any payloads exist for this field. returns true if any term vectors exist for this field. get a codec attribute value, or null if it does not exist puts a codec attribute value.  this is a key-value mapping for the field that the codec can use to store additional metadata, and will be available to the codec when reading the segment via {@link #getattribute(string)}  if a value already exists for the field, it will be replaced with the new value. returns internal codec attributes map. may be null if no mappings exist. true if any document indexed term vectors omit norms associated with indexed fields whether this field stores payloads together with term positions note: order is important here; fieldinfo uses this order to merge two conflicting indexoptions (always "downgrades" by picking the lowest). todo: maybe rename to just docs? for non-indexed fields, leave defaults cannot store payloads unless positions are indexed: should only be called by fieldinfos#addorupdate once indexed, always index if updated field data is not for indexing, leave the updates out once vector, always vector if one require omitnorms at least once, it remains off for life downgrade cannot store payloads if we don't store positions:"
org.apache.lucene.index.AtomicReaderContext "{@link indexreadercontext} for {@link atomicreader} instances. the readers ord in the top-level's leaves array the readers absolute doc base creates a new {@link atomicreadercontext}"
org.apache.lucene.index.DocConsumer ""
org.apache.lucene.index.SegmentMerger "the segmentmerger class combines two or more segments, represented by an indexreader ({@link #add}, into a single segment. after adding the appropriate readers, call the merge method to combine the segments. @see #merge @see #add add an indexreader to the collection of readers that are to be merged merges the readers specified by the {@link #add} method into the directory passed to the constructor number of documents that were merged @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error number of documents in all of the readers @throws corruptindexexception if the index is corrupt @throws ioexception if there is a low-level io error merge the termvectors from each of the segments into the new one. @throws ioexception if there is a low-level io error note, just like in codec apis directory 'dir' is not the same as segmentinfo.dir!! note: it's important to add calls to checkabort.work(...) if you make any changes to this method that will spend alot of time. the frequency of this check impacts how long indexwriter.close(false) takes to actually stop the threads. write the merged infos if the i'th reader is a segmentreader and has identical fieldname -> number mapping, then this array will be non-null at position i: if this reader is a segmentreader, and all of its field name -> number mappings match the "merged" fieldinfos, then we can do a bulk copy of the stored fields: todo: we may be able to broaden this to non-segmentreaders, since fieldinfos is now required? but... this'd also require exposing bulk-copy (tvs and stored fields) api in foreign readers.. returns an updated typepromoter (tracking type and size) given a previous one, and a newly encountered docvalues note: this is actually merging all the fieldinfos mapping from all docvalues fields found to their promoted types this is because fieldinfos does not store the valuesize update the type promotion mapping for this reader update any promoted doc values types: reset the type if we got promoted reset the type if we got promoted note: removes any "all deleted" readers from mergestate.readers remap docids todo: remove this check when 3.x indexes are no longer supported (3.x indexes don't have docvalues) todo: remove this check when 3.x indexes are no longer supported (3.x indexes don't have docvalues)"
org.apache.lucene.index.IndexReaderContext "a struct like class that represents a hierarchical relationship between {@link indexreader} instances. the reader context for this reader's immediate parent, or null if none true if this context struct represents the top level reader within the hierarchical context the doc base for this reader in the parent, 0 if parent is null the ord for this reader in the parent, 0 if parent is null returns the {@link indexreader}, this context represents. returns the context's leaves if this context is a top-level context. for convenience, if this is an {@link atomicreadercontext} this returns itself as the only leaf. note: this is convenience method since leaves can always be obtained by walking the context tree using {@link #children()}. @throws unsupportedoperationexception if this is not a top-level context. @see #children() returns the context's children iff this context is a composite context otherwise null."
org.apache.lucene.index.DocsAndPositionsEnum "also iterates through positions. flag to pass to {@link termsenum#docsandpositions(bits,docsandpositionsenum,int)} if you require offsets in the returned enum. flag to pass to {@link termsenum#docsandpositions(bits,docsandpositionsenum,int)} if you require payloads in the returned enum. sole constructor. (for invocation by subclass constructors, typically implicit.) returns the next position. you should only call this up to {@link docsenum#freq()} times else the behavior is not defined. if positions were not indexed this will return -1; this only happens if offsets were indexed and you passed needsoffset=true when pulling the enum. returns start offset for the current position, or -1 if offsets were not indexed. returns end offset for the current position, or -1 if offsets were not indexed. returns the payload at this position, or null if no payload was indexed. you should not modify anything (neither members of the returned bytesref nor bytes in the byte[]). javadocs"
org.apache.lucene.index.DocumentsWriterPerThread "the indexingchain must define the {@link #getchain(documentswriterperthread)} method which returns the docconsumer that the documentswriter calls to process the documents. this is the current indexing chain: docconsumer / docconsumerperthread --> code: docfieldprocessor / docfieldprocessorperthread --> docfieldconsumer / docfieldconsumerperthread / docfieldconsumerperfield --> code: docfieldconsumers / docfieldconsumersperthread / docfieldconsumersperfield --> code: docinverter / docinverterperthread / docinverterperfield --> inverteddocconsumer / inverteddocconsumerperthread / inverteddocconsumerperfield --> code: termshash / termshashperthread / termshashperfield --> termshashconsumer / termshashconsumerperthread / termshashconsumerperfield --> code: freqproxtermswriter / freqproxtermswriterperthread / freqproxtermswriterperfield --> code: termvectorstermswriter / termvectorstermswriterperthread / termvectorstermswriterperfield --> inverteddocendconsumer / inverteddocconsumerperthread / inverteddocconsumerperfield --> code: normswriter / normswriterperthread / normswriterperfield --> code: storedfieldswriter / storedfieldswriterperthread / storedfieldswriterperfield called if we hit an exception at a bad time (when updating the index files) and must discard all currently buffered docs. this resets our state, discarding any docs added since last flush. here we actually finish the document in two steps 1. push the delete into the queue and update our slice. 2. increment the dwpt private document id. the updated slice we get from 1. holds all the deletes that have occurred since we updated the slice the last time. returns the number of delete terms in this {@link documentswriterperthread} returns the number of ram resident documents in this {@link documentswriterperthread} reset after a flush prepares this dwpt for flushing. this method will freeze and return the {@link documentswriterdeletequeue}s global buffer and apply all pending deletes to this dwpt. deleteslice can possibly be null if we have hit non-aborting exceptions during indexing and never succeeded adding a document. flush all pending docs to a new segment seals the {@link segmentinfo} for the new flushed segment and persists the deleted documents {@link mutablebits}. get current segment info we are writing. initial chunks size of the shared byte[] blocks used to store postings data if you increase this, you must fix field cache impl for getterms/gettermsindex requires <= 32768 allocate another int[] from the shared pool build up indexing chain: only called by asserts don't hold onto doc nor analyzer, in case it is largish: system.out.println(thread.currentthread().getname() + ": now abort seg=" + segmentinfo.name); reset all postings data deletes for our still-in-ram (to be flushed next) segment current segment we are working on true if an abort is pending true if the last exception throws by #updatedocument was aborting this should be the last call in the ctor it really sucks that we need to pull this within the ctor and pass this ref to the chain! mark document as deleted an exc is being thrown... incr here because finishdocument will not be called (because an exc is being thrown): apply delterm only after all indexing has succeeded, but apply it only to docs prior to when this batch started: the iterator threw an exception that is not aborting go and mark all docs from this block as deleted buffer a specific docid for deletion. currently only used when we hit a exception when adding a document note: we do not trigger flush here. this is potentially a ram leak, if you have an app that tries to add docs but every single doc always hits a non-aborting exception. allowing a flush here gets very messy because we are only invoked when handling exceptions so to do this properly, while handling an exception we'd have to go off and flush new deletes which is risky (likely would hit some other confounding exception). public for flushpolicy public for flushpolicy apply all deletes before we flush and release the delete slice apply delete-by-docid now (delete-bydocid only happens when an exception is hit processing that doc, eg if analyzer has some problem w/ the text): now build compound file have codec write segmentinfo. must do this after creating cfs so that 1) .si isn't slurped into cfs, and 2) .si reflects usecompoundfile=true change above: todo: ideally we would freeze newsegment here!! because any changes after writing the .si will be lost... must write deleted docs after the cfs so we don't slurp the del file into cfs: todo: we should prune the segment if it's 100% deleted... but merge will also catch it. todo: in the nrt case it'd be better to hand this del vector over to the shortly-to-be-opened segmentreader and let it carry the changes; there's no reason to use filesystem as intermediary here."
org.apache.lucene.index.Norm "stores the normalization value computed in {@link similarity#computenorm(fieldinvertstate, norm)} per field. normalization values must be consistent within a single field, different value types are not permitted within a single field. all values set must be fixed size values ie. all values passed to {@link norm#setbytes(bytesref)} must have the same length per field. @lucene.experimental @lucene.internal sole constructor. returns the {@link indexablefield} representation for this norm returns the {@link type} for this norm. returns a spare {@link bytesref} sets a float norm value sets a double norm value sets a short norm value sets a int norm value sets a long norm value sets a byte norm value sets a fixed byte array norm value"
org.apache.lucene.index.DocumentsWriterStallControl "controls the health status of a {@link documentswriter} sessions. this class used to block incoming indexing threads if flushing significantly slower than indexing to ensure the {@link documentswriter}s healthiness. if flushing is significantly slower than indexing the net memory used within an {@link indexwriter} session can increase very quickly and easily exceed the jvm's available memory.  to prevent oom errors and ensure indexwriter's stability this class blocks incoming threads from indexing once 2 x number of available {@link threadstate}s in {@link documentswriterperthreadpool} is exceeded. once flushing catches up and the number of flushing dwpt is equal or lower than the number of active {@link threadstate}s threads are released and can continue indexing. update the stalled flag status. this method will set the stalled flag to true iff the number of flushing {@link documentswriterperthread} is greater than the number of active {@link documentswriterperthread}. otherwise it will reset the {@link documentswriterstallcontrol} to healthy and release all threads waiting on {@link #waitifstalled()} blocks if documents writing is currently in a stalled state. only with assert only with assert only with assert react on the first wakeup call! don't loop here, higher level logic will re-stall! for tests for tests volatile read! for tests for tests"
org.apache.lucene.index.DocFieldConsumer "called when documentswriterperthread decides to create a new segment called when an aborting exception is hit called when documentswriterperthread is using too much ram. the consumer should free ram, if possible, returning true if any ram was in fact freed."
org.apache.lucene.index.DocInverterPerField "holds state for inverting all occurrences of a single field in the document. this class doesn't do anything itself; instead, it forwards the tokens produced by analysis to its own consumer (inverteddocconsumerperfield). it also interacts with an endconsumer (inverteddocendconsumerperfield). todo fi: this should be "genericized" to querying consumer if it wants to see this particular field tokenized. if the field omits norms, the boost cannot be indexed. only bother checking offsets if something will consume them. todo: after we fix analyzers, also check if termvectoroffsets will be indexed. reset the tokenstream to the first token if we hit an exception in stream.next below (which is fairly common, eg if analyzer chokes on a given document), then it's non-aborting and (above) this one document will be marked as deleted, but still consume a docid note: confusing: this "mirrors" the position++ we do below position is legal, we can safely place it in fieldstate now. not sure if anything will use fieldstate after non-aborting exc... if we hit an exception in here, we abort all buffered documents since the last flush, on the likelihood that the internal state of the consumer is now corrupt and should not be flushed to a new segment: trigger streams to perform end-of-stream operations lucene-2387: don't hang onto the field, so gc can reclaim"
org.apache.lucene.index.DocFieldProcessorPerField "holds all per thread, per field state."
org.apache.lucene.index.MergedIterator "provides a merged sorted view from several sorted iterators, each iterating over a unique set of elements.  if an element appears in multiple iterators, it is deduplicated, that is this iterator returns the sorted union of elements.  caveats:  the behavior is undefined if the iterators are not actually sorted according to their comparator, or if a single iterator contains duplicates. null elements are unsupported. when an element e is a duplicate across multiple iterators, only one is returned, but it is undefined which one: not guaranteed to be a stable sort.  @lucene.internal restore queue gather equal top elements extract all subs from the queue that have the same top element call next() on each top, and put back into queue no more elements"
org.apache.lucene.index.StandardDirectoryReader "called only from static open() methods called from directoryreader.open(...) methods used by near real-time search this constructor is only used for {@link #doopenifchanged(segmentinfos, indexwriter)} indexwriter synchronizes externally before calling us, which ensures infos will not change; so there's no need to process segments in reverse order steal the ref: we put the old segmentreaders in a map, that allows us to lookup a reader using its segment name create a map segmentname->segmentreader remember which readers are shared between the old and the re-opened directoryreader - we have to incref those readers find segmentreader for this segment this is a new segment, no old segmentreader can be reused there is an old reader for this segment - we'll try to reopen it this is a new reader; in case we hit an exception we can close it safely no change; this reader will be shared between the old and the new one, so we must incref it: steal the ref returned by segmentreader ctor: this is a new subreader that is not used by the old one, we can close it this subreader is also used by the old reader, so instead closing we must decref it throw the first exception if we were obtained by writer.getreader(), re-ask the writer to get a new reader. if in fact no changes took place, return null: fully read the segments file: this ensures that it's completely written so that if indexwriter.preparecommit has been called (but not yet commit), then the reader will still see itself as current: we loaded segmentinfos from the directory try to close each reader, even if an exception is thrown since we just closed, writer may now be able to delete unused files: throw the first exception"
org.apache.lucene.index.NormsConsumerPerField "some similarity might not compute any norms null type - not omitted but not written"
org.apache.lucene.index.ReaderUtil "common util methods for dealing with {@link indexreader}s and {@link indexreadercontext}s. @lucene.internal walks up the reader tree and return the given context's top level reader context, or in other words the reader tree's root context. returns index of the searcher/reader for document n in the array used to construct this searcher/reader. returns index of the searcher/reader for document n in the array used to construct this searcher/reader. no instance find searcher/reader for doc n: search starts array for first element less than n, return its index found a match scan to last match find searcher/reader for doc n: search starts array for first element less than n, return its index found a match scan to last match"
org.apache.lucene.index.TypePromoter "type promoter that promotes {@link docvalues} during merge based on their {@link type} and {@link #getvaluesize()} @lucene.internal var & fixed == var if we have fixed & fixed with different size we promote to var straight & deref == straight (dense values win) more bits wins (int16 & int32 == int32) returns a positive value size if this {@link typepromoter} represents a fixed variant, otherwise -1 positive value size if this {@link typepromoter} represents a fixed variant, otherwise -1 creates a new {@link typepromoter} creates a new {@link typepromoter} the {@link type} this promoter represents the promoters flags the value size if {@link #is_fixed} or -1 otherwise. resets the {@link typepromoter} the {@link type} this promoter represents the promoters flags the value size if {@link #is_fixed} or -1 otherwise. creates a new promoted {@link typepromoter} based on this and the given {@link typepromoter} or null iff the {@link typepromoter} aren't compatible. the incoming promoter new promoted {@link typepromoter} based on this and the given {@link typepromoter} or null iff the {@link typepromoter} aren't compatible. returns the {@link type} of this {@link typepromoter} {@link type} of this {@link typepromoter} creates a new {@link typepromoter} for the given type and size per value. the {@link type} to create the promoter for the size per value in bytes or -1 iff the types have variable length. new {@link typepromoter} returns a {@link typepromoter} that always promotes to the type provided to {@link #promote(typepromoter)} todo: maybe we should not automagically promote types... and instead require a given field always has the same type? 8 9"
org.apache.lucene.index.TermsHashConsumerPerField "implement this class to plug into the termshash processor, which inverts & stores tokens into a hash table and provides an api for writing bytes into multiple streams for each unique token."
org.apache.lucene.index.NormsConsumer "writes norms. each thread x field accumulates the norms for the doc/fields it saw, then the flush method below merges all of these together into a single _x.nrm file. produce _x.nrm if any document had a field with norms not disabled todo fi: norms could actually be stored as doc store we must check the final value of omitnorms for the fieldinfo, it could have changed for this field since the first time we added it."
org.apache.lucene.index.SegmentCoreReaders "holds core readers that are shared (unchanged) when segmentreader is cloned or reopened counts how many other reader share the core objects (freqstream, proxstream, tis, etc.) of this reader; when coreref drops to 0, these core objects may be closed. a given instance of segmentreader may be closed, even those it shares core objects with other segmentreaders: confusing name: if (cfs) its the cfsdir, otherwise its the segment's directory. ask codec for its fields ask codec for its norms: todo: since we don't write any norms file if there are no norms, kinda jaky to assume the codec handles the case of no norms file at all gracefully?! open term vector files only as needed must assign this at the end -- if we hit an exception above core, we don't want to attempt to purge the fieldcache (will hit npe because core is not assigned yet). system.out.println("core.decref seg=" + owner.getsegmentinfo() + " rc=" + ref);"
org.apache.lucene.index.BaseCompositeReader "base class for implementing {@link compositereader}s based on an array of sub-readers. the implementing class has to add code for correctly refcounting and closing the sub-readers.  list view solely for {@link #getsequentialsubreaders()}, for effectiveness the array is used internally. constructs a {@code basecompositereader} on the given subreaders. the wrapped sub-readers. this array is returned by {@link #getsequentialsubreaders} and used to resolve the correct subreader for docid-based methods. please note: this array is not cloned and not protected for modification, the subclass is responsible to do this. overflow helper method for subclasses to get the corresponding reader for a doc id helper method for subclasses to get the docbase of the given sub-reader index. 1st docno for each reader build starts array compute maxdocs compute numdocs find subreader num dispatch to subreader don't call ensureopen() here (it could affect performance) don't call ensureopen() here (it could affect performance) find subreader num dispatch to subreader don't call ensureopen() here (it could affect performance) sum freqs in subreaders sum freqs in subreaders"
org.apache.lucene.index.MergeScheduler "expert: {@link indexwriter} uses an instance implementing this interface to execute the merges selected by a {@link mergepolicy}. the default mergescheduler is {@link concurrentmergescheduler}. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) run the merges provided by {@link indexwriter#getnextmerge()}. close this mergescheduler."
org.apache.lucene.index.FilterAtomicReader "a filteratomicreader contains another atomicreader, which it uses as its basic source of data, possibly transforming the data along the way or providing additional functionality. the class filteratomicreader itself simply implements all abstract methods of indexreader with versions that pass all requests to the contained index reader. subclasses of filteratomicreader may further override some of these methods and may also provide additional methods and fields. base class for filtering {@link fields} implementations. the underlying fields instance. creates a new filterfields. the underlying fields instance. base class for filtering {@link terms} implementations. the underlying terms instance. creates a new filterterms the underlying terms instance. base class for filtering {@link termsenum} implementations. the underlying termsenum instance. creates a new filtertermsenum the underlying termsenum instance. base class for filtering {@link docsenum} implementations. the underlying docsenum instance. create a new filterdocsenum the underlying docsenum instance. base class for filtering {@link docsandpositionsenum} implementations. the underlying docsandpositionsenum instance. create a new filterdocsandpositionsenum the underlying docsandpositionsenum instance. the underlying atomicreader. construct a filteratomicreader based on the specified base reader. note that base reader is closed if this filteratomicreader is closed. specified base reader. {@inheritdoc} if the subclass of filteredindexreader modifies the contents (but not livedocs) of the index, you must override this method to provide a different key. {@inheritdoc} if the subclass of filteredindexreader modifies the livedocs, you must override this method to provide a different key. don't call ensureopen() here (it could affect performance) don't call ensureopen() here (it could affect performance)"
org.apache.lucene.index.DocTermOrds "this class enables fast access to multiple term ords for a specified field across all docids. like fieldcache, it uninverts the index and holds a packed data structure in ram to enable fast access. unlike fieldcache, it can handle multi-valued fields, and, it does not hold the term bytes in ram. rather, you must obtain a termsenum from the {@link #getordtermsenum} method, and then seek-by-ord to get the term's bytes. while normally term ords are type long, in this api they are int as the internal representation here cannot address more than max_int unique terms. also, typically this class is used on fields with relatively few unique terms vs the number of documents. in addition, there is an internal limit (16 mb) on how many bytes each chunk of documents may consume. if you trip this limit you'll hit an illegalstateexception. deleted documents are skipped during uninversion, and if you look them up you'll get 0 ords. the returned per-document ords do not retain their original order in the document. instead they are returned in sorted (by ord, ie term's bytesref comparator) order. they are also de-dup'd (ie if doc has same term more than once in this field, you'll only get that ord back once). this class tests whether the provided reader is able to retrieve terms by ord (ie, it's single segment, and it uses an ord-capable terms index). if not, this class will create its own term index internally, allowing to create a wrapped termsenum that can handle ord. the {@link #getordtermsenum} method then provides this wrapped enum, if necessary. the ram consumption of this class can be high! @lucene.experimental final form of the un-inverted field: each document points to a list of term numbers that are contained in that document. term numbers are in sorted order, and are encoded as variable-length deltas from the previous term number. real term numbers start at 2 since 0 and 1 are reserved. a term number of 0 signals the end of the termnumber list. there is a single int[maxdoc()] which either contains a pointer into a byte[] for the termnumber lists, or directly contains the termnumber list if it fits in the 4 bytes of an integer. if the first byte in the integer is 1, the next 3 bytes are a pointer into a byte[] where the termnumber list starts. there are actually 256 byte arrays, to compensate for the fact that the pointers into the byte arrays are only 3 bytes long. the correct byte array for a document is a function of it's id. to save space and speed up faceting, any term that matches enough documents will not be un-inverted... it will be skipped while building the un-inverted field structure, and will use a set intersection method during faceting. to further save memory, the terms (the actual string values) are not all stored in memory, but a termindex is used to convert term numbers to term values only for the terms needed after faceting has completed. only every 128th term value is stored, along with it's corresponding term number, and this is used as an index to find the closest term and iterate until the desired number is hit (very much like lucene's own internal term index). every 128th term is indexed, by default. don't uninvert terms that exceed this count. field we are uninverting. number of terms in the field. total number of references to term numbers. total time to uninvert the field. time for phase1 of the uninvert process. holds the per-document ords or a pointer to the ords. holds term ords for documents. total bytes (sum of term lengths) for all indexed terms. holds the indexed (by default every 128th) terms. if non-null, only terms matching this prefix were indexed. ordinal of the first term in the field, or 0 if the {@link postingsformat} does not implement {@link termsenum#ord}. used while uninverting. returns total bytes used. inverts all terms inverts only terms starting w/ prefix inverts only terms starting w/ prefix, and only terms whose docfreq (not taking deletions into account) is note: you must pass the same reader that was used when creating this class returns the number of terms in this field returns {@code true} if no terms were indexed. subclass can override this invoked during {@link #uninvert(atomicreader,bytesref)} to record the document frequency for each uninverted term. call this only once (if you subclass!) for(byte b : arr) { //system.out.println(" b=" + integer.tohexstring((int) b)); } we don't have to worry about the array getting too large since the "pos" param will overflow first (only 24 bits available) if ((newlen<<1) <= 0) { // overflow... newlen = integer.max_value; if (newlen <= pos + len) { throw new solrexception(400,"too many terms to uninvert field!"); } } else { while (newlen <= pos + len) newlen<<=1; // doubling strategy } number of bytes to represent an unsigned int as a vint. iterates over the ords for a single document. buffer must be at least 5 ints long. returns number of term ords placed into buffer; if this count is less than buffer.length then that is the end. reset the iterator on a new document. returns an iterator to step through the term ords for this document. it's also possible to subclass this class and directly access members. only used if original indexreader doesn't implement ord; in this case we "wrap" our own terms index around it. returns the term ({@link bytesref}) corresponding to the provided ordinal. javadocs term ords are shifted by this, internally, to reserve values 0 (end term) and 1 (index is a pointer into byte array) decrease to a low number like 2 for testing can cache the mem size since it shouldn't change local fields system.out.println("dto init field=" + field + " maxtdfreq=" + maxtermdocfreq); system.out.println("get normal enum"); system.out.println("get wrapped enum ordbase=" + ordbase); system.out.println("dto uninvert field=" + field + " prefix=" + termprefix); immediate term numbers, or the index into the byte[] representing the last number last term we saw for this document list of term numbers for the doc (delta encoded vints) no terms no terms system.out.println("seekstart=" + seekstart.utf8tostring()); no terms match if we need our "term index wrapper", these will be init'd below: we need a minimum of 9 bytes, but round up to 12 since the space would be wasted with most allocators anyway.  enumerate all terms, and build an intermediate form of the un-inverted field.  during this intermediate form, every document has a (potential) byte[] and the int[maxdoc()] array either contains the termnumber list directly or the end offset of the termnumber list in it's byte array (for faster appending and faster creation of the final form).  idea... if things are too large while building, we could do a range of docs at a time (but it would be a fair amount slower to build) could also do ranges in parallel to take advantage of multiple cpus optional: remap the largest df terms to the lowest 128 (single byte) values. this requires going over the field first to find the most frequent terms ahead of time. loop begins with te positioned to first term (we call seek above): system.out.println("visit term=" + t.utf8tostring() + " " + t + " termnum=" + termnum); system.out.println("got ordbase=" + ordbase); reader cannot provide ord support, so we wrap our own support by creating our own terms index: system.out.println("no ords"); index this term todo: really should 1) strip off useless suffix, and 2) use fst not array/pagedbytes df, but takes deletions into account system.out.println(" chunk=" + chunk + " docs"); system.out.println(" docid=" + doc); add tnum_offset to the term number to make room for special reserved values: 0 (end term) and 1 (index into byte array follows) index into byte array (actually the end of the doc-specific byte[] when building) we avoid a doubling strategy to lower memory usage. this faceting method isn't for docs with many terms. in hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary. todo: figure out what array lengths we can round up to w/o actually using more memory (how much space does a byte[] take up? is data preceded by a 32 bit length only? it should be safe to round up to the nearest 32 bits in any case. 4 byte alignment update pointer to end index in byte[] ok, this int has data in it... find the end (a zero starting byte - not part of another number, hence not following a byte with the high bit set). system.out.println(" ipos=" + ipos); system.out.println(" endpos=" + endpos); system.out.println(" fits!"); value will fit in the integer... move bytes back value won't fit... move integer into byte[] point at the end index in the byte[] we didn't invert anything lower memory consumption.  transform intermediate form into the final form, building a single byte[] at a time, and releasing the intermediate byte[]s as we go to avoid increasing the memory footprint.  end in target; loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx where pp is the pass (which array we are building), and xx is all values. each pass shares the same byte[] for termnumber lists. system.out.println(" pass=" + pass + " process docid=" + doc); system.out.println(" ptr pos=" + pos); change index to point to start of array we only have 24 bits for the array index system.out.println(" b=" + integer.tohexstring((int) b)); important: allow gc to avoid oom overflow... doubling strategy doubling strategy skip single byte at end and leave it 0 for terminator shrink array todo: if we know the size of the vint already, we could do a single switch on the size code is inlined into upto system.out.println("inlined"); system.out.println(" tnum=" + tnum); code is a pointer system.out.println(" cycle: upto=" + upto + " delta=" + delta + " b=" + b); system.out.println(" delta=" + delta); system.out.println(" tnum=" + tnum); system.out.println(" reset docid=" + docid); a pointer system.out.println(" pointer! upto=" + upto); system.out.println(" inline!"); force "real" seek this is extra work if we know we are in bounds... already here we hit the term exactly... lucky us! we didn't hit the term exactly our target occurs before the first term back up to the start of the block we are already in the right block and the current term is before the term we want, so we don't need to seek. seek to the right block should be non-null since it's in the index system.out.println(" seek(ord) targetord=" + targetord + " delta=" + delta + " ord=" + ord + " ii=" + indexinterval); system.out.println(" do seek term=" + base.utf8tostring()); system.out.println("seek w/in block"); system.out.println(" setterm() term=" + term.utf8tostring() + " vs prefix=" + (prefix == null ? "null" : prefix.utf8tostring()));"
org.apache.lucene.index.Terms "access to the terms in a specific field. see {@link fields}. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns an iterator that will step through all terms. this method will not return null. if you have a previous termsenum, for example from a different field, you can pass it for possible reuse if the implementation can do so. returns a termsenum that iterates over all terms that are accepted by the provided {@link compiledautomaton}. if the startterm is provided then the returned enum will only accept terms > startterm, but you still must call next() first to get to the first term. note that the provided startterm must be accepted by the automaton. note: the returned termsenum cannot seek. return the bytesref comparator used to sort terms provided by the iterator. this method may return null if there are no terms. this method may be invoked many times; it's best to cache a single instance & reuse it. returns the number of terms for this field, or -1 if this measure isn't stored by the codec. note that, just like other term measures, this measure does not take deleted documents into account. returns the sum of {@link termsenum#totaltermfreq} for all terms in this field, or -1 if this measure isn't stored by the codec (or if this fields omits term freq and positions). note that, just like other term measures, this measure does not take deleted documents into account. returns the sum of {@link termsenum#docfreq()} for all terms in this field, or -1 if this measure isn't stored by the codec. note that, just like other term measures, this measure does not take deleted documents into account. returns the number of documents that have at least one term for this field, or -1 if this measure isn't stored by the codec. note that, just like other term measures, this measure does not take deleted documents into account. returns true if documents in this field store offsets. returns true if documents in this field store positions. returns true if documents in this field store payloads. zero-length array of {@link terms}. todo: eventually we could support seekceil/exact on the returned enum, instead of only being able to seek at the start"
org.apache.lucene.index.ParallelAtomicReader "an {@link atomicreader} which reads multiple, parallel indexes. each index added must have the same number of documents, but typically each contains different fields. deletions are taken from the first reader. each document contains the union of the fields of all documents with the same document number. when searching, matches for a query term are from the first index added that has the field. this is useful, e.g., with collections that have large fields which change rarely and small fields that change more frequently. the smaller fields may be re-indexed in a new index and both indexes may be searched together. warning: it is up to you to make sure all indexes are create a parallelatomicreader based on the provided readers; auto-closes the given readers on {@link #close()}. create a parallelatomicreader based on the provided readers. expert: create a parallelatomicreader based on the provided readers and storedfieldreaders; when a document is loaded, only storedfieldsreaders will be used. {@inheritdoc}  note: the returned field numbers will likely not correspond to the actual field numbers in the underlying readers, and codec metadata ({@link fieldinfo#getattribute(string)} will be unavailable. check compatibility: todo: make this read-only in a cleaner way? build fieldinfos and fieldtoreader map: note: first reader having a given field "wins": build fields instance only add if the reader responsible for that field name is the current: do this finally so any exceptions occurred before don't affect refcounts: single instance of this, per parallelreader instance don't call ensureopen() here (it could affect performance) don't call ensureopen() here (it could affect performance) throw the first exception"
org.apache.lucene.index.IndexCommit "expert: represents a single commit into an index as seen by the {@link indexdeletionpolicy} or {@link indexreader}.  changes to the content of an index are made visible only after the writer who made that change commits by writing a new segments file (segments_n). this point in time, when the action of writing of a new segments file to the directory is completed, is an index commit. each index commit point has a unique segments file associated with it. the segments file associated with a later index commit point would have a larger n. @lucene.experimental get the segments file (segments_n) associated with this commit point. returns all index files referenced by this commit point. returns the {@link directory} for the index. delete this commit point. this only applies when using the commit point in the context of indexwriter's indexdeletionpolicy.  upon calling this, the writer is notified that this commit point should be deleted.  decision that a commit-point should be deleted is taken by the {@link indexdeletionpolicy} in effect and therefore this should only be called by its {@link indexdeletionpolicy#oninit oninit()} or {@link indexdeletionpolicy#oncommit oncommit()} methods. returns true if this commit should be deleted; this is only used by {@link indexwriter} after invoking the {@link indexdeletionpolicy}. returns number of segments referenced by this commit. sole constructor. (for invocation by subclass constructors, typically implicit.) two indexcommits are equal if both their directory and versions are equal. returns the generation (the _n in segments_n) for this indexcommit returns"
org.apache.lucene.index.ParallelPostingsArray ""
org.apache.lucene.index.SegmentInfoPerCommit "embeds a [read-only] segmentinfo and adds per-commit fields. @lucene.experimental the {@link segmentinfo} that we wrap. sole constructor. {@link segmentinfo} that we wrap number of deleted documents in this segment deletion generation number (used to name deletion files) called when we succeed in writing deletes called if there was an exception while writing deletes, so that we don't try to write to the same file more than once. returns total size in bytes of all files for this segment. note: this value is not correct for 3.0 segments that have shared docstores. to get the correct value, upgrade! returns all files in use by this segment. sets the generation number of the live docs file. @see #getdelgen() returns true if there are any deletions for the segment at this commit. returns the next available generation number of the live docs file. returns generation number of the live docs file or -1 if there are no deletes yet. returns the number of deleted docs in the segment. returns a description of this segment. how many deleted docs in the segment: generation number of the live docs file (-1 if there are no deletes yet): normally 1+delgen, unless an exception was hit on last attempt to write: start from the wrapped info's files: must separately add any live docs files: note: only used in-ram by iw to track buffered deletes; this is never written to/read from the directory not clear that we need to carry over nextwritedelgen (i.e. do we ever clone after a failed write and before the next successful write?), but just do it to be safe:"
org.apache.lucene.index.MultiFields "exposes flex api, merged from flex api of sub-segments. this is useful when you're interacting with an {@link indexreader} implementation that consists of sequential sub-readers (eg {@link directoryreader} or {@link multireader}). note: for composite readers, you'll get better performance by gathering the sub readers using {@link indexreader#getcontext()} to get the atomic leaves and then operate per-atomicreader, instead of using this class. @lucene.experimental returns a single {@link fields} instance for this reader, merging fields/terms/docs/positions on the fly. this method will return null if the reader has no postings. note: this is a slow way to access postings. it's better to get the sub-readers and iterate through them yourself. returns a single {@link bits} instance for this reader, merging live documents on the fly. this method will return null if the reader has no deletions. note: this is a very slow way to access live docs. for example, each bits access will require a binary search. it's better to get the sub-readers and iterate through them yourself. this method may return null if the field does not exist. returns {@link docsenum} for the specified field & term. this will return null if the field or term does not exist. returns {@link docsenum} for the specified field & term, with control over whether freqs are required. some codecs may be able to optimize their implementation when freqs are not required. this will return null if the field or term does not exist. see {@link termsenum#docs(bits,docsenum,int)}. returns {@link docsandpositionsenum} for the specified field & term. this will return null if the field or term does not exist or positions were not indexed. @see #gettermpositionsenum(indexreader, bits, string, bytesref, int) returns {@link docsandpositionsenum} for the specified field & term, with control over whether offsets and payloads are required. some codecs may be able to optimize their implementation when offsets and/or payloads are not required. this will return null if the field or term does not exist or positions were not indexed. see {@link termsenum#docsandpositions(bits,docsandpositionsenum,int)}. expert: construct a new multifields instance directly. @lucene.internal returns the total number of occurrences of this term across all documents (the sum of the freq() for each doc that has this term). this will be -1 if the codec doesn't support this measure. note that, like other term measures, this measure does not take deleted documents into account. @see termsenum#totaltermfreq() call this to get the (merged) fieldinfos for a composite reader.  note: the returned field numbers will likely not correspond to the actual field numbers in the underlying readers, and codec metadata ({@link fieldinfo#getattribute(string)} will be unavailable. call this to get the (merged) fieldinfos representing the set of indexed fields only for a composite reader.  note: the returned field numbers will likely not correspond to the actual field numbers in the underlying readers, and codec metadata ({@link fieldinfo#getattribute(string)} will be unavailable. no fields already an atomic reader / reader with one leave record all livedocs, even if they are null todo: why is this public? lazy init: first time this field is requested, we create & add to terms: gather all sub-readers that share this field don't cache this case with an unbounded cache, since the number of fields that don't exist is unbounded."
org.apache.lucene.index.LiveIndexWriterConfig "holds all the configuration used by {@link indexwriter} with few setters for settings that can be changed on an {@link indexwriter} instance "live". {@link indexdeletionpolicy} controlling when commit points are deleted. {@link indexcommit} that {@link indexwriter} is opened on. {@link openmode} that {@link indexwriter} is opened with. {@link similarity} to use when encoding norms. {@link mergescheduler} to use for running merges. timeout when trying to obtain the write lock on init. {@link indexingchain} that determines how documents are indexed. {@link codec} used to write new segments. {@link infostream} for debugging messages. {@link mergepolicy} for selecting merges. {@code documentswriterperthreadpool} to control how threads are allocated to {@code documentswriterperthread}. true if readers should be pooled. {@link flushpolicy} to control when segments are flushed. sets the hard upper bound on ram usage for a single segment, after which the segment is forced to flush. {@link version} that {@link indexwriter} should emulate. creates a new config that that handles the live {@link indexwriter} settings. returns the default analyzer to use for indexing documents. expert: set the interval between indexed terms. large values cause less memory to be used by indexreader, but slow random-access to terms. small values cause more memory to be used by an indexreader, and speed random-access to terms.  this parameter determines the amount of computation required per query term, regardless of the number of documents that contain that term. in particular, it is the maximum number of other terms that must be scanned before a term is located and its frequency and position information may be processed. in a large index with returns the interval between indexed terms. @see #settermindexinterval(int) determines the minimal number of delete terms required before the buffered in-memory delete terms and queries are applied and flushed.  disabled by default (writer flushes by ram usage).  note: this setting won't trigger a segment flush.  takes effect immediately, but only the next time a document is added, updated or deleted. @throws illegalargumentexception if maxbuffereddeleteterms is enabled but smaller than 1 @see #setrambuffersizemb returns the number of buffered deleted terms that will trigger a flush of all buffered deletes if enabled. @see #setmaxbuffereddeleteterms(int) determines the amount of ram that may be used for buffering added documents and deletions before they are flushed to the directory. generally for faster indexing performance it's best to flush by ram usage instead of document count and use as large a ram buffer as you can.  when this is set, the writer will flush whenever buffered documents and deletions use this much ram. pass in {@link indexwriterconfig#disable_auto_flush} to prevent triggering a flush due to ram usage. note that if flushing by document count is also enabled, then the flush will be triggered by whichever comes first.  the maximum ram limit is inherently determined by the jvms available memory. yet, an {@link indexwriter} session can consume a significantly larger amount of memory than the given ram limit since this limit is just an indicator when to flush memory resident documents to the directory. flushes are likely happen concurrently while other threads adding documents to the writer. for application stability the available memory in the jvm should be significantly larger than the ram buffer used for indexing.  note: the account of ram usage for pending deletions is only approximate. specifically, if you delete by query, lucene currently has no way to measure the ram usage of individual queries so the accounting will under-estimate and you should compensate by either calling commit() periodically yourself, or by using {@link #setmaxbuffereddeleteterms(int)} to flush and apply buffered deletes by count instead of ram usage (for each buffered delete query a constant number of bytes is used to estimate ram usage). note that enabling {@link #setmaxbuffereddeleteterms(int)} will not trigger any segment flushes.  note: it's not guaranteed that all memory resident documents are flushed once this limit is exceeded. depending on the configured {@link flushpolicy} only a subset of the buffered documents are flushed and therefore only parts of the ram buffer is released.  the default value is {@link indexwriterconfig#default_ram_buffer_size_mb}.  takes effect immediately, but only the next time a document is added, updated or deleted. @see indexwriterconfig#setramperthreadhardlimitmb(int) @throws illegalargumentexception if rambuffersize is enabled but non-positive, or it disables rambuffersize when maxbuffereddocs is already disabled returns the value set by {@link #setrambuffersizemb(double)} if enabled. determines the minimal number of documents required before the buffered in-memory documents are flushed as a new segment. large values generally give faster indexing.  when this is set, the writer will flush every maxbuffereddocs added documents. pass in {@link indexwriterconfig#disable_auto_flush} to prevent triggering a flush due to number of buffered documents. note that if flushing by ram usage is also enabled, then the flush will be triggered by whichever comes first.  disabled by default (writer flushes by ram usage).  takes effect immediately, but only the next time a document is added, updated or deleted. @see #setrambuffersizemb(double) @throws illegalargumentexception if maxbuffereddocs is enabled but smaller than 2, or it disables maxbuffereddocs when rambuffersize is already disabled returns the number of buffered added documents that will trigger a flush if enabled. @see #setmaxbuffereddocs(int) set the merged segment warmer. see {@link indexreaderwarmer}.  takes effect on the next merge. returns the current merged segment warmer. see {@link indexreaderwarmer}. sets the termsindexdivisor passed to any readers that indexwriter opens, for example when applying deletes or creating a near-real-time reader in {@link directoryreader#open(indexwriter, boolean)}. if you pass -1, the terms index won't be loaded by the readers. this is only useful in advanced situations when you will only .next() through all terms; attempts to seek will hit an exception.  takes effect immediately, but only applies to readers opened after this call  note: divisor settings &gt; 1 do not apply to all postingsformat implementations, including the default one in this release. it only makes sense for terms indexes that can efficiently re-sample terms at load time. returns the {@code terminfosindexdivisor}. @see #setreadertermsindexdivisor(int) returns the {@link openmode} set by {@link indexwriterconfig#setopenmode(openmode)}. returns the {@link indexdeletionpolicy} specified in {@link indexwriterconfig#setindexdeletionpolicy(indexdeletionpolicy)} or the default {@link keeponlylastcommitdeletionpolicy}/ returns the {@link indexcommit} as specified in {@link indexwriterconfig#setindexcommit(indexcommit)} or the default, {@code null} which specifies to open the latest index commit point. expert: returns the {@link similarity} implementation used by this {@link indexwriter}. returns the {@link mergescheduler} that was set by {@link indexwriterconfig#setmergescheduler(mergescheduler)}. returns allowed timeout when acquiring the write lock. @see indexwriterconfig#setwritelocktimeout(long) returns the current {@link codec}. returns the current mergepolicy in use by this writer. @see indexwriterconfig#setmergepolicy(mergepolicy) returns the configured {@link documentswriterperthreadpool} instance. @see indexwriterconfig#setindexerthreadpool(documentswriterperthreadpool) configured {@link documentswriterperthreadpool} instance. returns the max number of simultaneous threads that may be indexing documents at once in indexwriter. returns {@code true} if {@link indexwriter} should pool readers even if {@link directoryreader#open(indexwriter, boolean)} has not been called. returns the indexing chain set on {@link indexwriterconfig#setindexingchain(indexingchain)}. returns the max amount of memory each {@link documentswriterperthread} can consume until forcefully flushed. @see indexwriterconfig#setramperthreadhardlimitmb(int) @see indexwriterconfig#setflushpolicy(flushpolicy) returns {@link infostream} used for debugging. @see indexwriterconfig#setinfostream(infostream) javadocs todo: this should be private to the codec, not settable here modified by indexwriterconfig used by indexwriterconfig todo: this should be private to the codec, not settable here customize lucene41postingsformat, passing minblocksize=50, maxblocksize=100 todo: this should be private to the codec, not settable here todo: this should be private to the codec, not settable here todo: this should be private to the codec, not settable here"
org.apache.lucene.index.IndexDeletionPolicy "expert: policy for deletion of stale {@link indexcommit index commits}. implement this interface, and pass it to one of the {@link indexwriter} or {@link indexreader} constructors, to customize when older {@link indexcommit point-in-time commits} are deleted from the index directory. the default deletion policy is {@link keeponlylastcommitdeletionpolicy}, which always removes old commits as soon as a new commit is done (this matches the behavior before 2.2). one expected use case for this (and the reason why it was first this is called once when a writer is first instantiated to give the policy a chance to remove old commit points. the writer locates all index commits present in the index directory and calls this method. the policy may choose to delete some of the commit points, doing so by calling method {@link indexcommit#delete delete()} of {@link indexcommit}. note: the last commitpoint is the most recent one, i.e. the "front index state". be careful not to delete it, unless you know for sure what you are doing, and unless you can afford to lose the index content while doing that. list of current {@link indexcommit point-in-time commits}, sorted by age (the 0th one is the oldest commit). this is called each time the writer completed a commit. this gives the policy a chance to remove old commit points with each commit. the policy may now choose to delete old commit points by calling method {@link indexcommit#delete delete()} of {@link indexcommit}. this method is only called when {@link indexwriter#commit} or {@link indexwriter#close} is called, or possibly not at all if the {@link indexwriter#rollback} is called. note: the last commitpoint is the most recent one, i.e. the "front index state". be careful not to delete it, unless you know for sure what you are doing, and unless you can afford to lose the index content while doing that. list of {@link indexcommit}, sorted by age (the 0th one is the oldest commit)."
org.apache.lucene.index.DocFieldConsumerPerField "processes all occurrences of a single field"
org.apache.lucene.index.FreqProxTermsWriterPerField ") that can // be configured as any number of files 1..n final class freqproxtermswriterperfield extends termshashconsumerperfield implements comparable { final freqproxtermswriter parent; final termshashperfield termshashperfield; final fieldinfo fieldinfo; final documentswriterperthread.docstate docstate; final fieldinvertstate fieldstate; private boolean hasfreq; private boolean hasprox; private boolean hasoffsets; payloadattribute payloadattribute; offsetattribute offsetattribute; public freqproxtermswriterperfield(termshashperfield termshashperfield, freqproxtermswriter parent, fieldinfo fieldinfo) { this.termshashperfield = termshashperfield; this.parent = parent; this.fieldinfo = fieldinfo; docstate = termshashperfield.docstate; fieldstate = termshashperfield.fieldstate; setindexoptions(fieldinfo.getindexoptions()); } @override int getstreamcount() { if (!hasprox) { return 1; } else { return 2; } } @override void finish() { if (haspayloads) { fieldinfo.setstorepayloads(); } } boolean haspayloads; @override void skippinglongterm() {} @override public int compareto(freqproxtermswriterperfield other) { return fieldinfo.name.compareto(other.fieldinfo.name); } // called after flush void reset() { // record, up front, whether our in-ram format will be // with or without term freqs: setindexoptions(fieldinfo.getindexoptions()); payloadattribute = null; } private void setindexoptions(indexoptions indexoptions) { if (indexoptions == null) { // field could later be updated with indexed=true, so set everything on hasfreq = hasprox = hasoffsets = true; } else { hasfreq = indexoptions.compareto(indexoptions.docs_and_freqs) >= 0; hasprox = indexoptions.compareto(indexoptions.docs_and_freqs_and_positions) >= 0; hasoffsets = indexoptions.compareto(indexoptions.docs_and_freqs_and_positions_and_offsets) >= 0; } } @override boolean start(indexablefield[] fields, int count) { for(int i=0;i 0) { termshashperfield.writevint(1, (proxcode= 0; termshashperfield.writevint(1, startoffset - postings.lastoffsets[termid]); termshashperfield.writevint(1, endoffset - startoffset); postings.lastoffsets[termid] = startoffset; } @override void newterm(final int termid) { // first time we're seeing this term since the last // flush assert docstate.testpoint("freqproxtermswriterperfield.newterm start"); freqproxpostingsarray postings = (freqproxpostingsarray) termshashperfield.postingsarray; postings.lastdocids[termid] = docstate.docid; if (!hasfreq) { postings.lastdoccodes[termid] = docstate.docid; } else { postings.lastdoccodes[termid] = docstate.docid  0; if (!hasfreq) { assert postings.termfreqs == null; if (docstate.docid != postings.lastdocids[termid]) { assert docstate.docid > postings.lastdocids[termid]; termshashperfield.writevint(0, postings.lastdoccodes[termid]); postings.lastdoccodes[termid] = docstate.docid - postings.lastdocids[termid]; postings.lastdocids[termid] = docstate.docid; fieldstate.uniquetermcount++; } } else if (docstate.docid != postings.lastdocids[termid]) { assert docstate.docid > postings.lastdocids[termid]:"id: "+docstate.docid + " postings id: "+ postings.lastdocids[termid] + " termid: "+termid; // term not yet seen in the current doc but previously // seen in other doc(s) since the last flush // now that we know doc freq for previous doc, // write it & lastdoccode if (1 == postings.termfreqs[termid]) { termshashperfield.writevint(0, postings.lastdoccodes[termid]|1); } else { termshashperfield.writevint(0, postings.lastdoccodes[termid]); termshashperfield.writevint(0, postings.termfreqs[termid]); } postings.termfreqs[termid] = 1; fieldstate.maxtermfrequency = math.max(1, fieldstate.maxtermfrequency); postings.lastdoccodes[termid] = (docstate.docid - postings.lastdocids[termid]) << 1; postings.lastdocids[termid] = docstate.docid; if (hasprox) { writeprox(termid, fieldstate.position); if (hasoffsets) { postings.lastoffsets[termid] = 0; writeoffsets(termid, fieldstate.offset); } } else { assert !hasoffsets; } fieldstate.uniquetermcount++; } else { fieldstate.maxtermfrequency = math.max(fieldstate.maxtermfrequency, ++postings.termfreqs[termid]); if (hasprox) { writeprox(termid, fieldstate.position-postings.lastpositions[termid]); } if (hasoffsets) { writeoffsets(termid, fieldstate.offset); } } } @override parallelpostingsarray createpostingsarray(int size) { return new freqproxpostingsarray(size, hasfreq, hasprox, hasoffsets); } static final class freqproxpostingsarray extends parallelpostingsarray { public freqproxpostingsarray(int size, boolean writefreqs, boolean writeprox, boolean writeoffsets) { super(size); if (writefreqs) { termfreqs = new int[size]; } lastdocids = new int[size]; lastdoccodes = new int[size]; if (writeprox) { lastpositions = new int[size]; if (writeoffsets) { lastoffsets = new int[size]; } } else { assert !writeoffsets; } //system.out.println("pa init freqs=" + writefreqs + " pos=" + writeprox + " offs=" + writeoffsets); } int termfreqs[]; // # times this term occurs in the current doc int lastdocids[]; // last docid where this term occurred int lastdoccodes[]; // code for prior doc int lastpositions[]; // last position where this term occurred int lastoffsets[]; // last endoffset where this term occurred @override parallelpostingsarray newinstance(int size) { return new freqproxpostingsarray(size, termfreqs != null, lastpositions != null, lastoffsets != null); } @override void copyto(parallelpostingsarray toarray, int numtocopy) { assert toarray instanceof freqproxpostingsarray; freqproxpostingsarray to = (freqproxpostingsarray) toarray; super.copyto(toarray, numtocopy); system.arraycopy(lastdocids, 0, to.lastdocids, 0, numtocopy); system.arraycopy(lastdoccodes, 0, to.lastdoccodes, 0, numtocopy); if (lastpositions != null) { assert to.lastpositions != null; system.arraycopy(lastpositions, 0, to.lastpositions, 0, numtocopy); } if (lastoffsets != null) { assert to.lastoffsets != null; system.arraycopy(lastoffsets, 0, to.lastoffsets, 0, numtocopy); } if (termfreqs != null) { assert to.termfreqs != null; system.arraycopy(termfreqs, 0, to.termfreqs, 0, numtocopy); } } @override int bytesperposting() { int bytes = parallelpostingsarray.bytes_per_posting + 2 ramusageestimator.num_bytes_int; if (lastpositions != null) { bytes += ramusageestimator.num_bytes_int; } if (lastoffsets != null) { bytes += ramusageestimator.num_bytes_int; } if (termfreqs != null) { bytes += ramusageestimator.num_bytes_int; } return bytes; } } public void abort() {} bytesref payload; / walk through all unique text tokens (posting instances) found in this field and serialize them into a single ram segment. todo: break into separate freq and prox writers as codecs; make separate container (tii/tis/skip/) that can be configured as any number of files 1..n called after flush record, up front, whether our in-ram format will be with or without term freqs: field could later be updated with indexed=true, so set everything on system.out.println("writeprox termid=" + termid + " proxcode=" + proxcode); system.out.println("writeoffsets termid=" + termid + " prevoffset=" + prevoffset + " startoff=" + startoffset + " endoff=" + endoffset); first time we're seeing this term since the last flush term not yet seen in the current doc but previously seen in other doc(s) since the last flush now that we know doc freq for previous doc, write it & lastdoccode system.out.println("pa init freqs=" + writefreqs + " pos=" + writeprox + " offs=" + writeoffsets); # times this term occurs in the current doc last docid where this term occurred code for prior doc last position where this term occurred last endoffset where this term occurred nothing to flush, don't bother the codec with the unindexed field confusing: this.indexoptions holds the index options that were current when we first saw this field. but it's possible this has changed, eg when other documents are indexed that cause a "downgrade" of the indexoptions. so we must decode the in-ram buffer according to this.indexoptions, but then write the new segment to the directory according to currentfieldindexoptions: system.out.println("flush readtf=" + readtermfreq + " readpos=" + readpositions + " readoffs=" + readoffsets); make sure fieldinfo.update is working correctly!: system.out.println("term=" + termid); get bytesref todo: really termshashperfield should take over most of this loop, including merge sort of terms from multiple threads and interacting with the termsconsumer, only calling out to us (passing us the docsconsumer) to handle delivery of docs/positions now termstates has numtomerge fieldmergestates which all share the same term. now we must interleave the docid streams. system.out.println(" cycle"); return last doc eof note: we could check here if the docid was deleted, and skip it. however, this is somewhat dangerous because it can yield non-deterministic behavior since we may see the docid before we see the term that caused it to be deleted. this would mean some (but not all) of its postings may make it into the index, which'd alter the docfreq for those terms. we could fix this by doing two passes, ie first sweep marks all del docs, and 2nd sweep does the real flush, but i suspect that'd add too much time to flush. mark it deleted. todo: we could also skip writing its postings; this would be deterministic (just for this term's docs). todo: can we do this reach-around in a cleaner way???? carefully copy over the prox + payload info, changing the format to match lucene's segment format. we did record positions (& maybe payload) and/or offsets this position has a payload"
org.apache.lucene.index.InvertedDocConsumer "abort (called after hitting abortexception) flush a new segment attempt to free ram, returning true if any ram was freed"
org.apache.lucene.index.ReaderManager "utility class to safely share {@link directoryreader} instances across multiple threads, while periodically reopening. this class ensures each reader is closed only once all threads have finished using it. @see searchermanager @lucene.experimental creates and returns a new readermanager from the given {@link indexwriter}. the indexwriter to open the indexreader from. if true, all buffered deletes will be applied (made visible) in the {@link indexsearcher} / {@link directoryreader}. if false, the deletes may or may not be applied, but remain buffered (in indexwriter) so that they will be applied in the future. applying deletes can be costly, so if your app can tolerate deleted documents being returned you might gain some performance by passing false. see {@link directoryreader#openifchanged(directoryreader, indexwriter, boolean)}. @throws ioexception if there is a low-level i/o error creates and returns a new readermanager from the given {@link directory}. the directory to open the directoryreader on. @throws ioexception if there is a low-level i/o error"
org.apache.lucene.index.ThreadAffinityDocumentsWriterThreadPool "a {@link documentswriterperthreadpool} implementation that tries to assign an indexing thread to the same {@link threadstate} each time the thread tries to obtain a {@link threadstate}. once a new {@link threadstate} is creates a new {@link threadaffinitydocumentswriterthreadpool} with a given maximum of {@link threadstate}s. todo -- another thread could lock the minthreadstate we just got while we should somehow prevent this. no new threadstate available we just take the mincontented one this must return a valid thread state since we accessed the synced context in newthreadstate() above. javadoc find the state that has minimum number of threads waiting state is already locked if non-null"
org.apache.lucene.store.MMapDirectory "file-based {@link directory} implementation that uses mmap for reading, and {@link fsdirectory.fsindexoutput} for writing. note: memory mapping uses up a portion of the virtual memory address space in your process equal to the size of the file being mapped. before using this class, be sure your have plenty of virtual address space, e.g. by using a 64 bit jre, or a 32 bit jre with indexes that are guaranteed to fit within the address space. on 32 bit platforms also consult {@link #mmapdirectory(file, lockfactory, int)} if you have problems with mmap failing because of fragmented address space. if you get an outofmemoryexception, it is recommended to reduce the chunk size, until it works. due to  this bug in sun's jre, mmapdirectory's {@link indexinput#close} is unable to close the underlying os file handle. only when gc finally collects the underlying objects, which could be quite some time later, will the file handle be closed. this will consume additional transient disk usage: on windows, attempts to delete or overwrite the files will result in an exception; on other platforms, which typically have a &quot;delete on last close&quot; semantics, while such operations will succeed, the bytes are still consuming space on disk. for many applications this limitation is not a problem (e.g. if you have plenty of disk space, and you don't rely on overwriting files on windows) but it's still an important limitation to be aware of. this class supplies the workaround mentioned in the bug report (see {@link #setuseunmap}), which may fail on non-sun jvms. it forcefully unmaps the buffer on close by using an undocumented internal cleanup functionality. {@link #unmap_supported} is true, if the workaround can be enabled (with no guarantees).  note: accessing this class either directly or indirectly from a thread while it's interrupted can close the underlying channel immediately if at the same time the thread is blocked on io. the channel will remain closed and subsequent access to {@link mmapdirectory} will throw a {@link closedchannelexception}.  default max chunk size. @see #mmapdirectory(file, lockfactory, int) create a new mmapdirectory for the named location. the path of the directory the lock factory to use, or null for the default ({@link nativefslockfactory}); @throws ioexception if there is a low-level i/o error create a new mmapdirectory for the named location and {@link nativefslockfactory}. the path of the directory @throws ioexception if there is a low-level i/o error create a new mmapdirectory for the named location, specifying the maximum chunk size used for memory mapping. the path of the directory the lock factory to use, or null for the default ({@link nativefslockfactory}); maximum chunk size (default is 1 gibytes for 64 bit jvms and 256 mibytes for 32 bit jvms) used for memory mapping.  especially on 32 bit platform, the address space can be very fragmented, so large index files cannot be mapped. using a lower chunk size makes the directory implementation a little bit slower (as the correct chunk may be resolved on lots of seeks) but the chance is higher that mmap does not fail. on 64 bit java platforms, this parameter should always be {@code 1  please note: the chunk size is always rounded down to a power of 2. @throws ioexception if there is a low-level i/o error true, if this platform supports unmapping mmapped files. this method enables the workaround for unmapping the buffers from address space after closing {@link indexinput}, that is mentioned in the bug report. this hack may fail on non-sun jvms. it forcefully unmaps the buffer on close by using an undocumented internal cleanup functionality. note: enabling this is completely unsupported by java and may lead to jvm crashes if indexinput is closed while another thread is still accessing it (sigsegv). @throws illegalargumentexception if {@link #unmap_supported} is false and the workaround cannot be enabled. returns true, if the unmap workaround is enabled. @see #setuseunmap try to unmap the buffer, this method silently fails if no support for that in the jvm. on windows, this leads to the fact, that mmapped files cannot be modified or deleted. returns the current mmap chunk size. @see #mmapdirectory(file, lockfactory, int) creates an indexinput for the file with the given name. maps a file into a set of buffers javadoc @link we always allocate one more buffer, the last one may be a 0 byte one"
org.apache.lucene.store.MergeInfo "a mergeinfo provides information required for a merge context. it is used as part of an {@link iocontext} in case of merge context. creates a new {@link mergeinfo} instance from the values required for a merge {@link iocontext} context. these values are only estimates and are not the actual values."
org.apache.lucene.store.LockObtainFailedException "this exception is thrown when the write.lock could not be acquired. this happens when a writer tries to open an index that another writer already has open. @see lock#obtain(long)"
org.apache.lucene.store.FSDirectory "base class for directory implementations that store index files in the file system.  there are currently three core subclasses:   {@link simplefsdirectory} is a straightforward implementation using java.io.randomaccessfile. however, it has poor concurrent performance (multiple threads will bottleneck) as it synchronizes when multiple threads read from the same file.  {@link niofsdirectory} uses java.nio's filechannel's positional io when reading to avoid synchronization when reading from the same file. unfortunately, due to a windows-only sun jre bug this is a poor choice for windows, but on all other platforms this is the preferred choice. applications using {@link thread#interrupt()} or {@link future#cancel(boolean)} should use {@link simplefsdirectory} instead. see {@link niofsdirectory} java doc for details.  {@link mmapdirectory} uses memory-mapped io when reading. this is a good choice if you have plenty of virtual memory relative to your index size, eg if you are running on a 64 bit jre, or you are running on a 32 bit jre but your index sizes are small enough to fit into the virtual memory space. java has currently the limitation of not being able to unmap files from default read chunk size. this is a conditional default: on 32bit jvms, it defaults to 100 mb. on 64bit jvms, it's integer.max_value. @see #setreadchunksize create a new fsdirectory for the named location (ctor for subclasses). the path of the directory the lock factory to use, or null for the default ({@link nativefslockfactory}); @throws ioexception if there is a low-level i/o error creates an fsdirectory instance, trying to pick the best implementation given the current environment. the directory returned uses the {@link nativefslockfactory}. currently this returns {@link mmapdirectory} for most solaris and windows 64-bit jres, {@link niofsdirectory} for other non-windows jres, and {@link simplefsdirectory} for other jres on windows. it is highly recommended that you consult the implementation's documentation for your platform before using this method. note: this method may suddenly change which implementation is returned from release to release, in the event that higher performance defaults become possible; if the precise implementation is important to your application, please instantiate it directly, instead. for optimal performance you should consider using {@link mmapdirectory} on 64 bit jvms. see above just like {@link #open(file)}, but allows you to also specify a custom {@link lockfactory}. lists all files (not subdirectories) in the directory. this method never returns null (throws {@link ioexception} instead). @throws nosuchdirectoryexception if the directory does not exist, or does exist but is not a directory. @throws ioexception if list() returns null lists all files (not subdirectories) in the directory. @see #listall(file) returns true iff a file with the given name exists. returns the time the named file was last modified. returns the length in bytes of a file in the directory. removes an existing file in the directory. creates an indexoutput for the file with the given name. closes the store to future operations. underlying filesystem directory for debug output. sets the maximum number of bytes read at once from the underlying file during {@link indexinput#readbytes}. the default value is {@link #default_read_chunk_size};  this was introduced due to sun jvm bug 6478546, which throws an incorrect outofmemoryerror when attempting to read too many bytes at once. it only happens on 32bit jvms with a large maximum heap size. changes to this value will not impact any already-opened {@link indexinput}s. you should call this before attempting to open an index on the directory.  note: this value should be as large as possible to reduce any possible performance impact. if you still encounter an incorrect outofmemoryerror, trying lowering the chunk size. the maximum number of bytes to read at once from the underlying file during {@link indexinput#readbytes}. @see #setreadchunksize base class for reading input from a randomaccessfile the underlying randomaccessfile maximum read length on a 32bit jvm to prevent incorrect oom, see lucene-1566 start offset: non-zero in the slice case end offset (start+length) create a new fsindexinput, reading the entire file from path create a new fsindexinput, representing a slice of an existing open file method used for testing. returns true if the underlying file descriptor is valid. writes output with {@link randomaccessfile#write(byte[], int, int)} output methods: random-access methods the underlying filesystem directory files written, but not yet sync'ed lucene-1566 returns the canonical version of the directory, creating it if it doesn't exist. new ctors use always nativefslockfactory as default: for filesystem based lockfactory, delete the lockprefix, if the locks are placed in index dir. if no index dir is given, set ourselves if the lock factory has no lockdir set, use the this directory as lockdir exclude subdirs delete existing, if any name to be hashed lucene-1566 lucene-1566 well, we are sorta? only close the file if this is not a clone remember if the file is open, so that we don't try to close it more than once only close the file if it has not been closed yet suppress so we don't mask original exception pause 5 msec throw original exception"
org.apache.lucene.store.ByteArrayDataInput "datainput backed by a byte array. warning: this class omits all low-level checks. @lucene.experimental note: sets pos to 0, which is not right if you had called reset w/ non-zero offset!! warning: the next ands use 0x0f / 0xf0 - beware copy/paste errors: note: aioobe not eof if you read too much note: aioobe not eof if you read too much"
org.apache.lucene.store.ByteBufferIndexInput "base indexinput implementation that uses an array of bytebuffers to represent a file.  because java's bytebuffer uses an int to address the values, it's necessary to access a file greater integer.max_value in size using multiple byte buffers.  for efficiency, this class requires that the buffers are a power-of-two (chunksizepower). creates a slice of this index input, with the given description, offset, and length. the slice is seeked to the beginning. returns a sliced view from a set of already-existing buffers: the last buffer's limit() will be correct, but you must deal with offset separately (the first buffer will not be adjusted) called when the contents of a buffer will be no longer needed. redundant for speed: buffers[curbufindex] necessary in case offset != 0 and pos = -offset we use >> here to preserve negative, so we will catch aioobe, in case pos + offset overflows. write values, on exception all is unchanged well we could, but this is stupid include our own offset into the final offset: we keep clone.clones, so it shares the same map with original and we have no additional cost on clones register the new clone in our clone list to clean it up on closing: we always allocate one more slice, the last one may be a 0 byte one set the last buffer's limit for the sliced view. make local copy, then un-set early for extra safety unset also all clones' buffers:"
org.apache.lucene.store.Lock "an interprocess mutex lock. typical use might look like: new lock.with(directory.makelock("my.lock")) { public object dobody() { ... code to execute while locked ... } }.run();  @see directory#makelock(string) how long {@link #obtain(long)} waits, in milliseconds, in between attempts to acquire the lock. pass this value to {@link #obtain(long)} to try forever to obtain the lock. attempts to obtain exclusive access and immediately return upon success or failure. iff exclusive access is obtained if a lock obtain called, this failurereason may be set with the "root cause" exception as to why the lock was not obtained. attempts to obtain an exclusive lock within amount of time given. polls once per {@link #lock_poll_interval} (currently 1000) milliseconds until lockwaittimeout is passed. length of time to wait in milliseconds or {@link #lock_obtain_wait_forever} to retry forever if lock was obtained @throws lockobtainfailedexception if lock wait times out @throws illegalargumentexception if lockwaittimeout is out of bounds @throws ioexception if obtain() throws ioexception releases exclusive access. returns true if the resource is currently locked. note that one must still call {@link #obtain()} before using the resource. utility class for executing code with exclusive access. constructs an executor that will grab the named lock. code to execute with exclusive access. calls {@link #dobody} while lock is obtained. blocks if lock cannot be obtained immediately. retries to obtain lock once per second until it is obtained, or until it has tried ten times. lock is released when {@link #dobody} exits. @throws lockobtainfailedexception if lock could not be obtained @throws ioexception if {@link lock#obtain} throws ioexception"
org.apache.lucene.store.NoLockFactory "use this {@link lockfactory} to disable locking entirely. only one instance of this lock is single instance returned whenever makelock is called."
org.apache.lucene.store.BufferedIndexOutput "base implementation class for buffered {@link indexoutput}. expert: implements buffer write. writes bytes at the current position in the output. the bytes to write the number of bytes to write expert: implements buffer write. writes bytes at the current position in the output. the bytes to write the offset in the byte array the number of bytes to write position in file of buffer position in buffer is there enough space in the buffer? we add the data to the end of the buffer if the buffer is full, flush it is data larger then buffer? we flush the buffer and write data at once we fill/flush the buffer (until the input is written) position in the input data if the buffer is full, flush it"
org.apache.lucene.store.RAMOutputStream "a memory-resident {@link indexoutput} implementation. @lucene.internal construct an empty output buffer. copy the current contents of this buffer to the named output. copy the current contents of this buffer to output byte array resets this to an empty file. returns byte usage of all buffers. make sure that we switch to the first needed buffer lazily at the last buffer at the last buffer set the file length in case we seek back and flush() has not been called yet"
org.apache.lucene.store.RAMDirectory "a memory-resident {@link directory} implementation. locking implementation is by default the {@link singleinstancelockfactory} but can be changed with {@link #setlockfactory}. warning: this class is not intended to work with huge indexes. everything beyond several hundred megabytes will waste resources (gc cycles), because it uses an internal buffer size of 1024 bytes, producing millions of {@code byte[1024]} arrays. this class is optimized for small memory-resident indexes. it also has bad concurrency on multithreaded environments. it is recommended to materialize large indexes on disk and use {@link mmapdirectory}, which is a high-performance directory implementation working directly on the file system cache of the operating system, so copying data to java heap space is not useful. constructs an empty {@link directory}. creates a new ramdirectory instance from a different directory implementation. this can be used to load a disk-based index into memory. warning: this class is not intended to work with huge indexes. everything beyond several hundred megabytes will waste resources (gc cycles), because it uses an internal buffer size of 1024 bytes, producing millions of {@code byte[1024]} arrays. this class is optimized for small memory-resident indexes. it also has bad concurrency on multithreaded environments. for disk-based indexes it is recommended to use {@link mmapdirectory}, which is a high-performance directory implementation working directly on the file system cache of the operating system, so copying data to java heap space is not useful. note that the resulting ramdirectory instance is fully independent from the original directory (it is a complete copy). any subsequent changes to the original directory will not be visible in the ramdirectory instance. a directory value @exception ioexception if an error occurs returns true iff the named file exists in this directory. returns the length in bytes of a file in the directory. @throws ioexception if the file does not exist return total size in bytes of all files in this directory. this is currently quantized to ramoutputstream.buffer_size. removes an existing file in the directory. @throws ioexception if the file does not exist creates a new, empty file in the directory with the given name. returns a stream writing this file. returns a new {@link ramfile} for storing data. this method can be overridden to return different {@link ramfile} impls, that e.g. override {@link ramfile#newbuffer(int)}. returns a stream reading an existing file. closes the store to future operations, releasing associated memory.  lock acquisition sequence: ramdirectory, then ramfile  cannot happen note: filemap.keyset().toarray(new string[0]) is broken in non sun jdks, and the code below is resilient to map changes during the array population."
org.apache.lucene.store.CompoundFileWriter "combines multiple files into a single compound file. @see compoundfiledirectory @lucene.internal source file temporary holder for the start of this file's data section the directory which contains the file. create the compound stream in the specified file. the file name is the entire name (no extensions are added). @throws nullpointerexception if dir or name is null returns the directory of the compound file. returns the name of the compound file. closes all resources and writes the entry table @throws illegalstateexception if close() had been called before or if no file has been added to this object copy the contents of the file with specified extension into the provided output stream. before versioning started. segment name is not written in the file names. versioning for the .cfs file versioning for the .cfe file all entries that are written to a sep. file but not yet moved into cfs todo this code should clean up after itself (remove partial .cfs/.cfe) open the compound stream verify that the output length diff is equal to original file copy successful - delete file release the output lock if not successful claim the output and copy all pending files in we are a separate file - push into the pending entries we have been written into the cfs directly - release the lock now prune all pending entries and push them into the cfs"
org.apache.lucene.store.FlushInfo "a flushinfo provides information required for a flush context. it is used as part of an {@link iocontext} in case of flush context. creates a new {@link flushinfo} instance from the values required for a flush {@link iocontext} context. these values are only estimates and are not the actual values."
org.apache.lucene.store.RateLimiter "abstract base class to rate limit io. typically implementations are shared across multiple indexinputs or indexoutputs (for example those involved all merging). those indexinputs and indexoutputs would call {@link #pause} whenever they want to read bytes or write bytes. sets an updated mb per second rate limit. the current mb per second rate limit. pauses, if necessary, to keep the instantaneous io rate at or below the target.  note: the implementation is thread-safe  pause time in nano seconds simple class to rate limit io. mbpersec is the mb/sec max io rate sets an updated mb per second rate limit. the current mb per second rate limit. pauses, if necessary, to keep the instantaneous io rate at or below the target. note: multiple threads may safely use this, however the implementation is not perfectly thread safe but likely in practice this is harmless (just means in some rare cases the rate might exceed the target). it's best to call this with a biggish count, not one byte at a time. pause time in nano seconds todo: we could also allow eg a sub class to dynamically determine the allowed rate, eg if an app wants to change the allowed rate over time or something todo: this is purely instantaneous rate; maybe we should also offer decayed recent history one? while loop because thread.sleep doesn't always sleep enough:"
org.apache.lucene.store.Directory "a directory is a flat list of files. files may be written once, when they are holds the lockfactory instance (implements locking for this directory instance). returns an array of strings, one for each file in the directory. @throws nosuchdirectoryexception if the directory is not prepared for any write operations (such as {@link #createoutput(string, iocontext)}). @throws ioexception in case of other io errors returns true iff a file with the given name exists. removes an existing file in the directory. returns the length of a file in the directory. this method follows the following contract:  throws {@link filenotfoundexception} if the file does not exist returns a value &ge;0 if the file exists, which specifies its length.  the name of the file for which to return the length. @throws filenotfoundexception if the file does not exist. @throws ioexception if there was an io error while retrieving the file's length. creates a new, empty file in the directory with the given name. returns a stream writing this file. ensure that any writes to these files are moved to stable storage. lucene uses this to properly commit changes to the index, to prevent a machine/os crash from corrupting the index.  note: clients may call this method for same files over and over again, so some impls might optimize for that. for other impls the operation can be a noop, for various reasons. returns a stream reading an existing file, with the specified read buffer size. the particular directory implementation may ignore the buffer size. currently the only directory implementations that respect this parameter are {@link fsdirectory} and {@link compoundfiledirectory}. construct a {@link lock}. the name of the lock file attempt to clear (forcefully unlock and remove) the specified lock. only call this at a time when you are certain this lock is no longer in use. name of the lock to be cleared. closes the store. set the lockfactory that this directory instance should use for its locking implementation. each instance of lockfactory should only be used for one directory (ie, do not share a single instance across multiple directories). instance of {@link lockfactory}. get the lockfactory that this directory instance is using for its locking implementation. note that this may be null for directory implementations that provide their own locking implementation. return a string identifier that uniquely differentiates this directory instance from other directory instances. this id should be the same if two directory instances (even in different jvms and/or on different machines) are considered "the same index". this is how locking "scopes" to the right index. copies the file src to {@link directory} to under the new file name dest.  if you want to copy the entire source directory to the destination one, you can do so like this:  directory to; // the directory to copy to for (string file : dir.listall()) { dir.copy(to, file, newfile); // newfile can be either file, or a new name }   note: this method does not check whether dest exist and will overwrite it if it does. creates an {@link indexinputslicer} for the given file name. indexinputslicer allows other {@link directory} implementations to efficiently open one or more sliced {@link indexinput} instances from a single file handle. the underlying file handle is kept open until the {@link indexinputslicer} is closed. @throws ioexception if an {@link ioexception} occurs @lucene.internal @lucene.experimental @throws alreadyclosedexception if this directory is closed allows to create one or more sliced {@link indexinput} instances from a single file handle. some {@link directory} implementations may be able to efficiently map slices of a file into memory when only certain parts of a file are required. @lucene.internal @lucene.experimental returns an {@link indexinput} slice starting at the given offset with the given length. returns an {@link indexinput} slice starting at offset 0 with a length equal to the length of the underlying file @deprecated only for reading cfs files from 3.x indexes. implementation of an indexinput that reads from a portion of a file. expert: implements buffer refill. reads bytes from the current position in the input. the array to read bytes into the offset in the array to start storing bytes the number of bytes to read expert: implements seek. sets current position in this file, where the next {@link #readinternal(byte[],int,int)} will occur. @see #readinternal(byte[],int,int) closes the stream to further operations. for javadocs the directory to copy to newfile can be either file, or a new name can we remove this somehow?"
org.apache.lucene.store.InputStreamDataInput "a {@link datainput} wrapping a plain {@link inputstream}. partially read the input, but no more data available in the stream."
org.apache.lucene.store.NIOFSDirectory "an {@link fsdirectory} implementation that uses java.nio's filechannel's positional read, which allows multiple threads to read from the same file without synchronizing.  this class only uses filechannel when reading; writing is achieved with {@link fsdirectory.fsindexoutput}.  note: niofsdirectory is not recommended on windows because of a bug in how filechannel.read is implemented in sun's jre. inside of the implementation the position is apparently synchronized. see here for details.   note: accessing this class either directly or indirectly from a thread while it's interrupted can close the underlying file descriptor immediately if at the same time the thread is blocked on io. the file descriptor will remain closed and subsequent access to {@link niofsdirectory} will throw a {@link closedchannelexception}. if your application uses either {@link thread#interrupt()} or {@link future#cancel(boolean)} you should use {@link simplefsdirectory} in favor of {@link niofsdirectory}.  create a new niofsdirectory for the named location. the path of the directory the lock factory to use, or null for the default ({@link nativefslockfactory}); @throws ioexception if there is a low-level i/o error create a new niofsdirectory for the named location and {@link nativefslockfactory}. the path of the directory @throws ioexception if there is a low-level i/o error creates an indexinput for the file with the given name. reads bytes with {@link filechannel#read(bytebuffer, long)} javadoc @link javadoc wraps the buffer for nio determine the bytebuffer we should use use our own pre-wrapped bytebuf: lucene-1566 - work around jvm bug by breaking very large reads into chunks propagate oom up and add a hint for 32bit vm with a large chunk size in the fast path."
org.apache.lucene.store.LockVerifyServer "simple standalone server that must be running when you use {@link verifyinglockfactory}. this server simply verifies at most one process holds the lock at a time. run without any args to see usage. @see verifyinglockfactory @see lockstresstest locked"
org.apache.lucene.store.LockFactory "base class for locking implementation. {@link directory} uses instances of this class to implement locking. note that there are some useful tools to verify that your lockfactory is working correctly: {@link verifyinglockfactory}, {@link lockstresstest}, {@link lockverifyserver}. @see lockverifyserver @see lockstresstest @see verifyinglockfactory set the prefix in use for all locks get the prefix in use for all locks return a new lock instance identified by lockname. name of the lock to be attempt to clear (forcefully unlock and remove) the specified lock. only call this at a time when you are certain this lock is no longer in use. name of the lock to be cleared."
org.apache.lucene.store.DataInput "abstract base class for performing read operations of lucene's low-level data types. {@code datainput} may only be used from one thread, because it is not thread safe (it keeps internal state like file position). to allow multithreaded use, every {@code datainput} instance must be cloned before used in another thread. subclasses must therefore implement {@link #clone()}, returning a new {@code datainput} which operates on the same underlying resource, but positioned independently. reads and returns a single byte. @see dataoutput#writebyte(byte) reads a specified number of bytes into an array at the specified offset. the array to read bytes into the offset in the array to start storing bytes the number of bytes to read @see dataoutput#writebytes(byte[],int) reads a specified number of bytes into an array at the specified offset with control over whether the read should be buffered (callers who have their own buffer should pass in "false" for usebuffer). currently only {@link bufferedindexinput} respects this parameter. the array to read bytes into the offset in the array to start storing bytes the number of bytes to read set to false if the caller will handle buffering. @see dataoutput#writebytes(byte[],int) reads two bytes and returns a short. @see dataoutput#writebyte(byte) reads four bytes and returns an int. @see dataoutput#writeint(int) reads an int stored in variable-length format. reads between one and five bytes. smaller values take fewer bytes. negative numbers are not supported.  the format is described further in {@link dataoutput#writevint(int)}. @see dataoutput#writevint(int) this is the original code of this method, but a hotspot bug (see lucene-2975) corrupts the for-loop if readbyte() is inlined. so the loop was unwinded! byte b = readbyte(); int i = b & 0x7f; for (int shift = 7; (b & 0x80) != 0; shift += 7) { b = readbyte(); i |= (b & 0x7f)  the format is described further in {@link dataoutput#writevint(int)}. @see dataoutput#writevlong(long) this is the original code of this method, but a hotspot bug (see lucene-2975) corrupts the for-loop if readbyte() is inlined. so the loop was unwinded! byte b = readbyte(); long i = b & 0x7f; for (int shift = 7; (b & 0x80) != 0; shift += 7) { b = readbyte(); i |= (b & 0x7fl) clones of a stream access the same data, and are positioned at the same point as the stream they were cloned from. expert: subclasses must ensure that clones may be positioned at different points in the input from each other and from the stream they were cloned from. reads a map&lt;string,string&gt; previously written with {@link dataoutput#writestringstringmap(map)}. reads a set&lt;string&gt; previously written with {@link dataoutput#writestringset(set)}. default to ignoring usebuffer entirely warning: the next ands use 0x0f / 0xf0 - beware copy/paste errors:"
org.apache.lucene.store.FSLockFactory "base class for file system based locking implementation. directory for the lock files. set the lock directory. this method can be only called once to initialize the lock directory. it is used by {@link fsdirectory} to set the lock directory to itself. subclasses can also use this method to set the directory in the constructor. retrieve the lock directory."
org.apache.lucene.store.IndexOutput "abstract base class for output to a file in a directory. a random-access output stream. used for all lucene index output operations. {@code indexoutput} may only be used from one thread, because it is not thread safe (it keeps internal state like file position). @see directory @see indexinput forces any buffered output to be written. closes this stream to further operations. returns the current position in this file, where the next write will occur. @see #seek(long) sets current position in this file, where the next write will occur. @see #getfilepointer() @deprecated (4.1) this method will be removed in lucene 5.0 the number of bytes in the file. set the file length. by default, this method does nothing (it's optional for a directory to implement it). but, certain directory implementations (for example @see fsdirectory) can use this to inform the underlying io system to pre-allocate the file to the specified size. if the length is longer than the current file length, the bytes added to the file are undefined. otherwise the file is truncated. file length"
org.apache.lucene.store.FileSwitchDirectory "expert: a directory instance that switches files between two other directory instances. files with the specified extensions are placed in the primary directory; others are placed in the secondary directory. the provided set must not change once passed to this class, and must allow multiple threads to call contains at once. @lucene.experimental return the primary directory return the secondary directory utility method to return a file's extension. lucene-3380: either or both of our dirs could be fsdirs, but if one underlying delegate is an fsdir and mkdirs() has not yet been called, because so far everything is written to the other, in this case, we don't want to throw a nosuchdirectoryexception we got nosuchdirectoryexception from both dirs rethrow the first. we got nosuchdirectoryexception from the secondary, and the primary is empty. we got nosuchdirectoryexception from the primary, and the secondary is empty."
org.apache.lucene.store.RateLimitedIndexOutput "a {@link ratelimiter rate limiting} {@link indexoutput} @lucene.internal todo should we make buffer size configurable"
org.apache.lucene.store.ChecksumIndexInput "reads bytes through to a primary indexinput, computing checksum as it goes. note that you cannot use seek(). @lucene.internal"
org.apache.lucene.store.OutputStreamDataOutput "a {@link dataoutput} wrapping a plain {@link outputstream}."
org.apache.lucene.store.NativeFSLockFactory "implements {@link lockfactory} using native os file locks. note that because this lockfactory relies on java.nio. apis for locking, any problems with those apis will cause locking to fail. specifically, on certain nfs environments the java.nio. locks will fail (the lock can incorrectly be double acquired) whereas {@link simplefslockfactory} worked perfectly in those same environments. for nfs based access to an index, it's recommended that you try {@link simplefslockfactory} first and work around the one limitation that a lock file could be left when the jvm exits abnormally. the primary benefit of {@link nativefslockfactory} is that lock files will be properly removed (by the os) if the jvm has an abnormal exit. note that, unlike {@link simplefslockfactory}, the existence of leftover lock files in the filesystem on exiting the jvm is fine because the os will free the locks held against these files even though the files still remain. if you suspect that this or any other lockfactory is not working properly in your environment, you can easily test it by using {@link verifyinglockfactory}, {@link lockverifyserver} and {@link lockstresstest}. @see lockfactory create a nativefslockfactory instance, with null (unset) lock directory. when you pass this factory to a {@link fsdirectory} subclass, the lock directory is automatically set to the directory itself. be sure to create one instance for each directory your create! create a nativefslockfactory instance, storing lock files into the specified lockdirname: where lock files are create a nativefslockfactory instance, storing lock files into the specified lockdir: where lock files are the javadocs for filechannel state that you should have a single instance of a filechannel (per jvm) for all locking against a given file (locks are tracked per filechannel instance in java 1.4/1.5). even using the same filechannel instance is not completely thread-safe with java 1.4/1.5 though. to work around this, we have a single (static) hashset that contains the file paths of all currently locked locks. this protects against possible cases where different directory instances in one jvm (each with their own nativefslockfactory instance) have set the same lock dir and lock prefix. however, this will not work when lockfactorys are note that this isn't strictly required anymore because the existence of these files does not mean they are locked, but, still do this in case people really want to see the files go away: try to release the lock first - if it's held by another process, this method should not silently fail. note: makelock fixes the lock name by prefixing it w/ lockprefix. therefore it should be called before the code block next which prefixes the given name. as mentioned above, we don't care if the deletion of the file failed. our instance is already locked: ensure that lockdir exists and is a directory. todo: nosuchdirectoryexception instead? make sure nobody else in-process has this lock held already, and, mark it held if not: someone else in this jvm already has the lock: this "reserves" the fact that we are the one thread trying to obtain this lock, so we own the only instance of a channel against this file: on windows, we can get intermittent "access denied" here. so, we treat this as failure to acquire the lock, but, store the reason in case there is in fact a real error case. at least on os x, we will sometimes get an intermittent "permission denied" ioexception, which seems to simply mean "you failed to get the lock". but other ioexceptions could be "permanent" (eg, locking is not supported via the filesystem). so, we record the failure reason here; the timeout obtain (usually the one calling us) will use this as "root cause" if it fails to get the lock. lucene-2421: we don't care anymore if the file cannot be deleted because it's held up by another process (e.g. antivirus). nativefslock does not depend on the existence/absence of the lock file if we don't hold the lock, and somebody still called release(), for example as a result of calling indexwriter.unlock(), we should attempt to obtain the lock and release it. if the obtain fails, it means the lock cannot be released, and we should throw a proper exception rather than silently failing/not doing anything. the test for is islocked is not directly possible with native file locks: first a shortcut, if a lock reference in this instance is available look if lock file is present; if not, there can definitely be no lock! try to obtain and release (if was locked) the lock"
org.apache.lucene.store.SingleInstanceLockFactory "implements {@link lockfactory} for a single in-process instance, meaning all locking will take place through this one instance. only use this {@link lockfactory} when you are certain all indexreaders and indexwriters for a given index are running against a single shared in-process directory instance. this is currently the default locking for ramdirectory. @see lockfactory we do not use the lockprefix at all, because the private hashset instance effectively scopes the locking to this single directory instance."
org.apache.lucene.store.IndexInput "abstract base class for input from a file in a {@link directory}. a random-access input stream. used for all lucene index input operations. {@code indexinput} may only be used from one thread, because it is not thread safe (it keeps internal state like file position). to allow multithreaded use, every {@code indexinput} instance must be cloned before used in another thread. subclasses must therefore implement {@link #clone()}, returning a new {@code indexinput} which operates on the same underlying resource, but positioned independently. lucene never closes cloned {@code indexinput}s, it will only do this on the original one. the original instance must take care that cloned instances throw {@link alreadyclosedexception} when the original one is closed. @see directory resourcedescription should be a non-null, opaque string describing this resource; it's returned from {@link #tostring}. closes the stream to further operations. returns the current position in this file, where the next read will occur. @see #seek(long) sets current position in this file, where the next read will occur. @see #getfilepointer() the number of bytes in the file. {@inheritdoc} warning: lucene never closes cloned {@code indexinput}s, it will only do this on the original one. the original instance must take care that cloned instances throw {@link alreadyclosedexception} when the original one is closed."
org.apache.lucene.store.AlreadyClosedException "this exception is thrown when there is an attempt to access something that has already been closed."
org.apache.lucene.store.LockStressTest "simple standalone tool that forever acquires & releases a lock using a specific lockfactory. run without any args to see usage. @see verifyinglockfactory @see lockverifyserver"
org.apache.lucene.store.SimpleFSDirectory "a straightforward implementation of {@link fsdirectory} using java.io.randomaccessfile. however, this class has poor concurrent performance (multiple threads will bottleneck) as it synchronizes when multiple threads read from the same file. it's usually better to use {@link niofsdirectory} or {@link mmapdirectory} instead. create a new simplefsdirectory for the named location. the path of the directory the lock factory to use, or null for the default ({@link nativefslockfactory}); @throws ioexception if there is a low-level i/o error create a new simplefsdirectory for the named location and {@link nativefslockfactory}. the path of the directory @throws ioexception if there is a low-level i/o error creates an indexinput for the file with the given name. reads bytes with {@link randomaccessfile#seek(long)} followed by {@link randomaccessfile#read(byte[], int, int)}. indexinput methods lucene-1566 - work around jvm bug by breaking very large reads into chunks propagate oom up and add a hint for 32bit vm with a large chunk size in the fast path."
org.apache.lucene.store.RateLimitedDirectoryWrapper "a {@link directory} wrapper that allows {@link indexoutput} rate limiting using {@link iocontext.context io context} specific {@link ratelimiter rate limiters}. @see #setratelimiter(ratelimiter, iocontext.context) @lucene.experimental sets the maximum (approx) mb/sec allowed by all write io performed by {@link indexoutput} sets the rate limiter to be used to limit (approx) mb/sec allowed by all io performed with the given {@link iocontext.context context}. pass null to have no limit.  passing an instance of rate limiter compared to setting it using {@link #setmaxwritembpersec(double, iocontext.context)} allows to use the same limiter instance across several directories globally limiting io across them. @throws illegalargumentexception if context is null @throws alreadyclosedexception if the {@link directory} is already closed @lucene.experimental see {@link #setmaxwritembpersec}. @throws illegalargumentexception if context is null @throws alreadyclosedexception if the {@link directory} is already closed @lucene.experimental we need to be volatile here to make sure we see all the values that are set / modified concurrently cross the mem barrier again"
org.apache.lucene.store.SimpleFSLockFactory "implements {@link lockfactory} using {@link file#createnewfile()}. note: the javadocs for file.createnewfile contain a vague yet spooky warning about not using the api for file locking. this warning was added due to this bug, and in fact the only known problem with using this api for locking is that the lucene write lock may not be released when the jvm exits abnormally. when this happens, a {@link lockobtainfailedexception} is hit when trying to create a writer, in which case you need to explicitly clear the lock file first. you can either manually remove the file, or use the {@link org.apache.lucene.index.indexwriter#unlock(directory)} api. but, first be certain that no writer is in fact writing to the index otherwise you can easily corrupt your index. if you suspect that this or any other lockfactory is not working properly in your environment, you can easily test it by using {@link verifyinglockfactory}, {@link lockverifyserver} and {@link lockstresstest}. @see lockfactory create a simplefslockfactory instance, with null (unset) lock directory. when you pass this factory to a {@link fsdirectory} subclass, the lock directory is automatically set to the directory itself. be sure to create one instance for each directory your create! instantiate using the provided directory (as a file instance). where lock files should be instantiate using the provided directory name (string). where lock files should be ensure that lockdir exists and is a directory: todo: nosuchdirectoryexception instead?"
org.apache.lucene.store.NoSuchDirectoryException "this exception is thrown when you try to list a non-existent directory."
org.apache.lucene.store.TrackingDirectoryWrapper "a delegating directory that records which files were written to and deleted. maybe clone before returning.... all callers are cloning anyway...."
org.apache.lucene.store.IOContext "iocontext holds additional details on the merge/search context. a iocontext object can never be initialized as null as passed as a parameter to either {@link org.apache.lucene.store.directory#openinput(string, iocontext)} or {@link org.apache.lucene.store.directory#createoutput(string, iocontext)} context is a enumerator which specifies the context in which the directory is being used for. an object of a enumerator context type this constructor is used to initialize a {@link iocontext} instance with a new value for the readonce variable. {@link iocontext} object whose information is used to create the new instance except the readonce variable. the new {@link iocontext} object will use this value for readonce."
org.apache.lucene.store.ChecksumIndexOutput "writes bytes through to a primary indexoutput, computing checksum. note that you cannot use seek(). @lucene.internal writes the checksum"
org.apache.lucene.store.NRTCachingDirectory "wraps a {@link ramdirectory} around any provided delegate directory, to be used during nrt search. this class is likely only useful in a near-real-time context, where indexing rate is lowish but reopen rate is highish, resulting in many tiny files being written. this directory keeps such segments (as well as the segments produced by merging them, as long as they are small enough), in ram. this is safe to use: when your app calls {indexwriter#commit}, all cached files will be flushed from the cached and sync'd. here's a simple example usage:  directory fsdir = fsdirectory.open(new file("/path/to/index")); nrtcachingdirectory cachedfsdir = new nrtcachingdirectory(fsdir, 5.0, 60.0); indexwriterconfig conf = new indexwriterconfig(version.lucene_32, analyzer); indexwriter writer = new indexwriter(cachedfsdir, conf);  this will cache all newly flushed segments, all merges whose expected segment size is  @lucene.experimental we will cache a newly returns how many bytes are being used by the ramdirectory cache close this directory, which flushes any cached files to the delegate and then closes the delegate. subclass can override this to customize logic; return true if this file should be written to the ramdirectory. javadocs todo - let subclass dictate policy...? - rename to mergecacheingdir? nrtcachingdir lucene-1468: our nrtcachingdirectory will actually exist (ramdir!), but if the underlying delegate is an fsdir and mkdirs() has not yet been called, because so far everything is a cached write, in this case, we don't want to throw a nosuchdirectoryexception cannot do this -- if lucene calls createoutput but file already exists then this falsely trips: assert !files.contains(f): "file \"" + f + "\" is in both dirs"; however, if there are no cached files, then the directory truly does not "exist" this is fine: file may not exist this is fine: file may not exist note: technically we shouldn't have to do this, ie, indexwriter should have sync'd all files, but we do it for defensive reasons... or in case the app is doing something custom (creating outputs directly w/o using indexwriter): system.out.println(thread.currentthread().getname() + ": cache check merge=" + merge + " size=" + (merge==null ? 0 : merge.estimatedmergebytes)); only let one thread uncache at a time; this only happens during commit() or close(): another thread beat us... lock order: uncachelock -> this must sync here because other sync methods have if (cache.fileexists(name)) { ... } else { ... }:"
org.apache.lucene.store.ByteArrayDataOutput "dataoutput backed by a byte array. warning: this class omits most low-level checks, so be sure to test heavily with assertions enabled. @lucene.experimental"
org.apache.lucene.store.LockReleaseFailedException "this exception is thrown when the write.lock could not be released. @see lock#release()"
org.apache.lucene.store.DataOutput "abstract base class for performing write operations of lucene's low-level data types. {@code dataoutput} may only be used from one thread, because it is not thread safe (it keeps internal state like file position). writes a single byte.  the most primitive data type is an eight-bit byte. files are accessed as sequences of bytes. all other data types are defined as sequences of bytes, so file formats are byte-order independent. @see indexinput#readbyte() writes an array of bytes. the bytes to write the number of bytes to write @see datainput#readbytes(byte[],int,int) writes an array of bytes. the bytes to write the offset in the byte array the number of bytes to write @see datainput#readbytes(byte[],int,int) writes an int as four bytes.  32-bit unsigned integer written as four bytes, high-order bytes first. @see datainput#readint() writes a short as two bytes. @see datainput#readshort() writes an int in a variable-length format. writes between one and five bytes. smaller values take fewer bytes. negative numbers are supported, but should be avoided. vbyte is a variable-length format for positive integers is defined where the high-order bit of each byte indicates whether more bytes remain to be read. the low-order seven bits are appended as increasingly more significant bits in the resulting integer value. thus values from zero to 127 may be stored in a single byte, values from 128 to 16,383 may be stored in two bytes, and so on. vbyte encoding example       value byte 1 byte 2 byte 3   0 00000000     1 00000001     2 00000010     ...      127 01111111     128 10000000 00000001    129 10000001 00000001    130 10000010 00000001    ...      16,383 11111111 01111111    16,384 10000000 10000000 00000001   16,385 10000001 10000000 00000001   ...      this provides compression while still being efficient to decode. smaller values take fewer bytes. negative numbers are supported, but should be avoided. @throws ioexception if there is an i/o error writing to the underlying medium. @see datainput#readvint() writes a long as eight bytes.  64-bit unsigned integer written as eight bytes, high-order bytes first. @see datainput#readlong() writes an long in a variable-length format. writes between one and nine bytes. smaller values take fewer bytes. negative numbers are not supported.  the format is described further in {@link dataoutput#writevint(int)}. @see datainput#readvlong() writes a string.  writes strings as utf-8 encoded bytes. first the length, in bytes, is written as a {@link #writevint vint}, followed by the bytes. @see datainput#readstring() copy numbytes bytes from input to ourself. writes a string map.  first the size is written as an {@link #writeint(int) int32}, followed by each key-value pair written as two consecutive {@link #writestring(string) string}s. input map. may be null (equivalent to an empty map) writes a string set.  first the size is written as an {@link #writeint(int) int32}, followed by each value written as a {@link #writestring(string) string}. input set. may be null (equivalent to an empty set)"
org.apache.lucene.store.VerifyingLockFactory "a {@link lockfactory} that wraps another {@link lockfactory} and verifies that each lock obtain/release is "correct" (never results in two processes holding the lock at the same time). it does this by contacting an external server ({@link lockverifyserver}) to assert that at most one process holds the lock at a time. to use this, you should also run {@link lockverifyserver} on the host & port matching what you pass to the constructor. @see lockverifyserver @see lockstresstest should be a unique id across all clients the lockfactory that we are testing host or ip where {@link lockverifyserver} is running the port {@link lockverifyserver} is listening on"
org.apache.lucene.store.CompoundFileDirectory "class for accessing a compound stream. this class implements a directory, but is limited to only read operations. directory methods that would normally modify data throw an exception.  all files belonging to a segment have the same name with varying extensions. the extensions correspond to the different file formats used by the {@link codec}. when using the compound file format these files are collapsed into a single .cfs file (except for the {@link livedocsformat}, with a corresponding .cfe file indexing its sub-files.  files:  .cfs: an optional "virtual" file consisting of all the other index files for systems that frequently run out of file handles. .cfe: the "virtual" compound file's entry table holding all entries in the corresponding .cfs file.  description:  compound (.cfs) --&gt; header, filedata filecount compound entry table (.cfe) --&gt; header, filecount, &lt;filename, dataoffset, datalength&gt; filecount header --&gt; {@link codecutil#writeheader codecheader} filecount --&gt; {@link dataoutput#writevint vint} dataoffset,datalength --&gt; {@link dataoutput#writelong uint64} filename --&gt; {@link dataoutput#writestring string} filedata --&gt; raw file data  notes:  filecount indicates how many files are contained in this compound file. the entry table that follows has that many entries. each directory entry contains a long pointer to the start of this file's data section, the files length, and a string with that file's name.  @lucene.experimental offset/length for a slice inside of a compound file create a new compoundfiledirectory. helper method that reads cfs entries from an input stream returns an array of strings, one for each file in the directory. returns true iff a file with the given name exists. not implemented @throws unsupportedoperationexception always: not supported by cfs not implemented @throws unsupportedoperationexception always: not supported by cfs returns the length of a file in the directory. @throws ioexception if the file does not exist not implemented @throws unsupportedoperationexception always: not supported by cfs javadocs javadocs javadocs read the first vint. if it is negative, it's the version number otherwise it's the count (pre-3.1 indexes) impossible for 3.0 to have 63 files in a .cfs, cfs writer was not visible and separate norms/etc are outside of cfs. todo remove once 3.x is not supported anymore this is needed until java 7's real try-with-resources: it's a post-3.1 index, read the count. read the directory and init files fix the id to not include the segment names. this is relevant for pre-3.1 indexes. set length of the previous entry set the length of the final entry allow double close - usually to be consistent with other closeables already closed add the segment name"
org.apache.lucene.store.RAMInputStream "a memory-resident {@link indexinput} implementation. @lucene.internal make sure that we switch to the first needed buffer lazily nothing to do here end of file reached, no more buffers left force eof if a read takes place at this position"
org.apache.lucene.store.RAMFile "represents a file in ram as a list of byte[] buffers. @lucene.internal expert: allocate a new buffer. subclasses can allocate differently. size of allocated buffer. buffer. file used as buffer, in no ramdirectory for non-stream access from thread that might be concurrent with writing"
org.apache.lucene.store.BufferedIndexInput "base implementation class for buffered {@link indexinput}. default buffer size set to 1024 a buffer size for merges set to 4096 inits bufferedindexinput with a specific buffersize change the buffer size used by this indexinput returns buffer size. @see #setbuffersize expert: implements buffer refill. reads bytes from the current position in the input. the array to read bytes into the offset in the array to start storing bytes the number of bytes to read expert: implements seek. sets current position in this file, where the next {@link #readinternal(byte[],int,int)} will occur. @see #readinternal(byte[],int,int) flushes the in-memory bufer to the given output, copying at most numbytes.  note: this method does not refill the buffer, however it does advance the buffer position. number of bytes actually flushed from the in-memory buffer. returns default buffer sizes for the given {@link iocontext} the normal read buffer size defaults to 1024, but increasing this during merging seems to yield performance gains. however we don't want to increase it too much because there are quite a few bufferedindexinputs lucene-888 for details. position in file of buffer end of valid bytes next byte to read resize the existing buffer and carefully save as many bytes as possible starting from the current bufferposition subclasses can do something here the buffer contains enough data to satisfy this request to allow b to be null if len is 0... the buffer does not have enough data. first serve all we've got. and now, read the remaining 'len' bytes: if the amount left to read is small enough, and we are allowed to use our buffer, do it in the usual buffered way: fill the buffer and copy from it: throw an exception when refill() could not read len bytes: the amount left to read is larger than the buffer or we've been asked to not use our buffer - there's no performance reason not to read it all at once. note that unlike the previous code of this function, there is no need to do a seek here, because there's no need to reread what we had in the buffer. trigger refill() on read warning: the next ands use 0x0f / 0xf0 - beware copy/paste errors: don't read past eof allocate buffer lazily seek within buffer trigger refill() on read()"
org.apache.lucene.LucenePackage "lucene's package information, including version. return lucene's package, including version information. can't construct"
org.apache.lucene.search.MaxNonCompetitiveBoostAttributeImpl "implementation class for {@link maxnoncompetitiveboostattribute}. @lucene.internal"
org.apache.lucene.search.Filter "abstract base class for restricting which documents may be returned during searching. creates a {@link docidset} enumerating the documents that should be permitted in search results. note: null can be returned if no documents are accepted by this filter.  note: this method will be called once per segment in the index during searching. the returned {@link docidset} must refer to document ids for that segment, not for the top-level reader. a {@link atomicreadercontext} instance opened on the index currently searched on. note, it is likely that the provided reader info does not represent the whole underlying index i.e. if the index has more than one segment the given reader only represents a single segment. the provided context is always an atomic context, so you can call {@link atomicreader#fields()} on the context's reader, for example. bits that represent the allowable docs to match (typically deleted docs but possibly filtering other documents) docidset that provides the documents which should be permitted or prohibited in search results. note: null can be returned if no documents will be accepted by this filter. javadocs javadocs"
org.apache.lucene.search.ConjunctionScorer "scorer for conjunctions, sets of queries, all of which are required. if even one of the sub-scorers does not have any documents, this scorer should not attempt to do any more work. sort the array the first time... we don't need to sort the array in any future calls because we know it will already start off sorted (all scorers on same doc). note that this comparator is not consistent with equals! also we use mergesort here to be stable (so order of scoreres that match on first document keeps preserved): sort the array note: donext() must be called before the re-sorting of the array later on. the reason is this: assume there are 5 scorers, whose first docs are 1, 2, 3, 5, 5 respectively. sorting (above) leaves the array as is. calling donext() here advances all the first scorers to 5 (or a larger doc id they all agree on). however, if we re-sort before donext() is called, the order will be 5, 3, 2, 1, 5 and then donext() will stop immediately, since the first scorer's docs equals the last one. so the invariant that after calling donext() all scorers are on the same doc id is broken. the scorers did not agree on any document. if first-time skip distance is any predictor of scorer sparseness, then we should always try to skip first on those scorers. keep last scorer in it's last place (it will be the first to be skipped on), but reverse all of the others so that they will be skipped on in order of original high skip. todo: sum into a double and cast to float if we ever send required clauses to bs1"
org.apache.lucene.search.ReqOptSumScorer "a scorer for queries with a required part and an optional part. delays skipto() on the optional part until a score() is needed.  this scorer implements {@link scorer#advance(int)}. the scorers passed from the constructor. these are set to null as soon as their next() or skipto() returns false. construct a reqoptscorer. the required scorer. this must match. the optional scorer. this is used for scoring only. returns the score of the current document matching the query. initially invalid, until {@link #nextdoc()} is called the first time. score of the required scorer, eventually increased by the score of the optional scorer when it also matches the current document. todo: sum into a double and cast to float if we ever send required clauses to bs1 we might have deferred advance()"
org.apache.lucene.search.NRTManagerReopenThread "utility class that runs a reopen thread to periodically reopen the nrt searchers in the provided {@link nrtmanager}.  typical usage looks like this:  ... open your own writer ... nrtmanager manager = new nrtmanager(writer); // refreshes searcher every 5 seconds when nobody is waiting, and up to 100 msec delay // when somebody is waiting: nrtmanagerreopenthread reopenthread = new nrtmanagerreopenthread(manager, 5.0, 0.1); reopenthread.setname("nrt reopen thread"); reopenthread.setpriority(math.min(thread.currentthread().getpriority()+2, thread.max_priority)); reopenthread.setdaemon(true); reopenthread.start();  then, for each incoming query, do this:  // for each incoming query: indexsearcher searcher = manager.get(); try { // use searcher to search... } finally { manager.release(searcher); }  you should make changes using the nrtmanager; if you later need to obtain a searcher reflecting those changes:  // ... or updatedocument, deletedocuments, etc: long gen = manager.adddocument(...); // returned searcher is guaranteed to reflect the just added document indexsearcher searcher = manager.get(gen); try { // use searcher to search... } finally { manager.release(searcher); }  when you are done be sure to close both the manager and the reopen thrad:  reopenthread.close(); manager.close();  @lucene.experimental create nrtmanagerreopenthread, to periodically reopen the nrt searcher. maximum time until a new reader must be opened; this sets the upper bound on how slowly reopens may occur mininum time until a new reader can be opened; this sets the lower bound on how quickly reopens may occur, when a caller is waiting for a specific indexing change to become visible. refreshes searcher every 5 seconds when nobody is waiting, and up to 100 msec delay when somebody is waiting: for each incoming query: use searcher to search... ... or updatedocument, deletedocuments, etc: returned searcher is guaranteed to reflect the just added document use searcher to search... system.out.println("nrt: set finish"); system.out.println(thread.currentthread().getname() + ": force wakeup waitinggen=" + waitinggen + " applydeletes=" + applydeletes); todo: maybe use private thread ticktock timer, in case clock shift messes up nanotime? system.out.println("reopen: start"); todo: try to guestimate how long reopen might take based on past data? system.out.println("reopen: cycle"); true if we have someone waiting for reopen'd searcher: system.out.println("reopen: sleep " + (sleepns/1000000.0) + " ms (haswaiting=" + haswaiting + ")"); system.out.println("nrt: set finish on interrupt"); system.out.println("reopen: finish"); system.out.println("reopen: start haswaiting=" + haswaiting); final long t0 = system.nanotime(); system.out.println("reopen took " + ((system.nanotime()-t0)/1000000.0) + " msec"); system.out.println(thread.currentthread().getname() + ": ioe"); ioe.printstacktrace(); system.out.println("reopen exc"); t.printstacktrace(system.out);"
org.apache.lucene.search.ConjunctionTermScorer "scorer for conjunctions, sets of terms, all of which are required. sort the array the first time to allow the least frequent docsenum to lead the matching. least frequent docsenum leads the intersection docsenum beyond the current doc - break and advance lead success - all docsenums are on the same doc advance head for next iteration todo: sum into a double and cast to float if we ever send required clauses to bs1"
org.apache.lucene.search.TermRangeTermsEnum "subclass of filteredtermenum for enumerating all terms that match the specified range parameters. term enumerations are always ordered by {@link #getcomparator}. each term in the enumeration is greater than all that precede it. enumerates all terms greater/equal than lowerterm but less/equal than upperterm. if an endpoint is null, it is said to be "open". either or both endpoints may be open. open endpoints may not be exclusive (you can't select all but the first or last term without explicitly specifying the term to exclude.) termsenum to filter the term text at the lower end of the range the term text at the upper end of the range if true, the lowerterm is included in the range. if true, the upperterm is included in the range. if beyond the upper term, or is exclusive and this is equal to the upper term, break out do a little bit of normalization... open ended range queries should always be inclusive. use this field's default sort ordering"
org.apache.lucene.search.DisjunctionMaxScorer "the scorer for disjunctionmaxquery. the union of all documents generated by the the subquery scorers is generated in document number order. the score for each document is the maximum of the scores computed by the subquery scorers that generate that document, plus tiebreakermultiplier times the sum of the scores for the other subqueries that generate the document. multiplier applied to non-maximum-scoring subqueries for a document as they are summed into the result. used when scoring currently matching doc. creates a new instance of disjunctionmaxscorer the weight to be used. multiplier applied to non-maximum-scoring subqueries for a document as they are summed into the result. the sub scorers this scorer should iterate on the actual number of scorers to iterate on. note that the array's length may be larger than the actual number of scorers. determine the current document score. initially invalid, until {@link #nextdoc()} is called the first time. score of the current generated document recursively iterate all subscorers that generated last doc computing sum and max recursively iterate all subscorers that generated last doc computing sum and max"
org.apache.lucene.search.NumericRangeFilter "a {@link filter} that only accepts numeric values within a specified range. to use this, you must first index the numeric values using {@link intfield}, {@link floatfield}, {@link longfield} or {@link doublefield} (expert: {@link numerictokenstream}). you create a new numericrangefilter with the static factory methods, eg:  filter f = numericrangefilter.newfloatrange("weight", 0.03f, 0.10f, true, true);  accepts all documents whose float valued "weight" field ranges from 0.03 to 0.10, inclusive. see {@link numericrangequery} for details on how lucene indexes and searches numeric valued fields. factory that creates a numericrangefilter, that filters a long range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangefilter, that queries a long range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangefilter, that filters a int range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangefilter, that queries a int range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangefilter, that filters a double range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link double#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == double.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangefilter, that queries a double range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link double#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == double.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangefilter, that filters a float range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link float#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == float.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangefilter, that queries a float range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link float#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == float.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. returns true if the lower endpoint is inclusive returns true if the upper endpoint is inclusive returns the lower value of this range filter returns the upper value of this range filter returns the precision step. for javadocs for javadocs for javadocs for javadocs for javadocs for javadocs"
org.apache.lucene.search.BooleanQuery "a query that matches documents matching boolean combinations of other queries, e.g. {@link termquery}s, {@link phrasequery}s or other booleanquerys. thrown when an attempt is made to add more than {@link #getmaxclausecount()} clauses. this typically happens if a prefixquery, fuzzyquery, wildcardquery, or termrangequery is expanded to many terms during search. return the maximum number of clauses permitted, 1024 by default. attempts to add more than the permitted number of clauses cause {@link toomanyclauses} to be thrown. @see #setmaxclausecount(int) set the maximum number of clauses permitted per booleanquery. default value is 1024. constructs an empty boolean query. constructs an empty boolean query. {@link similarity#coord(int,int)} may be disabled in scoring, as appropriate. for example, this score factor does not make sense for most automatically generated queries, like {@link wildcardquery} and {@link fuzzyquery}. disables {@link similarity#coord(int,int)} in scoring. returns true iff {@link similarity#coord(int,int)} is disabled in scoring for this query instance. @see #booleanquery(boolean) specifies a minimum number of the optional booleanclauses which must be satisfied.  by default no optional clauses are necessary for a match (unless there are no required clauses). if this method is used, then the specified number of clauses is required.   use of this method is totally independent of specifying that any specific clauses are required (or prohibited). this number will only be compared against the number of matching optional clauses.  the number of optional clauses that must match gets the minimum number of the optional booleanclauses which must be satisfied. adds a clause to a boolean query. @throws toomanyclauses if the new number of clauses exceeds the maximum clause number @see #getmaxclausecount() adds a clause to a boolean query. @throws toomanyclauses if the new number of clauses exceeds the maximum clause number @see #getmaxclausecount() returns the set of clauses in this query. returns the list of clauses in this query. returns an iterator on the clauses in this query. it implements the {@link iterable} interface to make it possible to do: for (booleanclause clause : booleanquery) {} expert: the weight for booleanquery, used to normalize, score and explain these queries. note: this api and implementation is subject to change suddenly in the next release. the similarity implementation. prints a returns true iff o is equal to this. returns a hash code value for this object. num optional + num required call sumofsquaredweights for all clauses in case of side effects sum sub weights only add to sum for non-prohibited clauses boost each sub-weight lucene-4300: in most cases of maxoverlap=1, bq rewrites itself away, so coord() is not applied. but when bq cannot optimize itself away for a single clause (minnrshouldmatch, prohibited clauses, etc), its important not to apply coord(1,1) for consistency, it might not be 1.0f incorporate boost normalize all clauses, (even if prohibited in case of side affects) eliminate wrapper specialized scorer for term conjunctions note: we could also use booleanscorer, if we knew this booleanquery was embedded in another booleanquery that was also using booleanscorer (ie, booleanscorer can nest). but this is hard to detect and we never do so today... (ie, we only return booleanscorer for topscorer): check if we can return a booleanscorer no required and optional clauses. either >1 req scorer, or there are 0 req scorers and at least 1 optional scorer. therefore if there are not enough optional scorers no documents will be matched by the query return a booleanscorer2 todo: fix scorer api to specify "needsscores" up front, so we can do match-only if caller doesn't needs scores bs2 (in-order) will be used by scorer() scorer() will return an out-of-order scorer if requested. optimize 1-clause queries just return clause rewrite first incorporate boost if rewrite was no-op then clone before boost since the booleanquery only has 1 clause, the booleanquery will be written out. therefore the rewritten query's boost must incorporate both the clause's boost, and the boost of the booleanquery itself recursively rewrite clause rewrote: must clone the booleanquery clone is lazily initialized so only initialize it if a rewritten clause differs from the original clause (and hasn't been initialized already). if nothing differs, the clone isn't needlessly some clauses rewrote no clauses rewrote inherit javadoc wrap sub-bools in parens"
org.apache.lucene.search.ScoringRewrite "base rewrite method that translates each term into a query, and keeps the scores as computed by the query.  @lucene.internal only public to be accessible by spans package. a rewrite method that first translates each term into {@link booleanclause.occur#should} clause in a booleanquery, and keeps the scores as computed by the query. note that typically such scores are meaningless to the like {@link #scoring_boolean_query_rewrite} except scores are not computed. instead, each matching document receives a constant score equal to the query's boost. note: this rewrite method will hit {@link booleanquery.toomanyclauses} if the number of terms exceeds {@link booleanquery#getmaxclausecount}. @see multitermquery#setrewritemethod this method is called after every new term to check if the number of max clauses (e.g. in booleanquery) is not exceeded. throws the corresponding {@link runtimeexception}. special implementation of bytesstartarray that keeps parallel arrays for boost and docfreq todo: if empty boolean query return nullquery? strip the scores off duplicate term: update docfreq new entry: we populate the entry initially"
org.apache.lucene.search.FieldComparator "expert: a fieldcomparator compares hits so as to determine their sort order when collecting the top results with {@link topfieldcollector}. the concrete public fieldcomparator classes here correspond to the sortfield types. this api is designed to achieve high performance sorting, by exposing a tight interaction with {@link fieldvaluehitqueue} as it visits hits. whenever a hit is competitive, it's enrolled into a virtual slot, which is an int ranging from 0 to numhits-1. the {@link fieldcomparator} is made aware of segment transitions during searching in case any internal state it's tracking needs to be recomputed during these transitions. a comparator must define these functions:   {@link #compare} compare a hit at 'slot a' with hit 'slot b'.  {@link #setbottom} this method is called by {@link fieldvaluehitqueue} to notify the fieldcomparator of the current weakest ("bottom") slot. note that this slot may not hold the weakest value according to your comparator, in cases where your comparator is not the primary one (ie, is only used to break ties from the comparators before it).  {@link #comparebottom} compare a new hit (docid) against the "weakest" (bottom) entry in the queue.  {@link #copy} installs a new hit into the priority queue. the {@link fieldvaluehitqueue} calls this method when a new hit is competitive.  {@link #setnextreader(atomicreadercontext)} invoked when the search is switching to the next segment. you may need to update internal state of the comparator, for example retrieving new values from the {@link fieldcache}.  {@link #value} return the sort value stored in the specified slot. this is only called at the end of the search, in order to populate {@link fielddoc#fields} when returning the top results.  @lucene.experimental compare hit at slot1 with hit at slot2. 1 first slot to compare 2 second slot to compare n  0 if the slot2's value is sorted before slot1 and 0 if they are equal set the bottom slot, ie the "weakest" (sorted last) entry in the queue. when {@link #comparebottom} is called, you should compare against this slot. this will always be called before {@link #comparebottom}. the currently weakest (sorted last) slot in the queue compare the bottom of the queue with doc. this will only invoked after setbottom has been called. this should return the same result as {@link #compare(int,int)}} as if bottom were slot1 and the new document were slot 2. for a search that hits many results, this method will be the hotspot (invoked by far the most frequently). that was hit n  0 if the doc's value is sorted before the bottom entry and 0 if they are equal. this method is called when a new hit is competitive. you should copy any state associated with this document that will be required for future comparisons, into the specified slot. which slot to copy the hit to docid relative to current reader set a new {@link atomicreadercontext}. all subsequent docids are relative to the current reader (you must add docbase if you need to map it to a top-level docid). current reader context comparator to use for this segment; most comparators can just return "this" to reuse the same comparator across segments @throws ioexception if there is a low-level io error sets the scorer to use in case a document's score is needed. scorer instance that you should use to obtain the current hit's score, if necessary. return the actual value in the slot. the value in this slot returns -1 if first is less than second. default impl to assume the type implements comparable and invoke .compareto; be sure to override this method if your fieldcomparator's type isn't a comparable or if your values may sometimes be null returns negative result if the doc's value is less than the provided value. base fieldcomparator class for numeric types parses field's values as byte (using {@link fieldcache#getbytes} and sorts by ascending value parses field's values as double (using {@link fieldcache#getdoubles} and sorts by ascending value uses float index values to sort by ascending value parses field's values as float (using {@link fieldcache#getfloats} and sorts by ascending value parses field's values as short (using {@link fieldcache#getshorts} and sorts by ascending value parses field's values as int (using {@link fieldcache#getints} and sorts by ascending value loads int index values and sorts by ascending value. parses field's values as long (using {@link fieldcache#getlongs} and sorts by ascending value sorts by descending relevance. note: if you are sorting only by descending relevance and then secondarily by ascending docid, performance is faster using {@link topscoredoccollector} directly (which {@link indexsearcher#search} uses when no {@link sort} is specified). sorts by ascending docid sorts by field's natural term sort order, using ordinals. this is functionally equivalent to {@link org.apache.lucene.search.fieldcomparator.termvalcomparator}, but it first resolves the string to their relative ordinal positions (using the index returned by {@link fieldcache#gettermsindex}), and does most comparisons using the ordinals. for medium to large results, this comparator will be much faster than {@link org.apache.lucene.search.fieldcomparator.termvalcomparator}. for very small result sets it may be slower. ords for each slot. @lucene.internal values for each slot. @lucene.internal which reader last copied a value into the slot. when we compare two slots, we just compare-by-ord if the readergen is the same; else we must compare the values (slower). @lucene.internal gen of current reader we are on. @lucene.internal current reader's doc ord/values. @lucene.internal bottom slot, or -1 if queue isn't full yet @lucene.internal bottom ord (same as ords[bottomslot] once bottomslot is set). cached for faster compares. @lucene.internal true if current bottom slot matches the current reader. @lucene.internal bottom value (same as values[bottomslot] once bottomslot is set). cached for faster compares. @lucene.internal base class for specialized (per bit width of the ords) per-segment comparator. note: this is messy; we do this only because hotspot can't reliably inline the underlying array access when looking up doc->ord @lucene.internal sorts by field's natural term sort order, using ordinals; this is just like {@link org.apache.lucene.search.fieldcomparator.termvalcomparator} except it uses docvalues to retrieve the sort ords saved during indexing. ords for each slot. @lucene.internal values for each slot. @lucene.internal which reader last copied a value into the slot. when we compare two slots, we just compare-by-ord if the readergen is the same; else we must compare the values (slower). @lucene.internal gen of current reader we are on. @lucene.internal current reader's doc ord/values. @lucene.internal comparator for comparing by value. @lucene.internal bottom slot, or -1 if queue isn't full yet @lucene.internal bottom ord (same as ords[bottomslot] once bottomslot is set). cached for faster compares. @lucene.internal true if current bottom slot matches the current reader. @lucene.internal bottom value (same as values[bottomslot] once bottomslot is set). cached for faster compares. @lucene.internal @lucene.internal base class for specialized (per bit width of the ords) per-segment comparator. note: this is messy; we do this only because hotspot can't reliably inline the underlying array access when looking up doc->ord @lucene.internal sorts by field's natural term sort order. all comparisons are done using bytesref.compareto, which is slow for medium to large result sets but possibly very fast for very small results sets. sorts by field's natural term sort order. all comparisons are done using bytesref.compareto, which is slow for medium to large result sets but possibly very fast for very small results sets. the bytesref values are obtained using {@link atomicreader#docvalues}. javadocs empty implementation since most comparators don't need the score. this can be overridden by those that need it. optimization to remove unneeded checks on the bit interface: test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): note: must do this before calling super otherwise we compute the docswithfield bits twice! test for docvalue == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): note: must do this before calling super otherwise we compute the docswithfield bits twice! test for docvalue == 0 to save bits.get method call for the common case (doc has value and value is non-zero): todo: are there sneaky non-branch ways to compute sign of float? todo: are there sneaky non-branch ways to compute sign of float? test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): note: must do this before calling super otherwise we compute the docswithfield bits twice! test for docvalue == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): note: must do this before calling super otherwise we compute the docswithfield bits twice! test for docvalue == 0 to save bits.get method call for the common case (doc has value and value is non-zero): value of bottom of queue todo: there are sneaky non-branch ways to compute -1/+1/0 sign cannot return values[slot1] - values[slot2] because that may overflow todo: there are sneaky non-branch ways to compute -1/+1/0 sign cannot return bottom - values[slot2] because that may overflow test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): note: must do this before calling super otherwise we compute the docswithfield bits twice! test for docvalue == 0 to save bits.get method call for the common case (doc has value and value is non-zero): todo: there are sneaky non-branch ways to compute -1/+1/0 sign todo: there are sneaky non-branch ways to compute -1/+1/0 sign todo: there are sneaky non-branch ways to compute -1/+1/0 sign todo: there are sneaky non-branch ways to compute -1/+1/0 sign test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): test for v2 == 0 to save bits.get method call for the common case (doc has value and value is non-zero): note: must do this before calling super otherwise we compute the docswithfield bits twice! test for docvalue == 0 to save bits.get method call for the common case (doc has value and value is non-zero): wrap with a scorecachingwrappingscorer so that successive calls to score() will not incur score computation over and over again. override because we sort reverse of natural float order: reversed intentionally because relevance by default sorts descending: reverse of floatcomparator reverse of floatcomparator no overflow risk because docids are non-negative no overflow risk because docids are non-negative todo: can we "map" our docids to the current reader? saves having to then subtract on every compare call used per-segment when bit width of doc->ord is 8: ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): used per-segment when bit width of doc->ord is 16: ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): used per-segment when bit width of doc->ord is 32: ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): used per-segment when bit width is not a native array size (8, 16, 32): ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): don't specialize the long[] case since it's not possible, ie, worse case is max_int-1 docs with every one having a unique value. 0 ord is null for all segments exact value match todo: would be nice to share these specialized impls w/ termordvalcomparator used per-segment when bit width of doc->ord is 8: ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): used per-segment when bit width of doc->ord is 16: ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): used per-segment when bit width of doc->ord is 32: ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): used per-segment when bit width is not a native array size (8, 16, 32): ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): used per-segment when dv doesn't use packed ints for doctoords: ord is precisely comparable, even in the equal case the equals case always means bottom is > doc (because we set bottomord to the lower bound in setbottom): this may mean entire segment had no docs with this dv field; use default field value (empty byte[]) in this case: this means segment has doc values, but they are not able to provide a sorted source; consider this a hard error: 8 bit packed 16 bit packed 32 bit packed 0 ord is null for all segments exact value match"
org.apache.lucene.search.MultiTermQueryWrapperFilter "a wrapper for {@link multitermquery}, that exposes its functionality as a {@link filter}.  multitermquerywrapperfilter is not designed to be used by itself. normally you subclass it to provide a filter counterpart for a {@link multitermquery} subclass.  for example, {@link termrangefilter} and {@link prefixfilter} extend multitermquerywrapperfilter. this class also provides the functionality behind {@link multitermquery#constant_score_filter_rewrite}; this is why it is not abstract. wrap a {@link multitermquery} as a filter. returns the field name for this query returns a docidset with documents that should be permitted in search results. query.tostring should be ok for the filter, too, if the query boost is 1.0f reader has no fields field does not exist fill into a fixedbitset system.out.println(" iter termcount=" + termcount + " term=" + enumerator.term().tobytesstring()); system.out.println(" done termcount=" + termcount);"
org.apache.lucene.search.ScoreDoc "holds one hit in {@link topdocs}. the score of this document for the query. a hit document's number. @see indexsearcher#doc(int) only set by {@link topdocs#merge} constructs a scoredoc. constructs a scoredoc. a convenience method for debugging."
org.apache.lucene.search.MultiCollector "a {@link collector} which allows running a search with several {@link collector}s. it offers a static {@link #wrap} method which accepts a list of collectors and wraps them with {@link multicollector}, while filtering out the null null ones. wraps a list of {@link collector}s with a {@link multicollector}. this method works as follows:  filters out the null collectors, so they are not used during search time. if the input contains 1 real collector (i.e. non-null ), it is returned. otherwise the method returns a {@link multicollector} which wraps the non-null ones.  @throws illegalargumentexception if either 0 collectors were input, or all collectors are null. for the however, to improve performance, these null collectors are found and dropped from the array we save for actual collection time. only 1 collector - return it."
org.apache.lucene.search.ConstantScoreAutoRewrite "if the number of terms in this query is equal to or larger than this setting then {@link multitermquery#constant_score_filter_rewrite} is used. @see #settermcountcutoff if the number of documents to be visited in the postings exceeds this specified percentage of the maxdoc() for the index, then {@link multitermquery#constant_score_filter_rewrite} is used. 0.0 to 100.0 @see #setdoccountpercent ignored special implementation of bytesstartarray that keeps parallel arrays for {@link termcontext} defaults derived from rough tests with a 20.0 million doc wikipedia index. with more than 350 terms in the query, the filter method is fastest: if the query will hit more than 1 in 1000 of the docs in the index (0.1%), the filter method is fastest: get the enum and start visiting terms. if we exhaust the enum before hitting either of the cutoffs, we use constantbooleanqueryrewrite; else, constantfilterrewrite: docfreq is not used for constant score here, we pass 1 to explicitely set a fake value, so it's not calculated strip scores"
org.apache.lucene.search.CollectionStatistics "contains statistics for a collection (field) @lucene.experimental returns the field name returns the total number of documents, regardless of whether they all contain values for this field. @see indexreader#maxdoc() returns the total number of documents that have at least one term for this field. @see terms#getdoccount() returns the total number of tokens for this field @see terms#getsumtotaltermfreq() returns the total number of postings for this field @see terms#getsumdocfreq() javadocs javadocs #docs with field must be = #docs with field #positions must be >= #postings"
org.apache.lucene.search.PositiveScoresOnlyCollector "a {@link collector} implementation which wraps another {@link collector} and makes sure only documents with scores &gt; 0 are collected. set a scorecachingwrappingscorer in case the wrapped collector will call score() also."
org.apache.lucene.search.TermScorer "expert: a scorer for documents matching a term. construct a termscorer. the weight of the term in the query. an iterator over the documents matching the term. the similarity.exactsimscorer implementation to be used for score computations. per-segment docfreq of this term advances to the next document matching the query.  document matching the query or no_more_docs if there are no more documents. advances to the first match beyond the current whose document number is greater than or equal to a given target.  the implementation uses {@link docsenum#advance(int)}. the target document number. matching document or no_more_docs if none exist. returns a string representation of this termscorer. todo: benchmark if the specialized conjunction really benefits from this, or if instead its from sorting by docfreq, or both todo: generalize something like this for scorers? even this is just an estimation..."
org.apache.lucene.search.Sort "encapsulates sort criteria for returned hits. the fields used to determine sort order must be carefully chosen. documents must contain a single term in such a field, and the value of the term should indicate the document's relative position in a given sort order. the field must be indexed, but should not be tokenized, and does not need to be stored (unless you happen to want it back with the rest of your document data). in other words: document.add (new field ("bynumber", integer.tostring(x), field.store.no, field.index.not_analyzed)); valid types of values there are four possible kinds of term values which may be put into sorting fields: integers, longs, floats, or strings. unless {@link sortfield sortfield} objects are specified, the type of value in the field is determined by parsing the first term in the field. integer term values should contain only digits and an optional preceding negative sign. values must be base 10 and in the range integer.min_value and integer.max_value inclusive. documents which should appear first in the sort should have low value integers, later documents high values (i.e. the documents should be numbered 1..n where 1 is the first and n the last). long term values should contain only digits and an optional preceding negative sign. values must be base 10 and in the range long.min_value and long.max_value inclusive. documents which should appear first in the sort should have low value integers, later documents high values. float term values should conform to values accepted by {@link float float.valueof(string)} (except that nan and infinity are not supported). documents which should appear first in the sort should have low values, later documents high values. string term values can contain any valid string, but should not be tokenized. the values are sorted according to their {@link comparable natural order}. note that using this type of term value has higher memory requirements than the other two types. object reuse one of these objects can be used multiple times and the sort order changed between usages. this class is thread safe. memory usage sorting uses of caches of term values maintained by the internal hitqueue(s). the cache is static and contains an integer or float array of length indexreader.maxdoc() for each field name for which a sort is performed. in other words, the size of the cache in bytes is: 4 indexreader.maxdoc() (# of different fields actually used to sort) for string fields, the cache is larger: in addition to the above array, the value of every term in the field is kept in memory. if there are many unique terms in the field, this could be quite large. note that the size of the cache is not affected by how many fields are in the index and might be used to sort - only by the ones actually used to sort a result set.  represents sorting by computed relevance. using this sort criteria returns the same results as calling {@link indexsearcher#search(query,int) indexsearcher#search()}without a sort criteria, only with slightly more overhead. represents sorting by index order. sorts by computed relevance. this is the same sort criteria as calling {@link indexsearcher#search(query,int) indexsearcher#search()}without a sort criteria, only with slightly more overhead. sorts by the criteria in the given sortfield. sorts in succession by the criteria in each sortfield. sets the sort to the given criteria. sets the sort to the given criteria in succession. representation of the sort criteria. of sortfield objects used in this sort criteria rewrites the sortfields in this sort, returning a new sort if any of the fields changes during their rewriting. indexsearcher to use in the rewriting {@code this} if the sort/fields have not changed, or a new sort if there is a change @throws ioexception can be thrown by the rewriting @lucene.experimental returns true if o is equal to this. returns a hash code value for this object. internal representation of the sort criteria"
org.apache.lucene.search.CachingWrapperFilter "wraps another {@link filter}'s result and caches it. the purpose is to allow filters to simply filter, and then wrap with this class to add caching. wraps another filter's result and caches it. filter to cache results of provide the docidset to be cached, using the docidset provided by the wrapped filter. this implementation returns the given {@link docidset}, if {@link docidset#iscacheable} returns true, else it copies the {@link docidsetiterator} into a {@link fixedbitset}. javadocs todo: make this filter aware of readercontext. a cached filter could specify the actual readers key or something similar to indicate on which level of the readers hierarchy it should be cached. this is better than returning null, as the nonnull result can be cached null is allowed to be returned by iterator(), in this case we wrap with the empty set, which is cacheable. for testing"
org.apache.lucene.search.FieldValueHitQueue "expert: a hit queue for sorting by hits by terms in more than one field. uses fieldcache.default for maintaining internal term lookup tables. @lucene.experimental @see indexsearcher#search(query,filter,int,sort) @see fieldcache extension of scoredoc to also store the {@link fieldcomparator} slot. an implementation of {@link fieldvaluehitqueue} which is optimized in case there is just one comparator. returns whether hita is less relevant than hitb. entry entry true if document hita should be sorted after document hitb. an implementation of {@link fieldvaluehitqueue} which is optimized in case there is more than one comparator. creates a hit queue sorted by the given list of fields. note: the instances returned by this method pre-allocate a full array of length numhits. sortfield array we are sorting by in priority order (highest priority first); cannot be null or empty the number of hits to retain. must be greater than zero. @throws ioexception if there is a low-level io error stores the sort criteria being used. given a queue entry, creates a corresponding fielddoc that contains the values used to sort the given document. these values are not the raw values out of the index, but the internal representation of them. this is so the given search hit can be collated by a multisearcher with other search hits. the entry used to create a fielddoc newly returns the sortfields being used by this hit queue. avoid random sort order that could lead to duplicates (bug #31241): short circuit avoid random sort order that could lead to duplicates (bug #31241): prevent instantiation and extension. when we get here, fields.length is guaranteed to be > 0, therefore no need to check it again. all these are required by this class's api - need to return arrays. therefore even in the case of a single comparator, create an array anyway. use setcomparator to change this array this must always be equal to comparators[0] if (maxscore > 1.0f) doc.score /= maxscore; // normalize scores"
org.apache.lucene.search.FilteredQuery "a query that applies a filter to the results of another query. note: the bits are retrieved from the filter each time this query is used in a search - use a cachingwrapperfilter to avoid regenerating the bits every time. 1.4 @see cachingwrapperfilter constructs a new query which applies a filter to the results of the original query. {@link filter#getdocidset} will be called every time this query is used in a search. query to be filtered, cannot be null. filter to apply to query results, cannot be null. expert: constructs a new query which applies a filter to the results of the original query. {@link filter#getdocidset} will be called every time this query is used in a search. query to be filtered, cannot be null. filter to apply to query results, cannot be null. a filter strategy used to create a filtered scorer. @see filterstrategy returns a weight that applies the filter to the enclosed query's weight. this is accomplished by overriding the scorer returned by the weight. a scorer that consults the filter iff a document was matched by the delegate scorer. this is useful if the filter computation is more expensive than document scoring or if the filter has a linear running time to compute the next matching doc like exact geo distances. a scorer that uses a "leap-frog" approach (also called "zig-zag join"). the scorer and the filter take turns trying to advance to each other's next matching document, often jumping past the target document. when both land on the same document, it's collected. rewrites the query. if the wrapped is an instance of {@link matchalldocsquery} it returns a {@link constantscorequery}. otherwise it returns a new {@code filteredquery} wrapping the rewritten query. returns this filteredquery's (unfiltered) query returns this filteredquery's filter prints a returns true iff o is equal to this. returns a hash code value for this object. a {@link filterstrategy} that conditionally uses a random access filter if the given {@link docidset} supports random access (returns a non-null value from {@link docidset#bits()}) and {@link randomaccessfilterstrategy# a filter strategy that uses a "leap-frog" approach (also called "zig-zag join"). the scorer and the filter take turns trying to advance to each other's next matching document, often jumping past the target document. when both land on the same document, it's collected.  note: this strategy uses the filter to lead the iteration.  a filter strategy that uses a "leap-frog" approach (also called "zig-zag join"). the scorer and the filter take turns trying to advance to each other's next matching document, often jumping past the target document. when both land on the same document, it's collected.  note: this strategy uses the query to lead the iteration.  a filter strategy that advances the query or rather its {@link scorer} first and consults the filter {@link docidset} for each matched document.  note: this strategy requires a {@link docidset#bits()} to return a non-null value. otherwise this strategy falls back to {@link filteredquery#leap_frog_query_first_strategy}   use this strategy if the filter computation is more expensive than document scoring or if the filter has a linear running time to compute the next matching doc like exact geo distances.  abstract class that defines how the filter ({@link docidset}) applied during document collection. returns a filtered {@link scorer} based on this strategy. the {@link atomicreadercontext} for which to return the {@link scorer}. specifies whether in-order scoring of documents is required. note that if set to false (i.e., out-of-order scoring is required), this method can return whatever scoring mode it supports, as every in-order scorer is also an out-of-order one. however, an out-of-order scorer may not support {@link scorer#nextdoc()} and/or {@link scorer#advance(int)}, therefore it is recommended to request an in-order scorer if use of these methods is required. if true, {@link scorer#score(collector)} will be called; if false, {@link scorer#nextdoc()} and/or {@link scorer#advance(int)} will be called. the {@link filteredquery} {@link weight} to create the filtered scorer. the filter {@link docidset} to apply filtered scorer @throws ioexception if an {@link ioexception} occurs a {@link filterstrategy} that conditionally uses a random access filter if the given {@link docidset} supports random access (returns a non-null value from {@link docidset#bits()}) and {@link randomaccessfilterstrategy# expert: decides if a filter should be executed as "random-access" or not. random-access means the filter "filters" in a similar way as deleted docs are filtered in lucene. this is faster when the filter accepts many documents. however, when the filter is very sparse, it can be faster to execute the query+filter as a conjunction in some cases. the default implementation returns true if the first document accepted by the filter is  note: this strategy requires a {@link docidset#bits()} to return a non-null value. otherwise this strategy falls back to {@link filteredquery#leap_frog_query_first_strategy}   use this strategy if the filter computation is more expensive than document scoring or if the filter has a linear running time to compute the next matching doc like exact geo distances.  boost sub-weight incorporate boost return this query return a filtering scorer this means the filter does not accept any documents. optimization: we are topscorer and collect directly the normalization trick already applies the boost of this query, so we can use the wrapped scorer directly: optimization: we are topscorer and collect directly using short-circuited algo the normalization trick already applies the boost of this query, so we can use the wrapped scorer directly: check if scorer has exhausted, only before collecting. todo once we have way to figure out if we use ra or leapfrog we can remove this scorer initialize to prevent and advance call to move it further special case: if the query is a matchalldocsquery, we only return a csq(filter). combine boost of matchalldocsquery and the wrapped rewritten query: rewrite to a new filteredquery wrapping the rewritten query nothing to rewrite, we are done! inherit javadoc this means the filter does not accept any documents. force if ra is requested if we are using random access, we return the inner scorer, just with other acceptdocs we are gonna advance() this scorer, so we set inorder=true/toplevel=false we pass null as acceptdocs, as our filter has already respected acceptdocs, no need to do twice todo once we have way to figure out if we use ra or leapfrog we can remove this scorer todo once we have a cost api on filters and scorers we should rethink this heuristic this means the filter does not accept any documents. we are gonna advance() this scorer, so we set inorder=true/toplevel=false we pass null as acceptdocs, as our filter has already respected acceptdocs, no need to do twice"
org.apache.lucene.search.spans.SpanPositionCheckQuery "base class for filtering a spanquery based on the position of a match. spanquery whose matches are filtered. return value for {@link spanpositioncheckquery#acceptposition(spans)}. indicates the match should be accepted indicates the match should be rejected indicates the match should be rejected, and the enumeration should advance to the next document. implementing classes are required to return whether the current position is a match for the passed in "match" {@link org.apache.lucene.search.spans.spanquery}. this is only called if the underlying {@link org.apache.lucene.search.spans.spans#next()} for the match is successful the {@link org.apache.lucene.search.spans.spans} instance, positioned at the spot to check the match is accepted, rejected, or rejected and should move to the next doc. @see org.apache.lucene.search.spans.spans#next() some clauses rewrote no clauses rewrote todo: remove warning after api has been finalized todo: remove warning after api has been finalized"
org.apache.lucene.search.spans.SpanMultiTermQueryWrapper "wraps any {@link multitermquery} as a {@link spanquery}, so it can be nested within other spanquery classes.  the query is rewritten by default to a {@link spanorquery} containing the expanded terms, but this can be customized.  example:  {@code wildcardquery wildcard = new wildcardquery(new term("field", "bro?n")); spanquery spanwildcard = new spanmultitermquerywrapper(wildcard); // do something with spanwildcard, such as use it in a spanfirstquery }  create a new spanmultitermquerywrapper. query to wrap.  note: this will call {@link multitermquery#setrewritemethod(multitermquery.rewritemethod)} on the wrapped query, changing its rewrite method to a suitable one for spans. be sure to not change the rewrite method on the wrapped query afterwards! doing so will throw {@link unsupportedoperationexception} on rewriting this query! expert: returns the rewritemethod expert: sets the rewrite method. this only makes sense to be a span rewrite method. abstract class that defines how the query is rewritten. a rewrite method that first translates each term into a spantermquery in a {@link occur#should} clause in a booleanquery, and keeps the scores as computed by the query. @see #setrewritemethod a rewrite method that first translates each term into a spantermquery in a {@link occur#should} clause in a booleanquery, and keeps the scores as computed by the query.  this rewrite method only uses the top scoring terms so it will not overflow the boolean max clause count. @see #setrewritemethod create a toptermsspanbooleanqueryrewrite for at most size terms. return the maximum priority queue size javadocs only do something with spanwildcard, such as use it in a spanfirstquery we accept all terms as spanorquery has no limits todo: would be nice to not lose term-state here. we could add a hack option to spanorquery, but the hack would only work if this is the top-level span (if you put this thing in another span query, it would extractterms/double-seek anyway)"
org.apache.lucene.search.spans.SpanQuery "base class for span-based queries. expert: returns the matches for this query in an index. used internally to search for spans. returns the name of the field matched by this query."
org.apache.lucene.search.spans.FieldMaskingSpanQuery "wrapper to allow {@link spanquery} objects participate in composite single-field spanqueries by 'lying' about their search field. that is, the masked spanquery will function as normal, but {@link spanquery#getfield()} simply hands back the value supplied in this class's constructor. this can be used to support queries like {@link spannearquery} or {@link spanorquery} across different fields, which is not ordinarily permitted. this can be useful for denormalized relational data: for example, when indexing a document with conceptually many 'children':   teacherid: 1 studentfirstname: james studentsurname: jones teacherid: 2 studenfirstname: james studentsurname: smith studentfirstname: sally studentsurname: jones  a spannearquery with a slop of 0 can be applied across two {@link spantermquery} objects as follows:  spanquery q1 = new spantermquery(new term("studentfirstname", "james")); spanquery q2 = new spantermquery(new term("studentsurname", "jones")); spanquery q2m = new fieldmaskingspanquery(q2, "studentfirstname"); query q = new spannearquery(new spanquery[]{q1, q2m}, -1, false);  to search for 'studentfirstname:james studentsurname:jones' and find teacherid 1 without matching teacherid 2 (which has a 'james' in position 0 and 'jones' in position 1).  note: as {@link #getfield()} returns the masked field, scoring will be done using the similarity and collection statistics of the field name supplied, but with the term statistics of the real field. this may lead to exceptions, poor performance, and unexpected scoring behaviour. :note: getboost and setboost are not proxied to the maskedquery ...this is done to be more consistent with things like spanfirstquery"
org.apache.lucene.search.spans.SpanFirstQuery "matches spans near the beginning of a field.  this class is a simple extension of {@link spanpositionrangequery} in that it assumes the start to be zero and only checks the end boundary. construct a spanfirstquery matching spans in match whose end position is less than or equal to end. reversible"
org.apache.lucene.search.spans.NearSpansOrdered "a spans that is formed from the ordered subspans of a spannearquery where the subspans do not overlap and have a maximum slop between them.  the formed spans only contains minimum slop matches. the matching slop is computed from the distance(s) between the non overlapping matching spans. successive matches are always formed from the successive spans of the spannearquery.  the formed spans may contain overlaps when the slop is at least 1. for example, when querying using t1 t2 t3 with slop at least 1, the fragment: t1 t2 t1 t3 t2 t3 matches twice: t1 t2 .. t3   t1 .. t2 t3 expert: only public for subclassing. most implementations should not need this class the spans in the same order as the spannearquery indicates that all subspans have same doc() advances the subspans to just after an ordered match with a minimum slop that is smaller than the slop allowed by the spannearquery. iff there is such a match. advance the subspans to the same document check whether two spans in the same document are ordered. iff spans1 starts before spans2 or the spans start at the same position, and spans1 ends before spans2. do not call docspansordered(int,int,int,int) to avoid invoking .end() : like {@link #docspansordered(spans,spans)}, but use the spans starts and ends as parameters. order the subspans within the same document by advancing all later spans after the previous one. the subspans are ordered in the same doc, so there is a possible match. compute the slop while making the match as short as possible by advancing all subspans except the last one in reverse order. do not break on (matchslop > allowedslop) here to make sure that subspans[0] is advanced after the match, if any. used in tosamedoc() kept for tostring() only. inherit javadocs inherit javadocs inherit javadocs todo: remove warning after api has been finalized todo: would be nice to be able to lazy load payloads todo: remove warning after api has been finalized inherit javadocs inherit javadocs no more matches advance prevspans until after (laststart, lastend) check remaining subspans for final match. the last subspans is not advanced here. check remaining subspans for last match in this document. cannot avoid invoking .end() check remaining subspans. prevspans still before (laststart, lastend) only non overlapping spans add to slop. ordered and allowed slop"
org.apache.lucene.search.spans.TermSpans "expert: public for extension only only for emptytermspans (below) todo: remove warning after api has been finalized todo: remove warning after api has been finalized"
org.apache.lucene.search.spans.NearSpansUnordered "similar to {@link nearspansordered}, but for the unordered case. expert: only public for subclassing. most implementations should not need this class wraps a spans, and can be used to form a linked list. warning: the list is not necessarily in order of the the positions of byte[] payloads @throws ioexception if there is a low-level i/o error spans in query order from query linked list of spans sorted by doc only sum of current lengths sorted queue of spans max element in queue true iff not done true before first next() subtract old length add new length todo: remove warning after api has been finalized todo: remove warning after api has been finalized initialize queue trigger further scanning maintain queue maintain list skip to doc w/ all clauses skip first upto last and move it to the end found doc w/ all clauses maintain the queue maintain queue no more matches initialize skip all normal case skip as needed todo: remove warning after api has been finalized todo: remove warning after api has been finalized move to first entry add to list add next to end of list move first to end of list rebuild queue add to queue from list"
org.apache.lucene.search.spans.SpanWeight "expert-only. public for use by other weight implementations"
org.apache.lucene.search.spans.SpanOrQuery "matches the union of its clauses. construct a spanorquery merging the provided clauses. adds a clause to this query return the clauses whose spans are matched. copy clauses array into an arraylist clause rewrote: must clone some clauses rewrote no clauses rewrote optimize 1-clause case all done move to next exhausted a clause"
org.apache.lucene.search.spans.SpanNotQuery "removes matches which overlap with another spanquery. construct a spannotquery matching spans from include which have no overlap with spans from exclude. return the spanquery whose matches are filtered. return the spanquery whose matches must not overlap those returned. returns true iff o is equal to this. move to next include skip exclude while exclude is before increment exclude if no intersection we found a match intersected: keep scanning skip include skip exclude while exclude is before increment exclude if no intersection we found a match scan to next match todo: remove warning after api has been finalized todo: remove warning after api has been finalized some clauses rewrote no clauses rewrote rotate left rotate left"
org.apache.lucene.search.spans.SpanNearQuery "matches spans which are near one another. one can specify slop, the maximum number of intervening unmatched positions, as well as whether matches are required to be in-order. construct a spannearquery. matches spans matching a span from each clause, with up to slop total unmatched positions between them. when inorder is true, the spans from each clause must be ordered as in clauses. the clauses to find near each other the slop value true if order is important return the clauses whose spans are matched. return the maximum number of intervening unmatched positions permitted. return true if matches are required to be in-order. returns true iff o is equal to this. copy clauses array into an arraylist check field optimize 0-clause case optimize 1-clause case clause rewrote: must clone some clauses rewrote no clauses rewrote mix bits before folding in things like boost, since it could cancel the last element of clauses. this particular mix also serves to differentiate spannearquery hashcodes from others. reversible"
org.apache.lucene.search.spans.SpanPayloadCheckQuery "only return those matches that have a specific payload at the given position.  do not use this with an spanquery that contains a {@link org.apache.lucene.search.spans.spannearquery}. instead, use {@link spannearpayloadcheckquery} since it properly handles the fact that payloads aren't ordered by {@link org.apache.lucene.search.spans.spannearquery}. the underlying {@link org.apache.lucene.search.spans.spanquery} to check the {@link java.util.collection} of payloads to match todo: check the byte arrays are the same check each of the byte arrays, in order hmm, can't rely on order here if one is a mismatch, then return false we've verified all the bytes reversible todo: is this right?"
org.apache.lucene.search.spans.Spans "expert: an enumeration of span matches. used to implement span searching. each span represents a range of term positions within a document. matches are enumerated in order, by increasing document number, within that by increasing start position and finally by increasing end position. move to the next match, returning true iff any such exists. skips to the first match beyond the current, whose document number is greater than or equal to target. returns true iff there is such a match. behaves as if written:  boolean skipto(int target) { do { if (!next()) return false; } while (target > doc()); return true; }  most implementations are considerably more efficient than that. returns the document number of the current match. initially invalid. returns the start position of the current match. initially invalid. returns the end position of the current match. initially invalid. returns the payload data for the current span. this is invalid until {@link #next()} is called for the first time. this method must not be called more than once after each call of {@link #next()}. however, most payloads are loaded lazily, so if the payload data for the current position is not needed, this method may not be called at all for performance reasons. an ordered spanquery does not lazy load, so if you have payloads in your index and you do not want ordered spannearquerys to collect payloads, you can disable collection with a constructor option.  note that the return type is a collection, thus the ordering should not be relied upon.  @lucene.experimental list of byte arrays containing the data of this payload, otherwise null if ispayloadavailable is false @throws ioexception if there is a low-level i/o error checks if a payload can be loaded at this position.  payloads can only be loaded once per call to {@link #next()}. if there is a payload available at this position that can be loaded todo: remove warning after api has been finalized"
org.apache.lucene.search.spans.SpanScorer "public for extension only. returns the intermediate "sloppy freq" adjusted for edit distance @lucene.internal setfreqcurrentdoc() leaves spans.doc() ahead only public so .payloads can see it."
org.apache.lucene.search.spans.SpanNearPayloadCheckQuery "only return those matches that have a specific payload at the given position.  the underlying {@link spanquery} to check the {@link java.util.collection} of payloads to match todo: check the byte arrays are the same hmm, can't rely on order here unfortunately, we can't rely on order, so we need to compare all we've verified all the bytes reversible todo: is this right?"
org.apache.lucene.search.spans.SpanTermQuery "matches spans containing a term. construct a spantermquery matching the named term's spans. return the term whose spans are matched. this happens with span-not query, as it doesn't include the not side in extractterms() so we seek to the term now in this segment..., this sucks because its ugly mostly! term is not present in that reader term does exist, but has no positions"
org.apache.lucene.search.spans.SpanPositionRangeQuery "checks to see if the {@link #getmatch()} lies between a start and end position @see org.apache.lucene.search.spans.spanfirstquery for a derivation that is optimized for the case where start position is 0 minimum position permitted in a match maximum end position permitted in a match. reversible"
org.apache.lucene.search.FieldCacheRangeFilter "a range filter built on top of a cached single term field (in {@link fieldcache}). {@code fieldcacherangefilter} builds a single cache for the field the first time it is used. each subsequent {@code fieldcacherangefilter} on the same field then reuses this cache, even if the range itself changes. this means that {@code fieldcacherangefilter} is much faster (sometimes more than 100x as fast) as building a {@link termrangefilter}, if using a {@link #newstringrange}. however, if the range never changes it is slower (around 2x as slow) than building a cachingwrapperfilter on top of a single {@link termrangefilter}. for numeric data types, this filter may be significantly faster than {@link numericrangefilter}. furthermore, it does not need the numeric values encoded by {@link intfield}, {@link floatfield}, {@link longfield} or {@link doublefield}. but it has the problem that it only works with exact one value/document (see below). as with all {@link fieldcache} based functionality, {@code fieldcacherangefilter} is only valid for fields which exact one term for each document (except for {@link #newstringrange} where 0 terms are also allowed). due to a restriction of {@link fieldcache}, for numeric ranges all terms that do not have a numeric value, 0 is assumed. thus it works on dates, prices and other single value fields but will not work on regular text fields. it is preferable to use a not_analyzed field to ensure that there is only a single term. this class does not have an constructor, use one of the static factory methods available, that create a correct instance for different data types supported by {@link fieldcache}. this method is implemented for each data type creates a string range filter using {@link fieldcache#gettermsindex}. this works with all fields containing zero or one term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getbytes(atomicreader,string,boolean)}. this works with all byte fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getbytes(atomicreader,string,fieldcache.byteparser,boolean)}. this works with all byte fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getshorts(atomicreader,string,boolean)}. this works with all short fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getshorts(atomicreader,string,fieldcache.shortparser,boolean)}. this works with all short fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getints(atomicreader,string,boolean)}. this works with all int fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getints(atomicreader,string,fieldcache.intparser,boolean)}. this works with all int fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getlongs(atomicreader,string,boolean)}. this works with all long fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getlongs(atomicreader,string,fieldcache.longparser,boolean)}. this works with all long fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getfloats(atomicreader,string,boolean)}. this works with all float fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getfloats(atomicreader,string,fieldcache.floatparser,boolean)}. this works with all float fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getdoubles(atomicreader,string,boolean)}. this works with all double fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. creates a numeric range filter using {@link fieldcache#getdoubles(atomicreader,string,fieldcache.doubleparser,boolean)}. this works with all double fields containing exactly one numeric term in the field. the range can be half-open by setting one of the values to null. returns the field name for this filter returns true if the lower endpoint is inclusive returns true if the upper endpoint is inclusive returns the lower value of this range filter returns the upper value of this range filter returns the current numeric parser ({@code null} for {@code t} is {@code string}} for javadocs for javadocs for javadocs for javadocs for javadocs hints: binarysearchlookup returns 0, if value was null. the value is <0 if no exact hit was found, the returned value is (-(insertion point) - 1) we transform the floating point numbers to sortable integers using numericutils to easier find the next bigger/lower value we transform the floating point numbers to sortable integers using numericutils to easier find the next bigger/lower value ignore deleted docs if range doesn't contain 0 rotate to distinguish lower from upper"
org.apache.lucene.search.DisjunctionScorer "base class for scorers that score disjunctions. currently this just provides helper methods to manage the heap. organize subscorers into a min heap with scorers generating the earliest document on top. the subtree of subscorers at root is a min heap except possibly for its root element. bubble the root down as required to make the subtree a heap. remove the root scorer from subscorers and re-establish it as a heap"
org.apache.lucene.search.FieldDoc "expert: a scoredoc which also contains information about how to sort the referenced document. in addition to the document number and score, this object contains an array of values for the document from the field(s) used to sort. for example, if the sort criteria was to sort by fields "a", "b" then "c", the fields object array will have three elements, corresponding respectively to the term values for the document in fields "a", "b" and "c". the class of each element in the array will be either integer, float or string depending on the type of values in the terms of each field.  expert: the values which are used to sort the referenced document. the order of these will match the original sort criteria given by a sort object. each object will have been returned from the value method corresponding fieldcomparator used to sort this field. @see sort @see indexsearcher#search(query,filter,int,sort) expert: creates one of these objects with empty sort information. expert: creates one of these objects with the given sort information. expert: creates one of these objects with the given sort information. a convenience method for debugging. super.tostring returns the doc and score information, so just add the fields information discard last ", ""
org.apache.lucene.search.PhraseQueue "same doc and pp.position, so decide by actual term positions. rely on: pp.position == tp.position - offset."
org.apache.lucene.search.MultiTermQuery "an abstract {@link query} that matches documents containing a subset of terms provided by a {@link filteredtermsenum} enumeration. this query cannot be used directly; you must subclass it and define {@link #gettermsenum(terms,attributesource)} to provide a {@link filteredtermsenum} that iterates through the terms to be matched. note: if {@link #setrewritemethod} is either {@link #constant_score_boolean_query_rewrite} or {@link #scoring_boolean_query_rewrite}, you may encounter a {@link booleanquery.toomanyclauses} exception during searching, which happens when the number of terms to be searched exceeds {@link booleanquery#getmaxclausecount()}. setting {@link #setrewritemethod} to {@link #constant_score_filter_rewrite} prevents this. the recommended rewrite method is {@link #constant_score_auto_rewrite_default}: it doesn't spend cpu computing unhelpful scores, and it tries to pick the most performant rewrite method given the query. if you need scoring (like {@link fuzzyquery}, use {@link toptermsscoringbooleanqueryrewrite} which uses a priority queue to only collect competitive terms and not hit this limitation. note that org.apache.lucene.queryparser.classic.queryparser produces multitermqueries using {@link #constant_score_auto_rewrite_default} by default. abstract class that defines how the query is rewritten. returns the {@link multitermquery}s {@link termsenum} @see multitermquery#gettermsenum(terms, attributesource) a rewrite method that first creates a private filter, by visiting each term in sequence and marking all docs for that term. matching documents are assigned a constant score equal to the query's boost.  this method is faster than the booleanquery rewrite methods when the number of matched terms or matched documents is non-trivial. also, it will never hit an errant {@link booleanquery.toomanyclauses} exception. @see #setrewritemethod a rewrite method that first translates each term into {@link booleanclause.occur#should} clause in a booleanquery, and keeps the scores as computed by the query. note that typically such scores are meaningless to the like {@link #scoring_boolean_query_rewrite} except scores are not computed. instead, each matching document receives a constant score equal to the query's boost. note: this rewrite method will hit {@link booleanquery.toomanyclauses} if the number of terms exceeds {@link booleanquery#getmaxclausecount}. @see #setrewritemethod a rewrite method that first translates each term into {@link booleanclause.occur#should} clause in a booleanquery, and keeps the scores as computed by the query.  this rewrite method only uses the top scoring terms so it will not overflow the boolean max clause count. it is the default rewrite method for {@link fuzzyquery}. @see #setrewritemethod create a toptermsscoringbooleanqueryrewrite for at most size terms.  note: if {@link booleanquery#getmaxclausecount} is smaller than size, then it will be used instead. a rewrite method that first translates each term into {@link booleanclause.occur#should} clause in a booleanquery, but the scores are only computed as the boost.  this rewrite method only uses the top scoring terms so it will not overflow the boolean max clause count. @see #setrewritemethod create a toptermsboostonlybooleanqueryrewrite for at most size terms.  note: if {@link booleanquery#getmaxclausecount} is smaller than size, then it will be used instead. a rewrite method that tries to pick the best constant-score rewrite method based on term and document counts from the query. if both the number of terms and documents is small enough, then {@link #constant_score_boolean_query_rewrite} is used. otherwise, {@link #constant_score_filter_rewrite} is used. read-only default instance of {@link constantscoreautorewrite}, with {@link constantscoreautorewrite#settermcountcutoff} set to {@link constantscoreautorewrite#default_term_count_cutoff} and {@link constantscoreautorewrite#setdoccountpercent} set to {@link constantscoreautorewrite#default_doc_count_percent}. note that you cannot alter the configuration of this instance; you'll need to create a private instance instead. constructs a query matching terms that cannot be represented with a single term. returns the field name for this query construct the enumeration to be used, expanding the pattern term. this method should only be called if the field exists (ie, implementations can assume the field does exist). this method should not return null (should instead return {@link termsenum#empty} if no terms match). the termsenum must already be positioned to the first matching term. the given {@link attributesource} is passed by the {@link rewritemethod} to provide attributes, the rewrite method uses to inform about e.g. maximum competitive boosts. this is currently only used by {@link toptermsrewrite} convenience method, if no attributes are needed: this simply passes empty attributes and is equal to: gettermsenum(terms, new attributesource()) to rewrite to a simpler form, instead return a simpler enum from {@link #gettermsenum(terms, attributesource)}. for example, to rewrite to a single term, return a {@link singletermsenum} @see #setrewritemethod sets the rewrite method to be used when executing the query. you can use one of the four core methods, or implement your own subclass of {@link rewritemethod}. javadocs javadocs allow rewritemethod subclasses to pull a termsenum from the mtq"
org.apache.lucene.search.BitsFilteredDocIdSet "this implementation supplies a filtered docidset, that excludes all docids which are not in a bits instance. this is especially useful in {@link org.apache.lucene.search.filter} to apply the {@code acceptdocs} passed to {@code getdocidset()} before returning the final docidset. @see docidset @see org.apache.lucene.search.filter convenience wrapper method: if {@code acceptdocs == null} it returns the original set without wrapping. underlying docidset. if {@code null}, this method returns {@code null} allowed docs, all docids not in this set will not be returned by this docidset. if {@code null}, this method returns the original set without wrapping. constructor. underlying docidset allowed docs, all docids not in this set will not be returned by this docidset"
org.apache.lucene.search.TopDocs "represents hits returned by {@link indexsearcher#search(query,filter,int)} and {@link indexsearcher#search(query,int)}. the total number of hits for the query. the top hits for the query. stores the maximum score value encountered, needed for normalizing. returns the maximum score value encountered. note that in case scores are not tracked, this returns {@link float#nan}. sets the maximum score value encountered. constructs a topdocs with a default maxscore=float.nan. returns a new topdocs, containing topn results across the provided topdocs, sorting by the specified {@link sort}. each of the topdocs must have been sorted by the same sort, and sort field values must have been filled (ie, fillfields=true must be passed to {@link topfieldcollector#create}. pass sort=null to merge sort by score descending. @lucene.experimental refers to one hit: which shard (index into shardhits[]): which hit within the shard: specialized mergesortqueue that just merges by relevance score, descending: returns true if first is < second tie break: earlier shard wins tie break in same shard: resolve however the shard had resolved it: these are really fielddoc instances: system.out.println(" init shardidx=" + shardidx + " hits=" + shard); fail gracefully if api is misused: returns true if first is < second system.out.println(" lessthan:\n first=" + first + " doc=" + firstfd.doc + " score=" + firstfd.score + "\n second=" + second + " doc=" + secondfd.doc + " score=" + secondfd.score); system.out.println(" cmp idx=" + compidx + " cmp1=" + firstfd.fields[compidx] + " cmp2=" + secondfd.fields[compidx] + " reverse=" + reversemul[compidx]); system.out.println(" return " + (cmp < 0)); tie break: earlier shard wins system.out.println(" return tb true"); system.out.println(" return tb false"); tie break in same shard: resolve however the shard had resolved it: system.out.println(" return tb " + (first.hitindex < second.hitindex)); totalhits can be non-zero even if no hits were collected, when searchafter was used: system.out.println(" maxscore now " + maxscore + " vs " + shard.getmaxscore()); system.out.println(" hitupto=" + hitupto); system.out.println(" doc=" + hits[hitupto].doc + " score=" + hits[hitupto].score); not done with this these topdocs yet:"
org.apache.lucene.search.SortField "stores information about how to sort documents by terms in an individual field. fields must be indexed in order to sort by them.  specifies the type of the terms to be sorted, or special types such as custom sort by document score (relevance). sort values are float and higher values are at the front. sort by document number (index order). sort values are integer and lower values are at the front. sort using term values as strings. sort values are string and lower values are at the front. sort using term values as encoded integers. sort values are integer and lower values are at the front. sort using term values as encoded floats. sort values are float and lower values are at the front. sort using term values as encoded longs. sort values are long and lower values are at the front. sort using term values as encoded doubles. sort values are double and lower values are at the front. sort using term values as encoded shorts. sort values are short and lower values are at the front. sort using a custom comparator. sort values are any comparable and sorting is done according to natural order. sort using term values as encoded bytes. sort values are byte and lower values are at the front. sort using term values as strings, but comparing by value (using string.compareto) for all comparisons. this is typically slower than {@link #string}, which uses ordinals to do the sorting. sort use byte[] index values. force rewriting of sortfield using {@link sortfield#rewrite(indexsearcher)} before it can be used for sorting represents sorting by document score (relevance). represents sorting by document number (index order). creates a sort by terms in the given field with the type of term values explicitly given. name of field to sort by. can be null if type is score or doc. type of values in the terms. creates a sort, possibly in reverse, by terms in the given field with the type of term values explicitly given. name of field to sort by. can be null if type is score or doc. type of values in the terms. true if natural order should be reversed. creates a sort by terms in the given field, parsed to numeric values using a custom {@link fieldcache.parser}. name of field to sort by. must not be null. instance of a {@link fieldcache.parser}, which must subclass one of the existing numeric parsers from {@link fieldcache}. sort type is inferred by testing which numeric parser the parser subclasses. @throws illegalargumentexception if the parser fails to subclass an existing numeric parser, or field is null creates a sort, possibly in reverse, by terms in the given field, parsed to numeric values using a custom {@link fieldcache.parser}. name of field to sort by. must not be null. instance of a {@link fieldcache.parser}, which must subclass one of the existing numeric parsers from {@link fieldcache}. sort type is inferred by testing which numeric parser the parser subclasses. true if natural order should be reversed. @throws illegalargumentexception if the parser fails to subclass an existing numeric parser, or field is null creates a sort with a custom comparison function. name of field to sort by; cannot be null. returns a comparator for sorting hits. creates a sort, possibly in reverse, with a custom comparison function. name of field to sort by; cannot be null. returns a comparator for sorting hits. true if natural order should be reversed. returns the name of the field. could return null if the sort is by score or doc. of field, possibly null. returns the type of contents in the field. of the constants score, doc, string, int or float. returns the instance of a {@link fieldcache} parser that fits to the given sort type. may return null if no parser was specified. sorting is using the default parser then. instance of a {@link fieldcache} parser, or null. returns whether the sort should be reversed. true if natural order should be reversed. returns the {@link fieldcomparatorsource} used for custom sorting returns true if o is equal to this. if a {@link fieldcomparatorsource} or {@link fieldcache.parser} was provided, it must properly implement equals (unless a singleton is always used). returns true if o is equal to this. if a {@link fieldcomparatorsource} or {@link fieldcache.parser} was provided, it must properly implement hashcode (unless a singleton is always used). returns the {@link fieldcomparator} to use for sorting. @lucene.experimental number of top hits the queue will store position of this sortfield within {@link sort}. the comparator is primary if sortpos==0, secondary if sortpos==1, etc. some comparators can optimize themselves when they are the primary sort. {@link fieldcomparator} to use when sorting rewrites this sortfield, returning a new sortfield if a change is made. subclasses should override this define their rewriting behavior when this sortfield is of type {@link sortfield.type#rewriteable} indexsearcher to use during rewriting rewritten sortfield, or {@code this} if nothing has changed. @throws ioexception can be thrown by the rewriting @lucene.experimental todo(simonw) -- for cleaner transition, maybe we should make a new sortfield that subclasses this one and always uses index values? defaults to determining type dynamically defaults to natural order used for custom sort used for 'sortmissingfirst/last' sets field & type, and ensures field is not null unless type is score or doc"
org.apache.lucene.search.TotalHitCountCollector "just counts the total number of hits. returns how many hits matched the search."
org.apache.lucene.search.DisjunctionMaxQuery "a query that generates the union of documents produced by its subqueries, and that scores each document with the maximum score for that document as produced by any subquery, plus a tie breaking increment for any additional matching subqueries. this is useful when searching for a word in multiple fields with different boost factors (so that the fields cannot be combined equivalently into a single search field). we want the primary score to be the one associated with the highest boost, not the sum of the field scores (as booleanquery would give). if the query is "albino elephant" this ensures that "albino" matching one field and "elephant" matching another gets a higher score than "albino" matching both fields. to get this result, use both booleanquery and disjunctionmaxquery: for each term a disjunctionmaxquery searches for it in each field, while the set of these disjunctionmaxquery's is combined into a booleanquery. the tie breaker capability allows results that include the same term in multiple fields to be judged better than results that include this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields. the subqueries multiple of the non-max disjunct scores added into our final score. non-zero values support tie-breaking. creates a new empty disjunctionmaxquery. use add() to add the subqueries. the score of each non-maximum disjunct for a document is multiplied by this weight and added into the final score. if non-zero, the value should be small, on the order of 0.1, which says that 10 occurrences of word in a lower-scored field that is also in a higher scored field is just as good as a unique word in the lower scored field (i.e., one that is not in any higher scored field. creates a new disjunctionmaxquery a {@code collection} of all the disjuncts to add the weight to give to each matching non-maximum disjunct add a subquery to this disjunction the disjunct added add a collection of disjuncts to this disjunction via {@code iterable} a collection of queries to add as disjuncts. {@code iterator} over the disjuncts disjuncts. breaker value for multiple matches. expert: the weight for disjunctionmaxquery, used to normalize, score and explain these queries. note: this api and implementation is subject to change suddenly in the next release. the weights for our subqueries, in 1-1 correspondence with disjuncts construct the weight for this query searched by searcher. recursively construct subquery weights. return our associated disjunctionmaxquery compute the sub of squared weights of us applied to our subqueries. used for normalization. apply the computed normalization factor to our subqueries create the scorer used to score our associated disjunctionmaxquery explain the score we computed for doc create the weight used to score us optimize our representation and our subqueries representations the indexreader we query optimized copy of us (which may not be a copy if there is nothing to optimize) create a shallow copy of us -- used in rewriting if necessary copy of us (but reuse, don't copy, our subqueries) prettyprint us. the field to which we are applied string that shows what we do, of the form "(disjunct1 | disjunct2 | ... | disjunctn)^boost" return true iff we represent the same query as o another object iff o is a disjunctionmaxquery with the same boost and the same subqueries, in the same order, as us compute a hash code for hashing us hash code the weight's for our subqueries, in 1-1 correspondence with disjuncts incorporate our boost we will advance() subscorers all scorers did not have documents end of disjunctionmaxweight inner class inherit javadoc wrap sub-bools in parens"
org.apache.lucene.search.FilteredDocIdSetIterator "abstract decorator class of a docidsetiterator implementation that provides on-demand filter/validation mechanism on an underlying docidsetiterator. see {@link filtereddocidset}. constructor. underlying docidsetiterator. validation method to determine whether a docid should be in the result set. docid to be tested if input docid should be in the result set, false otherwise. @see #filtereddocidsetiterator(docidsetiterator)"
org.apache.lucene.search.NGramPhraseQuery "this is a {@link phrasequery} which is optimized for n-gram phrase query. for example, when you query "abcd" on a 2-gram field, you may want to use ngramphrasequery rather than {@link phrasequery}, because ngramphrasequery will {@link #rewrite(indexreader)} the query to "ab/0 cd/2", while {@link phrasequery} will query "ab/0 bc/1 cd/2" (where term/position). constructor that takes gram size. n-gram size returns true iff o is equal to this. returns a hash code value for this object. check whether optimizable or not non-overlap n-gram cannot be optimized too short to optimize check all posincrement is 1 if not, cannot optimize now create the new optimized phrase query for n-gram"
org.apache.lucene.search.MaxNonCompetitiveBoostAttribute "add this {@link attribute} to a fresh {@link attributesource} before calling {@link multitermquery#gettermsenum(terms,attributesource)}. {@link fuzzyquery} is using this to control its internal behaviour to only return competitive terms. please note: this attribute is intended to be added by the {@link multitermquery.rewritemethod} to an empty {@link attributesource} that is shared for all segments during query rewrite. this attribute source is passed to all segment enums on {@link multitermquery#gettermsenum(terms,attributesource)}. {@link toptermsrewrite} uses this attribute to inform all enums about the current boost, that is not competitive. @lucene.internal this is the maximum boost that would not be competitive. this is the maximum boost that would not be competitive. default is negative infinity, which means every term is competitive. this is the term or null of the term that triggered the boost change. this is the term or null of the term that triggered the boost change. default is null, which means every term is competitoive. javadocs only javadocs only"
org.apache.lucene.search.WildcardQuery "implements the wildcard search query. supported wildcards are , which matches any character sequence (including the empty one), and ?, which matches any single character. '\' is the escape character.  note this query can be slow, as it needs to iterate over many terms. in order to prevent extremely slow wildcardqueries, a wildcard term should not start with the wildcard  this query uses the {@link multitermquery#constant_score_auto_rewrite_default} rewrite method. @see automatonquery string equality with support for wildcards char equality with support for wildcards escape character constructs a query for terms matching term. convert lucene wildcard syntax into an automaton. @lucene.internal returns the pattern term. prints a add the next codepoint instead, if it exists else fallthru, lenient parsing with a trailing \"
org.apache.lucene.search.MultiPhraseQuery "multiphrasequery is a generalized version of phrasequery, with an added method {@link #add(term[])}. to use this class, to search for the phrase "microsoft app" first use add(term) on the term "microsoft", then find all terms that have "app" as prefix using indexreader.terms(term), and use multiphrasequery.add(term[] terms) to add them to the query. sets the phrase slop for this query. @see phrasequery#setslop(int) sets the phrase slop for this query. @see phrasequery#getslop() add a single term at the next position in the phrase. @see phrasequery#add(term) add multiple terms at the next position in the phrase. any of the terms may match. @see phrasequery#add(term) allows to specify the relative position of terms within the phrase. @see phrasequery#add(term, int) returns a list of the terms in the multiphrase. do not modify the list or its contents. returns the relative positions of terms in this phrase. prints a returns true if o is equal to this. returns a hash code value for this object. takes the logical union of multiple docsenum iterators. inherit javadoc compute idf reuse single termsenum below: coarse -- this overcounts since a given doc can have more than one term: term not in reader none of the terms are in this reader term not in reader term does exist, but has no positions sort by increasing docfreq order optimize one-term case breakout calculation of the termarrays hashcode breakout calculation of the termarrays equals todo: if ever we allow subclassing of the phrasescorer term doesn't exist in reader term does exist, but has no positions todo: move this init into positions(): if the search doesn't need the positions for this doc then don't waste cpu merging them: merge sort all positions together"
org.apache.lucene.search.ScoreCachingWrappingScorer "a {@link scorer} which wraps another scorer and caches the score of the current document. successive calls to {@link #score()} will return the same result and will not invoke the wrapped scorer's score() method, unless the current document has changed. this class might be useful due to the changes done to the {@link collector} interface, in which the score is not computed for a document by default, only if the collector requests it. some collectors may need to use the score in several places, however all they have in hand is a {@link scorer} object, and might end up computing the score of a document more than once. creates a new instance by wrapping the given scorer."
org.apache.lucene.search.FuzzyTermsEnum "subclass of termsenum for enumerating all terms that are similar to the specified filter term. term enumerations are always ordered by {@link #getcomparator}. each term in the enumeration is greater than all that precede it. constructor for enumeration of all terms from specified reader which share a prefix of length prefixlength with term and which have a fuzzy similarity &gt; minsimilarity.  after calling the constructor the enumeration is already pointing to the first valid term if such a term exists. delivers terms. {@link attributesource} return an automata-based enum for matching up to editdistance from lastterm, if possible initialize levenshtein dfas up to maxdistance, if possible swap in a new actual enum to proxy to fired when the max non-competitive boost has changed. this is the hook to swap in a smarter actualenum implement fuzzy enumeration with terms.intersect.  this is the fastest method as opposed to linearfuzzytermsenum: as enumeration is logarithmic to the number of terms (instead of linear) and comparison is linear to length of the term (rather than quadratic) finds the smallest lev(n) dfa that accepts the term. returns true if term is within k edits of the query term @lucene.internal @lucene.internal reuses compiled automata across different segments, because they are independent of the index @lucene.internal stores compiled automata as a list (indexed by edit distance) @lucene.internal todo: chicken-and-egg convert the string into a utf32 int[] representation for fast comparisons the prefix could be longer than the word. it's kind of silly though. it means we must match the entire word. if minsimilarity >= 1, we treat it as number of edits just driven by number of edits calculate the maximum k edits for this similarity if (blocktreetermswriter.debug) system.out.println("fuzzyte.getaenum: ed=" + editdistance + " lastterm=" + (lastterm==null ? "null" : lastterm.utf8tostring())); system.out.println("cached automata size: " + runautomata.size()); system.out.println("compute automaton n=" + i); constant prefix true if the last term encountered is lexicographically equal or after the bottom term in the pq as long as the max non-competitive boost is >= the max boost for some edit distance, keep dropping the max edit distance. the maximum n has changed instead of assert, we do a hard check in case someone uses our enum directly assert newenum != null; for some raw min similarity and input term length, the maximum # of edits for some number of edits, the maximum possible scaled boost clone the term before potentially doing something with it this is a rare but wonderful occurrence anyway proxy all other enum calls to the actual enum system.out.println("afte.accept term=" + term); we are wrapping either an intersect() termsenum or an automatontermsenum, so we know the outer dfa always matches. now compute exact edit distance system.out.println("check term=" + term.utf8tostring() + " ed=" + ed); scale to a boost and return (if similarity > minsimilarity) exact match system.out.println(" yes"); system.out.println(" yes");"
org.apache.lucene.search.Weight "expert: calculate query weights and build query scorers.  the purpose of {@link weight} is to ensure searching does not modify a {@link query}, so that a {@link query} instance can be reused.  {@link indexsearcher} dependent state of the query should reside in the {@link weight}.  {@link atomicreader} dependent state should reside in the {@link scorer}.  since {@link weight} creates {@link scorer} instances for a given {@link atomicreadercontext} ({@link #scorer(atomicreadercontext, boolean, boolean, bits)}) callers must maintain the relationship between the searcher's top-level {@link indexreadercontext} and the context used to create a {@link scorer}.  a weight is used in the following way:  a weight is constructed by a top-level query, given a indexsearcher ({@link query#createweight(indexsearcher)}). the {@link #getvaluefornormalization()} method is called on the weight to compute the query normalization factor {@link similarity#querynorm(float)} of the query clauses contained in the query. the query normalization factor is passed to {@link #normalize(float, float)}. at this point the weighting is complete. a scorer is constructed by {@link #scorer(atomicreadercontext, boolean, boolean, bits)}.  an explanation of the score computation for the named document. the readers context to create the {@link explanation} for. the document's id relative to the given context's reader explanation for the score @throws ioexception if an {@link ioexception} occurs the query that this concerns. the value for normalization of contained query clauses (e.g. sum of squared weights). assigns the query normalization factor and boost from parent queries to this. returns a {@link scorer} which scores documents in/out-of order according to scoredocsinorder.  note: even if scoredocsinorder is false, it is recommended to check whether the returned scorer indeed scores documents out of order (i.e., call {@link #scoresdocsoutoforder()}), as some scorer implementations will always return documents in-order. note: null can be returned if no documents will be scored by this query. the {@link atomicreadercontext} for which to return the {@link scorer}. specifies whether in-order scoring of documents is required. note that if set to false (i.e., out-of-order scoring is required), this method can return whatever scoring mode it supports, as every in-order scorer is also an out-of-order one. however, an out-of-order scorer may not support {@link scorer#nextdoc()} and/or {@link scorer#advance(int)}, therefore it is recommended to request an in-order scorer if use of these methods is required. if true, {@link scorer#score(collector)} will be called; if false, {@link scorer#nextdoc()} and/or {@link scorer#advance(int)} will be called. bits that represent the allowable docs to match (typically deleted docs but possibly filtering other documents) {@link scorer} which scores documents in/out-of order. @throws ioexception if there is a low-level i/o error returns true iff this implementation scores docs only out of order. this method is used in conjunction with {@link collector}'s {@link collector#acceptsdocsoutoforder() acceptsdocsoutoforder} and {@link #scorer(atomicreadercontext, boolean, boolean, bits)} to create a matching {@link scorer} instance for a given {@link collector}, or vice versa.  note: the default implementation returns false, i.e. the scorer scores documents in-order. javadocs javadocs"
org.apache.lucene.search.ReqExclScorer "a scorer for queries with a required subscorer and an excluding (prohibited) sub docidsetiterator.  this scorer implements {@link scorer#advance(int)}, and it uses the skipto() on the given scorers. construct a reqexclscorer. the scorer that must match, except where indicates exclusion. advance to non excluded doc. on entry:  reqscorer != null, exclscorer != null, reqscorer was advanced once via next() or skipto() and reqscorer.doc() may still be excluded.  advances reqscorer a non excluded required doc, if any. iff there is a non excluded required doc. returns the score of the current document matching the query. initially invalid, until {@link #nextdoc()} is called the first time. score of the required scorer. exhausted, nothing left may be excluded reqscorer advanced to before exclscorer, ie. not excluded exhausted, no more exclusions not excluded exhausted, nothing left reqscorer may be null when next() or skipto() already return false"
org.apache.lucene.search.TermQuery "a query that matches documents containing a term. this may be combined with other terms with a {@link booleanquery}. returns a {@link termsenum} positioned at this weights term or null if the term does not exist in the given context constructs a query for the term t. expert: constructs a termquery that will use the provided docfreq instead of looking up the docfreq against the searcher. expert: constructs a termquery that will use the provided docfreq instead of looking up the docfreq against the searcher. returns the term of this query. prints a returns true iff o is equal to this. returns a hash code value for this object. term is not present in that reader system.out.println("ld=" + reader.getlivedocs() + " set?=" + (reader.getlivedocs() != null ? reader.getlivedocs().get(0) : "null")); only called from assert system.out.println("tq.termnotinreader reader=" + reader + " term=" + field + ":" + bytes.utf8tostring()); make termquery single-pass if we don't have a prts or if the context differs! cache term lookups! prts was pre-build for this is we must not ignore the given docfreq - if set use the given value (lie)"
org.apache.lucene.search.SearcherLifetimeManager "keeps track of current plus old indexsearchers, closing the old ones once they have timed out. use it like this:  searcherlifetimemanager mgr = new searcherlifetimemanager();  per search-request, if it's a "new" search request, then obtain the latest searcher you have (for example, by using {@link searchermanager} or {@link nrtmanager}), and then record this searcher:  // record the current searcher, and save the returend // token into records that you are now using this indexsearcher. always call this when you've obtained a possibly new {@link indexsearcher}, for example from one of the get methods in {@link nrtmanager} or {@link searchermanager}. it's fine if you already passed the same searcher to this method before. this returns the long token that you can later pass to {@link #acquire} to retrieve the same indexsearcher. you should record this long token in the search results sent to your retrieve a previously recorded {@link indexsearcher}, if it has not yet been closed note: this may return null when the requested searcher has already timed out. when this happens you should notify your release a searcher previously obtained from {@link #acquire}. note: it's fine to call this after close. see {@link #prune}. return true if this searcher should be removed. how much time has passed since this searcher was the current (live) searcher searcher simple pruner that drops any searcher older by more than the specified seconds, than the newest searcher. calls provided {@link pruner} to prune entries. the entries are passed to the pruner in sorted (newest to oldest indexsearcher) order. note: you must peridiocally call this, ideally from the same background thread that opens new searchers. close this to future searching; any searches still in process in other threads won't be affected, and they should still call {@link #release} after they are done. note: you must ensure no other threads are calling {@link #record} while you call close(); otherwise it's possible not all searcher references will be freed. javadocs record the current searcher, and save the returend token into html form field): if possible, obtain the same searcher as the last search: searcher is still here do searching... do not use searcher after this! searcher was pruned -- notify out, or, pull fresh searcher again use nanotime not currenttimemillis since it [in theory] reduces risk from clock shift newer searchers are sort before older ones: be defensive: cannot subtract since it could technically overflow long, though, we'd never hit that in practice: todo: we could get by w/ just a "set"; need to have tracker hash by its version and have compareto(long) compare to its version todo: we don't have to use ir.getversion to track; could be risky (if it's buggy); we could get better bug isolation if we assign our own private id: system.out.println("record version=" + version + " ms=" + system.currenttimemillis()); another thread beat us -- must decref to undo incref done by searchertracker ctor: cannot just pass searchers.values() to arraylist ctor (not thread-safe since the values can change while arraylist is init'ing itself); must instead iterate ourselves: first tracker is always age 0.0 sec, since it's still "live"; second tracker's age (= seconds since it was "live") is now minus first tracker's recordtime, etc: system.out.println("prune version=" + tracker.version + " age=" + agesec + " ms=" + system.currenttimemillis()); remove up front in case exc below, so we don't over-decref on double-close: make some effort to catch mis-use:"
org.apache.lucene.search.FieldCacheDocIdSet "base class for docidset to be used with fieldcache. the implementation of its iterator is very stupid and slow if the implementation of the {@link #matchdoc} method is not optimized, as iterators simply increment the document id until {@code matchdoc(int)} returns true. because of this {@code matchdoc(int)} must be as fast as possible and in no case do any i/o. @lucene.internal this method checks, if a doc is a hit this docidset is always cacheable (does not go back to the reader for iteration) specialization optimization disregard acceptdocs special case for fixedbitset / openbitset: use the iterator and filter it (used e.g. when filters are chained by filteredquery) stupid consultation of acceptdocs and matchdoc()"
org.apache.lucene.search.Explanation "expert: describes the score computation for document and query. indicates whether or not this explanation models a good match.  by default, an explanation represents a "match" if the value is positive.  @see #getvalue the value assigned to this explanation node. sets the value assigned to this explanation node. a description of this explanation node. sets the description of this explanation node. a short one line summary which should contain all high level information about this explanation, without the "details" the sub-nodes of this explanation node. adds a sub-node to this explanation node. render an explanation as text. render an explanation as html. the value of this node what it represents sub-explanations"
org.apache.lucene.search.NRTManager "utility class to manage sharing near-real-time searchers across multiple searching thread. the difference vs searchermanager is that this class enables individual requests to wait until specific indexing changes are visible. you must create an indexwriter, then create a {@link nrtmanager.trackingindexwriter} from it, and pass that to the nrtmanager. you may want to create two nrtmanagers, once that always applies deletes on refresh and one that does not. in this case you should use a single {@link nrtmanager.trackingindexwriter} instance for both. then, use {@link #acquire} to obtain the {@link indexsearcher}, and {@link #release} (ideally, from within a finally clause) to release it. note: to use this class, you must call {@link #mayberefresh()} periodically. the {@link nrtmanagerreopenthread} is a simple class to do this on a periodic basis, and reopens more quickly if a request is waiting. if you implement your own reopener, be sure to call {@link #addwaitinglistener} so your reopener is notified when a caller is waiting for a specific generation searcher.  @see searcherfactory @lucene.experimental create new nrtmanager. trackingindexwriter to open near-real-time readers an optional {@link searcherfactory}. pass null if you don't require the searcher to be warmed before going live or other custom behavior. expert: just like {@link #nrtmanager(trackingindexwriter,searcherfactory)}, but you can also specify whether each reopened searcher must apply deletes. this is useful for cases where certain uses can tolerate seeing some deleted docs, since reopen time is faster if deletes need not be applied. nrtmanager invokes this interface to notify it when a caller is waiting for a specific generation searcher to be visible. adds a listener, to be notified when a caller is waiting for a specific generation searcher to be visible. remove a listener added with {@link #addwaitinglistener}. class that tracks changes to a delegated indexwriter. create this class (passing your indexwriter), and then pass this class to nrtmanager. be sure to make all changes via the trackingindexwriter, otherwise nrtmanager won't know about the changes. @lucene.experimental waits for the target generation to become visible in the searcher. if the current searcher is older than the target generation, this method will block until the searcher is reopened, by another via {@link #mayberefresh} or until the {@link nrtmanager} is closed. the generation to wait for waits for the target generation to become visible in the searcher. if the current searcher is older than the target generation, this method will block until the searcher has been reopened by another thread via {@link #mayberefresh}, the given waiting time has elapsed, or until the nrtmanager is closed.  note: if the waiting time elapses before the requested target generation is available the current {@link searchermanager} is returned instead. the generation to wait for the time to wait for the target generation the waiting time's time unit returns generation of current searcher. returns true if no changes have occured since this searcher ie. reader was opened, otherwise false. @see directoryreader#iscurrent() javadocs javadocs javadocs return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: return gen as of when indexing finished: record gen as of when reopen started: update searchinggen: wake up threads if we have a new generation: max it out to make sure nobody can wait on another gen"
org.apache.lucene.search.SearcherManager "utility class to safely share {@link indexsearcher} instances across multiple threads, while periodically reopening. this class ensures each searcher is closed only once all threads have finished using it.  use {@link #acquire} to obtain the current searcher, and {@link #release} to release it, like this:  indexsearcher s = manager.acquire(); try { // do searching, doc retrieval, etc. with s } finally { manager.release(s); } // do not use s after this! s = null;   in addition you should periodically call {@link #mayberefresh}. while it's possible to call this just before running each query, this is discouraged since it penalizes the unlucky queries that do the reopen. it's better to use a separate background thread, that periodically calls maybereopen. finally, be sure to call {@link #close} once you are done. @see searcherfactory @lucene.experimental creates and returns a new searchermanager from the given {@link indexwriter}. the indexwriter to open the indexreader from. if true, all buffered deletes will be applied (made visible) in the {@link indexsearcher} / {@link directoryreader}. if false, the deletes may or may not be applied, but remain buffered (in indexwriter) so that they will be applied in the future. applying deletes can be costly, so if your app can tolerate deleted documents being returned you might gain some performance by passing false. see {@link directoryreader#openifchanged(directoryreader, indexwriter, boolean)}. an optional {@link searcherfactory}. pass null if you don't require the searcher to be warmed before going live or other custom behavior. @throws ioexception if there is a low-level i/o error creates and returns a new searchermanager from the given {@link directory}. the directory to open the directoryreader on. an optional {@link searcherfactory}. pass null if you don't require the searcher to be warmed before going live or other custom behavior. @throws ioexception if there is a low-level i/o error returns true if no changes have occured since this searcher ie. reader was opened, otherwise false. @see directoryreader#iscurrent() do searching, doc retrieval, etc. with s do not use s after this! note: decrefs incoming reader on throwing an exception"
org.apache.lucene.search.TermRangeFilter "a filter that restricts search results to a range of term values in a given field. this filter matches the documents looking for terms that fall into the supplied range according to {@link byte#compareto(byte)}, it is not intended for numerical ranges; use {@link numericrangefilter} instead. if you construct a large number of range filters with different ranges but on the same field, {@link fieldcacherangefilter} may have significantly better performance. the field this range applies to the lower bound on this range the upper bound on this range does this range include the lower bound? does this range include the upper bound? @throws illegalargumentexception if both terms are null or if lowerterm is null and includelower is true (similar for upperterm and includeupper) factory that creates a new termrangefilter using strings for term text. constructs a filter for field fieldname matching less than or equal to upperterm. constructs a filter for field fieldname matching greater than or equal to lowerterm. returns the lower value of this range filter returns the upper value of this range filter returns true if the lower endpoint is inclusive returns true if the upper endpoint is inclusive"
org.apache.lucene.search.ComplexExplanation "expert: describes the score computation for document and query, and can distinguish a match independent of a positive value. the match status of this explanation node. be null if match status is unknown sets the match status assigned to this explanation node. may be null if match status is unknown indicates whether or not this explanation models a good match.  if the match status is explicitly set (i.e.: not null) this method uses it; otherwise it defers to the superclass.  @see #getmatch note: use of "boolean" instead of "boolean" in params is conscious choice to encourage clients to be specific."
org.apache.lucene.search.Collector "expert: collectors are primarily meant to be used to gather raw results from a search, and implement sorting or custom result filtering, collation, etc.  lucene's core collectors are derived from collector. likely your application can use one of these classes, or subclass {@link topdocscollector}, instead of implementing collector directly:  {@link topdocscollector} is an abstract base class that assumes you will retrieve the top n docs, according to some criteria, after collection is done.  {@link topscoredoccollector} is a concrete subclass {@link topdocscollector} and sorts according to score + docid. this is used internally by the {@link indexsearcher} search methods that do not take an explicit {@link sort}. it is likely the most frequently used collector. {@link topfieldcollector} subclasses {@link topdocscollector} and sorts according to a specified {@link sort} object (sort by field). this is used internally by the {@link indexsearcher} search methods that take an explicit {@link sort}. {@link timelimitingcollector}, which wraps any other collector and aborts the search if it's taken too much time. {@link positivescoresonlycollector} wraps any other collector and prevents collection of hits whose score is &lt;= 0.0  collector decouples the score from the collected doc: the score computation is skipped entirely if it's not needed. collectors that do need the score should implement the {@link #setscorer} method, to hold onto the passed {@link scorer} instance, and call {@link scorer#score()} within the collect method to compute the current hit's score. if your collector may request the score for a single hit multiple times, you should use {@link scorecachingwrappingscorer}.  note: the doc that is passed to the collect method is relative to the current reader. if your collector needs to resolve this to the docid space of the multireader, you must re-base it by recording the docbase from the most recent setnextreader call. here's a simple example showing how to collect docids into a bitset:  indexsearcher searcher = new indexsearcher(indexreader); final bitset bits = new bitset(indexreader.maxdoc()); searcher.search(query, new collector() { private int docbase; // ignore scorer public void setscorer(scorer scorer) { } // accept docs out of order (for a bitset it doesn't matter) public boolean acceptsdocsoutoforder() { return true; } public void collect(int doc) { bits.set(doc + docbase); } public void setnextreader(atomicreadercontext context) { this.docbase = context.docbase; } });  not all collectors will need to rebase the docid. for example, a collector that simply counts the total number of hits would skip it. note: prior to 2.9, lucene silently filtered out hits with score  @lucene.experimental called before successive calls to {@link #collect(int)}. implementations that need the score of the current document (passed-in to {@link #collect(int)}), should save the passed-in scorer and call scorer.score() when needed. called once for every document matching a query, with the unbased document number.  note: this is called in an inner search loop. for good search performance, implementations of this method should not call {@link indexsearcher#doc(int)} or {@link org.apache.lucene.index.indexreader#document(int)} on every hit. doing so can slow searches by an order of magnitude or more. called before collecting from each {@link atomicreadercontext}. all doc ids in {@link #collect(int)} will correspond to {@link indexreadercontext#reader}. add {@link atomicreadercontext#docbase} to the current {@link indexreadercontext#reader}'s internal document id to re-base ids in {@link #collect(int)}. next atomic reader context return true if this collector does not require the matching docids to be delivered in int sort order (smallest to largest) to {@link #collect}.  most lucene query implementations will visit matching docids in order. however, some queries (currently limited to certain cases of {@link booleanquery}) can achieve faster searching if the collector allows them to deliver the docids out of order.  many collectors don't mind getting docids out of order, so it's important to return true here."
org.apache.lucene.search.payloads.MinPayloadFunction "calculates the minimum payload seen"
org.apache.lucene.search.payloads.PayloadSpanUtil "experimental class to get set of payloads for most standard lucene queries. operates like highlighter - indexreader should only contain doc of interest, best to use memoryindex. @lucene.experimental that contains doc with payloads to extract @see indexreader#getcontext() query should be rewritten for wild/fuzzy support. rewritten query collection @throws ioexception if there is a low-level i/o error"
org.apache.lucene.search.payloads.MaxPayloadFunction "returns the maximum payload score seen, else 1 if there are no payloads on the doc.  is thread safe and completely reusable."
org.apache.lucene.search.payloads.PayloadNearQuery "this class is very similar to {@link org.apache.lucene.search.spans.spannearquery} except that it factors in the value of the payloads located at each of the positions where the {@link org.apache.lucene.search.spans.termspans} occurs.  note: in order to take advantage of this with the default scoring implementation ({@link defaultsimilarity}), you must override {@link defaultsimilarity#scorepayload(int, int, int, bytesref)}, which returns 1 by default.  payload scores are aggregated using a pluggable {@link payloadfunction}. @see org.apache.lucene.search.similarities.similarity.sloppysimscorer#computepayloadfactor(int, int, int, bytesref) by default, uses the {@link payloadfunction} to score the payloads, but can be overridden to do other things. the payloads the start position of the span being scored the end position of the span being scored @see spans all clauses must have same field now the payloads part combined get the payloads associated with all underlying subspans todo change the whole spans api to use bytesref, or nuke spans"
org.apache.lucene.search.payloads.PayloadFunction "an abstract class that defines a way for payloadquery instances to transform the cumulative effects of payload scores for a document. @see org.apache.lucene.search.payloads.payloadtermquery for more information @lucene.experimental this class and its derivations are experimental and subject to change calculate the score up to this point for this doc and field the current doc the field the start position of the matching span the end position of the matching span the number of payloads seen so far the current score so far the score for the current payload new current score @see org.apache.lucene.search.spans.spans calculate the final score for all the payloads seen so far for this doc/field the current doc the current field the total number of payloads seen on this document the raw score for those payloads final score for the payloads"
org.apache.lucene.search.payloads.AveragePayloadFunction "calculate the final score as the average score of all payloads seen.  is thread safe and completely reusable."
org.apache.lucene.search.payloads.PayloadTermQuery "this class is very similar to {@link org.apache.lucene.search.spans.spantermquery} except that it factors in the value of the payload located at each of the positions where the {@link org.apache.lucene.index.term} occurs.  note: in order to take advantage of this with the default scoring implementation ({@link defaultsimilarity}), you must override {@link defaultsimilarity#scorepayload(int, int, int, bytesref)}, which returns 1 by default.  payload scores are aggregated using a pluggable {@link payloadfunction}. @see org.apache.lucene.search.similarities.similarity.sloppysimscorer#computepayloadfactor(int, int, int, bytesref) {@link #getspanscore()} {@link #getpayloadscore()} @throws ioexception if there is a low-level i/o error returns the spanscorer score only.  should not be overridden without good cause! score for just the span part w/o the payload @throws ioexception if there is a low-level i/o error @see #score() the score for the payload score, as calculated by {@link payloadfunction#docscore(int, string, int, float)} document zero out the payload? now the payloads part question: is there a way to avoid this skipto call? we need to know whether to load the payload or not gsi: i suppose we could tostring the payload, but i don't think that would be a good idea combined lucene-1303"
org.apache.lucene.search.similarities.BasicModel "this class acts as the base class for the specific basic model implementations in the dfr framework. basic models compute the informative content inf1 = -log2prob1 . @see dfrsimilarity @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns the informative content score. returns an explanation for the score. most basic models use the number of documents and the total term frequency to compute inf1. this method provides a generic explanation for such models. subclasses that use other statistics must override this method. subclasses must override this method to return the code of the basic model formula. refer to the original paper for the list."
org.apache.lucene.search.similarities.DistributionLL "log-logistic distribution. unlike for dfr, the natural logarithm is used, as it is faster to compute and the original paper does not express any preference to a specific base. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.BasicModelBE "limiting form of the bose-einstein model. the formula used in lucene differs slightly from the one in the original paper: {@code f} is increased by {@code tfn+1} and {@code n} is increased by {@code f} @lucene.experimental note: in some corner cases this model may give poor performance with normalizations that return large values for {@code tfn} such as normalizationh3. consider using the geometric approximation ({@link basicmodelg}) instead, which provides the same relevance but with less practical problems. sole constructor: parameter-free the f helper function defined for be. approximation only holds true when f << n, so we use n += f"
org.apache.lucene.search.similarities.AfterEffectB "model of the information gain based on the ratio of two bernoulli processes. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.BM25Similarity "bm25 similarity. introduced in stephen e. robertson, steve walker, susan jones, micheline hancock-beaulieu, and mike gatford. okapi at trec-3. in proceedings of the third text retrieval conference (trec 1994). gaithersburg, usa, november 1994. @lucene.experimental bm25 with the supplied parameter values. 1 controls non-linear term frequency normalization (saturation). controls to what degree document length normalizes tf values. bm25 with these default values:  {@code k1 = 1.2}, {@code b = 0.75}.  implemented as log(1 + (numdocs - docfreq + 0.5)/(docfreq + 0.5)). implemented as 1 / (distance + 1). the default implementation returns 1 the default implementation computes the average as sumtotaltermfreq / maxdoc, or returns 1 if the index does not store sumtotaltermfreq (lucene 3.x indexes or any field that omits frequency information). the default implementation encodes boost / sqrt(length) with {@link smallfloat#floattobyte315(float)}. this is compatible with lucene's default implementation. if you change this, then you should change {@link #decodenormvalue(byte)} to match. the default implementation returns 1 / f2 where f is {@link smallfloat#byte315tofloat(byte)}. true if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. sets whether overlap tokens (tokens with 0 position increment) are ignored when computing norm. by default this is true, meaning overlap tokens do not count when computing norms. returns true if overlap tokens are discounted from the document's length. @see #setdiscountoverlaps cache of decoded bytes. computes a score factor for a simple term and returns an explanation for that score factor.  the default implementation uses:  idf(docfreq, searcher.maxdoc());  note that {@link collectionstatistics#maxdoc()} is used instead of {@link org.apache.lucene.index.indexreader#numdocs() indexreader#numdocs()} because also {@link termstatistics#docfreq()} is used, and when the latter is inaccurate, so is {@link collectionstatistics#maxdoc()}, and in the same direction. in addition, {@link collectionstatistics#maxdoc()} is more efficient to compute collection-level statistics term-level statistics for the term explain object that includes both an idf score factor and an explanation for the term. computes a score factor for a phrase.  the default implementation sums the idf factor for each term in the phrase. collection-level statistics term-level statistics for the terms in the phrase explain object that includes both an idf score factor for the phrase and an explanation for each term. there are no norms, we act as if b=0 collection statistics for the bm25 model. bm25's idf the average document length. query's inner boost query's outer boost (only for explain) weight (idf boost) field name, for pulling norms precomputed norm[256] with k1 ((1 - b) + b dl / avgdl) returns the k1 parameter @see #bm25similarity(float, float) returns the b parameter @see #bm25similarity(float, float) todo: should we add a delta like sifaka.cs.uiuc.edu/~ylv2/pub/sigir11-bm25l.pdf ? field does not exist, or stat is unsupported compute freq-independent part of bm25 equation across all norm values boost idf (k1 + 1) boost idf (k1 + 1) todo: maybe score cache is more trouble than its worth? check cache cache hit cache miss boost idf (k1 + 1) if there are no norms, we act as if b=0 we return a tf-idf like normalization to be nice, but we don't actually normalize ourselves. we don't normalize with querynorm at all, we just capture the top-level boost"
org.apache.lucene.search.similarities.BasicModelIn "the basic tf-idf model of randomness. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.DefaultSimilarity "expert: default scoring implementation. sole constructor: parameter-free implemented as overlap / maxoverlap. implemented as 1/sqrt(sumofsquaredweights). implemented as state.getboost()lengthnorm(numterms), where numterms is {@link fieldinvertstate#getlength()} if {@link #setdiscountoverlaps} is false, else it's {@link fieldinvertstate#getlength()} - {@link fieldinvertstate#getnumoverlap()}. @lucene.experimental implemented as sqrt(freq). implemented as 1 / (distance + 1). the default implementation returns 1 implemented as log(numdocs/(docfreq+1)) + 1. true if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. determines whether overlap tokens (tokens with 0 position increment) are ignored when computing norm. by default this is true, meaning overlap tokens do not count when computing norms. @lucene.experimental @see #computenorm returns true if overlap tokens are discounted from the document's length. @see #setdiscountoverlaps"
org.apache.lucene.search.similarities.BasicModelG "geometric as limiting form of the bose-einstein model. the formula used in lucene differs slightly from the one in the original paper: {@code f} is increased by {@code 1} and {@code n} is increased by {@code f}. @lucene.experimental sole constructor: parameter-free just like in be, approximation only holds true when f  log(lambda + 1)"
org.apache.lucene.search.similarities.BasicModelD "implements the approximation of the binomial model with the divergence for dfr. the formula used in lucene differs slightly from the one in the original paper: to avoid underflow for small values of {@code n} and {@code f}, {@code n} is increased by {@code 1} and {@code f} is always increased by {@code tfn+1}.  warning: for terms that do not meet the expected random distribution (e.g. stopwords), this model may give poor performance, such as abnormally high scores for low tf values. @lucene.experimental sole constructor: parameter-free we have to ensure phi is always < 1 for tiny ttf values, otherwise nphi can go negative, resulting in nan. cleanest way is to unconditionally always add tfn to totaltermfreq to create a 'normalized' f."
org.apache.lucene.search.similarities.AfterEffect "this class acts as the base class for the implementations of the first normalization of the informative content in the dfr framework. this component is also called the after effect and is defined by the formula inf2 = 1 - prob2, where prob2 measures the information gain. @see dfrsimilarity @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns the aftereffect score. returns an explanation for the score. implementation used when there is no aftereffect. sole constructor: parameter-free subclasses must override this method to return the code of the after effect formula. refer to the original paper for the list."
org.apache.lucene.search.similarities.BasicModelP "implements the poisson approximation for the binomial model for dfr. @lucene.experimental  warning: for terms that do not meet the expected random distribution (e.g. stopwords), this model may give poor performance, such as abnormally high scores for low tf values. {@code log2(math.e)}, precomputed. sole constructor: parameter-free"
org.apache.lucene.search.similarities.NormalizationZ "pareto-zipf normalization @lucene.experimental calls {@link #normalizationz(float) normalizationz(0.3)} creates normalizationz with the supplied parameter z. represents a/(a+1) where a measures the specificity of the language. returns the parameter z @see #normalizationz(float)"
org.apache.lucene.search.similarities.DistributionSPL "the smoothed power-law (spl) distribution for the information-based framework that is described in the original paper. unlike for dfr, the natural logarithm is used, as it is faster to compute and the original paper does not express any preference to a specific base. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.BasicModelIne "tf-idf model of randomness, based on a mixture of poisson and inverse document frequency. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.BasicStats "stores all statistics commonly used ranking methods. @lucene.experimental the number of documents. the total number of tokens in the field. the average field length. the document frequency. the total number of occurrences of this term across all documents. query's inner boost. any outer query's boost. for most similarities, the immediate and the top level query boosts are not handled differently. hence, this field is just the product of the other two. constructor. sets the query boost. returns the number of documents. sets the number of documents. returns the total number of tokens in the field. @see terms#getsumtotaltermfreq() sets the total number of tokens in the field. @see terms#getsumtotaltermfreq() returns the average field length. sets the average field length. returns the document frequency. sets the document frequency. returns the total number of occurrences of this term across all documents. sets the total number of occurrences of this term across all documents. the square of the raw normalization value. @see #rawnormalizationvalue() computes the raw normalization value. this basic implementation returns the query boost. subclasses may override this method to include other factors (such as idf), or to save the value for inclusion in {@link #normalize(float, float)}, etc. no normalization is done. {@code toplevelboost} is saved in the object, however. returns the total boost. -------------------------- boost-related stuff -------------------------- ------------------------- getter/setter methods ------------------------- -------------------------- boost-related stuff --------------------------"
org.apache.lucene.search.similarities.NormalizationH3 "dirichlet priors normalization @lucene.experimental calls {@link #normalizationh3(float) normalizationh3(800)} creates normalizationh3 with the supplied parameter &mu;. smoothing parameter &mu; returns the parameter &mu; @see #normalizationh3(float)"
org.apache.lucene.search.similarities.NormalizationH2 "normalization model in which the term frequency is inversely related to the length. while this model is parameterless in the  original article, the thesis introduces the parameterized variant. the default value for the {@code c} parameter is {@code 1}. @lucene.experimental creates normalizationh2 with the supplied parameter c. hyper-parameter that controls the term frequency normalization with respect to the document length. calls {@link #normalizationh2(float) normalizationh2(1)} returns the c parameter. @see #normalizationh2(float)"
org.apache.lucene.search.similarities.LMSimilarity "abstract superclass for language modeling similarities. the following inner types are introduced:  {@link lmstats}, which defines a new statistic, the probability that the collection language model generates the current term; {@link collectionmodel}, which is a strategy interface for object that compute the collection language model {@code p(w|c)}; {@link defaultcollectionmodel}, an implementation of the former, that computes the term probability as the number of occurrences of the term in the collection, divided by the total number of tokens.  @lucene.experimental the collection model. creates a new instance with the specified collection language model. creates a new instance with the default collection language model. computes the collection probability of the current term in addition to the usual statistics. returns the name of the lm method. the values of the parameters should be included as well. used in {@link #tostring()}. returns the name of the lm method. if a custom collection model strategy is used, its name is included as well. @see #getname() @see collectionmodel#getname() @see defaultcollectionmodel stores the collection distribution of the current term. the probability that the current term is generated by the collection. creates lmstats for the provided field and query-time boost returns the probability that the current term is generated by the collection. sets the probability that the current term is generated by the collection. a strategy for computing the collection language model. computes the probability {@code p(w|c)} according to the language model strategy for the current term. the name of the collection model strategy. models {@code p(w|c)} as the number of occurrences of the term in the collection, divided by the total number of tokens {@code + 1}. sole constructor: parameter-free"
org.apache.lucene.search.similarities.Similarity "similarity defines the components of lucene scoring.  expert: scoring api.  this is a low-level api, you should only extend this api if you want to implement an information retrieval model. if you are instead looking for a convenient way to alter lucene's scoring, consider extending a higher-level implementation such as {@link tfidfsimilarity}, which implements the vector space model with this api, or just tweaking the default implementation: {@link defaultsimilarity}.  similarity determines how lucene weights terms, and lucene interacts with this class at both index-time and query-time.   at indexing time, the indexer calls {@link #computenorm(fieldinvertstate, norm)}, allowing the similarity implementation to set a per-document value for the field that will be later accessible via {@link atomicreader#normvalues(string)}. lucene makes no assumption about what is in this norm, but it is most useful for encoding length normalization information.  implementations should carefully consider how the normalization is encoded: while lucene's classical {@link tfidfsimilarity} encodes a combination of index-time boost and length normalization information with {@link smallfloat} into a single byte, this might not be suitable for all purposes.  many formulas require the use of average document length, which can be computed via a combination of {@link collectionstatistics#sumtotaltermfreq()} and {@link collectionstatistics#maxdoc()} or {@link collectionstatistics#doccount()}, depending upon whether the average should reflect field sparsity.  additional scoring factors can be stored in named docvaluesfields (such as {@link bytedocvaluesfield} or {@link floatdocvaluesfield}), and accessed at query-time with {@link atomicreader#docvalues(string)}.  finally, using index-time boosts (either via folding into the normalization byte or via docvalues), is an inefficient way to boost the scores of different fields if the boost will be the same for every document, instead the similarity can simply take a constant boost parameter c, and {@link perfieldsimilaritywrapper} can return different instances with different boosts depending upon field name.   at query-time, queries interact with the similarity via these steps:  the {@link #computeweight(float, collectionstatistics, termstatistics...)} method is called a single time, allowing the implementation to compute any statistics (such as idf, average document length, etc) across the entire collection. the {@link termstatistics} and {@link collectionstatistics} passed in already contain all of the raw statistics involved, so a similarity can freely use any combination of statistics without causing any additional i/o. lucene makes no assumption about what is stored in the returned {@link similarity.simweight} object. the query normalization process occurs a single time: {@link similarity.simweight#getvaluefornormalization()} is called for each query leaf node, {@link similarity#querynorm(float)} is called for the top-level query, and finally {@link similarity.simweight#normalize(float, float)} passes down the normalization value and any top-level boosts (e.g. from enclosing {@link booleanquery}s). for each segment in the index, the query creates a {@link #exactsimscorer(simweight, atomicreadercontext)} (for queries with exact frequencies such as termquerys and exact phrasequeries) or a {@link #sloppysimscorer(simweight, atomicreadercontext)} (for queries with sloppy frequencies such as spanquerys and sloppy phrasequeries). the score() method is called for each matching document.    when {@link indexsearcher#explain(org.apache.lucene.search.query, int)} is called, queries consult the similarity's docscorer for an explanation of how it computed its score. the query passes in a the document id and an explanation of how the frequency was computed. @see org.apache.lucene.index.indexwriterconfig#setsimilarity(similarity) @see indexsearcher#setsimilarity(similarity) @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) hook to integrate coordinate-level matching.  by default this is disabled (returns 1), as with most modern models this will only skew performance, but some implementations such as {@link tfidfsimilarity} override this. the number of query terms matched in the document the total number of terms in the query score factor based on term overlap with the query computes the normalization value for a query given the sum of the normalized weights {@link simweight#getvaluefornormalization()} of each of the query terms. this value is passed back to the weight ({@link simweight#normalize(float, float)} of each query term, to provide a hook to attempt to make scores from different queries comparable.  by default this is disabled (returns 1), but some implementations such as {@link tfidfsimilarity} override this. the sum of the term normalization values normalization factor for query weights computes the normalization value for a field, given the accumulated state of term processing for this field (see {@link fieldinvertstate}). implementations should calculate a norm value based on the field state and set that value to the given {@link norm}. matches in longer fields are less precise, so implementations of this method usually set smaller values when state.getlength() is large, and larger values when state.getlength() is small. @lucene.experimental current processing state for this field holds the computed norm value when this method returns compute any collection-level weight (e.g. idf, average document length, etc) needed for scoring a query. the query-time boost. collection-level statistics, such as the number of tokens in the collection. term-level statistics, such as the document frequency of a term across the collection. object with the information this similarity needs to score a query. creates a new {@link similarity.exactsimscorer} to score matching documents from a segment of the inverted index. collection information from {@link #computeweight(float, collectionstatistics, termstatistics...)} segment of the inverted index to be scored. for scoring documents across context @throws ioexception if there is a low-level i/o error creates a new {@link similarity.sloppysimscorer} to score matching documents from a segment of the inverted index. collection information from {@link #computeweight(float, collectionstatistics, termstatistics...)} segment of the inverted index to be scored. for scoring documents across context @throws ioexception if there is a low-level i/o error api for scoring exact queries such as {@link termquery} and exact {@link phrasequery}.  frequencies are integers (the term or phrase frequency within the document) sole constructor. (for invocation by subclass constructors, typically implicit.) score a single document document id term frequency 's score explain the score for a single document document id explanation of how the term frequency was computed 's score api for scoring "sloppy" queries such as {@link spanquery} and sloppy {@link phrasequery}.  frequencies are floating-point values: an approximate within-document frequency adjusted for "sloppiness" by {@link sloppysimscorer#computeslopfactor(int)}. sole constructor. (for invocation by subclass constructors, typically implicit.) score a single document document id within the inverted index segment sloppy term frequency 's score computes the amount of a sloppy phrase match, based on an edit distance. calculate a scoring factor based on the data in the payload. explain the score for a single document document id within the inverted index segment explanation of how the sloppy term frequency was computed 's score stores the weight for a query across the indexed collection. this abstract implementation is empty; descendants of {@code similarity} should subclass {@code simweight} and define the statistics they require in the subclass. examples include idf, average field length, etc. sole constructor. (for invocation by subclass constructors, typically implicit.) the value for normalization of contained query clauses (e.g. sum of squared weights).  note: a similarity implementation might not use any query normalization at all, its not required. however, if it wants to participate in query normalization, it can return a value here. assigns the query normalization factor and boost from parent queries to this.  note: a similarity implementation might not use this normalized value at all, its not required. however, its usually a good idea to at least incorporate the toplevelboost (e.g. from an outer booleanquery) into its score. javadoc javadoc javadoc javadoc javadoc"
org.apache.lucene.search.similarities.LambdaDF "computes lambda as {@code docfreq+1 / numberofdocuments+1}. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.TFIDFSimilarity "implementation of {@link similarity} with the vector space model.  expert: scoring api. tfidfsimilarity defines the components of lucene scoring. overriding computation of these components is a convenient way to alter lucene scoring. suggested reading:  introduction to information retrieval, chapter 6. the following describes how lucene scoring evolves from underlying information retrieval models to (efficient) implementation. we first brief on vsm score, then derive from it lucene's conceptual scoring formula, from which, finally, evolves lucene's practical scoring function (the latter is connected directly with lucene classes and methods). lucene combines  boolean model (bm) of information retrieval with  vector space model (vsm) of information retrieval - documents "approved" by bm are scored by vsm. in vsm, documents and queries are represented as weighted vectors in a multi-dimensional space, where each distinct index term is a dimension, and weights are tf-idf values. vsm does not require weights to be tf-idf values, but tf-idf values are believed to produce search results of high quality, and so lucene is using tf-idf. tf and idf are described in more detail below, but for now, for completion, let's just say that for given term t and document (or query) x, tf(t,x) varies with the number of occurrences of term t in x (when one increases so does the other) and idf(t) similarly varies with the inverse of the number of index documents containing term t. vsm score of document d for query q is the  cosine similarity of the weighted query vectors v(q) and v(d): &nbsp;        cosine-similarity(q,d) &nbsp; = &nbsp;    v(q)&nbsp;&middot;&nbsp;v(d) &ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash; |v(q)|&nbsp;|v(d)|         vsm score   &nbsp; where v(q) &middot; v(d) is the dot product of the weighted vectors, and |v(q)| and |v(d)| are their euclidean norms. note: the above equation can be viewed as the dot product of the normalized weighted vectors, in the sense that dividing v(q) by its euclidean norm is normalizing it to a unit vector. lucene refines vsm score for both search quality and usability:  normalizing v(d) to the unit vector is known to be problematic in that it removes all document length information. for some documents removing this info is probably ok, e.g. a document made by duplicating a certain paragraph 10 times, especially if that paragraph is made of distinct terms. but for a document which contains no duplicated paragraphs, this might be wrong. to avoid this problem, a different document length normalization factor is used, which normalizes to a vector equal to or larger than the unit vector: doc-len-norm(d).  at indexing, sole constructor. (for invocation by subclass constructors, typically implicit.) computes a score factor based on the fraction of all query terms that a document contains. this value is multiplied into scores. the presence of a large portion of the query terms indicates a better match with the query, so implementations of this method usually return larger values when the ratio between these parameters is large and smaller values when the ratio between them is small. the number of query terms matched in the document the total number of terms in the query score factor based on term overlap with the query computes the normalization value for a query given the sum of the squared weights of each of the query terms. this value is multiplied into the weight of each query term. while the classic query normalization factor is computed as 1/sqrt(sumofsquaredweights), other implementations might completely ignore sumofsquaredweights (ie return 1). this does not affect ranking, but the default implementation does make scores from different queries more comparable than they would be by eliminating the magnitude of the query vector as a factor in the score. the sum of the squares of query term weights normalization factor for query weights computes a score factor based on a term or phrase's frequency in a document. this value is multiplied by the {@link #idf(long, long)} factor for each term in the query and these products are then summed to form the initial score for a document. terms and phrases repeated in a document indicate the topic of the document, so implementations of this method usually return larger values when freq is large, and smaller values when freq is small. the default implementation calls {@link #tf(float)}. the frequency of a term within a document score factor based on a term's within-document frequency computes a score factor based on a term or phrase's frequency in a document. this value is multiplied by the {@link #idf(long, long)} factor for each term in the query and these products are then summed to form the initial score for a document. terms and phrases repeated in a document indicate the topic of the document, so implementations of this method usually return larger values when freq is large, and smaller values when freq is small. the frequency of a term within a document score factor based on a term's within-document frequency computes a score factor for a simple term and returns an explanation for that score factor.  the default implementation uses:  idf(docfreq, searcher.maxdoc());  note that {@link collectionstatistics#maxdoc()} is used instead of {@link org.apache.lucene.index.indexreader#numdocs() indexreader#numdocs()} because also {@link termstatistics#docfreq()} is used, and when the latter is inaccurate, so is {@link collectionstatistics#maxdoc()}, and in the same direction. in addition, {@link collectionstatistics#maxdoc()} is more efficient to compute collection-level statistics term-level statistics for the term explain object that includes both an idf score factor and an explanation for the term. computes a score factor for a phrase.  the default implementation sums the idf factor for each term in the phrase. collection-level statistics term-level statistics for the terms in the phrase explain object that includes both an idf score factor for the phrase and an explanation for each term. computes a score factor based on a term's document frequency (the number of documents which contain the term). this value is multiplied by the {@link #tf(int)} factor for each term in the query and these products are then summed to form the initial score for a document. terms that occur in fewer documents are better indicators of topic, so implementations of this method usually return larger values for rare terms, and smaller values for common terms. the number of documents which contain the term the total number of documents in the collection score factor based on the term's document frequency compute an index-time normalization value for this field instance.  this value will be stored in a single byte lossy representation by {@link #encodenormvalue(float)}. statistics of the current field (such as length, boost, etc) index-time normalization value cache of decoded bytes. decodes a normalization factor stored in an index. @see #encodenormvalue(float) encodes a normalization factor for storage in an index. the encoding uses a three-bit mantissa, a five-bit exponent, and the zero-exponent point at 15, thus representing values from around 7x10^9 to 2x10^-9 with about one significant decimal digit of accuracy. zero is also represented. negative numbers are rounded up to zero. values too large to represent are rounded down to the largest representable value. positive values too small to represent are rounded up to the smallest positive representable value. @see org.apache.lucene.document.field#setboost(float) @see org.apache.lucene.util.smallfloat computes the amount of a sloppy phrase match, based on an edit distance. this value is summed for each sloppy phrase match in a document to form the frequency to be used in scoring instead of the exact term count. a phrase match with a small edit distance to a document passage more closely matches the document, so implementations of this method usually return larger values when the edit distance is small and smaller values when it is large. @see phrasequery#setslop(int) the edit distance of this sloppy phrase match frequency increment for this match calculate a scoring factor based on the data in the payload. implementations are responsible for interpreting what is in the payload. lucene makes no assumptions about what is in the byte array. the docid currently being scored. the start position of the payload the end position of the payload the payload byte array to be scored implementation dependent float to be used as a scoring factor collection statistics for the tf-idf model. the only statistic of interest to this model is idf. the idf and its explanation & 0xff maps negative bytes to positive above 127 todo: we can specialize these for omitnorms up front, but we should test that it doesn't confuse stupid hotspot. compute tf(f)weight check cache cache hit cache miss normalize for field compute tf(f)weight normalize for field todo: validate? compute query weight todo: (sorta lucene-1907) make non-static class and expose this squaring via a nice method to subclasses? sum of squared weights normalize query weight idf for document explain query weight explain field weight combine them"
org.apache.lucene.search.similarities.Normalization "this class acts as the base class for the implementations of the term frequency normalization methods in the dfr framework. @see dfrsimilarity @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns the normalized term frequency. the field length. returns an explanation for the normalized term frequency. the default normalization methods use the field length of the document and the average field length to compute the normalized term frequency. this method provides a generic explanation for such methods. subclasses that use other statistics must override this method. implementation used when there is no normalization. sole constructor: parameter-free subclasses must override this method to return the code of the normalization formula. refer to the original paper for the list."
org.apache.lucene.search.similarities.MultiSimilarity "implements the combsum method for combining evidence from multiple similarity values described in: joseph a. shaw, edward a. fox. in text retrieval conference (1993), pp. 243-252 @lucene.experimental the sub-similarities used to create the combined score creates a multisimilarity which will sum the scores of the provided sims."
org.apache.lucene.search.similarities.Lambda "the lambda (&lambda;w) parameter in information-based models. @see ibsimilarity @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) computes the lambda parameter. explains the lambda parameter. subclasses must override this method to return the code of the lambda formula. since the original paper is not very clear on this matter, and also uses the dfr naming scheme incorrectly, the codes here were chosen arbitrarily."
org.apache.lucene.search.similarities.LambdaTTF "computes lambda as {@code totaltermfreq+1 / numberofdocuments+1}. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.DFRSimilarity "implements the divergence from randomness (dfr) framework introduced in gianni amati and cornelis joost van rijsbergen. 2002. probabilistic models of information retrieval based on measuring the divergence from randomness. acm trans. inf. syst. 20, 4 (october 2002), 357-389. the dfr scoring formula is composed of three separate components: the basic model, the aftereffect and an additional normalization component, represented by the classes {@code basicmodel}, {@code aftereffect} and {@code normalization}, respectively. the names of these classes were chosen to match the names of their counterparts in the terrier ir engine. to construct a dfrsimilarity, you must specify the implementations for all three components of dfr:  {@link basicmodel}: basic model of information content:  {@link basicmodelbe}: limiting form of bose-einstein {@link basicmodelg}: geometric approximation of bose-einstein {@link basicmodelp}: poisson approximation of the binomial {@link basicmodeld}: divergence approximation of the binomial {@link basicmodelin}: inverse document frequency {@link basicmodeline}: inverse expected document frequency [mixture of poisson and idf] {@link basicmodelif}: inverse term frequency [approximation of i(ne)]  {@link aftereffect}: first normalization of information gain:  {@link aftereffectl}: laplace's law of succession {@link aftereffectb}: ratio of two bernoulli processes {@link noaftereffect}: no first normalization  {@link normalization}: second (length) normalization:  {@link normalizationh1}: uniform distribution of term frequency {@link normalizationh2}: term frequency density inversely related to length {@link normalizationh3}: term frequency normalization provided by dirichlet prior {@link normalizationz}: term frequency normalization provided by a zipfian relation {@link nonormalization}: no second normalization   note that qtf, the multiplicity of term-occurrence in the query, is not handled by this implementation. @see basicmodel @see aftereffect @see normalization @lucene.experimental the basic model for information content. the first normalization of the information content. the term frequency normalization. creates dfrsimilarity from the three components.  note that null values are not allowed: if you want no normalization or after-effect, instead pass {@link nonormalization} or {@link noaftereffect} respectively. basic model of information content first normalization of information gain second (length) normalization returns the basic model of information content returns the first normalization returns the second normalization"
org.apache.lucene.search.similarities.SimilarityBase "a subclass of {@code similarity} that provides a simplified api for its descendants. subclasses are only required to implement the {@link #score} and {@link #tostring()} methods. implementing {@link #explain(explanation, basicstats, int, float, float)} is optional, inasmuch as similaritybase already provides a basic explanation of the score and the term frequency. however, implementers of a subclass are encouraged to include as much detail about the scoring method as possible.  note: multi-word queries such as phrase queries are scored in a different way than lucene's default ranking algorithm: whereas it "fakes" an idf value for the phrase as a whole (since it does not know it), this class instead scores phrases as a summation of the individual term scores. @lucene.experimental for {@link #log2(double)}. precomputed for efficiency reasons. true if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. sole constructor. (for invocation by subclass constructors, typically implicit.) determines whether overlap tokens (tokens with 0 position increment) are ignored when computing norm. by default this is true, meaning overlap tokens do not count when computing norms. @lucene.experimental @see #computenorm returns true if overlap tokens are discounted from the document's length. @see #setdiscountoverlaps factory method to return a custom stats object fills all member fields defined in {@code basicstats} in {@code stats}. subclasses can override this method to fill additional stats. scores the document {@code doc}. subclasses must apply their scoring formula in this class. the corpus level statistics. the term frequency. the document length. score. subclasses should implement this method to explain the score. {@code expl} already contains the score, the name of the class and the doc id, as well as the term frequency and its explanation; subclasses can add additional clauses to explain details of their scoring formulae. the default implementation does nothing. the explanation to extend with details. the corpus level statistics. the document id. the term frequency. the document length. explains the score. the implementation here provides a basic explanation in the format score(name-of-similarity, doc=doc-id, freq=term-frequency), computed from:, and attaches the score (computed via the {@link #score(basicstats, float, float)} method) and the explanation for the term frequency. subclasses content with this format may add additional details in {@link #explain(explanation, basicstats, int, float, float)}. the corpus level statistics. the document id. the term frequency and its explanation. the document length. explanation. subclasses must override this method to return the name of the similarity and preferably the values of parameters (if any) as well. norm -> document length map. encodes the document length in the same way as {@link tfidfsimilarity}. decodes a normalization factor (document length) stored in an index. @see #encodenormvalue(float,float) encodes the length to a byte via smallfloat. returns the base two logarithm of {@code x}. delegates the {@link #score(int, int)} and {@link #explain(int, explanation)} methods to {@link similaritybase#score(basicstats, float, float)} and {@link similaritybase#explain(basicstats, int, explanation, float)}, respectively. delegates the {@link #score(int, float)} and {@link #explain(int, explanation)} methods to {@link similaritybase#score(basicstats, float, float)} and {@link similaritybase#explain(basicstats, int, explanation, float)}, respectively. #positions(field) must be >= #positions(term) codec does not supply totaltermfreq: substitute docfreq field does not exist; we have to provide something if codec doesnt supply these measures, or if someone omitted frequencies for the field... negative values cause nan/inf for some scorers. todo: add sumdocfreq for field (numberoffieldpostings) a multi term query (e.g. phrase). return the summation, scoring almost as if it were boolean query a multi term query (e.g. phrase). return the summation, scoring almost as if it were boolean query ------------------------------ norm handling ------------------------------ & 0xff maps negative bytes to positive above 127 ----------------------------- static methods ------------------------------ put this to a 'util' class if we need more of these. --------------------------------- classes --------------------------------- we have to supply something in case norms are omitted we have to supply something in case norms are omitted"
org.apache.lucene.search.similarities.BasicModelIF "an approximation of the i(ne) model. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.similarities.IBSimilarity "provides a framework for the family of information-based models, as described in st&eacute;phane clinchant and eric gaussier. 2010. information-based models for ad hoc ir. in proceeding of the 33rd international acm sigir conference on research and development in information retrieval (sigir '10). acm, new york, ny, usa, 234-241. the retrieval function is of the form rsv(q, d) = &sum; -xqw log prob(xw &ge; tdw | &lambda;w), where  xqw is the query boost; xw is a random variable that counts the occurrences of word w; tdw is the normalized term frequency; &lambda;w is a parameter.   the framework described in the paper has many similarities to the dfr framework (see {@link dfrsimilarity}). it is possible that the two similarities will be merged at one point. to construct an ibsimilarity, you must specify the implementations for all three components of the information-based model.  {@link distribution}: probabilistic distribution used to model term occurrence  {@link distributionll}: log-logistic {@link distributionll}: smoothed power-law   {@link lambda}: &lambda;w parameter of the probability distribution  {@link lambdadf}: nw/n or average number of documents where w occurs {@link lambdattf}: fw/n or average number of occurrences of w in the collection   {@link normalization}: term frequency normalization any supported dfr normalization (listed in {@link dfrsimilarity})    @see dfrsimilarity @lucene.experimental the probabilistic distribution used to model term occurrence. the lambda (&lambda;w) parameter. the term frequency normalization. creates ibsimilarity from the three components.  note that null values are not allowed: if you want no normalization, instead pass {@link nonormalization}. probabilistic distribution modeling term occurrence distribution's &lambda;w parameter term frequency normalization the name of ib methods follow the pattern {@code ib  }. the name of the distribution is the same as in the original paper; for the names of lambda parameters, refer to the javadoc of the {@link lambda} classes. returns the distribution returns the distribution's lambda parameter returns the term frequency normalization"
org.apache.lucene.search.similarities.PerFieldSimilarityWrapper "provides the ability to use a different {@link similarity} for different fields.  subclasses should implement {@link #get(string)} to return an appropriate similarity (for example, using field-specific parameter values) for the field. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns a {@link similarity} for scoring a field."
org.apache.lucene.search.similarities.NormalizationH1 "normalization model that assumes a uniform distribution of the term frequency. while this model is parameterless in the  original article,  information-based models (see {@link ibsimilarity}) introduced a multiplying factor. the default value for the {@code c} parameter is {@code 1}. @lucene.experimental creates normalizationh1 with the supplied parameter c. hyper-parameter that controls the term frequency normalization with respect to the document length. calls {@link #normalizationh1(float) normalizationh1(1)} returns the c parameter. @see #normalizationh1(float)"
org.apache.lucene.search.similarities.LMJelinekMercerSimilarity "language model based on the jelinek-mercer smoothing method. from chengxiang zhai and john lafferty. 2001. a study of smoothing methods for language models applied to ad hoc information retrieval. in proceedings of the 24th annual international acm sigir conference on research and development in information retrieval (sigir '01). acm, new york, ny, usa, 334-342. the model has a single parameter, &lambda;. according to said paper, the optimal value depends on both the collection and the query. the optimal value is around {@code 0.1} for title queries and {@code 0.7} for long queries. @lucene.experimental the &lambda; parameter. instantiates with the specified collectionmodel and &lambda; parameter. instantiates with the specified &lambda; parameter. returns the &lambda; parameter."
org.apache.lucene.search.similarities.Distribution "the probabilistic distribution used to model term occurrence in information-based models. @see ibsimilarity @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) computes the score. explains the score. returns the name of the model only, since both {@code tfn} and {@code lambda} are explained elsewhere. subclasses must override this method to return the name of the distribution."
org.apache.lucene.search.similarities.LMDirichletSimilarity "bayesian smoothing using dirichlet priors. from chengxiang zhai and john lafferty. 2001. a study of smoothing methods for language models applied to ad hoc information retrieval. in proceedings of the 24th annual international acm sigir conference on research and development in information retrieval (sigir '01). acm, new york, ny, usa, 334-342.  the formula as defined the paper assigns a negative score to documents that contain the term, but with fewer occurrences than predicted by the collection language model. the lucene implementation returns {@code 0} for such documents.  @lucene.experimental the &mu; parameter. instantiates the similarity with the provided &mu; parameter. instantiates the similarity with the provided &mu; parameter. instantiates the similarity with the default &mu; value of 2000. instantiates the similarity with the default &mu; value of 2000. returns the &mu; parameter."
org.apache.lucene.search.similarities.AfterEffectL "model of the information gain based on laplace's law of succession. @lucene.experimental sole constructor: parameter-free"
org.apache.lucene.search.TimeLimitingCollector "the {@link timelimitingcollector} is used to timeout search requests that take longer than the maximum allowed search time limit. after this time is exceeded, the search thread is stopped by throwing a {@link timeexceededexception}. thrown when elapsed search time exceeds allowed search time. returns allowed time (milliseconds). returns elapsed time (milliseconds). returns last doc (absolute doc id) that was collected when the search time exceeded. create a timelimitedcollector wrapper over another {@link collector} with a specified timeout. the wrapped {@link collector} the timer clock max time allowed for collecting hits after which {@link timeexceededexception} is thrown sets the baseline for this collector. by default the collectors baseline is initialized once the first reader is passed to the collector. to include operations executed in prior to the actual document collection set the baseline through this method in your prelude.  example usage:  counter clock = ...; long baseline = clock.get(); // ... prepare search timelimitingcollector collector = new timelimitingcollector(c, clock, numticks); collector.setbaseline(baseline); indexsearcher.search(query, collector);   @see #setbaseline() syntactic sugar for {@link #setbaseline(long)} using {@link counter#get()} on the clock passed to the construcutor. checks if this time limited collector is greedy in collecting the last hit. a non greedy collector, upon a timeout, would throw a {@link timeexceededexception} without allowing the wrapped collector to collect current doc. a greedy one would first allow the wrapped hit collector to collect current doc and only then throw a {@link timeexceededexception}. @see #setgreedy(boolean) sets whether this time limited collector is greedy. true to make this time limited greedy @see #isgreedy() calls {@link collector#collect(int)} on the decorated {@link collector} unless the allowed time has passed, in which case it throws an exception. @throws timeexceededexception if the time allowed has exceeded. this is so the same timer can be used with a multi-phase search process such as grouping. we don't want to create a new timelimitingcollector for each phase because that would reset the timer for each phase. once time is up subsequent phases need to timeout quickly. the actual collector performing search functionality returns the global timerthreads {@link counter}  invoking this creates may create a new instance of {@link timerthread} iff the global {@link timerthread} has never been accessed before. the thread returned from this method is started on creation and will be alive unless you stop the {@link timerthread} via {@link timerthread#stoptimer()}.  global timerthreads {@link counter} @lucene.experimental returns the global {@link timerthread}.  invoking this creates may create a new instance of {@link timerthread} iff the global {@link timerthread} has never been accessed before. the thread returned from this method is started on creation and will be alive unless you stop the {@link timerthread} via {@link timerthread#stoptimer()}.  global {@link timerthread} @lucene.experimental thread used to timeout search requests. can be stopped completely with {@link timerthread#stoptimer()} @lucene.experimental get the timer value in milliseconds. stops the timer thread return the timer resolution. @see #setresolution(long) set the timer resolution. the default timer resolution is 20 milliseconds. this means that a search required to take no longer than 800 milliseconds may be stopped after 780 to 820 milliseconds. note that:  finer (smaller) resolution is more accurate but less efficient. setting resolution to less than 5 milliseconds will be silently modified to 5 milliseconds. setting resolution smaller than current resolution might take effect only after current resolution. (assume current resolution of 20 milliseconds is modified to 5 milliseconds, then it can take up to 20 milliseconds for the change to have effect.  ... prepare search system.out.println(this+" greedy: before failing, collecting doc: "+(docbase + doc)+" "+(time-t0)); system.out.println(this+" failing on: "+(docbase + doc)+" "+(time-t0)); system.out.println(this+" collecting: "+(docbase + doc)+" "+(time-t0)); note: we can avoid explicit synchronization here for several reasons: updates to volatile long variables are atomic only single thread modifies this value use of volatile keyword ensures that it does not reside in a register, but in main memory (so that changes are visible to other threads). visibility of changes does not need to be instantaneous, we can afford losing a tick or two.  see section 17 of the java language specification for details. todo: use system.nanotime() when lucene moves to java se 5. 5 milliseconds is about the minimum reasonable time for a object.wait(long) call."
org.apache.lucene.search.CachingCollector "caches all docs, and optionally also scores, coming from a search, and is then able to replay them to another collector. you specify the max ram this class may use. once the collection is done, call {@link #iscached}. if this returns true, you can use {@link #replay(collector)} against a new collector. if it returns false, this means too much ram was required and you must instead re-run the original search. note: this class consumes 4 (or 8 bytes, if scoring is cached) per collected document. if the result set is large this can easily be a very substantial amount of ram! note: this class caches at least 128 documents before checking ram limits. see the lucene modules/grouping module for more details including a full code example. @lucene.experimental creates a {@link cachingcollector} which does not wrap another collector. the cached documents and scores can later be {@link #replay(collector) replayed}. whether documents are allowed to be collected out-of-order create a new {@link cachingcollector} that wraps the given collector and caches documents and scores up to the specified ram threshold. the collector to wrap and delegate calls to. whether to cache scores in addition to document ids. note that this increases the ram consumed per doc the maximum ram in mb to consume for caching the documents and scores. if the collector exceeds the threshold, no documents and scores are cached. create a new {@link cachingcollector} that wraps the given collector and caches documents and scores up to the specified max docs threshold. the collector to wrap and delegate calls to. whether to cache scores in addition to document ids. note that this increases the ram consumed per doc the maximum number of documents for caching the documents and possible the scores. if the collector exceeds the threshold, no documents and scores are cached. reused by the specialized inner classes. replays the cached doc ids (and scores) to the given collector. if this instance does not cache scores, then scorer is not set on {@code other.setscorer} as well as scores are not replayed. @throws illegalstateexception if this collector is not cached (i.e., if the ram limits were too low for the number of documents + scores to cache). @throws illegalargumentexception if the given collect's does not support out-of-order collection, while the collector passed to the ctor does. max out at 512k arrays note: these members are package-private b/c that way accessing them from the outer class does not incur access check by the jvm. the same situation would be if they were defined in the outer class as private members. a cachingcollector which caches scores cache was too large allocate a bigger array or abort caching compute next array length - don't allocate too big arrays try to allocate a smaller array too many docs to collect -- clear cache a cachingcollector which does not cache scores cache was too large allocate a bigger array or abort caching compute next array length - don't allocate too big arrays try to allocate a smaller array too many docs to collect -- clear cache todo: would be nice if a collector defined a needsscores() method so we can specialize / do checks up front. this is only relevant for the scorecaching version -- if the wrapped collector does not need scores, it can avoid cachedscorer entirely. prevent extension from non-internal classes system.out.println("cc: replay tothits=" + (upto + base));"
org.apache.lucene.search.Query "the abstract base class for queries. instantiable subclasses are:   {@link termquery}  {@link booleanquery}  {@link wildcardquery}  {@link phrasequery}  {@link prefixquery}  {@link multiphrasequery}  {@link fuzzyquery}  {@link regexpquery}  {@link termrangequery}  {@link numericrangequery}  {@link constantscorequery}  {@link disjunctionmaxquery}  {@link matchalldocsquery}  see also the family of {@link org.apache.lucene.search.spans span queries} and additional queries available in the queries module sets the boost for this query clause to b. documents matching this clause will (in addition to the normal weightings) have their score multiplied by b. gets the boost for this clause. documents matching this clause will (in addition to the normal weightings) have their score multiplied by b. the boost is 1.0 by default. prints a query to a string, with field assumed to be the default field and omitted. prints a query to a string. expert: constructs an appropriate weight implementation for this query.  only implemented by primitive queries, which re-write to themselves. expert: called to re-write queries into primitive queries. for example, a prefixquery will be rewritten into a booleanquery that consists of termquerys. expert: adds all terms occurring in this query to the terms set. only works if this query is in its {@link #rewrite rewritten} form. @throws unsupportedoperationexception if this query is not yet rewritten returns a clone of this query. query boost factor needs to be implemented by query subclasses"
org.apache.lucene.search.HitQueue "creates a new instance with size elements. if prepopulate is set to true, the queue will pre-populate itself with sentinel objects and set its {@link #size()} to size. in that case, you should not rely on {@link #size()} to get the number of actual elements that were added to the queue, but keep track yourself. note: in case prepopulate is true, you should pop elements from the queue using the following code example:  priorityqueue&lt;scoredoc&gt; pq = new hitqueue(10, true); // pre-populate. scoredoc top = pq.top(); // add/update one element. top.score = 1.0f; top.doc = 0; top = (scoredoc) pq.updatetop(); int totalhits = 1; // now pop only the elements that were truly inserted. // first, pop all the sentinel elements (there are pq.size() - totalhits). for (int i = pq.size() - totalhits; i &gt; 0; i--) pq.pop(); // now pop the truly added elements. scoredoc[] results = new scoredoc[totalhits]; for (int i = totalhits - 1; i &gt;= 0; i--) { results[i] = (scoredoc) pq.pop(); }  note: this class pre-allocate a full array of length size. the requested size of this queue. specifies whether to pre-populate the queue with sentinel values. @see #getsentinelobject() pre-populate. add/update one element. now pop only the elements that were truly inserted. first, pop all the sentinel elements (there are pq.size() - totalhits). now pop the truly added elements. always set the doc id to max_value so that it won't be favored by lessthan. this generally should not happen since if score is not neg_inf, topscoredoccollector will always add the object to the queue."
org.apache.lucene.search.Scorer "expert: common scoring functionality for different types of queries.  a scorer iterates over documents matching a query in increasing order of doc id.   document scores are computed using a given similarity implementation.  note: the values float.nan, float.negative_infinity and float.positive_infinity are not valid scores. certain collectors (eg {@link topscoredoccollector}) will not properly collect hits with these scores. the scorer's parent weight. in some cases this may be null constructs a scorer the scorers weight. scores and collects all matching documents. the collector to which all matching documents are passed. expert: collects matching documents in a range. hook for optimization. note, firstdocid is added to ensure that {@link #nextdoc()} was called before this method. the collector to which all matching documents are passed. do not score documents past this. the first document id (ensures {@link #nextdoc()} is called before this method. if more matching documents may remain. returns the score of the current document matching the query. initially invalid, until {@link #nextdoc()} or {@link #advance(int)} is called the first time, or when called from within {@link collector#collect}. returns parent weight @lucene.experimental returns child sub-scorers @lucene.experimental a child scorer and its relationship to its parent. the meaning of the relationship depends upon the parent query. @lucene.experimental child scorer. (note this is typically a direct child, and may itself also have children). an arbitrary string relating this scorer to the parent. creates a new childscorer node with the specified relationship.  the relationship can be any be any string that makes sense to the parent scorer. todo can we clean this up?"
org.apache.lucene.search.FieldComparatorSource "provides a {@link fieldcomparator} for custom field sorting. @lucene.experimental creates a comparator for the field in the given index. name of the field to create comparator for. . @throws ioexception if an error occurs reading the index."
org.apache.lucene.search.RegexpQuery "a fast regular expression query based on the {@link org.apache.lucene.util.automaton} package.  comparisons are fast the term dictionary is enumerated in an intelligent way, to avoid comparisons. see {@link automatonquery} for more details.   the supported syntax is documented in the {@link regexp} class. note this might be different than other regular expression implementations. for some alternatives with different syntax, look under the sandbox.   note this query can be slow, as it needs to iterate over many terms. in order to prevent extremely slow regexpqueries, a regexp term should not start with the expression . @see regexp @lucene.experimental a provider that provides no named automata constructs a query for terms matching term.  by default, all regular expression features are enabled.  regular expression. constructs a query for terms matching term. regular expression. optional regexp features from {@link regexp} constructs a query for terms matching term. regular expression. optional regexp features from {@link regexp} custom automatonprovider for named automata prints a"
org.apache.lucene.search.TermRangeQuery "a query that matches documents within an range of terms. this query matches the documents looking for terms that fall into the supplied range according to {@link byte#compareto(byte)}. it is not intended for numerical ranges; use {@link numericrangequery} instead. this query uses the {@link multitermquery#constant_score_auto_rewrite_default} rewrite method. constructs a query selecting all terms greater/equal than lowerterm but less/equal than upperterm.  if an endpoint is null, it is said to be "open". either or both endpoints may be open. open endpoints may not be exclusive (you can't select all but the first or last term without explicitly specifying the term to exclude.) the field that holds both lower and upper terms. the term text at the lower end of the range the term text at the upper end of the range if true, the lowerterm is included in the range. if true, the upperterm is included in the range. factory that creates a new termrangequery using strings for term text. returns the lower value of this range query returns the upper value of this range query returns true if the lower endpoint is inclusive returns true if the upper endpoint is inclusive prints a todo: all these tostrings for queries should just output the bytes, it might not be utf-8!"
org.apache.lucene.search.PhraseQuery "a query that matches documents containing a particular sequence of terms. a phrasequery is built by queryparser for input like "new york". this query may be combined with other terms or queries with a {@link booleanquery}. constructs an empty phrase query. sets the number of other words permitted between words in query phrase. if zero, then this is an exact phrase search. for larger values this works like a within or near operator. the slop is in fact an edit-distance, where the units correspond to moves of terms in the query phrase out of position. for example, to switch the order of two words requires two moves (the first move places the words atop one another), so to permit re-orderings of phrases, the slop must be at least two. more exact matches are scored higher than sloppier matches, thus search results are sorted by exactness. the slop is zero by default, requiring exact matches. returns the slop. see setslop(). adds a term to the end of the query phrase. the relative position of the term is the one immediately after the last term added. adds a term to the end of the query phrase. the relative position of the term within the phrase is specified explicitly. this allows e.g. phrases with more than one term at the same position or phrases with gaps (e.g. in connection with stopwords). returns the set of terms in this phrase. returns the relative positions of terms in this phrase. term doesnt exist in this segment @see org.apache.lucene.search.query#extractterms(set) prints a returns true iff o is equal to this. returns a hash code value for this object. for faster comparisons reuse single termsenum below: phrasequery on a field that did not index positions. term does exist, but has no positions sort by increasing docfreq order optimize exact case only called from assert"
org.apache.lucene.search.SearcherFactory "factory class used by {@link searchermanager} and {@link nrtmanager} to create new indexsearchers. the default implementation just creates an indexsearcher with no custom behavior:  public indexsearcher newsearcher(indexreader r) throws ioexception { return new indexsearcher(r); }  you can pass your own factory instead if you want custom behavior, such as:  setting a custom scoring model: {@link indexsearcher#setsimilarity(similarity)} parallel per-segment search: {@link indexsearcher#indexsearcher(indexreader, executorservice)} return custom subclasses of indexsearcher (for example that implement distributed scoring) run queries to warm your indexsearcher before it is used. note: when using near-realtime search you may want to also {@link indexwriterconfig#setmergedsegmentwarmer(indexwriter.indexreaderwarmer)} to warm newly merged segments in the background, outside of the reopen path.  @lucene.experimental returns a new indexsearcher over the given reader. javadocs javadocs javadocs javadocs"
org.apache.lucene.search.TopFieldCollector "a {@link collector} that sorts by {@link sortfield} using {@link fieldcomparator}s.  see the {@link #create(org.apache.lucene.search.sort, int, boolean, boolean, boolean, boolean)} method for instantiating a topfieldcollector. @lucene.experimental implements a topfieldcollector over one sortfield criteria, without tracking document scores and maxscore. implements a topfieldcollector over one sortfield criteria, without tracking document scores and maxscore, and assumes out of orderness in doc ids collection. implements a topfieldcollector over one sortfield criteria, while tracking document scores but no maxscore. implements a topfieldcollector over one sortfield criteria, while tracking document scores but no maxscore, and assumes out of orderness in doc ids collection. implements a topfieldcollector over one sortfield criteria, with tracking document scores and maxscore. implements a topfieldcollector over one sortfield criteria, with tracking document scores and maxscore, and assumes out of orderness in doc ids collection. implements a topfieldcollector over multiple sortfield criteria, without tracking document scores and maxscore. implements a topfieldcollector over multiple sortfield criteria, without tracking document scores and maxscore, and assumes out of orderness in doc ids collection. implements a topfieldcollector over multiple sortfield criteria, with tracking document scores and maxscore. implements a topfieldcollector over multiple sortfield criteria, with tracking document scores and maxscore, and assumes out of orderness in doc ids collection. implements a topfieldcollector over multiple sortfield criteria, with tracking document scores and maxscore. implements a topfieldcollector over multiple sortfield criteria, with tracking document scores and maxscore, and assumes out of orderness in doc ids collection. implements a topfieldcollector when after != null. stores the maximum score value encountered, needed for normalizing. if document scores are not tracked, this value is initialized to nan. creates a new {@link topfieldcollector} from the given arguments. note: the instances returned by this method pre-allocate a full array of length numhits. the sort criteria (sortfields). the number of results to collect. specifies whether the actual field values should be returned on the results (fielddoc). specifies whether document scores should be tracked and set on the results. note that if set to false, then the results' scores will be set to float.nan. setting this to true affects performance, as it incurs the score computation on each competitive result. therefore if document scores are not required by the application, it is recommended to set it to false. specifies whether the query's maxscore should be tracked and set on the resulting {@link topdocs}. note that if set to false, {@link topdocs#getmaxscore()} returns float.nan. setting this to true affects performance as it incurs the score computation on each result. also, setting this true automatically sets trackdocscores to true as well. specifies whether documents are scored in doc id order or not by the given {@link scorer} in {@link #setscorer(scorer)}. {@link topfieldcollector} instance which will sort the results by the sort criteria. @throws ioexception if there is a low-level i/o error creates a new {@link topfieldcollector} from the given arguments. note: the instances returned by this method pre-allocate a full array of length numhits. the sort criteria (sortfields). the number of results to collect. only hits after this fielddoc will be collected specifies whether the actual field values should be returned on the results (fielddoc). specifies whether document scores should be tracked and set on the results. note that if set to false, then the results' scores will be set to float.nan. setting this to true affects performance, as it incurs the score computation on each competitive result. therefore if document scores are not required by the application, it is recommended to set it to false. specifies whether the query's maxscore should be tracked and set on the resulting {@link topdocs}. note that if set to false, {@link topdocs#getmaxscore()} returns float.nan. setting this to true affects performance as it incurs the score computation on each result. also, setting this true automatically sets trackdocscores to true as well. specifies whether documents are scored in doc id order or not by the given {@link scorer} in {@link #setscorer(scorer)}. {@link topfieldcollector} instance which will sort the results by the sort criteria. @throws ioexception if there is a low-level i/o error only the following callback methods need to be overridden since topdocs(int, int) calls them to return the results. todo: one optimization we could do is to pre-fill the queue with sentinel value that guaranteed to always compare lower than a real hit; this would save having to check queuefull on each insert bottom.score is already set to float.nan in add(). since docs are visited in doc id order, if compare is 0, it means this document is larger than anything else in the queue, and therefore not competitive. this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue fastmatch: return if this hit is not competitive this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue since docs are visited in doc id order, if compare is 0, it means this document is largest than anything else in the queue, and therefore not competitive. compute the score only if the hit is competitive. this hit is competitive - replace bottom element in queue & adjusttop compute the score only if the hit is competitive. startup transient: queue hasn't gathered numhits yet copy hit into queue fastmatch: return if this hit is not competitive compute the score only if the hit is competitive. this hit is competitive - replace bottom element in queue & adjusttop compute the score only if the hit is competitive. startup transient: queue hasn't gathered numhits yet copy hit into queue must set maxscore to neg_inf, or otherwise math.max always returns nan. since docs are visited in doc id order, if compare is 0, it means this document is largest than anything else in the queue, and therefore not competitive. this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue fastmatch: return if this hit is not competitive this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue bottom.score is already set to float.nan in add(). fastmatch: return if this hit is not competitive definitely not competitive. definitely competitive. here c=0. if we're at the last comparator, this doc is not competitive, since docs are visited in doc id order, which means this doc cannot compete with any other document in the queue. this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue set the scorer on all comparators fastmatch: return if this hit is not competitive definitely not competitive. definitely competitive. this is the equals case. definitely not competitive this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue must set maxscore to neg_inf, or otherwise math.max always returns nan. fastmatch: return if this hit is not competitive definitely not competitive. definitely competitive. here c=0. if we're at the last comparator, this doc is not competitive, since docs are visited in doc id order, which means this doc cannot compete with any other document in the queue. this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue fastmatch: return if this hit is not competitive definitely not competitive. definitely competitive. this is the equals case. definitely not competitive this hit is competitive - replace bottom element in queue & adjusttop startup transient: queue hasn't gathered numhits yet copy hit into queue fastmatch: return if this hit is not competitive definitely not competitive. definitely competitive. here c=0. if we're at the last comparator, this doc is not competitive, since docs are visited in doc id order, which means this doc cannot compete with any other document in the queue. this hit is competitive - replace bottom element in queue & adjusttop compute score only if it is competitive. startup transient: queue hasn't gathered numhits yet copy hit into queue compute score only if it is competitive. fastmatch: return if this hit is not competitive definitely not competitive. definitely competitive. this is the equals case. definitely not competitive this hit is competitive - replace bottom element in queue & adjusttop compute score only if it is competitive. startup transient: queue hasn't gathered numhits yet copy hit into queue compute score only if it is competitive. must set maxscore to neg_inf, or otherwise math.max always returns nan. system.out.println(" collect doc=" + doc); check if this hit was already collected on a previous page: already collected on a previous page system.out.println(" skip: before"); not yet collected system.out.println(" keep: after"); tie-break by docid: already collected on a previous page system.out.println(" skip: tie-break"); fastmatch: return if this hit is not competitive definitely not competitive. definitely competitive. this is the equals case. definitely not competitive this hit is competitive - replace bottom element in queue & adjusttop compute score only if it is competitive. startup transient: queue hasn't gathered numhits yet system.out.println(" slot=" + slot); copy hit into queue compute score only if it is competitive. declaring the constructor private prevents extending this class by anyone else. note that the class cannot be final since it's extended by the internal versions. if someone will define a constructor with any other visibility, then anyone will be able to extend the class, which is not what we want. multiple comparators. avoid casting if unnecessary. set maxscore to nan, in case this is a maxscore tracking collector. if this is a maxscoring tracking collector and there were no results,"
org.apache.lucene.search.BooleanClause "a clause in a booleanquery. specifies how clauses are to occur in matching documents. use this operator for clauses that must appear in the matching documents. use this operator for clauses that should appear in the matching documents. for a booleanquery with no must clauses one or more should clauses must match a document for the booleanquery to match. @see booleanquery#setminimumnumbershouldmatch use this operator for clauses that must not appear in the matching documents. note that it is not possible to search for queries that only consist of a must_not clause. the query whose matching documents are combined by the boolean query. constructs a booleanclause. returns true if o is equal to this. returns a hash code value for this object."
org.apache.lucene.search.TopFieldDocs "represents hits returned by {@link indexsearcher#search(query,filter,int,sort)}. the fields which were used to sort results by. creates one of these objects. total number of hits for the query. the top hits for the query. the sort criteria used to find the top hits. the maximum score encountered."
org.apache.lucene.search.BooleanScorer2 "see the description in booleanscorer.java, comparing booleanscorer & booleanscorer2 an alternative to booleanscorer that also allows a minimum number of optional scorers that should match. implements skipto(), and has no limitations on the numbers of added scorers. uses conjunctionscorer, disjunctionscorer, reqoptscorer and reqexclscorer. the scorer to which all scoring will be delegated, except for computing and using the coordination factor. the number of optionalscorers that need to match (if there are any) creates a {@link scorer} with the given similarity and lists of required, prohibited and optional scorers. in no required scorers are added, at least one of the optional scorers will have to match during the search. the booleanweight to be used. if this parameter is true, coordination level matching ({@link similarity#coord(int, int)}) is not used. the minimum number of optional added scorers that should match during the search. in case no required scorers are added, at least one of the optional scorers will have to match during the search. the list of required scorers. the list of prohibited scorers. the list of optional scorers. count a scorer as a single match. returns the scorer to be used for match counting and score summing. uses requiredscorers, optionalscorers and prohibitedscorers. returns the scorer to be used for match counting and score summing. uses the given required scorer and the prohibitedscorers. a required scorer already built. scores and collects all matching documents. the collector to which all matching documents are passed through. to be increased by score() of match counting scorers. save the score of lastscoreddoc, so that we don't compute it more than once in score(). each scorer from the list counted as a single matcher save the score of lastscoreddoc, so that we don't compute it more than once in score(). each scorer from the list counted as a single matcher save the score of lastscoreddoc, so that we don't compute it more than once in score(). all scorers match, so defaultsimilarity super.score() always has 1 as the coordination factor. therefore the sum of the scores of the requiredscorers is used as score. non counting. all scorers match, so defaultsimilarity always has 1 as the coordination factor. therefore the sum of the scores of two scorers is used as score. each scorer counted as a single matcher no required scorers minnrshouldmatch optional scorers are required, but at least 1 at least one required scorer. all optional scorers also required. optionalscorers.size() > minnrshouldmatch, and at least one required scorer use a required disjunction scorer over the optional scorers non counting minnrshouldmatch == 0 require 1 in combined, optional scorer. no prohibited"
org.apache.lucene.search.FuzzyQuery "implements the fuzzy search query. the similarity measurement is based on the damerau-levenshtein (optimal string alignment) algorithm, though you can explicitly choose classic levenshtein by passing false to the transpositions parameter. this query uses {@link multitermquery.toptermsscoringbooleanqueryrewrite} as default. so terms will be collected and scored according to their edit distance. only the top terms are used for building the {@link booleanquery}. it is not recommended to change the rewrite mode for fuzzy queries. at most, this query will match terms up to {@value org.apache.lucene.util.automaton.levenshteinautomata#maximum_supported_distance} edits. higher distances (especially with transpositions enabled), are generally not useful and will match a significant amount of the term dictionary. if you really want this, consider using an n-gram indexing technique (such as the spellchecker in the suggest module) instead. create a new fuzzyquery that will match terms with an edit distance of at most maxedits to term. if a prefixlength &gt; 0 is specified, a common prefix of that length is also required. the term to search for must be >= 0 and <= {@link levenshteinautomata#maximum_supported_distance}. length of common (non-fuzzy) prefix the maximum number of terms to match. if this number is greater than {@link booleanquery#getmaxclausecount} when the query is rewritten, then the maxclausecount will be used instead. true if transpositions should be treated as a primitive edit operation. if this is false, comparisons will implement the classic levenshtein algorithm. calls {@link #fuzzyquery(term, int, int, int, boolean) fuzzyquery(term, minimumsimilarity, prefixlength, defaultmaxexpansions, defaulttranspositions)}. calls {@link #fuzzyquery(term, int, int) fuzzyquery(term, maxedits, defaultprefixlength)}. calls {@link #fuzzyquery(term, int) fuzzyquery(term, defaultmaxedits)}. maximum number of edit distances allowed for this query to match. returns the non-fuzzy prefix length. this is the number of characters at the start of a term that must be identical (not fuzzy) to the query term if the query is to match that term. returns the pattern term. @deprecated pass integer edit distances instead. helper function to convert from deprecated "minimumsimilarity" fractions to raw edit distances. scaled similarity length (in unicode codepoints) of the term. number of maxedits @deprecated pass integer edit distances instead. can only match if it's exact 0 means exact, not infinite # of edits!"
org.apache.lucene.search.ExactPhraseScorer "coarse optimization: advance(target) is fairly costly, so, if the relative freq of the 2nd rarest term is not that much (> 1/5th) rarer than the first term, then we just use .nextdoc() when anding. this buys ~15% gain for phrases where freq of rarest 2 terms is close: first (rarest) term not-first terms safety net -- fallback to .advance if we've done too many .nextdocs this doc has all the terms -- now test whether phrase occurs first term not-first terms this doc has all the terms -- now test whether phrase occurs init chunks process chunk by chunk todo: we could fold in chunkstart into offset and save one subtract per pos incr wraparound first term middle terms viable petered out for this chunk last term"
org.apache.lucene.search.ConstantScoreQuery "a query that wraps another query or a filter and simply returns a constant score equal to the query boost for every document that matches the filter or query. for queries it therefore simply strips of all scores and returns a constant one. strips off scores from the passed in query. the hits will get a constant score dependent on the boost factor of this query. wraps a filter as a query. the hits will get a constant score dependent on the boost factor of this query. if you simply want to strip off scores from a query, no longer use {@code new constantscorequery(new querywrapperfilter(query))}, instead use {@link #constantscorequery(query)}! returns the encapsulated filter, returns {@code null} if a query is wrapped. returns the encapsulated query, returns {@code null} if a filter is wrapped. todo: ok to not add any terms when wrapped a filter and used with multisearcher, but may not be ok for highlighting. if a query was wrapped, we delegate to query. we calculate sumofsquaredweights of the inner weight, but ignore it (just to initialize everything) we normalize the inner weight, but ignore it (just to initialize everything) we must wrap again here, but using the scorer passed in as parameter: this optimization allows out of order scoring as top scorer! this optimization allows out of order scoring as top scorer,"
org.apache.lucene.search.FieldCache "expert: maintains caches of term values.  placeholder indicating creation of this cache is currently in-progress. hack: when thrown from a parser (numeric_utils_ ones), this stops processing terms and returns the current fieldcache array. @lucene.internal marker interface as super-interface to all parsers. it is used to specify a custom parser to {@link sortfield#sortfield(string, fieldcache.parser)}. interface to parse bytes from document fields. @see fieldcache#getbytes(atomicreader, string, fieldcache.byteparser, boolean) return a single byte representation of this field's value. interface to parse shorts from document fields. @see fieldcache#getshorts(atomicreader, string, fieldcache.shortparser, boolean) return a short representation of this field's value. interface to parse ints from document fields. @see fieldcache#getints(atomicreader, string, fieldcache.intparser, boolean) return an integer representation of this field's value. interface to parse floats from document fields. @see fieldcache#getfloats(atomicreader, string, fieldcache.floatparser, boolean) return an float representation of this field's value. interface to parse long from document fields. @see fieldcache#getlongs(atomicreader, string, fieldcache.longparser, boolean) return an long representation of this field's value. interface to parse doubles from document fields. @see fieldcache#getdoubles(atomicreader, string, fieldcache.doubleparser, boolean) return an long representation of this field's value. expert: the cache used internally by sorting and range query classes. the default parser for byte values, which are encoded by {@link byte#tostring(byte)} the default parser for short values, which are encoded by {@link short#tostring(short)} the default parser for int values, which are encoded by {@link integer#tostring(int)} the default parser for float values, which are encoded by {@link float#tostring(float)} the default parser for long values, which are encoded by {@link long#tostring(long)} the default parser for double values, which are encoded by {@link double#tostring(double)} a parser instance for int values encoded by {@link numericutils}, e.g. when indexed via {@link intfield}/{@link numerictokenstream}. a parser instance for float values encoded with {@link numericutils}, e.g. when indexed via {@link floatfield}/{@link numerictokenstream}. a parser instance for long values encoded by {@link numericutils}, e.g. when indexed via {@link longfield}/{@link numerictokenstream}. a parser instance for double values encoded with {@link numericutils}, e.g. when indexed via {@link doublefield}/{@link numerictokenstream}. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field and returns a bit set at the size of reader.maxdoc(), with turned on bits for each docid that does have a value for this field. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as a single byte and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the single byte values. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as bytes and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the bytes. computes byte for string values. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as shorts and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the shorts. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as shorts and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the shorts. computes short for string values. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as integers and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the integers. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as integers and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the integers. computes integer for string values. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as floats and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the floats. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as floats and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the floats. computes float for string values. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as longs and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the longs. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws java.io.ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as longs and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the longs. computes integer for string values. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as integers and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the doubles. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. checks the internal cache for an appropriate entry, and if none is found, reads the terms in field as doubles and returns an array of size reader.maxdoc() of the value each document has in the given field. used to get field values. which field contains the doubles. computes integer for string values. if true then {@link #getdocswithfield} will also be computed and stored in the fieldcache. values in the given field for each document. @throws ioexception if any error occurs. returned by {@link #getterms} the bytesref argument must not be null; the method returns the same bytesref, or an empty (length=0) bytesref if the doc did not have this field or was deleted. returns true if this doc has this field and is not deleted. number of documents checks the internal cache for an appropriate entry, and if none is found, reads the term values in field and returns a {@link docterms} instance, providing a method to retrieve the term (as a bytesref) per document. used to get field values. which field contains the strings. values in the given field for each document. @throws ioexception if any error occurs. expert: just like {@link #getterms(atomicreader,string)}, but you can specify whether more ram should be consumed in exchange for faster lookups (default is "true"). note that the first call for a given reader and field "wins", subsequent calls will share the same cache entry. returned by {@link #gettermsindex} the bytesref argument must not be null; the method returns the same bytesref, or an empty (length=0) bytesref if this ord is the null ord (0). convenience method, to lookup the term for a doc. if this doc is deleted or did not have this field, this will return an empty (length=0) bytesref. returns sort ord for this document. ord 0 is reserved for docs that are deleted or did not have this field. returns total unique ord count; this includes +1 for the null ord (always 0). number of documents returns a termsenum that can iterate over the values in this index entry @lucene.internal checks the internal cache for an appropriate entry, and if none is found, reads the term values in field and returns a {@link docterms} instance, providing a method to retrieve the term (as a bytesref) per document. used to get field values. which field contains the strings. values in the given field for each document. @throws ioexception if any error occurs. expert: just like {@link #gettermsindex(atomicreader,string)}, but you can specify whether more ram should be consumed in exchange for faster lookups (default is "true"). note that the first call for a given reader and field "wins", subsequent calls will share the same cache entry. checks the internal cache for an appropriate entry, and if none is found, reads the term values in field and returns a {@link doctermords} instance, providing a method to retrieve the terms (as ords) per document. used to build a {@link doctermords} instance which field contains the strings. {@link doctermords} instance @throws ioexception if any error occurs. expert: a unique identifier/description for each item in the fieldcache. can be useful for logging/debugging. @lucene.experimental computes (and stores) the estimated size of the cache value @see #getestimatedsize the most recently estimated size of the value, null unless estimatesize has been called. expert: generates an array of cacheentry objects representing all items currently in the fieldcache.  note: these cacheentry objects maintain a strong reference to the cached values. maintaining references to a cacheentry the atomicindexreader associated with it has garbage collected will prevent the value itself from being garbage collected when the cache drops the weakreference.  @lucene.experimental  expert: instructs the fieldcache to forcibly expunge all entries from the underlying caches. this is intended only to be used for test methods as a way to ensure a known base state of the cache (with out needing to rely on gc to free weakreferences). it should not be relied on for "cache maintenance" in general application code.  @lucene.experimental expert: drops all cache entries associated with this reader. note: this reader must precisely match the reader that the cache entry is keyed on. if you pass a top-level reader, it usually will have no effect as lucene now caches at the segment reader level. if non-null, fieldcacheimpl will warn whenever entries are counterpart of {@link #setinfostream(printstream)} for javadocs for javadocs for javadocs for javadocs for javadocs todo: would be far better to directly parse from utf8 bytes... but really intfield, instead, which already decodes directly from byte[] todo: would be far better to directly parse from utf8 bytes... but really intfield, instead, which already decodes directly from byte[] todo: would be far better to directly parse from utf8 bytes... but really intfield, instead, which already decodes directly from byte[] todo: would be far better to directly parse from utf8 bytes... but really floatfield, instead, which already decodes directly from byte[] todo: would be far better to directly parse from utf8 bytes... but really longfield, instead, which already decodes directly from byte[] todo: would be far better to directly parse from utf8 bytes... but really doublefield, instead, which already decodes directly from byte[] this special case is the reason that arrays.binarysearch() isn't useful. key found key not found."
org.apache.lucene.search.FieldValueFilter "a {@link filter} that accepts all documents that have one or more values in a given field. this {@link filter} request {@link bits} from the {@link fieldcache} and build the bits if not present. creates a new {@link fieldvaluefilter} the field to filter creates a new {@link fieldvaluefilter} the field to filter iff true all documents with no value in the given field are accepted. returns the field this filter is applied on. field this filter is applied on. returns true iff this filter is negated, otherwise false true iff this filter is negated, otherwise false uwesays: this is always the case for our current impl - but who knows :-)"
org.apache.lucene.search.TermStatistics "contains statistics for a specific term @lucene.experimental returns the term text returns the number of documents this term occurs in @see termsenum#docfreq() returns the total number of occurrences of this term @see termsenum#totaltermfreq() javadocs #positions must be >= #postings"
org.apache.lucene.search.PrefixTermsEnum "subclass of filteredtermenum for enumerating all terms that match the specified prefix filter term. term enumerations are always ordered by {@link #getcomparator}. each term in the enumeration is greater than all that precede it."
org.apache.lucene.search.TopDocsCollector "a base class for all collectors that return a {@link topdocs} output. this collector allows easy extension by providing a single constructor which accepts a {@link priorityqueue} as well as protected members for that priority queue and a counter of the number of total hits. extending classes can override any of the methods to provide their own implementation, as well as avoid the use of the priority queue entirely by passing null to {@link #topdocscollector(priorityqueue)}. in that case however, you might want to consider overriding all methods, in order to avoid a nullpointerexception. this is used in case topdocs() is called with illegal parameters, or there simply aren't (enough) results. the priority queue which holds the top documents. note that different implementations of priorityqueue give different meaning to 'top documents'. hitqueue for example aggregates the top scoring documents, while other pq implementations may hold documents sorted by other criteria. the total number of documents that the collector encountered. populates the results array with the scoredoc instances. this can be overridden in case a different scoredoc type should be returned. returns a {@link topdocs} instance containing the given results. if results is null it means there are no results to return, either because there were 0 calls to collect() or because the arguments to topdocs were invalid. the total number of documents that matched this query. the number of valid pq entries returns the top docs that were collected by this collector. returns the documents in the rage [start .. pq.size()) that were collected by this collector. note that if start >= pq.size(), an empty topdocs is returned. this method is convenient to call if the application always asks for the last results, starting from the last 'page'. note: you cannot call this method more than once for each search execution. if you need to call it more than once, passing each time a different start, you should call {@link #topdocs()} and work with the returned {@link topdocs} object, which will contain all the results this search execution collected. returns the documents in the rage [start .. start+howmany) that were collected by this collector. note that if start >= pq.size(), an empty topdocs is returned, and if pq.size() - start &lt; howmany, then only the available documents in [start .. pq.size()) are returned. this method is useful to call in case pagination of search results is allowed by the search application, as well as it attempts to optimize the memory used by allocating only as much as requested by howmany. note: you cannot call this method more than once for each search execution. if you need to call it more than once, passing each time a different range, you should call {@link #topdocs()} and work with the returned {@link topdocs} object, which will contain all the results this search execution collected. in case pq was populated with sentinel values, there might be less results than pq.size(). therefore return all results until either pq.size() or totalhits. in case pq was populated with sentinel values, there might be less results than pq.size(). therefore return all results until either pq.size() or totalhits. in case pq was populated with sentinel values, there might be less results than pq.size(). therefore return all results until either pq.size() or totalhits. in case pq was populated with sentinel values, there might be less results than pq.size(). therefore return all results until either pq.size() or totalhits. don't bother to throw an exception, just return an empty topdocs in case the parameters are invalid or out of range. todo: shouldn't we throw iae if apps give bad params here so they dont have sneaky silent bugs? we know that start < pqsize, so just fix howmany. pq's pop() returns the 'least' element in the queue, therefore need to discard the first ones, until we reach the requested range. note that this loop will usually not be executed, since the common usage should be that the caller asks for the last howmany results. however it's needed here for completeness. get the requested results from pq."
org.apache.lucene.search.TermCollectingRewrite "return a suitable top-level query for holding all expanded terms. add a multitermquery term to the top-level query attributes used for communication with the enum return false to stop collecting the next segment's {@link termsenum} that is used to collect terms reader has no fields field does not exist check comparator compatibility: interrupt whole term collection, so also don't iterate other subreaders"
org.apache.lucene.search.SloppyPhraseScorer "score a candidate doc for all slop-valid position-combinations (matches) encountered while traversing/hopping the phrasepositions.  the score contribution of a match depends on the distance:  - highest score for distance=0 (exact match).  - score gets lower as distance gets higher. example: for query "a b"~2, a document "x a b a y" can be scored twice: once for "a b" (distance=0), and once for "b a" (distance=2). possibly not all valid combinations are encountered, because for efficiency we always propagate the least phraseposition. this allows to base on priorityqueue and move forward faster. as result, for example, document "a b c b a" would score differently for queries "a b c"~4 and "c b a"~4, although they really are equivalent. similarly, for doc "a b c b a f g", query "c b"~2 would get same score as "g f"~2, although "c b"~2 could be matched twice. we may want to fix this in the future (currently not, for performance reasons). advance a phraseposition and update 'end', return false if exhausted pp was just advanced. if that caused a repeater collision, resolve by advancing the lesser of the two colliding pps. note that there can only be one collision, as by the initialization there were no collisions before pp was advanced. compare two pps, but only by position and offset index of a pp2 colliding with pp, or -1 if none initialize phrasepositions in place. a one time initialization for this scorer (on first doc matching all terms):  check if there are repetitions if there are, find groups of repetitions.  examples:  no repetitions: "ho my"~2 repetitions: "ho my my"~2 repetitions: "my ho my"~2  if pps are exhausted (and so current doc will not be a match) no repeats: simplest case, and most common. it is important to keep this piece of the code simple and efficient with repeats: not so simple. move all pps to their first position fill the queue (all pps are already placed at initialization (each doc), each repetition group is sorted by (query) offset. this provides the start condition: no collisions. case 1: no multi-term repeats it is sufficient to advance each pp in the group by one less than its group index. so lesser pp is not advanced, 2nd one advance once, 3rd one advanced twice, etc. case 2: multi-term repeats if pps are exhausted. initialize with checking for repeats. heavy work, but done only for the first candidate doc. if there are repetitions, check if multi-term postings (mtp) are involved. without mtp, once pps are placed in the first candidate doc, repeats (and groups) are visible. with mtp, a more complex check is needed, up-front, as there may be "hidden collisions". for example p1 has {a,b}, p1 has {b,c}, and the first doc is: "a c b". at start, p1 would point to "a", p2 to "c", and it will not be identified that p1 and p2 are repetitions of each other. the more complex initialization has two parts: (1) identification of repetition groups. (2) advancing repeat groups at the start of the doc. for (1), a possible solution is to just create a single repetition group, made of all repeating pps. but this would slow down the check for collisions, as all pps would need to be checked. instead, we compute "connected regions" on the bipartite graph of postings and terms. sort each repetition group by (query) offset. done only once (at first doc) and allows to initialize faster for each doc. detect repetition groups. done once - for first doc actual position in doc of a phraseposition, relies on that position = tppos - offset) find repeating terms and assign them ordinal values find repeating pps, and for each, if has multi-terms, update this.hasmultitermrpts bit-sets - for each repeating pp, for each of its repeating terms, the term ordinal values is set union (term group) bit-sets until they are disjoint (o(n^^2)), and each group have different terms map each term to the single group that contains it phrase frequency in current doc as computed by phrasefreq(). for advancing min position current largest phrase position flag indicating that there are repetitions (as checked in first candidate doc) flag to only check for repetitions in first candidate doc  in each group are pps that repeats each other (i.e. same term), sorted by (query) offset temporary stack for switching colliding repeating pps convert tps to a list of phrase positions. note: phrase-position differs from term-position in that its position reflects the phrase offset: pp.pos = tp.pos - offset. this allows to easily identify a matching (exact) phrase when all phrasepositions have exactly the same position. make it cyclic for easier manipulation pps exhausted done minimizing current match-length score match score match not a repeater for re-queuing after collisions are resolved always advance the lesser of the (only) two colliding pps exhausted careful: mark only those currently in the queue mark that pp2 need to be re-queued collisions resolved, now re-queue empty (partially) the queue until seeing all pps advanced for resolving collisions add back to queue pps available system.err.println("initsimple: doc: "+min.doc); position pps and build queue from list iterate cyclic list: done once handled max system.err.println("initcomplex: doc: "+min.doc); pps exhausted pps available iterate cyclic list: done once handled max iterate cyclic list: done once handled max more involved, some may not collide at initialization always advance pp with higher offset exhausted should not happen? simpler, we know exactly how much to advance pps exhausted pps available system.err.println("initfirsttime: doc: "+min.doc); needed with repetitions pps exhausted pps available we use this index for efficient re-queuing simpler - no multi-terms - can base on positions in first doc already marked as a repetition already marked as a repetition not a repetition: two pps are originally in same offset in the query! not a repetition a repetition more involved - has multi-terms iterate cyclic list: done once handled max iterate cyclic list: done once handled max i is the group no. private void printqueue(printstream ps, phrasepositions ext, string title) { //if (min.doc != ?) return; ps.println(); ps.println("---- "+title); ps.println("ext: "+ext); phrasepositions[] t = new phrasepositions[pq.size()]; if (pq.size()>0) { t[0] = pq.pop(); ps.println(" " + 0 + " " + t[0]); for (int i=1; i=0; i--) { pq.add(t[i]); } } } for further calls to docid() cyclic cyclic found a doc with all of the terms check for phrase found a match"
org.apache.lucene.search.TopTermsRewrite "base rewrite method for collecting only the top terms via a priority queue. @lucene.internal only public to be accessible by spans package. create a toptermsbooleanqueryrewrite for at most size terms.  note: if {@link booleanquery#getmaxclausecount} is smaller than size, then it will be used instead. return the maximum priority queue size return the maximum size of the priority queue (for boolean rewrites this is booleanquery#getmaxclausecount). lazy init the initial scoreterm because comparator is not known on ctor: for assert: make sure within a single seg we always collect terms in order system.out.println("ttr.collect term=" + bytes.utf8tostring() + " boost=" + boost + " ord=" + readercontext.ord); ignore uncompetitive hits if the term is already in the pq, only update docfreq of term in pq add new entry in pq, we must clone the term, else it may get overwritten! possibly drop entries from queue reset the termstate! set maxboostatt with values to help fuzzytermsenum to optimize add to query"
org.apache.lucene.search.BooleanScorer "description from doug cutting (excerpted from lucene-1483): booleanscorer uses an array to score windows of 2k docs. so it scores docs 0-2k first, then docs 2k-4k, etc. for each window it iterates through all query terms and accumulates a score in table[doc%2k]. it also stores in the table a bitmask representing which terms contributed to the score. non-zero scores are chained in a linked list. at the end of scoring each window it then iterates through the linked list and, if the bitmask matches the boolean constraints, collects a hit. for boolean queries with lots of frequent terms this can be much faster, since it does not need to update a priority queue for each posting, instead performing constant-time operations per posting. the only downside is that it results in hits being delivered out-of-order within the window, which means it cannot be nested within other scorers. but it works well as a top-level scorer. the new booleanscorer2 implementation instead works by merging priority queues of postings, albeit with some clever tricks. for example, a pure conjunction (all terms required) does not require a priority queue. instead it sorts the posting streams at the start, then repeatedly skips the first to to the last. if the first ever equals the last, then there's a hit. when some terms are required and some terms are optional, the conjunction can be evaluated first, then the optional terms can all skip to the match and be added to the score. thus the conjunction can reduce the number of priority queue updates for the optional terms. a simple hash table of document scores within a range. invalid bucket set doc initialize score initialize mask initialize coord push onto valid list valid bucket increment score add bits in mask increment coord not needed by this implementation an internal class which is used in score(collector, int) for setting the current score. this is required since collector exposes a setscorer method and implementations that need the score will call scorer.score(). therefore the only methods that are implemented are score() and doc(). tells if bucket is valid incremental score todo: break out bool anyprohibited, int numrequiredmatched; then we can remove 32 limit on required clauses used for bool constraints count of terms in score next valid bucket head of valid list pre-fill to save the lazy init when collecting each sub: todo: re-enable this if bq ever sends us required clauses public boolean required = false; todo: re-enable this if bq ever sends us required clauses this.required = required; todo: re-enable this if bq ever sends us required clauses private int requiredmask = 0; any time a prohibited clause matches we set bit 0: firstdocid is ignored since nextdoc() initializes 'current' make sure it's only booleanscorer that calls us: the internal loop will set the score and doc before calling collect. more queued check prohibited & required todo: re-enable this if bq ever sends us required clauses && (current.bits & requiredmask) == requiredmask) { note: lucene always passes max = integer.max_value today, because we never embed a booleanscorer inside another (even though that should work)... but in theory an outside app could pass a different max so we must check it: pop the queue refill the queue"
org.apache.lucene.search.IndexSearcher "implements search over a single indexreader. applications usually need only call the inherited {@link #search(query,int)} or {@link #search(query,filter,int)} methods. for performance reasons, if your index is unchanging, you should share a single indexsearcher instance across multiple searches instead of creating a new one per-search. if your index has changed and you wish to see the changes reflected in searching, you should use {@link directoryreader#openifchanged(directoryreader)} to obtain a new reader and then create a new indexsearcher from that. also, for low-latency turnaround it's best to use a near-real-time reader ({@link directoryreader#open(indexwriter,boolean)}). once you have a new {@link indexreader}, it's relatively cheap to create a new indexsearcher from it. note: {@link indexsearcher} instances are completely thread safe, meaning multiple threads can call any of its methods, concurrently. if your application requires external synchronization, you should not synchronize on the indexsearcher instance; use your own (non-lucene) objects instead. used with executor - each slice holds a set of leafs executed within one thread expert: returns a default similarity instance. in general, this method is only called to initialize searchers and writers. the similarity implementation used by this searcher. creates a searcher searching the provided index. runs searches for each segment separately, using the provided executorservice. indexsearcher will not shutdown/awaittermination this executorservice on close; you must do so, eventually, on your own. note: if you are using {@link niofsdirectory}, do not use the shutdownnow method of executorservice as this uses thread.interrupt under-the-hood which can silently close file descriptors (see lucene-2239). @lucene.experimental creates a searcher searching the provided top-level {@link indexreadercontext}.  given a non-null {@link executorservice} this method runs searches for each segment separately, using the provided executorservice. indexsearcher will not shutdown/awaittermination this executorservice on close; you must do so, eventually, on your own. note: if you are using {@link niofsdirectory}, do not use the shutdownnow method of executorservice as this uses thread.interrupt under-the-hood which can silently close file descriptors (see lucene-2239). @see indexreadercontext @see indexreader#getcontext() @lucene.experimental creates a searcher searching the provided top-level {@link indexreadercontext}. @see indexreadercontext @see indexreader#getcontext() @lucene.experimental expert: creates an array of leaf slices each holding a subset of the given leaves. each {@link leafslice} is executed in a single thread. by default there will be one {@link leafslice} per leaf ({@link atomicreadercontext}). return the {@link indexreader} this searches. sugar for .getindexreader().document(docid) @see indexreader#document(int) sugar for .getindexreader().document(docid, fieldvisitor) @see indexreader#document(int, storedfieldvisitor) sugar for .getindexreader().document(docid, fieldstoload) @see indexreader#document(int, set) @deprecated use {@link #doc(int, set)} instead. expert: set the similarity implementation used by this indexsearcher. @lucene.internal finds the top n hits for query where all results are after a previous result (after).  by passing the bottom result from a previous page as after, this method can be used for efficient 'deep-paging' across potentially large result sets. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. finds the top n hits for query, applying filter if non-null, where all results are after a previous result (after).  by passing the bottom result from a previous page as after, this method can be used for efficient 'deep-paging' across potentially large result sets. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. finds the top n hits for query. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. finds the top n hits for query, applying filter if non-null. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. lower-level search api. {@link collector#collect(int)} is called for every matching document. to match documents if non-null, used to permit documents to be collected. to receive hits @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. lower-level search api. {@link collector#collect(int)} is called for every matching document. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. search implementation with arbitrary sorting. finds the top n hits for query, applying filter if non-null, and sorting the hits by the criteria in sort. note: this does not compute scores by default; use {@link indexsearcher#search(query,filter,int,sort,boolean,boolean)} to control scoring. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. search implementation with arbitrary sorting, plus control over whether hit scores and max score should be computed. finds the top n hits for query, applying filter if non-null, and sorting the hits by the criteria in sort. if dodocscores is true then the score of each hit will be computed and returned. if domaxscore is true then the maximum score over all collected hits will be computed. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. finds the top n hits for query, applying filter if non-null, where all results are after a previous result (after).  by passing the bottom result from a previous page as after, this method can be used for efficient 'deep-paging' across potentially large result sets. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. search implementation with arbitrary sorting and no filter. the query to search for return only the top n results the {@link org.apache.lucene.search.sort} object top docs, sorted according to the supplied {@link org.apache.lucene.search.sort} instance @throws ioexception if there is a low-level i/o error finds the top n hits for query where all results are after a previous result (after).  by passing the bottom result from a previous page as after, this method can be used for efficient 'deep-paging' across potentially large result sets. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. finds the top n hits for query where all results are after a previous result (after), allowing control over whether hit scores and max score should be computed.  by passing the bottom result from a previous page as after, this method can be used for efficient 'deep-paging' across potentially large result sets. if dodocscores is true then the score of each hit will be computed and returned. if domaxscore is true then the maximum score over all collected hits will be computed. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. expert: low-level search implementation. finds the top n hits for query, applying filter if non-null. applications should usually call {@link indexsearcher#search(query,int)} or {@link indexsearcher#search(query,filter,int)} instead. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. expert: low-level search implementation. finds the top n hits for query. applications should usually call {@link indexsearcher#search(query,int)} or {@link indexsearcher#search(query,filter,int)} instead. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. expert: low-level search implementation with arbitrary sorting and control over whether hit scores and max score should be computed. finds the top n hits for query and sorting the hits by the criteria in sort. applications should usually call {@link indexsearcher#search(query,filter,int,sort)} instead. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. just like {@link #search(weight, int, sort, boolean, boolean)}, but you choose whether or not the fields in the returned {@link fielddoc} instances should be set by specifying fillfields. note: this does not compute scores by default. if you need scores, create a {@link topfieldcollector} instance by calling {@link topfieldcollector#create} and then pass that to {@link #search(list, weight, collector)}. just like {@link #search(weight, int, sort, boolean, boolean)}, but you choose whether or not the fields in the returned {@link fielddoc} instances should be set by specifying fillfields. lower-level search api.  {@link collector#collect(int)} is called for every document.   note: this method executes the searches on all given leaves exclusively. to search across all the searchers leaves use {@link #leafcontexts}. the searchers leaves to execute the searches on to match documents to receive hits @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. expert: called to re-write queries into primitive queries. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. returns an explanation that describes how doc scored against query. this is intended to be used in developing similarity implementations, and, for good performance, should not be displayed with every hit. computing an explanation is as expensive as executing the query over the entire index. expert: low-level implementation method returns an explanation that describes how doc scored against weight. this is intended to be used in developing similarity implementations, and, for good performance, should not be displayed with every hit. computing an explanation is as expensive as executing the query over the entire index. applications should call {@link indexsearcher#explain(query, int)}. @throws booleanquery.toomanyclauses if a query would exceed {@link booleanquery#getmaxclausecount()} clauses. creates a normalized weight for a top-level {@link query}. the query is rewritten by this method and {@link query#createweight} called, afterwards the {@link weight} is normalized. the returned {@code weight} can then directly be used to get a {@link scorer}. @lucene.internal returns this searchers the top-level {@link indexreadercontext}. @see indexreader#getcontext() sugar for #getreader().gettopreadercontext() a thread subclass for searching a single searchable a thread subclass for searching a single searchable a helper class that wraps a {@link completionservice} and provides an iterable interface to the completed {@link callable} instances.  the type of the {@link callable} return value a class holding a subset of the {@link indexsearcher}s leaf contexts to be executed within a single thread. @lucene.experimental returns {@link termstatistics} for a term. this can be overridden for example, to return a term's statistics across a distributed collection. @lucene.experimental returns {@link collectionstatistics} for a field. this can be overridden for example, to return a field's statistics across a distributed collection. @lucene.experimental javadocs javadoc javadocs package private for testing! note: these members might change in incompatible ways in the next release these are only used for multi-threaded search the default similarity todo: if we fix type safety of topfielddocs we can remove this todo: if we fix type safety of topfielddocs we can remove this todo: if we fix type safety of topfielddocs we can remove this search each sub put docs in array single thread use all leaves here! search each leaf slice single thread todo: should we make this threaded...? the collector could be sync'd? always use single thread: search each subreader it would be so nice if we had a thread-safe insert merge scoredocs into hq carry over maxscore from sub: use the shortcut here - this is only used in a private context"
org.apache.lucene.search.NumericRangeQuery "a {@link query} that matches numeric values within a specified range. to use this, you must first index the numeric values using {@link intfield}, {@link floatfield}, {@link longfield} or {@link doublefield} (expert: {@link numerictokenstream}). if your terms are instead textual, you should use {@link termrangequery}. {@link numericrangefilter} is the filter equivalent of this query. you create a new numericrangequery with the static factory methods, eg:  query q = numericrangequery.newfloatrange("weight", 0.03f, 0.10f, true, true);  matches all documents whose float valued "weight" field ranges from 0.03 to 0.10, inclusive. the performance of numericrangequery is much better than the corresponding {@link termrangequery} because the number of terms that must be searched is usually far fewer, thanks to trie indexing, described below. you can optionally specify a precisionstep when creating this query. this is necessary if you've changed this configuration from its default (4) during indexing. lower values consume more disk space but speed up searching. suitable values are between 1 and 8. a good starting point to test is 4, which is the default value for all numeric classes. see below for details. this query defaults to {@linkplain multitermquery#constant_score_auto_rewrite_default}. with precision steps of &le;4, this query can be run with one of the booleanquery rewrite methods without changing booleanquery's default max clause count. how it works see the publication about panfmp, where this algorithm was described (referred to as trierangequery): schindler, u, diepenbroek, m, 2008. generic xml-based framework for metadata portals. computers &amp; geosciences 34 (12), 1947-1955. doi:10.1016/j.cageo.2008.02.023 a quote from this paper: because apache lucene is a full-text search engine and not a conventional database, it cannot handle numerical ranges (e.g., field value is inside factory that creates a numericrangequery, that queries a long range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangequery, that queries a long range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangequery, that queries a int range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangequery, that queries a int range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangequery, that queries a double range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link double#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == double.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangequery, that queries a double range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link double#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == double.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangequery, that queries a float range using the given precisionstep. you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link float#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == float.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. factory that creates a numericrangequery, that queries a float range using the default precisionstep {@link numericutils#precision_step_default} (4). you can have half-open ranges (which are in fact &lt;/&le; or &gt;/&ge; queries) by setting the min or max value to null. {@link float#nan} will never match a half-open range, to hit {@code nan} use a query with {@code min == max == float.nan}. by setting inclusive to false, it will match all documents excluding the bounds, with inclusive on, the boundaries are hits, too. returns true if the lower endpoint is inclusive returns true if the upper endpoint is inclusive returns the lower value of this range query returns the upper value of this range query returns the precision step. subclass of filteredtermsenum for enumerating all terms that match the sub-ranges for trie range queries, using flex api.  warning: this term enumeration is not guaranteed to be always ordered by {@link term#compareto}. the ordering depends on how {@link numericutils#splitlongrange} and {@link numericutils#splitintrange} generates the sub-ranges. for {@link multitermquery} ordering is not relevant. for javadocs for javadocs for javadocs for javadocs for javadocs for javadocs very strange: java.lang.number itself is not comparable, but all subclasses used here are members (package private, to be also fast accessible by numericrangetermenum) used to handle float/double infinity correcty lower upper lower upper should never happen if the new upper bound is before the term parameter, the sub-range is never a hit never seek backwards, so use current term if lower bound is smaller no more sub-range enums available peek next sub-range, only seek if the current term is smaller than next lower bound step forward to next range without seeking, as next lower range bound is less or equal current term"
org.apache.lucene.search.FilteredDocIdSet "abstract decorator class for a docidset implementation that provides on-demand filtering/validation mechanism on a given docidset.  technically, this same functionality could be achieved with chainedfilter (under queries/), however the benefit of this class is it never materializes the full bitset for the filter. instead, the {@link #match} method is invoked on-demand, per docid visited during searching. if you know few docids will be visited, and the logic behind {@link #match} is relatively costly, this may be a better way to filter than chainedfilter. @see docidset constructor. underlying docidset this docidset implementation is cacheable if the inner set is cacheable. validation method to determine whether a docid should be in the result set. docid to be tested if input docid should be in the result set, false otherwise. implementation of the contract to build a docidsetiterator. @see docidsetiterator @see filtereddocidsetiterator"
org.apache.lucene.search.ReferenceManager "utility class to safely share instances of a certain type across multiple threads, while periodically refreshing them. this class ensures each reference is closed only once all threads have finished using it. it is recommended to consult the documentation of {@link referencemanager} implementations for their {@link #mayberefresh()} semantics.  the concrete type that will be {@link #acquire() acquired} and {@link #release(object) released}. @lucene.experimental decrement reference counting on the given reference. @throws ioexception if reference decrement on the given resource failed. refresh the given reference if needed. returns {@code null} if no refresh was needed, otherwise a new refreshed reference. @throws alreadyclosedexception if the reference manager has been {@link #close() closed}. @throws ioexception if the refresh operation failed try to increment reference counting on the given reference. return true if the operation was successful. @throws alreadyclosedexception if the reference manager has been {@link #close() closed}. obtain the current reference. you must match every call to acquire with one call to {@link #release}; it's best to do so in a finally clause, and set the reference to {@code null} to prevent accidental usage after it has been released. @throws alreadyclosedexception if the reference manager has been {@link #close() closed}.  closes this referencemanager to prevent future {@link #acquire() acquiring}. a reference manager should be closed if the reference to the managed resource should be disposed or the application using the {@link referencemanager} is shutting down. the managed resource might not be released immediately, if the {@link referencemanager} called after close(), so subclass can free any resources. @throws ioexception if the after close operation in a sub-class throws an {@link ioexception} you must call this (or {@link #mayberefreshblocking()}), periodically, if you want that {@link #acquire()} will return refreshed instances.  threads: it's fine for more than one thread to call this at once. only the first thread will attempt the refresh; subsequent threads will see that another thread is already handling refresh and will return immediately. note that this means if another thread is already refreshing then subsequent threads will return right away without waiting for the refresh to complete.  if this method returns true it means the calling thread either refreshed or that there were no changes to refresh. if it returns false it means another thread is currently refreshing.  @throws ioexception if refreshing the resource causes an {@link ioexception} @throws alreadyclosedexception if the reference manager has been {@link #close() closed}. you must call this (or {@link #mayberefresh()}), periodically, if you want that {@link #acquire()} will return refreshed instances.  threads: unlike {@link #mayberefresh()}, if another thread is currently refreshing, this method blocks until that thread completes. it is useful if you want to guarantee that the next call to {@link #acquire()} will return a refreshed instance. otherwise, consider using the non-blocking {@link #mayberefresh()}. @throws ioexception if refreshing the resource causes an {@link ioexception} @throws alreadyclosedexception if the reference manager has been {@link #close() closed}. called after a refresh was attempted, regardless of whether a new reference was in fact release the reference previously obtained via {@link #acquire()}.  note: it's safe to call this after {@link #close()}. @throws ioexception if the release operation on the given resource throws an {@link ioexception} adds a listener, to be notified when a reference is refreshed/swapped. remove a listener added with {@link #addlistener(refreshlistener)}. use to receive notification when a refresh has finished. see {@link #addlistener}. called after a successful refresh and a new reference has been installed. when this is called {@link #acquire()} is guaranteed to return a new instance. make sure we can call this more than once closeable javadoc says: if this is already closed then invoking this method has no effect. it's ok to call lock() here (blocking) because we're supposed to get here from either mayberefreh() or mayberefreshblocking(), after the lock has already been obtained. doing that protects us from an accidental bug where this method will be called outside the scope of refreshlock. per reentrantlock's javadoc, calling lock() by the same thread more than once is ok, as long as unlock() is called a matching number of times. ensure only 1 thread does refresh at once; other threads just return immediately: ensure only 1 thread does refresh at once"
org.apache.lucene.search.BoostAttributeImpl "implementation class for {@link boostattribute}. @lucene.internal"
org.apache.lucene.search.DocIdSet "a docidset contains a set of doc ids. implementing classes must only implement {@link #iterator} to provide access to the set. an empty {@code docidset} instance for easy use, e.g. in filters that hit no documents. provides a {@link docidsetiterator} to access the set. this implementation can return null or {@linkplain #empty_docidset}.iterator() if there are no docs that match. optionally provides a {@link bits} interface for random access to matching documents. {@code null}, if this {@code docidset} does not support random access. in contrast to {@link #iterator()}, a return value of {@code null} does not imply that no documents match the filter! the default implementation does not provide random access, so you only need to implement this method if your docidset can guarantee random access to every docid in o(1) time without external disk access (as {@link bits} interface cannot throw {@link ioexception}). this is generally true for bit sets like {@link org.apache.lucene.util.fixedbitset}, which return itself if they are used as {@code docidset}. this method is a hint for {@link cachingwrapperfilter}, if this docidset should be cached without copying it into a bitset. the default is to return false. if you have an own docidset implementation that does its iteration very effective and fast without doing disk i/o, override this method and return true. we explicitely provide no random access, as this filter is 100% sparse and iterator exits faster"
org.apache.lucene.search.FieldCacheTermsFilter "a {@link filter} that only accepts documents whose single term value in the specified field is contained in the provided set of allowed terms.  this is the same functionality as termsfilter (from queries/), except this filter requires that the field contains only a single term for all documents. because of drastically different implementations, they also have different performance characteristics, as described below.  the first invocation of this filter on a given field will be slower, since a {@link fieldcache.doctermsindex} must be javadoc @link"
org.apache.lucene.search.PrefixFilter "a filter that restricts search results to values that have a matching prefix in a given field. prints a"
org.apache.lucene.search.FieldCacheImpl "expert: the default cache implementation, storing all values in memory. a weakhashmap is used for storage. lucene 1.4 hack: when thrown from a parser (numeric_utils_ ones), this stops processing terms and returns the current fieldcache array. expert: internal cache. remove this reader from the cache, if present. sets the key to the value for the provided reader; if the key is already set then this doesn't change it. expert: every composite-key in the internal cache is of this type. creates one of these objects for a custom comparator/parser. two of these are equal iff they reference the same field and type. composes a hashcode based on the field and type. ignored ignored ignored ignored ignored :hack: for testing. if (null != locale || sortfield.custom != sortfieldtype) { throw new runtimeexception("locale/sortfieldtype: " + this); } per-segment fieldcaches don't purge until the shared core closes. composite/slowmultireaderwrapper fieldcaches don't purge until composite reader is closed. we have a slow reader of some sort, try to register a purge event rather than relying on gc: last chance first time this reader is using fieldcache another thread beat us to it; leave the current value first time this reader is using fieldcache only check if key.custom (the parser) is non-null; else, we check twice for a single call to fieldcache.getxxx ok this insanity involves our entry which field which custom comparator or parser inherit javadocs inherit javadocs fast case: all docs have this field: lazy init inherit javadocs inherit javadocs fast case: all docs have this field: lazy init null bits means no docs matched the cardinality of the bitset is maxdoc if all documents have a value. inherit javadocs inherit javadocs fast case: all docs have this field: late init so numeric fields don't double allocate lazy init no values fast case: all docs have this field: lazy init todo: use bulk api the cardinality of the bitset is maxdoc if all documents have a value. inherit javadocs inherit javadocs fast case: all docs have this field: late init so numeric fields don't double allocate lazy init no values inherit javadocs fast case: all docs have this field: late init so numeric fields don't double allocate lazy init no values inherit javadocs inherit javadocs fast case: all docs have this field: late init so numeric fields don't double allocate lazy init no values end position in the current block key found todo: if gap is small, could iterate from current position? or let switch byte blocks special case of empty last array try for coarse estimate for number of bits; this should be an underestimate most of the time, which is fine -- growablewriter will reallocate as needed app is misusing the api (there is more than one term per doc); in this case we make best effort to load what we can (see lucene-2142) 0 is reserved for "unset" note: this code only runs if the incoming reader impl doesn't implement size (which should be uncommon) maybe an int-only impl? todo: this if doctermsindex was already should share it... holds the actual term data, expanded. try for coarse estimate for number of bits; this should be an underestimate most of the time, which is fine -- growablewriter will reallocate as needed pointer==0 means not set app is misusing the api (there is more than one term per doc); in this case we make best effort to load what we can (see lucene-2142) maybe an int-only impl?"
org.apache.lucene.search.QueryWrapperFilter "constrains search results to only match those which also match a provided query.  this could be used, for example, with a {@link termrangequery} on a suitably formatted date field to implement date filtering. one could re-use a single queryfilter that matches, e.g., only documents modified within the last week. the queryfilter and termrangequery would only need to be reconstructed once per day. constructs a filter which only matches documents matching query. returns the inner query get a private context that is used to rewrite, createweight and score eventually"
org.apache.lucene.search.PrefixQuery "a query that matches documents containing terms with a specified prefix. a prefixquery is built by queryparser for input like app. this query uses the {@link multitermquery#constant_score_auto_rewrite_default} rewrite method. constructs a query for terms starting with prefix. returns the prefix of this query. prints a no prefix -- match all terms for this field:"
org.apache.lucene.search.BoostAttribute "add this {@link attribute} to a {@link termsenum} returned by {@link multitermquery#gettermsenum(terms,attributesource)} and update the boost on each returned term. this enables to control the boost factor for each matching term in {@link multitermquery#scoring_boolean_query_rewrite} or {@link toptermsrewrite} mode. {@link fuzzyquery} is using this to take the edit distance into account. please note: this attribute is intended to be added only by the termsenum to itself in its constructor and consumed by the {@link multitermquery.rewritemethod}. @lucene.internal sets the boost in this attribute retrieves the boost, default is {@code 1.0f}. javadocs only javadocs only javadocs only"
org.apache.lucene.search.TopScoreDocCollector "a {@link collector} implementation that collects the top-scoring hits, returning them as a {@link topdocs}. this is used by {@link indexsearcher} to implement {@link topdocs}-based search. hits are sorted by score descending and then (when the scores are tied) docid ascending. when you create an instance of this collector you should know in advance whether documents are going to be collected in doc id order or not. note: the values {@link float#nan} and {@link float#negative_infinity} are not valid scores. this collector will not properly collect hits with such scores. creates a new {@link topscoredoccollector} given the number of hits to collect and whether documents are scored in order by the input {@link scorer} to {@link #setscorer(scorer)}. note: the instances returned by this method pre-allocate a full array of length numhits, and fill the array with sentinel objects. creates a new {@link topscoredoccollector} given the number of hits to collect, the bottom of the previous page, and whether documents are scored in order by the input {@link scorer} to {@link #setscorer(scorer)}. note: the instances returned by this method pre-allocate a full array of length numhits, and fill the array with sentinel objects. assumes docs are scored in order. this collector cannot handle these scores: since docs are returned in-order (i.e., increasing doc id), a document with equal score to pqtop.score cannot compete since hitqueue favors documents with lower doc ids. therefore reject those docs too. assumes docs are scored in order. this is always after.doc - docbase, to save an add when score == after.score this collector cannot handle these scores: hit was collected on a previous page since docs are returned in-order (i.e., increasing doc id), a document with equal score to pqtop.score cannot compete since hitqueue favors documents with lower doc ids. therefore reject those docs too. assumes docs are scored out of order. this collector cannot handle nan doesn't compete w/ bottom entry in queue break tie in score by doc id: assumes docs are scored out of order. this is always after.doc - docbase, to save an add when score == after.score this collector cannot handle nan hit was collected on a previous page doesn't compete w/ bottom entry in queue break tie in score by doc id: prevents instantiation hitqueue implements getsentinelobject to return a scoredoc, so we know that at this point top() is already initialized. we need to compute maxscore in order to set it in topdocs. if start == 0, it means the largest element is already in results, use its score as maxscore. otherwise pop everything else, until the largest element is extracted and use its score as maxscore."
org.apache.lucene.search.DocIdSetIterator "this abstract class defines methods to iterate over a set of non-decreasing doc ids. note that this class assumes it iterates on doc ids, and therefore {@link #no_more_docs} is set to {@value #no_more_docs} in order to be used as a sentinel object. implementations of this class are expected to consider {@link integer#max_value} as an invalid value. when returned by {@link #nextdoc()}, {@link #advance(int)} and {@link #docid()} it means there are no more docs in the iterator. returns the following:  -1 or {@link #no_more_docs} if {@link #nextdoc()} or {@link #advance(int)} were not called yet. {@link #no_more_docs} if the iterator has exhausted. otherwise it should return the doc id it is currently on.   advances to the next document in the set and returns the doc it is currently on, or {@link #no_more_docs} if there are no more docs in the set. note: after the iterator has exhausted you should not call this method, as it may result in unpredicted behavior. advances to the first beyond the current whose document number is greater than or equal to target, and returns the document number itself. exhausts the iterator and returns {@link #no_more_docs} if target is greater than the highest document number in the set.  the behavior of this method is undefined when called with  target &le; current, or after the iterator has exhausted. both cases may result in unpredicted behavior.  when  target &gt; current it behaves as if written:  int advance(int target) { int doc; while ((doc = nextdoc()) &lt; target) { } return doc; }  some implementations are considerably more efficient than that.  note: this method may be called with {@link #no_more_docs} for efficiency by some scorers. if your implementation cannot efficiently determine that it should exhaust, it is recommended that you check for that value in each call to this method. "
org.apache.lucene.search.DisjunctionSumScorer "a scorer for or like queries, counterpart of conjunctionscorer. this scorer implements {@link scorer#advance(int)} and uses advance() on the given scorers. the minimum number of scorers that should match. the document number of the current match. the number of subscorers that provide the current match. construct a disjunctionscorer. the weight to be used. a collection of at least two subscorers. the positive minimum number of subscorers that should match to match this query. when minimumnrmatchers is bigger than the number of subscorers, no matches will be produced. when minimumnrmatchers equals the number of subscorers, it more efficient to use conjunctionscorer. construct a disjunctionscorer, using one as the minimum number of matching subscorers. returns the score of the current document matching the query. initially invalid, until {@link #nextdoc()} is called the first time. advances to the first match beyond the current whose document number is greater than or equal to a given target.  the implementation uses the advance() method on the subscorers. the target document number. document whose number is greater than or equal to the given target, or -1 if none exist. stop looping todo: this currently scores, but so did the previous impl todo: remove recursion. todo: if we separate scoring, out of here, modify this and afternext() to terminate when nrmatchers == minimumnrmatchers then also change freq() to just always compute it from scratch"
org.apache.lucene.search.MatchAllDocsQuery "a query that matches all documents. explain query weight"
org.apache.lucene.search.AutomatonQuery "a {@link query} that will match terms against a finite-state machine.  this query will match documents that contain terms accepted by a given finite-state machine. the automaton can be constructed with the {@link org.apache.lucene.util.automaton} api. alternatively, it can be the automaton to match index terms against term containing the field, and possibly some pattern structure create a new automatonquery from an {@link automaton}. term containing field and possibly some pattern structure. the term text is ignored. automaton to run, terms that are accepted are considered a match. we already minimized the automaton in the ctor, so this hash code will be the same for automata that are the same:"
org.apache.lucene.search.PhrasePositions "position of a term in a document that takes into account the term offset within the phrase. go to next location of this term current document, and set position as location - offset, so that a matching exact phrase is easily identified when all phrasepositions have exactly the same position. for debug purposes current doc position in doc remaining pos in this doc position in phrase unique across all phrasepositions instances stream of docs & positions used to make lists >=0 indicates that this is a repeating pp index in the rptgroup for repetitions initialization increments to next doc read first pos read subsequent pos's"
org.apache.lucene.document.DocumentStoredFieldVisitor "a {@link storedfieldvisitor} that creates a {@link document} containing all stored fields, or only specific requested fields provided to {@link #documentstoredfieldvisitor(set)}.  this is used by {@link indexreader#document(int)} to load a document. @lucene.experimental load only fields named in the provided set&lt;string&gt;. set of fields to load, or null (all fields). load only fields named in the provided fields. load all stored fields. retrieve the visited document. populated with stored fields. note that only the stored information in the field instances is valid, data such as boosts, indexing options, term vector options, etc is not set."
org.apache.lucene.document.IntField " field that indexes int values for efficient range filtering and sorting. here's an example usage:  document.add(new intfield(name, 6, field.store.no));  for optimal performance, re-use the intfield and {@link document} instance for more than one document:  intfield field = new intfield(name, 6, field.store.no); document document = new document(); document.add(field); for(all documents) { ... field.setintvalue(value) writer.adddocument(document); ... }  see also {@link longfield}, {@link floatfield}, {@link doublefield}. to perform range querying or filtering against a intfield, use {@link numericrangequery} or {@link numericrangefilter}. to sort according to a intfield, use the normal numeric sort types, eg {@link org.apache.lucene.search.sortfield.type#int}. intfield values can also be loaded directly from {@link fieldcache}. you may add the same field name as an intfield to the same document more than once. range querying and filtering will be the logical or of all values; so a range query will hit all documents that have at least one value in the range. however sort behavior is not defined. if you need to sort, you should separately index a single-valued intfield. an intfield will consume somewhat more disk space in the index than an ordinary single-valued field. however, for a typical index that includes substantial textual content per document, this increase will likely be in the noise.  within lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). the step size between each successive bracket is called the precisionstep, measured in bits. smaller precisionstep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. the default value, 4, was selected for a reasonable tradeoff of disk space consumption versus performance. you can create a custom {@link fieldtype} and invoke the {@link fieldtype#setnumericprecisionstep} method if you'd like to change the value. note that you must also specify a congruent value when creating {@link numericrangequery} or {@link numericrangefilter}. for low cardinality fields larger precision steps are good. if the cardinality is &lt; 100, it is fair to use {@link integer#max_value}, which produces one term per value. for more information on the internals of numeric trie indexing, including the precisionstep configuration, see {@link numericrangequery}. the format of indexed values is described in {@link numericutils}. if you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionstep of {@link integer#max_value}. this will minimize disk space consumed.  more advanced type for an intfield that is not stored: normalization factors, frequencies, and positions are omitted. type for a stored intfield: normalization factors, frequencies, and positions are omitted. creates a stored or un-stored intfield with the provided value and default precisionstep {@link numericutils#precision_step_default} (4). field name 32-bit integer value store.yes if the content should also be stored @throws illegalargumentexception if the field name is null. expert: allows you to customize the {@link fieldtype}. field name 32-bit integer value customized field type: must have {@link fieldtype#numerictype()} of {@link fieldtype.numerictype#int}. @throws illegalargumentexception if the field name or type is null, or if the field type does not have a int numerictype() javadocs javadocs javadocs javadocs"
org.apache.lucene.document.StoredField "a field whose value is stored so that {@link indexsearcher#doc} and {@link indexreader#document} will return the field and its value. type for a stored-only field. create a stored-only field with the given binary value. note: the provided byte[] is not copied so be sure not to change it until you're done with this field. field name byte array pointing to binary content (not copied) @throws illegalargumentexception if the field name is null. create a stored-only field with the given binary value. note: the provided byte[] is not copied so be sure not to change it until you're done with this field. field name byte array pointing to binary content (not copied) starting position of the byte array valid length of the byte array @throws illegalargumentexception if the field name is null. create a stored-only field with the given binary value. note: the provided bytesref is not copied so be sure not to change it until you're done with this field. field name bytesref pointing to binary content (not copied) @throws illegalargumentexception if the field name is null. create a stored-only field with the given string value. field name string value @throws illegalargumentexception if the field name or value is null. create a stored-only field with the given integer value. field name integer value @throws illegalargumentexception if the field name is null. create a stored-only field with the given float value. field name float value @throws illegalargumentexception if the field name is null. create a stored-only field with the given long value. field name long value @throws illegalargumentexception if the field name is null. create a stored-only field with the given double value. field name double value @throws illegalargumentexception if the field name is null. javadocs javadocs todo: not great but maybe not a big problem?"
org.apache.lucene.document.DateTools "provides support for converting dates to strings and vice-versa. the strings are structured so that lexicographic sorting orders them by date, which makes them suitable for use as field values and search terms. this class also helps you to limit the resolution of your dates. do not save dates with a finer resolution than you really need, as then rangequery and prefixquery will require more memory and become slower.  another approach is {@link numericutils}, which provides a sortable binary representation (prefix encoded) of numeric values, which date/time are. for indexing a {@link date} or {@link calendar}, just get the unix timestamp as long using {@link date#gettime} or {@link calendar#gettimeinmillis} and index this as a numeric value with {@link longfield} and use {@link numericrangequery} to query it. converts a date to a string suitable for indexing. the date to be converted the desired resolution, see {@link #round(date, datetools.resolution)} string in format yyyymmddhhmmsssss or shorter, depending on resolution; using gmt as timezone converts a millisecond time to a string suitable for indexing. the date expressed as milliseconds since january 1, 1970, 00:00:00 gmt the desired resolution, see {@link #round(long, datetools.resolution)} string in format yyyymmddhhmmsssss or shorter, depending on resolution; using gmt as timezone converts a string produced by timetostring or datetostring back to a time, represented as the number of milliseconds since january 1, 1970, 00:00:00 gmt. the date string to be converted number of milliseconds since january 1, 1970, 00:00:00 gmt @throws parseexception if datestring is not in the expected format converts a string produced by timetostring or datetostring back to a time, represented as a date object. the date string to be converted parsed time as a date object @throws parseexception if datestring is not in the expected format limit a date's resolution. for example, the date 2004-09-21 13:50:11 will be changed to 2004-09-01 00:00:00 when using resolution.month. the desired resolution of the date to be returned date with all values more precise than resolution set to 0 or 1 limit a date's resolution. for example, the date 1095767411000 (which represents 2004-09-21 13:50:11) will be changed to 1093989600000 (2004-09-01 00:00:00) when using resolution.month. the desired resolution of the date to be returned date with all values more precise than resolution set to 0 or 1, expressed as milliseconds since january 1, 1970, 00:00:00 gmt specifies the time granularity. limit a date's resolution to year granularity. limit a date's resolution to month granularity. limit a date's resolution to day granularity. limit a date's resolution to hour granularity. limit a date's resolution to minute granularity. limit a date's resolution to second granularity. limit a date's resolution to millisecond granularity. this method returns the name of the resolution in lowercase (for backwards compatibility) for javadocs for javadocs indexed by format length cannot create, the class has static methods only note: switch statement fall-through is deliberate don't cut off anything formatlen 10's place: 11111111 formatlen 1's place: 12345678901234567"
org.apache.lucene.document.DerefBytesDocValuesField " field that stores a per-document {@link bytesref} value. the values are stored indirectly, such that many documents sharing the same value all point to a single copy of the value, which is a good fit when the fields share values. if values are (mostly) unique it's better to use {@link straightbytesdocvaluesfield}. here's an example usage:  document.add(new derefbytesdocvaluesfield(name, new bytesref("hello")));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for indirect bytes docvalues: all with the same length type for indirect bytes docvalues: can have variable lengths create a new variable-length indirect docvalues field.  this calls {@link derefbytesdocvaluesfield#derefbytesdocvaluesfield(string, bytesref, boolean) derefbytesdocvaluesfield(name, bytes, false}, meaning by default it allows for values of different lengths. if your values are all the same length, use that constructor instead. field name binary content @throws illegalargumentexception if the field name is null create a new fixed or variable length indirect docvalues field.  field name binary content true if all values have the same length. @throws illegalargumentexception if the field name is null todo: ideally indexer figures out var vs fixed on its own!?"
org.apache.lucene.document.FieldType "describes the properties of a field. data type of the numeric value 32-bit integer numeric type 64-bit long numeric type 32-bit float numeric type 64-bit double numeric type create a new mutable fieldtype with all of the properties from ref create a new fieldtype with default properties. prevents future changes. note, it is recommended that this is called once the fieldtypes's properties have been set, to prevent unintentional state changes. {@inheritdoc}  the default is false. @see #setindexed(boolean) set to true to index (invert) this field. true if this field should be indexed. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #indexed() {@inheritdoc}  the default is false. @see #setstored(boolean) set to true to store this field. true if this field should be stored. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #stored() {@inheritdoc}  the default is true. @see #settokenized(boolean) set to true to tokenize this field's contents via the configured {@link analyzer}. true if this field should be tokenized. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #tokenized() {@inheritdoc}  the default is false. @see #setstoretermvectors(boolean) set to true if this field's indexed form should be also stored into term vectors. true if this field should store term vectors. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #storetermvectors() {@inheritdoc}  the default is false. @see #setstoretermvectoroffsets(boolean) set to true to also store token character offsets into the term vector for this field. true if this field should store term vector offsets. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #storetermvectoroffsets() {@inheritdoc}  the default is false. @see #setstoretermvectorpositions(boolean) set to true to also store token positions into the term vector for this field. true if this field should store term vector positions. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #storetermvectorpositions() {@inheritdoc}  the default is false. @see #setstoretermvectorpayloads(boolean) set to true to also store token payloads into the term vector for this field. true if this field should store term vector payloads. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #storetermvectorpayloads() {@inheritdoc}  the default is false. @see #setomitnorms(boolean) set to true to omit normalization values for the field. true if this field should omit norms. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #omitnorms() {@inheritdoc}  the default is {@link indexoptions#docs_and_freqs_and_positions}. @see #setindexoptions(org.apache.lucene.index.fieldinfo.indexoptions) sets the indexing options for the field: indexing options @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #indexoptions() set's the field's docvalues.type docvalues type, or null if no docvalues should be stored. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #docvaluetype() {@inheritdoc}  the default is null (no docvalues) @see #setdocvaluetype(docvalues.type) specifies the field's numeric type. numeric type, or null if the field has no numeric type. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #numerictype() numerictype: if non-null then the field's value will be indexed numerically so that {@link numericrangequery} can be used at search time.  the default is null (no numeric type) @see #setnumerictype(numerictype) sets the numeric precision step for the field. numeric precision step for the field @throws illegalargumentexception if precisionstep is less than 1. @throws illegalstateexception if this fieldtype is frozen against future modifications. @see #numericprecisionstep() precision step for numeric field.  this has no effect if {@link #numerictype()} returns null.  the default is {@link numericutils#precision_step_default} @see #setnumericprecisionstep(int) prints a field for human consumption. javadocs javadocs do not copy frozen!"
org.apache.lucene.document.CompressionTools "simple utility class providing static methods to compress and decompress binary data for stored fields. this class uses java.util.zip.deflater and inflater classes to compress and decompress. compresses the specified byte range using the specified compressionlevel (constants are defined in java.util.zip.deflater). create an expandable byte array to hold the compressed data. you cannot use an array that's the same size as the orginal because there is no guarantee that the compressed data will be smaller than the uncompressed data. compresses the specified byte range, with default best_compression level compresses all bytes in the array, with default best_compression level compresses the string value, with default best_compression level compresses the string value using the specified compressionlevel (constants are defined in java.util.zip.deflater). decompress the byte array previously returned by compress (referenced by the provided bytesref) decompress the byte array previously returned by compress decompress the byte array previously returned by compress decompress the byte array previously returned by compressstring back into a string decompress the byte array previously returned by compressstring back into a string decompress the byte array (referenced by the provided bytesref) previously returned by compressstring back into a string export only static methods compress the data create an expandable byte array to hold the decompressed data decompress the data"
org.apache.lucene.document.FloatField " field that indexes float values for efficient range filtering and sorting. here's an example usage:  document.add(new floatfield(name, 6.0f, field.store.no));  for optimal performance, re-use the floatfield and {@link document} instance for more than one document:  floatfield field = new floatfield(name, 0.0f, field.store.no); document document = new document(); document.add(field); for(all documents) { ... field.setfloatvalue(value) writer.adddocument(document); ... }  see also {@link intfield}, {@link longfield}, {@link doublefield}. to perform range querying or filtering against a floatfield, use {@link numericrangequery} or {@link numericrangefilter}. to sort according to a floatfield, use the normal numeric sort types, eg {@link org.apache.lucene.search.sortfield.type#float}. floatfield values can also be loaded directly from {@link fieldcache}. you may add the same field name as an floatfield to the same document more than once. range querying and filtering will be the logical or of all values; so a range query will hit all documents that have at least one value in the range. however sort behavior is not defined. if you need to sort, you should separately index a single-valued floatfield. a floatfield will consume somewhat more disk space in the index than an ordinary single-valued field. however, for a typical index that includes substantial textual content per document, this increase will likely be in the noise.  within lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). the step size between each successive bracket is called the precisionstep, measured in bits. smaller precisionstep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. the default value, 4, was selected for a reasonable tradeoff of disk space consumption versus performance. you can create a custom {@link fieldtype} and invoke the {@link fieldtype#setnumericprecisionstep} method if you'd like to change the value. note that you must also specify a congruent value when creating {@link numericrangequery} or {@link numericrangefilter}. for low cardinality fields larger precision steps are good. if the cardinality is &lt; 100, it is fair to use {@link integer#max_value}, which produces one term per value. for more information on the internals of numeric trie indexing, including the precisionstep configuration, see {@link numericrangequery}. the format of indexed values is described in {@link numericutils}. if you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionstep of {@link integer#max_value}. this will minimize disk space consumed.  more advanced type for a floatfield that is not stored: normalization factors, frequencies, and positions are omitted. type for a stored floatfield: normalization factors, frequencies, and positions are omitted. creates a stored or un-stored floatfield with the provided value and default precisionstep {@link numericutils#precision_step_default} (4). field name 32-bit double value store.yes if the content should also be stored @throws illegalargumentexception if the field name is null. expert: allows you to customize the {@link fieldtype}. field name 32-bit float value customized field type: must have {@link fieldtype#numerictype()} of {@link fieldtype.numerictype#float}. @throws illegalargumentexception if the field name or type is null, or if the field type does not have a float numerictype() javadocs javadocs javadocs javadocs"
org.apache.lucene.document.Document "documents are the unit of indexing and search. a document is a set of fields. each field has a name and a textual value. a field may be {@link org.apache.lucene.index.indexablefieldtype#stored() stored} with the document, in which case it is returned with search hits on the document. thus each document should typically contain one or more stored fields which uniquely identify it. note that fields which are not {@link org.apache.lucene.index.indexablefieldtype#stored() stored} are not available in documents retrieved from the index, e.g. with {@link scoredoc#doc} or {@link indexreader#document(int)}. constructs a new document with no fields. adds a field to a document. several fields may be added with the same name. in this case, if the fields are indexed, their text is treated as though appended for the purposes of search.  note that add like the removefield(s) methods only makes sense prior to adding a document to an index. these methods cannot be used to change the content of an existing index! in order to achieve this, a document has to be deleted from an index and a new changed version of that document has to be added. removes field with the specified name from the document. if multiple fields exist with this name, this method removes the first field that has been added. if there is no field with the specified name, the document remains unchanged.  note that the removefield(s) methods like the add method only make sense prior to adding a document to an index. these methods cannot be used to change the content of an existing index! in order to achieve this, a document has to be deleted from an index and a new changed version of that document has to be added. removes all fields with the given name from the document. if there is no field with the specified name, the document remains unchanged.  note that the removefield(s) methods like the add method only make sense prior to adding a document to an index. these methods cannot be used to change the content of an existing index! in order to achieve this, a document has to be deleted from an index and a new changed version of that document has to be added. returns an array of byte arrays for of the fields that have the name specified as the method parameter. this method returns an empty array when there are no matching fields. it never returns null. the name of the field bytesref[] of binary field values returns an array of bytes for the first (or only) field that has the name specified as the method parameter. this method will return null if no binary fields with the specified name are available. there may be non-binary fields with the same name. the name of the field. bytesref containing the binary field value or null returns a field with the given name if any exist in this document, or null. if multiple fields exists with this name, this method returns the first value added. returns an array of {@link indexablefield}s with the given name. this method returns an empty array when there are no matching fields. it never returns null. the name of the field indexablefield[] array returns a list of all the fields in a document. note that fields which are not stored are not available in documents retrieved from the index, e.g. {@link indexsearcher#doc(int)} or {@link indexreader#document(int)}. returns an array of values of the field specified as the method parameter. this method returns an empty array when there are no matching fields. it never returns null. for {@link intfield}, {@link longfield}, {@link floatfield} and {@link doublefield} it returns the string value of the number. if you want the actual numeric field instances back, use {@link #getfields}. the name of the field string[] of field values returns the string value of the field with the given name if any exist in this document, or null. if multiple fields exist with this name, this method returns the first value added. if only binary fields with this name exist, returns null. for {@link intfield}, {@link longfield}, {@link floatfield} and {@link doublefield} it returns the string value of the number. if you want the actual numeric field instance back, use {@link #getfield}. prints the fields of a document for human consumption. for javadoc for javadoc for javadoc"
org.apache.lucene.document.IntDocValuesField " field that stores a per-document int value for scoring, sorting or value retrieval. here's an example usage:  document.add(new intdocvaluesfield(name, 22));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for 32-bit integer docvalues. creates a new docvalues field with the specified 32-bit integer value field name 32-bit integer value @throws illegalargumentexception if the field name is null"
org.apache.lucene.document.ByteDocValuesField " field that stores a per-document byte value for scoring, sorting or value retrieval. here's an example usage:  document.add(new bytedocvaluesfield(name, (byte) 22));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for 8-bit byte docvalues. creates a new docvalues field with the specified 8-bit byte value field name 8-bit byte value @throws illegalargumentexception if the field name is null."
org.apache.lucene.document.StringField "a field that is indexed but not tokenized: the entire string value is indexed as a single token. for example this might be used for a 'country' field or an 'id' field, or any field that you intend to use for sorting or access through the field cache. indexed, not tokenized, omits norms, indexes docs_only, not stored. indexed, not tokenized, omits norms, indexes docs_only, stored creates a new stringfield. field name string value store.yes if the content should also be stored @throws illegalargumentexception if the field name or value is null."
org.apache.lucene.document.LongDocValuesField " field that stores a per-document long value for scoring, sorting or value retrieval. here's an example usage:  document.add(new longdocvaluesfield(name, 22l));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for 64-bit long docvalues. creates a new docvalues field with the specified 64-bit long value field name 64-bit long value @throws illegalargumentexception if the field name is null"
org.apache.lucene.document.SortedBytesDocValuesField " field that stores a per-document {@link bytesref} value, indexed for sorting. here's an example usage:  document.add(new sortedbytesdocvaluesfield(name, new bytesref("hello")));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for sorted bytes docvalues: all with the same length type for sorted bytes docvalues: can have variable lengths create a new variable-length sorted docvalues field.  this calls {@link sortedbytesdocvaluesfield#sortedbytesdocvaluesfield(string, bytesref, boolean) sortedbytesdocvaluesfield(name, bytes, false}, meaning by default it allows for values of different lengths. if your values are all the same length, use that constructor instead. field name binary content @throws illegalargumentexception if the field name is null create a new fixed or variable length sorted docvalues field. field name binary content true if all values have the same length. @throws illegalargumentexception if the field name is null todo: ideally indexer figures out var vs fixed on its own!?"
org.apache.lucene.document.LongField " field that indexes long values for efficient range filtering and sorting. here's an example usage:  document.add(new longfield(name, 6l, field.store.no));  for optimal performance, re-use the longfield and {@link document} instance for more than one document:  longfield field = new longfield(name, 0l, field.store.no); document document = new document(); document.add(field); for(all documents) { ... field.setlongvalue(value) writer.adddocument(document); ... }  see also {@link intfield}, {@link floatfield}, {@link doublefield}. any type that can be converted to long can also be indexed. for example, date/time values represented by a {@link java.util.date} can be translated into a long value using the {@link java.util.date#gettime} method. if you don't need millisecond precision, you can quantize the value, either by dividing the result of {@link java.util.date#gettime} or using the separate getters (for year, month, etc.) to construct an int or long value. to perform range querying or filtering against a longfield, use {@link numericrangequery} or {@link numericrangefilter}. to sort according to a longfield, use the normal numeric sort types, eg {@link org.apache.lucene.search.sortfield.type#long}. longfield values can also be loaded directly from {@link fieldcache}. you may add the same field name as an longfield to the same document more than once. range querying and filtering will be the logical or of all values; so a range query will hit all documents that have at least one value in the range. however sort behavior is not defined. if you need to sort, you should separately index a single-valued longfield. a longfield will consume somewhat more disk space in the index than an ordinary single-valued field. however, for a typical index that includes substantial textual content per document, this increase will likely be in the noise.  within lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). the step size between each successive bracket is called the precisionstep, measured in bits. smaller precisionstep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. the default value, 4, was selected for a reasonable tradeoff of disk space consumption versus performance. you can create a custom {@link fieldtype} and invoke the {@link fieldtype#setnumericprecisionstep} method if you'd like to change the value. note that you must also specify a congruent value when creating {@link numericrangequery} or {@link numericrangefilter}. for low cardinality fields larger precision steps are good. if the cardinality is &lt; 100, it is fair to use {@link integer#max_value}, which produces one term per value. for more information on the internals of numeric trie indexing, including the precisionstep configuration, see {@link numericrangequery}. the format of indexed values is described in {@link numericutils}. if you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionstep of {@link integer#max_value}. this will minimize disk space consumed.  more advanced type for a longfield that is not stored: normalization factors, frequencies, and positions are omitted. type for a stored longfield: normalization factors, frequencies, and positions are omitted. creates a stored or un-stored longfield with the provided value and default precisionstep {@link numericutils#precision_step_default} (4). field name 64-bit long value store.yes if the content should also be stored @throws illegalargumentexception if the field name is null. expert: allows you to customize the {@link fieldtype}. field name 64-bit long value customized field type: must have {@link fieldtype#numerictype()} of {@link fieldtype.numerictype#long}. @throws illegalargumentexception if the field name or type is null, or if the field type does not have a long numerictype() javadocs javadocs javadocs javadocs"
org.apache.lucene.document.DoubleField " field that indexes double values for efficient range filtering and sorting. here's an example usage:  document.add(new doublefield(name, 6.0, field.store.no));  for optimal performance, re-use the doublefield and {@link document} instance for more than one document:  doublefield field = new doublefield(name, 0.0, field.store.no); document document = new document(); document.add(field); for(all documents) { ... field.setdoublevalue(value) writer.adddocument(document); ... }  see also {@link intfield}, {@link longfield}, {@link floatfield}. to perform range querying or filtering against a doublefield, use {@link numericrangequery} or {@link numericrangefilter}. to sort according to a doublefield, use the normal numeric sort types, eg {@link org.apache.lucene.search.sortfield.type#double}. doublefield values can also be loaded directly from {@link fieldcache}. you may add the same field name as an doublefield to the same document more than once. range querying and filtering will be the logical or of all values; so a range query will hit all documents that have at least one value in the range. however sort behavior is not defined. if you need to sort, you should separately index a single-valued doublefield. a doublefield will consume somewhat more disk space in the index than an ordinary single-valued field. however, for a typical index that includes substantial textual content per document, this increase will likely be in the noise.  within lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). the step size between each successive bracket is called the precisionstep, measured in bits. smaller precisionstep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. the default value, 4, was selected for a reasonable tradeoff of disk space consumption versus performance. you can create a custom {@link fieldtype} and invoke the {@link fieldtype#setnumericprecisionstep} method if you'd like to change the value. note that you must also specify a congruent value when creating {@link numericrangequery} or {@link numericrangefilter}. for low cardinality fields larger precision steps are good. if the cardinality is &lt; 100, it is fair to use {@link integer#max_value}, which produces one term per value. for more information on the internals of numeric trie indexing, including the precisionstep configuration, see {@link numericrangequery}. the format of indexed values is described in {@link numericutils}. if you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionstep of {@link integer#max_value}. this will minimize disk space consumed.  more advanced type for a doublefield that is not stored: normalization factors, frequencies, and positions are omitted. type for a stored doublefield: normalization factors, frequencies, and positions are omitted. creates a stored or un-stored doublefield with the provided value and default precisionstep {@link numericutils#precision_step_default} (4). field name 64-bit double value store.yes if the content should also be stored @throws illegalargumentexception if the field name is null. expert: allows you to customize the {@link fieldtype}. field name 64-bit double value customized field type: must have {@link fieldtype#numerictype()} of {@link fieldtype.numerictype#double}. @throws illegalargumentexception if the field name or type is null, or if the field type does not have a double numerictype() javadocs javadocs javadocs javadocs"
org.apache.lucene.document.FloatDocValuesField " field that stores a per-document float value for scoring, sorting or value retrieval. here's an example usage:  document.add(new floatdocvaluesfield(name, 22f));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for 32-bit float docvalues. creates a new docvalues field with the specified 32-bit float value field name 32-bit float value @throws illegalargumentexception if the field name is null"
org.apache.lucene.document.TextField "a field that is indexed and tokenized, without term vectors. for example this would be used on a 'body' field, that contains the bulk of a document's text. indexed, tokenized, not stored. indexed, tokenized, stored. creates a new un-stored textfield with reader value. field name reader value @throws illegalargumentexception if the field name is null @throws nullpointerexception if the reader is null creates a new textfield with string value. field name string value store.yes if the content should also be stored @throws illegalargumentexception if the field name or value is null. creates a new un-stored textfield with tokenstream value. field name tokenstream value @throws illegalargumentexception if the field name is null. @throws nullpointerexception if the tokenstream is null todo: add sugar for term vectors...?"
org.apache.lucene.document.PackedLongDocValuesField " field that stores a per-document long value for scoring, sorting or value retrieval. the values are encoded in the index an in ram (when loaded via {@link atomicreader#docvalues}) using packed ints. here's an example usage:  document.add(new packedlongdocvaluesfield(name, 22l));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for packed long docvalues. creates a new docvalues field with the specified long value field name 64-bit long value @throws illegalargumentexception if the field name is null javadocs"
org.apache.lucene.document.Field "expert: directly create a field for a document. most field's type field's name field's value pre-analyzed tokenstream for indexed fields; this is separate from fieldsdata because you are allowed to have both; eg maybe field has a string value but you customize how it's tokenized field's boost @see #boost() expert: creates a field with no initial value. intended only for custom field subclasses. field name field type @throws illegalargumentexception if either the name or type is null. create field with reader value. field name reader value field type @throws illegalargumentexception if either the name or type is null, or if the field's type is stored(), or if tokenized() is false. @throws nullpointerexception if the reader is null create field with tokenstream value. field name tokenstream value field type @throws illegalargumentexception if either the name or type is null, or if the field's type is stored(), or if tokenized() is false, or if indexed() is false. @throws nullpointerexception if the tokenstream is null create field with binary value. note: the provided byte[] is not copied so be sure not to change it until you're done with this field. field name byte array pointing to binary content (not copied) field type @throws illegalargumentexception if the field name is null, or the field's type is indexed() @throws nullpointerexception if the type is null create field with binary value. note: the provided byte[] is not copied so be sure not to change it until you're done with this field. field name byte array pointing to binary content (not copied) starting position of the byte array valid length of the byte array field type @throws illegalargumentexception if the field name is null, or the field's type is indexed() @throws nullpointerexception if the type is null create field with binary value. note: the provided bytesref is not copied so be sure not to change it until you're done with this field. field name bytesref pointing to binary content (not copied) field type @throws illegalargumentexception if the field name is null, or the field's type is indexed() @throws nullpointerexception if the type is null create field with string value. field name string value field type @throws illegalargumentexception if either the name or value is null, or if the field's type is neither indexed() nor stored(), or if indexed() is false but storetermvectors() is true. @throws nullpointerexception if the type is null the value of the field as a string, or null. if null, the reader value or binary value is used. exactly one of stringvalue(), readervalue(), and getbinaryvalue() must be set. the value of the field as a reader, or null. if null, the string value or binary value is used. exactly one of stringvalue(), readervalue(), and getbinaryvalue() must be set. the tokenstream for this field to be used when indexing, or null. if null, the reader value or string value is analyzed to produce the indexed tokens.  expert: change the value of this field. this can be used during indexing to re-use a single field instance to improve indexing speed by avoiding gc cost of new'ing and reclaiming field instances. typically a single {@link document} instance is re-used as well. this helps most on small documents.   each field instance should only be used once within a single {@link document} instance. see improveindexingspeed for details.  expert: change the value of this field. see {@link #setstringvalue(string)}. expert: change the value of this field. see {@link #setstringvalue(string)}. expert: change the value of this field. see {@link #setstringvalue(string)}. note: the provided bytesref is not copied so be sure not to change it until you're done with this field. expert: change the value of this field. see {@link #setstringvalue(string)}. expert: change the value of this field. see {@link #setstringvalue(string)}. expert: change the value of this field. see {@link #setstringvalue(string)}. expert: change the value of this field. see {@link #setstringvalue(string)}. expert: change the value of this field. see {@link #setstringvalue(string)}. expert: change the value of this field. see {@link #setstringvalue(string)}. expert: sets the token stream to be used for indexing and causes isindexed() and istokenized() to return true. may be combined with stored values from stringvalue() or getbinaryvalue() {@inheritdoc}  the default value is 1.0f (no boost). @see #setboost(float) sets the boost factor on this field. @throws illegalargumentexception if this field is not indexed, or if it omits norms. @see #boost() prints a field for human consumption. returns the {@link fieldtype} for this field. creates a new tokenstream that returns a string as single token. warning: does not initialize the value, you must call {@link #setvalue(string)} afterwards! sets the string value. specifies whether and how a field should be stored. store the original field value in the index. this is useful for short texts like a document's title which should be displayed with the results. the value is stored in its original form, i.e. no analyzer is used before it is stored. do not store the field's value in the index. specifies whether and how a field should be indexed. @deprecated this is here only to ease transition from the pre-4.0 apis. do not index the field value. this field can thus not be searched, but one can still access its contents provided it is {@link field.store stored}. index the tokens produced by running the field's value through an analyzer. this is useful for common text. index the field's value without using an analyzer, so it can be searched. as no analyzer is used the value will be stored as a single term. this is useful for unique ids like product numbers. expert: index the field's value without an analyzer, and also disable the indexing of norms. note that you can also separately enable/disable norms by calling {@link fieldtype#setomitnorms}. no norms means that index-time field and document boosting and field length normalization are disabled. the benefit is less memory usage as norms take up one byte of ram per indexed field for every document in the index, during searching. note that once you index a given field with norms enabled, disabling norms will have no effect. in other words, for this to have the above described effect on a field, all instances of that field must be indexed with not_analyzed_no_norms from the beginning. expert: index the tokens produced by running the field's value through an analyzer, and also separately disable the storing of norms. see {@link #not_analyzed_no_norms} for what norms are and why you may want to disable them. get the best representation of the index given the flags. expert: get the best representation of the index given the flags. specifies whether and how a field should have term vectors. @deprecated this is here only to ease transition from the pre-4.0 apis. do not store term vectors. store the term vectors of each document. a term vector is a list of the document's terms and their number of occurrences in that document. store the term vector + token position information @see #yes store the term vector + token offset information @see #yes store the term vector + token position and offset information @see #yes @see #with_positions @see #with_offsets get the best representation of a termvector given the flags. translates the pre-4.0 enums for specifying how a field should be indexed into the 4.0 {@link fieldtype} approach. @deprecated this is here only to ease transition from the pre-4.0 apis. create a field by specifying its name, value and how it will be saved in the index. term vectors will not be stored in the index. the name of the field the string to process whether value should be stored in the index whether the field should be indexed, and if so, if it should be tokenized before indexing @throws nullpointerexception if name or value is null @throws illegalargumentexception if the field is neither stored nor indexed @deprecated use {@link stringfield}, {@link textfield} instead. create a field by specifying its name, value and how it will be saved in the index. the name of the field the string to process whether value should be stored in the index whether the field should be indexed, and if so, if it should be tokenized before indexing whether term vector should be stored @throws nullpointerexception if name or value is null @throws illegalargumentexception in any of the following situations:  the field is neither stored nor indexed the field is not indexed but termvector is termvector.yes  @deprecated use {@link stringfield}, {@link textfield} instead. create a tokenized and indexed field that is not stored. term vectors will not be stored. the reader is read only when the document is added to the index, i.e. you may not close the reader until {@link indexwriter#adddocument} has been called. the name of the field the reader with the content @throws nullpointerexception if name or reader is null @deprecated use {@link textfield} instead. create a tokenized and indexed field that is not stored, optionally with storing term vectors. the reader is read only when the document is added to the index, i.e. you may not close the reader until {@link indexwriter#adddocument} has been called. the name of the field the reader with the content whether term vector should be stored @throws nullpointerexception if name or reader is null @deprecated use {@link textfield} instead. create a tokenized and indexed field that is not stored. term vectors will not be stored. this is useful for pre-analyzed fields. the tokenstream is read only when the document is added to the index, i.e. you may not close the tokenstream until {@link indexwriter#adddocument} has been called. the name of the field the tokenstream with the content @throws nullpointerexception if name or tokenstream is null @deprecated use {@link textfield} instead create a tokenized and indexed field that is not stored, optionally with storing term vectors. this is useful for pre-analyzed fields. the tokenstream is read only when the document is added to the index, i.e. you may not close the tokenstream until {@link indexwriter#adddocument} has been called. the name of the field the tokenstream with the content whether term vector should be stored @throws nullpointerexception if name or tokenstream is null @deprecated use {@link textfield} instead create a stored field with binary value. optionally the value may be compressed. the name of the field the binary value @deprecated use {@link storedfield} instead. create a stored field with binary value. optionally the value may be compressed. the name of the field the binary value starting offset in value where this field's bytes are number of bytes to use for this field, starting at offset @deprecated use {@link storedfield} instead. javadocs javadocs javadocs todo: allow direct construction of int, long, float, double value too..? lazy init the tokenstream as it is heavy to instantiate (attributes,...) if not needed (stored field loading) initialize value in tokenstream lazy init the tokenstream as it is heavy to instantiate (attributes,...) if not needed (stored field loading) this prevents npe when reading after close!  deprecated transition api below:  if it is not indexed nothing else matters typical, non-expert expert: norms omitted if it is not stored, nothing else matters."
org.apache.lucene.document.ShortDocValuesField " field that stores a per-document short value for scoring, sorting or value retrieval. here's an example usage:  document.add(new shortdocvaluesfield(name, (short) 22));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for 16-bit short docvalues. creates a new docvalues field with the specified 16-bit short value field name 16-bit short value @throws illegalargumentexception if the field name is null"
org.apache.lucene.document.DoubleDocValuesField " field that stores a per-document double value for scoring, sorting or value retrieval. here's an example usage:  document.add(new doubledocvaluesfield(name, 22.0));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for 64-bit double docvalues. creates a new docvalues field with the specified 64-bit double value field name 64-bit double value @throws illegalargumentexception if the field name is null"
org.apache.lucene.document.StraightBytesDocValuesField " field that stores a per-document {@link bytesref} value. the values are stored directly with no sharing, which is a good fit when the fields don't share (many) values, such as a title field. if values may be shared it's better to use {@link derefbytesdocvaluesfield}. here's an example usage:  document.add(new straightbytesdocvaluesfield(name, new bytesref("hello")));   if you also need to store the value, you should add a separate {@link storedfield} instance. @see docvalues type for direct bytes docvalues: all with the same length type for direct bytes docvalues: can have variable lengths create a new variable-length direct docvalues field.  this calls {@link straightbytesdocvaluesfield#straightbytesdocvaluesfield(string, bytesref, boolean) straightbytesdocvaluesfield(name, bytes, false}, meaning by default it allows for values of different lengths. if your values are all the same length, use that constructor instead. field name binary content @throws illegalargumentexception if the field name is null create a new fixed or variable length direct docvalues field.  field name binary content true if all values have the same length. @throws illegalargumentexception if the field name is null todo: ideally indexer figures out var vs fixed on its own!?"
org.apache.lucene.codecs.FilterCodec "a codec that forwards all its method calls to another codec.  extend this class when you need to reuse the functionality of an existing codec. for example, if you want to build a codec that redefines lucene41's {@link livedocsformat}:  public final class customcodec extends filtercodec { public customcodec() { super("customcodec", new lucene41codec()); } public livedocsformat livedocsformat() { return new customlivedocsformat(); } }  please note: don't call {@link codec#forname} from the no-arg constructor of your own codec. when the spi framework loads your own codec as spi component, spi has not yet fully initialized! if you want to extend another codec, instantiate it directly by calling its constructor. @lucene.experimental the codec to filter. sole constructor. when subclassing this codec, create a no-arg ctor and pass the delegate codec and a unique name to this ctor."
org.apache.lucene.codecs.PostingsBaseFormat "provides a {@link postingsreaderbase} and {@link postingswriterbase}. @lucene.experimental unique name that's used to retrieve this codec when reading the index sole constructor. creates the {@link postingsreaderbase} for this format. creates the {@link postingswriterbase} for this format. todo: find a better name; this defines the api that the terms dict impls use to talk to a postings impl. termsdict + postingsreader/writerbase == postingsconsumer/producer can we clean this up and do this some other way? refactor some of these classes and use covariant return?"
org.apache.lucene.codecs.SegmentInfoWriter "specifies an api for classes that can write out {@link segmentinfo} data. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) write {@link segmentinfo} data. @throws ioexception if an i/o error occurs"
org.apache.lucene.codecs.MappingMultiDocsEnum "exposes flex api, merged from flex api of sub-segments, remapping docids (this is used for segment merging). @lucene.experimental sole constructor. sets the {@link mergestate}, which is used to re-map document ids. how many sub-readers we are merging. @see #getsubs returns sub-readers we are merging. compact deletions"
org.apache.lucene.codecs.BlockTreeTermsReader "a block-based terms index and dictionary that assigns terms to variable length blocks according to how they share prefixes. the terms index is a prefix trie whose leaves are term blocks. the advantage of this approach is that seekexact is often able to determine a term cannot exist without doing any io, and intersection with automata is very fast. note that this terms dictionary has it's own fixed terms index (ie, it does not support a pluggable terms index implementation). note: this terms dictionary does not support index divisor when opening an indexreader. instead, you can change the min/maxitemsperblock during indexing. the data structure used by this implementation is very similar to a burst trie (http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499), but with added logic to break up too-large blocks of all terms sharing a given prefix into smaller ones. use {@link org.apache.lucene.index.checkindex} with the -verbose option to see summary statistics on the blocks in the dictionary. see {@link blocktreetermswriter}. @lucene.experimental file offset where the directory starts in the terms file. file offset where the directory starts in the index file. sole constructor. reads terms file header. reads index file header. seek {@code input} to the directory offset. blocktree statistics for a single field returned by {@link fieldreader#computestats()}. how many nodes in the index fst. how many arcs in the index fst. byte size of the index. total number of terms in the field. total number of bytes (sum of term lengths) across all terms in the field. the number of normal (non-floor) blocks in the terms file. the number of floor blocks (meta-blocks larger than the allowed {@code maxitemsperblock}) in the terms file. the number of sub-blocks within the floor blocks. the number of "internal" blocks (that have both terms and sub-blocks). the number of "leaf" blocks (blocks that have only terms). the number of "internal" blocks that do not contain terms (have only sub-blocks). total number of blocks. number of blocks at each prefix depth. total number of bytes used to store term suffixes. total number of bytes used to store term stats (not including what the {@link postingsbaseformat} stores. total bytes stored by the {@link postingsbaseformat}, plus the other few vints stored in the frame. segment name. field name. blocktree's implementation of {@link terms}. if (false) { final string dotfilename = segment + "_" + fieldinfo.name + ".dot"; writer w = new outputstreamwriter(new fileoutputstream(dotfilename)); util.todot(index, w, false, false); system.out.println("fst index: saved to " + dotfilename); w.close(); } for debugging -- used by checkindex too runs next() through the entire terms dict, computing aggregate statistics. decodes only the term bytes of the next term. if caller then asks for metadata, ie docfreq, totaltermfreq or pulls a d/&penum, we then (lazily) decode all metadata up to the current term. does initial decode of next block of terms; this doesn't actually decode the docfreq, totaltermfreq, postings details (frq/prx offset, etc.) metadata; it just loads them as byte[] blobs which are then decoded on-demand if the metadata is ever requested for any term in this block. this enables terms-only intensive consumes (eg certain mtqs, respelling) to not pay the price of decoding metadata they won't use. if (debug) { if (arc == null) { system.out.println(" loadblock (next) fp=" + fp + " entcount=" + entcount + " prefixlen=" + prefix + " islastinfloor=" + islastinfloor + " leaf?=" + isleafblock); } else { system.out.println(" loadblock (seek) fp=" + fp + " entcount=" + entcount + " prefixlen=" + prefix + " hasterms?=" + hasterms + " isfloor?=" + isfloor + " islastinfloor=" + islastinfloor + " leaf?=" + isleafblock); } } //system.out.println("rewind"); // keeps the block loaded, but rewinds its state: if (nextent > 0 || fp != fporig) { if (debug) { system.out.println(" rewind frame ord=" + ord + " fporig=" + fporig + " fp=" + fp + " hasterms?=" + hasterms + " isfloor?=" + isfloor + " nextent=" + nextent + " prefixlen=" + prefix); } if (fp != fporig) { fp = fporig; nextent = -1; } else { nextent = 0; } hasterms = hastermsorig; if (isfloor) { floordatareader.rewind(); numfollowfloorblocks = floordatareader.readvint(); nextfloorlabel = floordatareader.readbyte() & 0xff; } assert suffixbytes != null; suffixesreader.rewind(); assert statbytes != null; statsreader.rewind(); metadataupto = 0; state.termblockord = 0; // todo: skip this if !hasterms? then postings // impl wouldn't have to write useless 0 byte postingsreader.resettermsblock(fieldinfo, state); lastsubfp = -1; } else if (debug) { system.out.println(" skip rewind fp=" + fp + " fporig=" + fporig + " nextent=" + nextent + " ord=" + ord); } open input to the main terms dict file (_x.tib) private static final boolean debug = blocktreetermswriter.debug; reads the terms dict entries, to gather state to produce docsenum on demand have postingsreader init itself read per-field details #docs with field must be = #docs with field #positions must be >= #postings this.close() will close in: for debugging private static string tohex(int v) { return "0x" + integer.tohexstring(v); } clear so refs to terms index is gcable even if app hangs onto us: for debugging if bytesref isn't actually utf8, or it's eg a prefix of utf8 that ends mid-unicode-char, we fallback to hex: private boolean debug; debug = blocktreetermsreader.debug && fieldinfo.name.equals("id"); if (debug) { system.out.println("bttr: seg=" + segment + " field=" + fieldinfo.name + " rootblockcode=" + rootcode + " divisor=" + indexdivisor); } system.out.println("start=" + indexstartfp + " field=" + fieldinfo.name); todo: maybe push this into terms? note: cannot seek! todo: can we share this with the frame in ste? state in automaton length of prefix shared by all terms in this block number of entries (term or sub-block) in this block which term we will next read true if this block is either not a floor block, or, it's the last sub-block of a floor block true if all entries are terms cumulative output so far if (debug) system.out.println(" loadnextfoorblock trans=" + transitions[transitionindex]); if (debug) system.out.println(" skip floor block2! nextfloorlabel=" + (char) nextfloorlabel + " vs target=" + (char) transitions[transitionindex].getmin() + " newfp=" + fp + " numfollowfloorblocks=" + numfollowfloorblocks); if (debug) system.out.println(" nextfloorlabel=" + (char) nextfloorlabel); if (debug) system.out.println(" load fp=" + fp + " fporig=" + fporig + " frameindexdata=" + frameindexdata + " trans=" + (transitions.length != 0 ? transitions[0] : "n/a" + " state=" + state)); floor frame skip first long -- has redundant fp, hasterms flag, isfloor flag if (debug) system.out.println(" numfollowfloorblocks=" + numfollowfloorblocks + " nextfloorlabel=" + nextfloorlabel); if current state is accept, we must process first block in case it has empty suffix: maybe skip floor blocks: if (debug) system.out.println(" skip floor block! nextfloorlabel=" + (char) nextfloorlabel + " vs target=" + (char) transitions[0].getmin() + " newfp=" + fp + " numfollowfloorblocks=" + numfollowfloorblocks); term suffixes: if (debug) system.out.println(" entcount=" + entcount + " lastinfloor?=" + islastinfloor + " leafblock?=" + isleafblock + " numsuffixbytes=" + numbytes); stats sub-blocks of a single floor block are always written one after another -- tail recurse: todo: maybe add scantolabel; should give perf boost decodes next entry; returns true if it's a sub-block if (debug) system.out.println(" frame.next ord=" + ord + " nextent=" + nextent + " entcount=" + entcount); if (debug) system.out.println(" frame.next ord=" + ord + " nextent=" + nextent + " entcount=" + entcount); a normal term a sub-block; make sub-fp absolute: lazily catch up on metadata decode: we must set/incr state.termcount because postings impl can look at this todo: better api would be "jump straight to term=n"??? todo: we could make "tiers" of metadata, ie, decode docfreq/totaltf but don't decode postings metadata; this way caller could get docfreq/totaltf w/o paying decode cost for postings todo: if docfreq were bulk decoded we could just skipn here: if (debug) system.out.println(" df=" + state.docfreq); if (debug) system.out.println(" tottf=" + state.totaltermfreq); todo: in some cases we can filter by length? eg regexp foobar must be at least length 6 bytes if (debug) { system.out.println("\nintenum.init seg=" + segment + " commonsuffix=" + brtostring(compiled.commonsuffixref)); } todo: if the automaton is "smallish" we really should use the terms index to seek at least to the initial term and likely to subsequent terms (or, maybe just fallback to ate for such cases). else the seek cost of loading the frames will be too costly. empty string prefix must have an output in the index! special pushframe since it's the first one: for assert: only for assert: if (debug) system.out.println(" pushframe state=" + state + " prefix=" + f.prefix); walk the arc through the index -- we only "bother" with this so we can get the floor data from the index and skip floor blocks when possible: todo: we could be more efficient for the next() case by using current arc as starting point, passed to findtargetarc if (debug) system.out.println("btir.docfreq"); if (debug) system.out.println(" return " + currentframe.termstate.docfreq); positions were not indexed: note: specialized to only doing the first-time seek, but we could generalize it to allow arbitrary seekexact/ceil. note that this is a seekfloor! if (debug) system.out.println("seek to startterm=" + target.utf8tostring()); if (debug) system.out.println(" cycle ent=" + currentframe.nextent + " (of " + currentframe.entcount + ") prefix=" + currentframe.prefix + " suffix=" + currentframe.suffix + " isblock=" + issubblock + " firstlabel=" + (currentframe.suffix == 0 ? "" : (currentframe.suffixbytes[currentframe.startbytepos])&0xff)); recurse if (debug) system.out.println(" recurse!"); if (debug) system.out.println(" load floorblock"); if (debug) system.out.println(" return term=" + brtostring(term)); if (debug) system.out.println(" return term=" + brtostring(term)); fallback to prior entry: the semantics of this method is that the first call to next() will return the term after the requested term if the last entry was a block we don't need to bother recursing and pushing to the last term under it because the first next() will simply skip the frame anyway if (debug) { system.out.println("\nintenum.next seg=" + segment); system.out.println(" frame ord=" + currentframe.ord + " prefix=" + brtostring(new bytesref(term.bytes, term.offset, currentframe.prefix)) + " state=" + currentframe.state + " lastinfloor?=" + currentframe.islastinfloor + " fp=" + currentframe.fp + " trans=" + (currentframe.transitions.length == 0 ? "n/a" : currentframe.transitions[currentframe.transitionindex]) + " outputprefix=" + currentframe.outputprefix); } pop finished frames if (debug) system.out.println(" next-floor-block"); if (debug) system.out.println("\n frame ord=" + currentframe.ord + " prefix=" + brtostring(new bytesref(term.bytes, term.offset, currentframe.prefix)) + " state=" + currentframe.state + " lastinfloor?=" + currentframe.islastinfloor + " fp=" + currentframe.fp + " trans=" + (currentframe.transitions.length == 0 ? "n/a" : currentframe.transitions[currentframe.transitionindex]) + " outputprefix=" + currentframe.outputprefix); if (debug) system.out.println(" pop frame"); if (debug) system.out.println("\n frame ord=" + currentframe.ord + " prefix=" + brtostring(new bytesref(term.bytes, term.offset, currentframe.prefix)) + " state=" + currentframe.state + " lastinfloor?=" + currentframe.islastinfloor + " fp=" + currentframe.fp + " trans=" + (currentframe.transitions.length == 0 ? "n/a" : currentframe.transitions[currentframe.transitionindex]) + " outputprefix=" + currentframe.outputprefix); if (debug) { final bytesref suffixref = new bytesref(); suffixref.bytes = currentframe.suffixbytes; suffixref.offset = currentframe.startbytepos; suffixref.length = currentframe.suffix; system.out.println(" " + (issubblock ? "sub-block" : "term") + " " + currentframe.nextent + " (of " + currentframe.entcount + ") suffix=" + brtostring(suffixref)); } stop processing this frame -- no further matches are possible because we've moved beyond what the max transition will allow if (debug) system.out.println(" break: trans=" + (currentframe.transitions.length == 0 ? "n/a" : currentframe.transitions[currentframe.transitionindex])); sneaky! forces a pop above if (debug) system.out.println(" next trans=" + currentframe.transitions[currentframe.transitionindex]); first test the common suffix, if set: no match if (debug) { system.out.println(" skip: common suffix length"); } a prefix of the common suffix overlaps with the suffix of the block prefix so we first test whether the prefix part matches: if (debug) { system.out.println(" skip: common suffix mismatch (in prefix)"); } test overlapping suffix part: if (debug) { system.out.println(" skip: common suffix mismatch"); } todo: maybe we should do the same linear test that automatontermsenum does, so that if we reach a part of the automaton where . is "temporarily" accepted, we just blindly .next() until the limit see if the term prefix matches the automaton: no match system.out.println(" no s=" + state); system.out.println(" c s=" + state); match! recurse: if (debug) system.out.println(" sub-block match to state=" + state + "; recurse fp=" + currentframe.lastsubfp); if (debug) system.out.println("\n frame ord=" + currentframe.ord + " prefix=" + brtostring(new bytesref(term.bytes, term.offset, currentframe.prefix)) + " state=" + currentframe.state + " lastinfloor?=" + currentframe.islastinfloor + " fp=" + currentframe.fp + " trans=" + (currentframe.transitions.length == 0 ? "n/a" : currentframe.transitions[currentframe.transitionindex]) + " outputprefix=" + currentframe.outputprefix); if (debug) system.out.println(" term match to state=" + state + "; return term=" + brtostring(term)); system.out.println(" no s=" + state); system.out.println(" copyterm cur.prefix=" + currentframe.prefix + " cur.suffix=" + currentframe.suffix + " first=" + (char) currentframe.suffixbytes[currentframe.startbytepos]); iterates through terms in this field what prefix of the current term was present in the index: assert only: if (debug) system.out.println("bttr.init seg=" + segment); used to hold seek by termstate, or cached seek init w/ root block; don't use index since it may not (and need not) have been loaded empty string prefix must have an output in the index! currentframe = pushframe(arc, rootcode, 0); currentframe.loadblock(); if (debug) { system.out.println("init frame state " + currentframe.ord); printseekstate(); } system.out.println(); computeblockstats().print(system.out); not private to avoid synthetic access$nnn methods empty string prefix must have an output in the index! empty string prefix must have an output in the index! pop finished blocks if (debug) { system.out.println(" reset validindexprefix=" + validindexprefix); } push to new block: this is a "next" frame -- even if it's floor'd we must pretend it isn't so we don't try to scan to the right floor frame: currentframe.hasterms = true; put root frame back: empty string prefix must have an output in the index! pushes a frame we seek'd to pushes next'd frame or seek'd frame; we later lazy-load the frame only when needed if (debug) system.out.println(" push reused frame ord=" + f.ord + " fp=" + f.fp + " isfloor?=" + f.isfloor + " hasterms=" + f.hasterms + " pref=" + term + " nextent=" + f.nextent + " targetbeforecurrentlength=" + targetbeforecurrentlength + " term.length=" + term.length + " vs prefix=" + f.prefix); if (debug) { system.out.println(" skip rewind!"); } if (debug) { final int sav = term.length; term.length = length; system.out.println(" push new frame ord=" + f.ord + " fp=" + f.fp + " hasterms=" + f.hasterms + " isfloor=" + f.isfloor + " pref=" + brtostring(term)); term.length = sav; } asserts only asserts only if (debug) { system.out.println("\nbttr.seekexact seg=" + segment + " target=" + fieldinfo.name + ":" + brtostring(target) + " current=" + brtostring(term) + " (exists?=" + termexists + ") validindexprefix=" + validindexprefix); printseekstate(); } we are already seek'd; find the common prefix of new seek term vs current term and re-use the corresponding seek state. for example, if app first seeks to foobar, then seeks to foobaz, we can re-use the seek state for the first 5 bytes. if (debug) { system.out.println(" re-use current seek state validindexprefix=" + validindexprefix); } todo: reverse vlong byte order for better fst prefix output sharing first compare up to valid seek frames: if (debug) { system.out.println(" cycle targetupto=" + targetupto + " (vs limit=" + targetlimit + ") cmp=" + cmp + " (targetlabel=" + (char) (target.bytes[target.offset + targetupto]) + " vs termlabel=" + (char) (term.bytes[targetupto]) + ")" + " arc.output=" + arc.output + " output=" + output); } if (arc.label != (target.bytes[target.offset + targetupto] & 0xff)) { system.out.println("fail: arc.label=" + (char) arc.label + " targetlabel=" + (char) (target.bytes[target.offset + targetupto] & 0xff)); } second compare the rest of the term, but don't save arc/output/frame; we only do this to find out if the target term is before, equal or after the current term if (debug) { system.out.println(" cycle2 targetupto=" + targetupto + " (vs limit=" + targetlimit + ") cmp=" + cmp + " (targetlabel=" + (char) (target.bytes[target.offset + targetupto]) + " vs termlabel=" + (char) (term.bytes[targetupto]) + ")"); } common case: target term is after current term, ie, app is seeking multiple terms in sorted order if (debug) { system.out.println(" target is after current (shares prefixlen=" + targetupto + "); frame.ord=" + lastframe.ord); } uncommon case: target term is before current term; this means we can keep the currentframe but we must rewind it (so we scan from the start) if (debug) { system.out.println(" target is before current (shares prefixlen=" + targetupto + "); rewind frame ord=" + lastframe.ord); } target is exactly the same as current term if (debug) { system.out.println(" target is same as current; return true"); } if (debug) { system.out.println(" target is same as current but term doesn't exist"); } validindexprefix = currentframe.depth; term.length = target.length; return termexists; empty string prefix must have an output (block) in the index! if (debug) { system.out.println(" no seek state; push root frame"); } term.length = 0; if (debug) { system.out.println(" start index loop targetupto=" + targetupto + " output=" + output + " currentframe.ord=" + currentframe.ord + " targetbeforecurrentlength=" + targetbeforecurrentlength); } index is exhausted if (debug) { system.out.println(" index: index exhausted label=" + ((char) targetlabel) + " " + tohex(targetlabel)); } validindexprefix = targetupto; if (debug) { system.out.println(" fast not_found term=" + brtostring(term)); } if (debug) { system.out.println(" return found term=" + term.utf8tostring() + " " + term); } if (debug) { system.out.println(" got " + result + "; return not_found term=" + brtostring(term)); } follow this arc aggregate output as we go: if (debug) { system.out.println(" index: follow label=" + tohex(target.bytes[target.offset + targetupto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextfinaloutput); } if (debug) system.out.println(" arc is final!"); if (debug) system.out.println(" curframe.ord=" + currentframe.ord + " hasterms=" + currentframe.hasterms); validindexprefix = targetupto; target term is entirely contained in the index: if (debug) { system.out.println(" fast not_found term=" + brtostring(term)); } if (debug) { system.out.println(" return found term=" + term.utf8tostring() + " " + term); } if (debug) { system.out.println(" got result " + result + "; return not_found term=" + term.utf8tostring()); } if (debug) { system.out.println("\nbttr.seekceil seg=" + segment + " target=" + fieldinfo.name + ":" + target.utf8tostring() + " " + target + " current=" + brtostring(term) + " (exists?=" + termexists + ") validindexprefix= " + validindexprefix); printseekstate(); } we are already seek'd; find the common prefix of new seek term vs current term and re-use the corresponding seek state. for example, if app first seeks to foobar, then seeks to foobaz, we can re-use the seek state for the first 5 bytes. if (debug) { system.out.println(" re-use current seek state validindexprefix=" + validindexprefix); } tood: we should write our vlong backwards (msb first) to get better sharing from the fst first compare up to valid seek frames: if (debug) { system.out.println(" cycle targetupto=" + targetupto + " (vs limit=" + targetlimit + ") cmp=" + cmp + " (targetlabel=" + (char) (target.bytes[target.offset + targetupto]) + " vs termlabel=" + (char) (term.bytes[targetupto]) + ")" + " arc.output=" + arc.output + " output=" + output); } tood: we could save the outputs in local byte[][] instead of making new objs ever seek; but, often the fst doesn't have any shared bytes (but this could change if we reverse vlong byte order) second compare the rest of the term, but don't save arc/output/frame: if (debug) { system.out.println(" cycle2 targetupto=" + targetupto + " (vs limit=" + targetlimit + ") cmp=" + cmp + " (targetlabel=" + (char) (target.bytes[target.offset + targetupto]) + " vs termlabel=" + (char) (term.bytes[targetupto]) + ")"); } common case: target term is after current term, ie, app is seeking multiple terms in sorted order if (debug) { system.out.println(" target is after current (shares prefixlen=" + targetupto + "); clear frame.scanned ord=" + lastframe.ord); } uncommon case: target term is before current term; this means we can keep the currentframe but we must rewind it (so we scan from the start) if (debug) { system.out.println(" target is before current (shares prefixlen=" + targetupto + "); rewind frame ord=" + lastframe.ord); } target is exactly the same as current term if (debug) { system.out.println(" target is same as current; return found"); } if (debug) { system.out.println(" target is same as current but term doesn't exist"); } empty string prefix must have an output (block) in the index! if (debug) { system.out.println(" no seek state; push root frame"); } term.length = 0; if (debug) { system.out.println(" start index loop targetupto=" + targetupto + " output=" + output + " currentframe.ord+1=" + currentframe.ord + " targetbeforecurrentlength=" + targetbeforecurrentlength); } index is exhausted if (debug) { system.out.println(" index: index exhausted label=" + ((char) targetlabel) + " " + tohex(targetlabel)); } validindexprefix = targetupto; if (debug) { system.out.println(" return not_found term=" + brtostring(term) + " " + term); } if (debug) { system.out.println(" return end"); } if (debug) { system.out.println(" return " + result + " term=" + brtostring(term) + " " + term); } follow this arc aggregate output as we go: if (debug) { system.out.println(" index: follow label=" + tohex(target.bytes[target.offset + targetupto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextfinaloutput); } if (debug) system.out.println(" arc is final!"); if (debug) system.out.println(" curframe.ord=" + currentframe.ord + " hasterms=" + currentframe.hasterms); validindexprefix = targetupto; if (debug) { system.out.println(" return not_found term=" + term.utf8tostring() + " " + term); } if (debug) { system.out.println(" return end"); } fresh termsenum; seek to first term: empty string prefix must have an output in the index! if (debug) { system.out.println("\nbttr.next seg=" + segment + " term=" + brtostring(term) + " termexists?=" + termexists + " field=" + fieldinfo.name + " termblockord=" + currentframe.state.termblockord + " validindexprefix=" + validindexprefix); printseekstate(); } if seek was previously called and the term was cached, or seek(termstate) was called, usually caller is just going to pull a d/&penum or get docfreq, etc. but, if they then call next(), this method catches up all internal state so next() works properly: if (debug) system.out.println(" re-seek to pending term=" + term.utf8tostring() + " " + term); pop finished blocks if (debug) system.out.println(" pop frame"); if (debug) system.out.println(" return null"); we popped into a frame that's not loaded yet or not scan'd to the right entry note that the seek state (last seek) has been invalidated beyond this depth if (debug) { system.out.println(" reset validindexprefix=" + validindexprefix); } push to new block: if (debug) system.out.println(" push frame"); this is a "next" frame -- even if it's floor'd we must pretend it isn't so we don't try to scan to the right floor frame: currentframe.hasterms = true; if (debug) system.out.println(" return term=" + term.utf8tostring() + " " + term + " currentframe.ord=" + currentframe.ord); if (debug) system.out.println("btr.docfreq"); if (debug) system.out.println(" return " + currentframe.state.docfreq); if (debug) { system.out.println("bttr.docs seg=" + segment); } if (debug) { system.out.println(" state=" + currentframe.state); } positions were not indexed: if (debug) { system.out.println("bttr.seekexact termstate seg=" + segment + " target=" + target.utf8tostring() + " " + target + " state=" + otherstate); } if (debug) { system.out.println(" skip seek: already on target state=" + currentframe.state); } if (debug) system.out.println("bttr.termstate seg=" + segment + " state=" + ts); not static -- references term, postingsreader, fieldinfo, in our index in stack[]: file pointer where this block was loaded from length of prefix shared by all terms in this block number of entries (term or sub-block) in this block which term we will next read, or -1 if the block isn't loaded yet true if this block is either not a floor block, or, it's the last sub-block of a floor block true if all entries are terms next term to decode metadata; we decode metadata lazily so that scanning to find the matching term is fast and only if you find a match and app wants the stats or docs/positions enums, will we decode the metadata if (debug) { system.out.println(" setfloordata fporig=" + fporig + " bytes=" + new bytesref(source.bytes, source.offset + in.getposition(), numbytes) + " numfollowfloorblocks=" + numfollowfloorblocks + " nextfloorlabel=" + tohex(nextfloorlabel)); } if (debug) { system.out.println(" loadnextfloorblock fp=" + fp + " fpend=" + fpend); } clone the indexinput lazily, so that consumers that just pull a termsenum to seekexact(termstate) don't pay this cost: already loaded system.out.println("blc=" + blockloadcount); todo: if suffixes were stored in random-access array structure, then we could do binary search instead of linear scan to find target term; eg we could have simple array of offsets term suffixes: stats todo: we could skip this if !hasterms; but that's rare so won't help much sub-blocks of a single floor block are always written one after another -- tail recurse: if (debug) { system.out.println(" fpend=" + fpend); } force reload: system.out.println("rewind"); keeps the block loaded, but rewinds its state: todo: skip this if !hasterms? then postings impl wouldn't have to write useless 0 byte decodes next entry; returns true if it's a sub-block if (debug) system.out.println(" frame.next ord=" + ord + " nextent=" + nextent + " entcount=" + entcount); a normal term if (debug) system.out.println(" frame.next ord=" + ord + " nextent=" + nextent + " entcount=" + entcount); a normal term a sub-block; make sub-fp absolute: if (debug) { system.out.println(" lastsubfp=" + lastsubfp); } todo: make this array'd so we can do bin search? likely not worth it? need to measure how many floor blocks we "typically" get if (debug) { system.out.println(" scantofloorframe skip: isfloor=" + isfloor + " target.length=" + target.length + " vs prefix=" + prefix); } if (debug) { system.out.println(" scantofloorframe fporig=" + fporig + " targetlabel=" + tohex(targetlabel) + " vs nextfloorlabel=" + tohex(nextfloorlabel) + " numfollowfloorblocks=" + numfollowfloorblocks); } if (debug) { system.out.println(" already on correct block"); } if (debug) { system.out.println(" label=" + tohex(nextfloorlabel) + " fp=" + newfp + " hasterms?=" + hasterms + " numfollowfloor=" + numfollowfloorblocks); } if (debug) { system.out.println(" stop! last block nextfloorlabel=" + tohex(nextfloorlabel)); } if (debug) { system.out.println(" stop! nextfloorlabel=" + tohex(nextfloorlabel)); } force re-load of the block: if (debug) { system.out.println(" force switch to fp=" + newfp + " oldfp=" + fp); } if (debug) { system.out.println(" stay on same fp=" + newfp); } if (debug) system.out.println("\nbttr.decodemetadata seg=" + segment + " mdupto=" + metadataupto + " vs termblockord=" + state.termblockord); lazily catch up on metadata decode: we must set/incr state.termcount because postings impl can look at this todo: better api would be "jump straight to term=n"??? todo: we could make "tiers" of metadata, ie, decode docfreq/totaltf but don't decode postings metadata; this way caller could get docfreq/totaltf w/o paying decode cost for postings todo: if docfreq were bulk decoded we could just skipn here: if (debug) system.out.println(" df=" + state.docfreq); if (debug) system.out.println(" tottf=" + state.totaltermfreq); used only by assert scans to sub-block that has this target fp; only called by next(); note: does not set startbytepos/suffix as a side effect if (debug) system.out.println(" scantosubblock fp=" + fp + " subfp=" + subfp + " entcount=" + entcount + " lastsubfp=" + lastsubfp); assert nextent == 0; if (debug) system.out.println(" already positioned"); if (debug) system.out.println(" targetsubcode=" + targetsubcode); if (debug) system.out.println(" " + nextent + " (of " + entcount + ") ent issubblock=" + ((code&1)==1)); if (debug) system.out.println(" subcode=" + subcode); if (debug) system.out.println(" match!"); note: sets startbytepos/suffix as a side effect target's prefix matches this block's prefix; we scan the entries check if the suffix matches. if (debug) system.out.println(" scantotermleaf: block fp=" + fp + " prefix=" + prefix + " nextent=" + nextent + " (of " + entcount + ") target=" + brtostring(target) + " term=" + brtostring(term)); loop over each entry (term or sub-block) in this block: nextterm: while(nextent < entcount) { if (debug) { bytesref suffixbytesref = new bytesref(); suffixbytesref.bytes = suffixbytes; suffixbytesref.offset = suffixesreader.getposition(); suffixbytesref.length = suffix; system.out.println(" cycle: term " + (nextent-1) + " (of " + entcount + ") suffix=" + brtostring(suffixbytesref)); } loop over bytes in the suffix, comparing to the target current entry is still before the target; keep scanning we are done scanning this block done! current entry is after target -- return not_found: we are on a sub-block, and caller wants us to position to the next term after the target, so we must recurse into the sub-frame(s): if (debug) system.out.println(" not found"); exact match! this cannot be a sub-block because we would have followed the index to this sub-block from the start: if (debug) system.out.println(" found!"); it is possible (and ok) that terms index pointed us at this block, but, we scanned the entire block and did not find the term to position to. this happens when the target is after the last term in the block (but, before the next term in the index). eg target could be foozzz, and terms index pointed us to the foo block, but the last term in this block was fooz (and, eg, first term in the next block will bee fop). if (debug) system.out.println(" block end"); todo: not consistent that in the not-exact case we don't next() into the next frame here target's prefix matches this block's prefix; we scan the entries check if the suffix matches. if (debug) system.out.println(" scantotermnonleaf: block fp=" + fp + " prefix=" + prefix + " nextent=" + nextent + " (of " + entcount + ") target=" + brtostring(target) + " term=" + brtostring(term)); loop over each entry (term or sub-block) in this block: nextterm: while(nextent < entcount) { if (debug) { bytesref suffixbytesref = new bytesref(); suffixbytesref.bytes = suffixbytes; suffixbytesref.offset = suffixesreader.getposition(); suffixbytesref.length = suffix; system.out.println(" cycle: " + ((code&1)==1 ? "sub-block" : "term") + " " + (nextent-1) + " (of " + entcount + ") suffix=" + brtostring(suffixbytesref)); } loop over bytes in the suffix, comparing to the target current entry is still before the target; keep scanning termexists = true; we are done scanning this block done! current entry is after target -- return not_found: we are on a sub-block, and caller wants us to position to the next term after the target, so we must recurse into the sub-frame(s): if (debug) system.out.println(" not found"); exact match! this cannot be a sub-block because we would have followed the index to this sub-block from the start: if (debug) system.out.println(" found!"); it is possible (and ok) that terms index pointed us at this block, but, we scanned the entire block and did not find the term to position to. this happens when the target is after the last term in the block (but, before the next term in the index). eg target could be foozzz, and terms index pointed us to the foo block, but the last term in this block was fooz (and, eg, first term in the next block will bee fop). if (debug) system.out.println(" block end"); todo: not consistent that in the not-exact case we don't next() into the next frame here"
org.apache.lucene.codecs.TermStats "holder for per-term statistics. @see termsenum#docfreq @see termsenum#totaltermfreq how many documents have at least one occurrence of this term. total number of times this term occurs across all documents in the field. sole constructor. javadocs"
org.apache.lucene.codecs.compressing.Compressor "a data compressor. sole constructor, typically called from sub-classes. compress bytes into out. it it the responsibility of the compressor to add all necessary information so that a {@link decompressor} will know when to stop decompressing bytes from the stream."
org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexWriter "number of chunks to serialize at once means unset the trick here is that we only store the difference from the average start pointer or doc base, this helps save bits per value. and in order to prevent a few chunks that would be far from the average to raise the number of bits per value for all of them, we only encode blocks of 1024 chunks at once see lucene-4512 doc bases docbase start pointers end marker"
org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat "a {@link storedfieldsformat} that is very similar to {@link lucene40storedfieldsformat} but compresses documents in chunks in order to improve the compression ratio.  for a chunk size of chunksize bytes, this {@link storedfieldsformat} does not support documents larger than (231 - chunksize) bytes. in case this is a problem, you should use another format, such as {@link lucene40storedfieldsformat}.  for optimal performance, you should use a {@link mergepolicy} that returns segments that have the biggest byte size first. @lucene.experimental create a new {@link compressingstoredfieldsformat} with an empty segment suffix. @see compressingstoredfieldsformat#compressingstoredfieldsformat(string, string, compressionmode, int) create a new {@link compressingstoredfieldsformat}.  formatname is the name of the format. this name will be used in the file formats to perform {@link codecutil#checkheader(org.apache.lucene.store.datainput, string, int, int) codec header checks}.  segmentsuffix is the segment suffix. this suffix is added to the result file name only if it's not the empty string.  the compressionmode parameter allows you to choose between compression algorithms that have various compression and decompression speeds so that you can pick the one that best fits your indexing and searching throughput. you should never instantiate two {@link compressingstoredfieldsformat}s that have the same name but different {@link compressionmode}s.  chunksize is the minimum byte size of a chunk of documents. a value of 1 can make sense if there is redundancy across fields. in that case, both performance and compression ratio should be better than with {@link lucene40storedfieldsformat} with compressed fields.  higher values of chunksize should improve the compression ratio but will require more memory at indexing time and might make document loading a little slower (depending on the size of your os cache compared to the size of your index). the name of the {@link storedfieldsformat} the {@link compressionmode} to use the minimum number of bytes of a single chunk of stored documents @see compressionmode"
org.apache.lucene.codecs.compressing.Decompressor "a decompressor. sole constructor, typically called from sub-classes. decompress bytes that were stored between offsets offset and offset+length in the original stream from the compressed stream in to bytes. after returning, the length of bytes (bytes.length) must be equal to length. implementations of this method are free to resize bytes depending on their needs. the input that stores the compressed stream the length of the original data (before compression) bytes before this offset do not need to be decompressed bytes after offset+length do not need to be decompressed a {@link bytesref} where to store the decompressed data"
org.apache.lucene.codecs.compressing.LZ4 "lz4 compression and decompression routines. http://code.google.com/p/lz4/ http://fastcompression.blogspot.fr/p/lz4.html decompress at least decompressedlen bytes into dest[doff:]. please note that dest must be large enough to be able to hold all decompressed data (meaning that you need to know the total decompressed length). compress bytes[off:off+len] into out using at most 16kb of memory. compress bytes[off:off+len] into out. compared to {@link lz4#compress(byte[], int, int, dataoutput)}, this method is slower, uses more memory (~ 256kb), but should provide better compression ratios (especially on large inputs) because it chooses the best match among up to 256 candidates and then performs trade-offs to fix overlapping matches. minimum length of a match maximum distance of a reference the last 5 bytes must be encoded as literals log size of the dictionary for compresshc match length that doesn't require an additional byte literals matchs copying a multiple of 8 bytes can make decompression from 5% to 10% faster overlap -> naive incremental copy no overlap -> arraycopy encode literal length encode literals encode token encode match dec encode match len find a match compute match length last literals saved, in case we would skip too much no better match empirical first match too small : removed no better match -> 2 sequences to encode encode seq 1 encode seq 2 not enough space for match 2 : remove it // can write seq1 immediately ==> seq2 is removed, so seq3 becomes seq1 ok, now we have 3 ascending matches; let's write at least the first one"
org.apache.lucene.codecs.compressing.GrowableByteArrayDataOutput "a {@link dataoutput} that can be used to build a byte[]."
org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter "{@link storedfieldswriter} impl for {@link compressingstoredfieldsformat}. @lucene.experimental sole constructor. number of stored fields end offsets in buffereddocs doc id at the beginning of the chunk docbase + numbuffereddocs == current doc id save docbase and numbuffereddocs save numstoredfields save lengths chunks of at least chunksize bytes can be necessary if most docs are empty transform end offsets into lengths compress stored fields to fieldsstream reset we can only bulk-copy if the matching reader is also a compressingstoredfieldsreader naive merge... not all docs were deleted go to the next chunk that contains docid transform lengths into offsets same compression mode starting a new chunk chunk is small enough chunk is large enough no deletion in the chunk no need to decompress, just copy data decompress copy non-deleted docs"
org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader "{@link storedfieldsreader} impl for {@link compressingstoredfieldsformat}. @lucene.experimental sole constructor. @throws alreadyclosedexception if this fieldsreader is closed close the underlying {@link indexinput}s. return the decompressed size of the chunk go to the chunk containing the provided doc id. decompress the chunk. copy compressed data. used by clone nothing to do decompress data"
org.apache.lucene.codecs.compressing.CompressionMode "a compression mode. tells how much effort should be spent on compression and decompression of stored fields. @lucene.experimental a compression mode that trades compression ratio for speed. although the compression ratio might remain high, compression and decompression are very fast. use this mode with indices that have a high update rate but should be able to load documents from disk quickly. a compression mode that trades speed for compression ratio. although compression and decompression might be slow, this compression mode should provide a good compression ratio. this mode might be interesting if/when your index size is much bigger than your os cache. this compression mode is similar to {@link #fast} but it spends more time compressing in order to improve the compression ratio. this compression mode is best used with indices that have a low update rate but should be able to load documents from disk quickly. sole constructor. create a new {@link compressor} instance. create a new {@link decompressor} instance. add 7 padding bytes, this is not necessary but can help decompression run faster no output"
org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexReader "delta from the avg delta from the avg doc bases start pointers"
org.apache.lucene.codecs.BlockTreeTermsWriter "todo: - currently there is a one-to-one mapping of indexed term to term block, but we could decouple the two, ie, put more terms into the index than there are blocks. the index would take up more ram but then it'd be able to avoid seeking more often and could make pk/fuzzyq faster if the additional indexed terms could store the offset into the terms block. - the blocks are not written in true depth-first order, meaning if you just next() the file pointer will sometimes jump backwards. for example, block foo will be written before block f because it finished before. this could possibly hurt performance if the terms dict is not hot, since oss anticipate sequential file access. we could fix the writer to re-order the blocks as a 2nd pass. - each block encodes the term suffixes packed sequentially using a separate vint per term, which is 1) wasteful and 2) slow (must linear scan to find a particular suffix). we should instead 1) make random-access array so we can directly access the nth suffix, and 2) bulk-encode this array using bulk int[] codecs; then at search time we can binary search when we seek a particular term. block-based terms index and dictionary writer.  writes terms dict and index, block-encoding (column stride) each term's metadata for each set of terms between two index terms.  files:  .tim: term dictionary .tip: term index    term dictionary the .tim file contains the list of terms in each field along with per-term statistics (such as docfreq) and per-term metadata (typically pointers to the postings list for that term in the inverted index).  the .tim is arranged in blocks: with blocks containing a variable number of entries (by default 25-48), where each entry is either a term or a reference to a sub-block. note: the term dictionary can plug into different postings implementations: the postings writer/reader are actually responsible for encoding and decoding the postings metadata and term metadata sections.   termsdict (.tim) --&gt; header, postings metadata, blocknumblocks, fieldsummary, diroffset block --&gt; suffixblock, statsblock, metadatablock suffixblock --&gt; entrycount, suffixlength, bytesuffixlength statsblock --&gt; statslength, &lt;docfreq, totaltermfreq&gt;entrycount metadatablock --&gt; metalength, &lt;term metadata&gt;entrycount fieldsummary --&gt; numfields, &lt;fieldnumber, numterms, rootcodelength, byterootcodelength, sumdocfreq, doccount&gt;numfields header --&gt; {@link codecutil#writeheader codecheader} diroffset --&gt; {@link dataoutput#writelong uint64} entrycount,suffixlength,statslength,docfreq,metalength,numfields, fieldnumber,rootcodelength,doccount --&gt; {@link dataoutput#writevint vint} totaltermfreq,numterms,sumtotaltermfreq,sumdocfreq --&gt; {@link dataoutput#writevlong vlong}  notes:  header is a {@link codecutil#writeheader codecheader} storing the version information for the blocktree implementation. diroffset is a pointer to the fieldsummary section. docfreq is the count of documents which contain the term. totaltermfreq is the total number of occurrences of the term. this is encoded as the difference between the total number of occurrences and the docfreq. fieldnumber is the fields number from {@link fieldinfos}. (.fnm) numterms is the number of unique terms for the field. rootcode points to the root block for the field. sumdocfreq is the total number of postings, the number of term-document pairs across the entire field. doccount is the number of documents that have at least one posting for this field. postingsmetadata and termmetadata are plugged into by the specific postings implementation: these contain arbitrary per-file data (such as parameters or versioning information) and per-term data (such as pointers to inverted files).   term index the .tip file contains an index into the term dictionary, so that it can be accessed randomly. the index is also used to determine when a given term cannot exist on disk (in the .tim file), saving a disk seek.  termsindex (.tip) --&gt; header, fstindexnumfields &lt;indexstartfp&gt;numfields, diroffset header --&gt; {@link codecutil#writeheader codecheader} diroffset --&gt; {@link dataoutput#writelong uint64} indexstartfp --&gt; {@link dataoutput#writevlong vlong}  fstindex --&gt; {@link fst fst&lt;byte[]&gt;}  notes:  the .tip file contains a separate fst for each field. the fst maps a term prefix to the on-disk block that holds all terms starting with that prefix. each field's indexstartfp points to its fst. diroffset is a pointer to the start of the indexstartfps for all fields it's possible that an on-disk block would contain too many terms (more than the allowed maximum (default: 48)). when this happens, the block is sub-divided into new blocks (called "floor blocks"), and then the output in the fst for the block's prefix encodes the leading byte of each sub-block, and its file pointer.  @see blocktreetermsreader @lucene.experimental suggested default value for the {@code minitemsinblock} parameter to {@link #blocktreetermswriter(segmentwritestate,postingswriterbase,int,int)}. suggested default value for the {@code maxitemsinblock} parameter to {@link #blocktreetermswriter(segmentwritestate,postingswriterbase,int,int)}. extension of terms file initial terms format. append-only current terms format. extension of terms index file initial index format. append-only current index format. create a new writer. the number of items (terms or sub-blocks) per block will aim to be between minitemsperblock and maxitemsperblock, though in some cases the blocks may be smaller than the min. writes the terms file header. writes the index file header. writes the terms file trailer. writes the index file trailer. writer w = new outputstreamwriter(new fileoutputstream("out.dot")); util.todot(index, w, false, false); system.out.println("saved to out.dot"); w.close(); if (suffixleadlabel == -1) { system.out.println(" sub " + -1 + " termcount=" + termcount + " subcount=" + subcount); } else { system.out.println(" sub " + integer.tohexstring(suffixleadlabel) + " termcount=" + termcount + " subcount=" + subcount); } if (lastsuffixleadlabel == -1) { system.out.println(" sub " + -1 + " termcount=" + termcount + " subcount=" + subcount); } else { system.out.println(" sub " + integer.tohexstring(lastsuffixleadlabel) + " termcount=" + termcount + " subcount=" + subcount); } for(sub++;sub < numsubs;sub++) { system.out.println(" " + (subtermcounts[sub] + subsubcounts[sub])); } system.out.println(" = " + curstart); if (curstart < minitemsinblock) { system.out.println(" "); } if (fieldinfo.name.equals("id")) { postingswriter.termid = integer.parseint(text.utf8tostring()); } else { postingswriter.termid = -1; } public final static boolean debug = false; private final string segment; debug = state.segmentname.equals("_4a"); segment = state.segmentname; system.out.println("btw.init seg=" + state.segmentname); have consumer write its format/header debug = field.name.equals("id"); if (debug) system.out.println("\nbttw.addfield seg=" + segment + " field=" + field.name); todo: try writing the leading vlong in msb order (opposite of what lucene does today), for better outputs sharing in the fst if (debug) { system.out.println(" write floorleadbyte=" + integer.tohexstring(sub.floorleadbyte&0xff)); } if (debug) { system.out.println(" compile index for prefix=" + prefix); } indexbuilder.debug = false; copy over index for all sub-blocks todo: maybe we could add bulk-add method to builder? takes fst and unions it w/ current fst. if (debug) { system.out.println(" add sub=" + indexent.input + " " + indexent.input + " output=" + indexent.output); } used only to partition terms into the block tree; we don't pull an fst from this builder: pendingterm or pendingblock: index into pending of most recently written block re-used when segmenting a too-large block into floor blocks: this class assigns terms to blocks "naturally", ie, according to the number of terms under a given prefix that we encounter: if (debug) system.out.println(" freeze prefixlenplus1=" + prefixlenplus1); we are on a prefix node that has enough entries (terms or sub-blocks) under it to let us write a new block or multiple blocks (main block + follow on floor blocks): if (debug) { if (totcount < minitemsinblock && idx != 0) { system.out.println(" force block has terms"); } } stragglers! carry count upwards write the top count entries on the pending stack as one or more blocks. returns how many blocks were written. if the entry count is <= maxitemsperblock we just write a single block; else we break into primary (initial) block and then one or more following floor blocks: easy case: not floor block. eg, prefix is "foo", and we found 30 terms/sub-blocks starting w/ that prefix, and minitemsinblock <= 30 <= maxitemsinblock. floor block case. eg, prefix is "foo" but we have 100 terms/sub-blocks starting w/ that prefix. we segment the entries into a primary block and following floor blocks using the first label in the suffix to assign to floor blocks. todo: we could store min & max suffix start byte in each block, to make floor blocks if (debug) { final bytesref prefix = new bytesref(prefixlength); for(int m=0;m<prefixlength;m++) { prefix.bytes[m] = (byte) prevterm.ints[m]; } prefix.length = prefixlength; //system.out.println("\nwbs count=" + count + " prefix=" + prefix.utf8tostring() + " " + prefix); system.out.println("writeblocks: prefix=" + prefix + " " + prefix + " count=" + count + " pending.size()=" + pending.size()); } system.out.println("\nwbs count=" + count); count up how many items fall under each unique label after the prefix. todo: this is wasteful since the builder had already done this (partitioned these sub-terms according to their leading prefix byte) first byte in the suffix of this term suffix is 0, ie prefix 'foo' and term is 'foo' so the term has empty string suffix in this block roll up (backwards) the termcounts; postings impl needs this to know where to pull the term slice from its pending terms stack: todo: make a better segmenter? it'd have to absorb the too-small end blocks backwards into the previous blocks naive greedy segmentation; this is not always best (it can produce a too-small block as the last block): system.out.println(" " + (subtermcounts[sub] + subsubcounts[sub])); greedily make a floor block as soon as we've crossed the min count floor term: system.out.println(" " + subcount + " subs"); system.out.println(" = " + pendingcount); remainder is small enough to fit into a block. note that this may be too small (< minitemsinblock); need a true segmenter here system.out.println(" final " + (numsubs-sub-1) + " subs"); if (debug) system.out.println(" done pending.size()=" + pending.size()); for debugging if bytesref isn't actually utf8, or it's eg a prefix of utf8 that ends mid-unicode-char, we fallback to hex: writes all entries in the pending slice as a single block: write block header: if (debug) { system.out.println(" writeblock " + (isfloor ? "(floor) " : "") + "seg=" + segment + " pending.size()=" + pending.size() + " prefixlength=" + prefixlength + " indexprefix=" + tostring(prefix) + " entcount=" + length + " startfp=" + startfp + " futuretermcount=" + futuretermcount + (isfloor ? (" floorleadbyte=" + integer.tohexstring(floorleadbyte&0xff)) : "") + " islastinfloor=" + islastinfloor); } 1st pass: pack term suffix bytes into byte[] blob todo: cutover to bulk int codec... simple64? this block definitely does not contain sub-blocks: system.out.println("no scan true isfloor=" + isfloor); this block definitely does contain at least one sub-block: system.out.println("no scan false " + lastblockindex + " vs start=" + start + " len=" + length); must scan up-front to see if there is a sub-block system.out.println("scan " + lastblockindex + " vs start=" + start + " len=" + length); if (debug) { bytesref suffixbytes = new bytesref(suffix); system.arraycopy(term.term.bytes, prefixlength, suffixbytes.bytes, 0, suffix); suffixbytes.length = suffix; system.out.println(" write term suffix=" + suffixbytes); } for leaf block we write suffix straight write term stats, to separate byte[] blob: if (debug) { bytesref suffixbytes = new bytesref(suffix); system.arraycopy(term.term.bytes, prefixlength, suffixbytes.bytes, 0, suffix); suffixbytes.length = suffix; system.out.println(" write term suffix=" + suffixbytes); } for non-leaf block we borrow 1 bit to record if entry is term or sub-block write term stats, to separate byte[] blob: for non-leaf block we borrow 1 bit to record if entry is term or sub-block if (debug) { bytesref suffixbytes = new bytesref(suffix); system.arraycopy(block.prefix.bytes, prefixlength, suffixbytes.bytes, 0, suffix); suffixbytes.length = suffix; system.out.println(" write sub-block suffix=" + tostring(suffixbytes) + " subfp=" + block.fp + " subcode=" + (startfp-block.fp) + " floor=" + block.isfloor); } todo: we could block-write the term suffix pointers; this would take more space but would enable binary search on lookup write suffixes byte[] blob to terms dict output: write term stats byte[] blob have postings writer write block remove slice replaced by block: if (debug) { system.out.println(" fpend=" + out.getfilepointer()); } this builder is just used transiently to fragment terms into "good" blocks; we don't save the resulting fst: if (debug) system.out.println("\nbttw.startterm term=" + fieldinfo.name + ":" + tostring(text) + " seg=" + segment); if (debug) system.out.println("bttw.finishterm term=" + fieldinfo.name + ":" + tostring(text) + " seg=" + segment + " df=" + stats.docfreq); finishes all terms in this field we better have one final "root" block: write fst to index system.out.println(" write fst " + indexstartfp + " field=" + fieldinfo.name); if (save_dot_files || debug) { final string dotfilename = segment + "_" + fieldinfo.name + ".dot"; writer w = new outputstreamwriter(new fileoutputstream(dotfilename)); util.todot(root.index, w, false, false); system.out.println("saved to " + dotfilename); w.close(); } system.out.println(" field " + field.fieldinfo.name + " " + field.numterms + " terms");"
org.apache.lucene.codecs.LiveDocsFormat "format for live/deleted documents @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) creates a new mutablebits, with all bits set, for the specified size. creates a new mutablebits of the same bits set and size of existing. read live docs bits. persist live docs bits. use {@link segmentinfopercommit#getnextdelgen} to determine the generation of the deletes file you should write to. records all files in use by this {@link segmentinfopercommit} into the files argument."
org.apache.lucene.codecs.MultiLevelSkipListReader "this abstract class reads skip lists with multiple levels. see {@link multilevelskiplistwriter} for the information about the encoding of the multi level skip lists. subclasses must implement the abstract method {@link #readskipdata(int, indexinput)} which defines the actual format of the skip data. @lucene.experimental the maximum number of skip levels possible for this index skipstream for each level. the start pointer of each skip level. skipinterval of each level. number of docs skipped per level. doc id of current skip entry per level. doc id of last read skip entry with docid &lt;= target. child pointer of current skip entry per level. childpointer of last read skip entry with docid &lt;= target. creates a {@code multilevelskiplistreader}. creates a {@code multilevelskiplistreader}, where {@code skipinterval} and {@code skipmultiplier} are the same. returns the id of the doc to which the last call of {@link #skipto(int)} has skipped. skips entries to the first beyond the current whose document number is greater than or equal to target. returns the current doc count. seeks the skip entry on the given level initializes the reader, for reuse on a new term. loads the skip levels subclasses must implement the actual skip data encoding in this method. the level skip data shall be read from the skip stream to read from copies the values of the last read skip entry on this level used to buffer the top skip levels number of levels in this skip list expert: defines the number of top skip levels to buffer in memory. reducing this number results in less memory usage, but possibly slower performance due to more random i/os. please notice that the space each level occupies is limited by the skipinterval. the top level can not contain more than skiplevel entries, the second top level can not contain more than skiplevel^2 entries and so forth. cache skip intervals first time, load skip levels walk up the levels until highest level is found that has a skip for this target no more skips on this level, go down one level we have to skip, the target document is greater than the current skip list entry this skip list is exhausted read next skip entry read the child pointer if we are not on the leaf level the length of the current level the start pointer of the current level buffer this level clone this stream, it is already at the start of the current level move base stream beyond the current level use base stream for the lowest level"
org.apache.lucene.codecs.TermVectorsFormat "controls the format of term vectors sole constructor. (for invocation by subclass constructors, typically implicit.) returns a {@link termvectorsreader} to read term vectors. returns a {@link termvectorswriter} to write term vectors."
org.apache.lucene.codecs.StoredFieldsFormat "controls the format of stored fields sole constructor. (for invocation by subclass constructors, typically implicit.) returns a {@link storedfieldsreader} to load stored fields. returns a {@link storedfieldswriter} to write stored fields."
org.apache.lucene.codecs.FieldsConsumer "abstract api that consumes terms, doc, freq, prox, offset and payloads postings. concrete implementations of this actually do "something" with the postings (write it into the index in a specific format).  the lifecycle is:  fieldsconsumer is sole constructor. (for invocation by subclass constructors, typically implicit.) add a new field called when we are done adding everything. called during merging to merge all {@link fields} from sub-readers. this must recurse to merge all postings (terms, docs, positions, etc.). a {@link postingsformat} can override this default implementation to do its own merging. javadocs"
org.apache.lucene.codecs.DocValuesArraySource "docvalues {@link source} implementation backed by simple arrays. @lucene.experimental @lucene.internal returns the {@link docvaluesarraysource} for the given {@link type}. number of bytes to encode each doc value. creates a {@link docvaluesarraysource} by loading a previously saved one from an {@link indexinput}. creates {@link docvaluesarraysource} from a native array. encode a long value into the provided {@link bytesref}. encode a double value into the provided {@link bytesref}. we always read big_endian here since the writer serialized plain bytes we can simply read the ints / longs back in using readint / readlong we always read big_endian here since the writer serialized plain bytes we can simply read the ints / longs back in using readint / readlong copies the given long value and encodes it as 8 byte big-endian.  note: this method resets the offset to 0, length to 8 and resizes the reference array if needed. copies the given int value and encodes it as 4 byte big-endian.  note: this method resets the offset to 0, length to 4 and resizes the reference array if needed. copies the given short value and encodes it as a 2 byte big-endian.  note: this method resets the offset to 0, length to 2 and resizes the reference array if needed. converts 2 consecutive bytes from the current offset to a short. bytes are interpreted as big-endian (most significant bit first)  note: this method does not check the bounds of the referenced array. converts 4 consecutive bytes from the current offset to an int. bytes are interpreted as big-endian (most significant bit first)  note: this method does not check the bounds of the referenced array. converts 8 consecutive bytes from the current offset to a long. bytes are interpreted as big-endian (most significant bit first)  note: this method does not check the bounds of the referenced array."
org.apache.lucene.codecs.BlockTermState "holds all state required for {@link postingsreaderbase} to produce a {@link docsenum} without re-seeking the terms dict. how many docs have this term total number of occurrences of this term the term's ord in the current block fp into the terms dict primary file (_x.tim) that holds this term sole constructor. (for invocation by subclass constructors, typically implicit.) javadocs note: don't copy blocktermcount; it's "transient": used only by the "primary" termstate, and regenerated on seek by termstate"
org.apache.lucene.codecs.PostingsReaderBase "the core terms dictionaries (blocktermsreader, blocktreetermsreader) interact with a single instance of this class to manage creation of {@link docsenum} and {@link docsandpositionsenum} instances. it provides an indexinput (termsin) where this class may read any previously stored data that it had written in its corresponding {@link postingswriterbase} at indexing time. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) performs any initialization, such as reading and verifying the header from the provided terms dictionary {@link indexinput}. return a newly actually decode metadata for next term must fully consume state, since after this call that termstate may be reused. must fully consume state, since after this call that termstate may be reused. reads data for all terms in the next block; this method should merely load the byte[] blob but not decode, which is done in {@link #nextterm}. todo: find a better name; this defines the api that the terms dict impls use to talk to a postings impl. termsdict + postingsreader/writerbase == postingsconsumer/producer"
org.apache.lucene.codecs.Codec "encodes/decodes an inverted index segment.  note, when extending this class, the name ({@link #getname}) is written into the index. in order for the segment to be read, the name must resolve to your implementation via {@link #forname(string)}. this method uses java's {@link serviceloader service provider interface} (spi) to resolve codec names.  if you implement your own codec, make sure that it has a no-arg constructor so spi can load it. @see serviceloader creates a new codec.  the provided name will be written into the index segment: in order to for the segment to be read this class should be registered with java's spi mechanism (registered in meta-inf/ of your jar file, etc). must be all ascii alphanumeric, and less than 128 characters in length. returns this codec's name encodes/decodes postings encodes/decodes docvalues encodes/decodes stored fields encodes/decodes term vectors encodes/decodes field infos file encodes/decodes segment info file encodes/decodes document normalization values encodes/decodes live docs looks up a codec by name returns a list of all available codec names reloads the codec list from the given {@link classloader}. changes to the codecs are visible after the method ends, all iterators ({@link #availablecodecs()},...) stay consistent. note: only new codecs are added, existing ones are never removed or replaced. this method is expensive and should only be called for discovery of new codecs on the given classpath/classloader! expert: returns the default codec used for newly expert: sets the default codec used for newly returns the codec's name. subclasses can override to provide more detail (such as parameters). javadocs javadocs todo: should we use this, or maybe a system property is better?"
org.apache.lucene.codecs.CodecUtil "utility class for reading and writing versioned headers.  writing codec headers is useful to ensure that a file is in the format you think it is. @lucene.experimental constant to identify the start of a codec header. writes a codec header, which records both a string to identify the file and a version number. this header can be parsed and validated with {@link #checkheader(datainput, string, int, int) checkheader()}.  codecheader --&gt; magic,codecname,version  magic --&gt; {@link dataoutput#writeint uint32}. this identifies the start of the header. it is always {@value #codec_magic}. codecname --&gt; {@link dataoutput#writestring string}. this is a string to identify this file. version --&gt; {@link dataoutput#writeint uint32}. records the version of the file.   note that the length of a codec header depends only upon the name of the codec, so this length can be computed at any time with {@link #headerlength(string)}. output stream string to identify this file. it should be simple ascii, less than 128 characters in length. version number @throws ioexception if there is an i/o error writing to the underlying medium. computes the length of a codec header. codec name. of the entire codec header. @see #writeheader(dataoutput, string, int) reads and validates a header previously written with {@link #writeheader(dataoutput, string, int)}.  when reading a file, supply the expected codec and an expected version range (minversion to maxversion). input stream, positioned at the point where the header was previously written. typically this is located at the beginning of the file. the expected codec name. the minimum supported expected version number. the maximum supported expected version number. actual version found, when a valid header is found that matches codec, with an actual version where minversion . otherwise an exception is thrown. @throws corruptindexexception if the first four bytes are not {@link #codec_magic}, or if the actual codec found is not codec. @throws indexformattoooldexception if the actual version is less than minversion. @throws indexformattoonewexception if the actual version is greater than maxversion. @throws ioexception if there is an i/o error reading from the underlying medium. @see #writeheader(dataoutput, string, int) like {@link #checkheader(datainput,string,int,int)} except this version assumes the first int has already been read and validated from the input. no instance safety to guard against reading a bogus string:"
org.apache.lucene.codecs.PerDocProducerBase "abstract base class for perdocproducer implementations @lucene.experimental closes provided closables. returns a map, mapping field names to doc values. sole constructor. (for invocation by subclass constructors, typically implicit.) returns the comparator used to sort {@link bytesref} values. only opens files... doesn't actually load any values. returns true if this field indexed doc values. returns the doc values type for this field. returns true if any fields indexed doc values. returns the unique segment and field id for any per-field files this implementation needs to write. loads a {@link docvalues} instance depending on the given {@link type}. codecs that use different implementations for a certain {@link type} can simply override this method and return their custom implementations. number of documents in the segment the {@link directory} to load the {@link docvalues} from the unique file id within the segment the type to load {@link docvalues} instance for the given type @throws ioexception if an {@link ioexception} occurs @throws illegalargumentexception if the given {@link type} is not supported javadocs if we fail we must close all opened resources if there are any keep our original exception"
org.apache.lucene.codecs.FieldInfosReader "codec api for reading {@link fieldinfos}. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) read the {@link fieldinfos} previously written with {@link fieldinfoswriter}."
org.apache.lucene.codecs.FieldsProducer "abstract api that produces terms, doc, freq, prox, offset and payloads postings. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.)"
org.apache.lucene.codecs.DocValuesConsumer "abstract api that consumes {@link indexablefield}s. {@link docvaluesconsumer} are always associated with a specific field and segments. concrete implementations of this api write the given {@link indexablefield} into a implementation specific format depending on the fields meta-data. @lucene.experimental spare {@link bytesref} that subclasses can reuse. returns the {@link type} of this consumer. sole constructor. (for invocation by subclass constructors, typically implicit.) adds the given {@link indexablefield} instance to this {@link docvaluesconsumer} the document id to add the value for. the docid must always increase or be 0 if it is the first call to this method. the value to add @throws ioexception if an {@link ioexception} occurs called when the consumer of this api is done adding values. the total number of documents in this {@link docvaluesconsumer}. must be greater than or equal the last given docid to {@link #add(int, indexablefield)}. @throws ioexception if an i/o error occurs returns the value size this consumer accepts or -1 iff this consumer is value size agnostic ie. accepts variable length values.  note: the return value is undefined until the consumer has successfully consumed at least one value. value size this consumer accepts or -1 iff this consumer is value size agnostic ie. accepts variable length values. merges the given {@link org.apache.lucene.index.mergestate} into this {@link docvaluesconsumer}. the state to merge docvalues array containing one instance per reader ( {@link org.apache.lucene.index.mergestate#readers}) or null if the reader has no {@link docvalues} instance. @throws ioexception if an {@link ioexception} occurs merges the given {@link docvalues} into this {@link docvaluesconsumer}. @throws ioexception if an {@link ioexception} occurs merges a document with the given docid. the methods implementation obtains the value for the sourcedoc id from the current {@link source}.  this method is used during merging to provide implementation agnostic default merge implementation.   all documents ids between the given id and the previously given id or 0 if the method is call the first time are filled with default values depending on the implementation. the given document id must always be greater than the previous id or 0 if called the first time. only finish if no exception is thrown! this enables bulk copies in subclasses per mergestate, subclasses can simply override this and decide if they want to merge segments using this generic implementation or if a bulk merge is possible / feasible."
org.apache.lucene.codecs.TermVectorsWriter "codec api for writing term vectors:   for every document, {@link #startdocument(int)} is called, informing the codec how many fields will be written. {@link #startfield(fieldinfo, int, boolean, boolean, boolean)} is called for each field in the document, informing the codec how many terms will be written for that field, and whether or not positions, offsets, or payloads are enabled. within each field, {@link #startterm(bytesref, int)} is called for each term. if offsets and/or positions are enabled, then {@link #addposition(int, int, int, bytesref)} will be called for each term occurrence. after all documents have been written, {@link #finish(fieldinfos, int)} is called for verification/sanity-checks. finally the writer is closed ({@link #close()})  @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) called before writing the term vectors of the document. {@link #startfield(fieldinfo, int, boolean, boolean, boolean)} will be called numvectorfields times. note that if term vectors are enabled, this is called even if the document has no vector fields, in this case numvectorfields will be zero. called after a doc and all its fields have been added. called before writing the terms of the field. {@link #startterm(bytesref, int)} will be called numterms times. called after a field and all its terms have been added. adds a term and its term frequency freq. if this field has positions and/or offsets enabled, then {@link #addposition(int, int, int, bytesref)} will be called freq times respectively. called after a term and all its positions have been added. adds a term position and offsets aborts writing entirely, implementation should remove any partially-written files, etc. called before {@link #close()}, passing in the number of documents that were written. note that this is intentionally redundant (equivalent to the number of calls to {@link #startdocument(int)}, but a codec should check that this is the case to detect the jre bug described in lucene-1282. called by indexwriter when writing new segments.  this is an expert api that allows the codec to consume positions and offsets directly from the indexer.  the default implementation calls {@link #addposition(int, int, int, bytesref)}, but subclasses can override this if they want to efficiently write all the positions, then all the offsets, for example.  note: this api is extremely expert and subject to change or removal!!! @lucene.internal merges in the term vectors from the readers in mergestate. the default implementation skips over deleted documents, and uses {@link #startdocument(int)}, {@link #startfield(fieldinfo, int, boolean, boolean, boolean)}, {@link #startterm(bytesref, int)}, {@link #addposition(int, int, int, bytesref)}, and {@link #finish(fieldinfos, int)}, returning the number of documents that were written. implementations can override this method for more sophisticated merging (bulk-byte copying, etc). safe (but, slowish) default method to write every vector field in the document. return the bytesref comparator used to sort terms before feeding to this api. todo: we should probably nuke this and make a more efficient 4.x format preflex-rw could then be slow and buffer (its only used in tests...) this position has a payload skip deleted docs note: it's very important to first assign to vectors then pass it to termvectorswriter.addalldocvectors; see lucene-1282 count manually! todo: maybe enforce that fields.size() returns something valid? fieldsenum shouldn't lie... count manually. it is stupid, but needed, as terms.size() is not a mandatory statistics function"
org.apache.lucene.codecs.PostingsWriterBase "extension of {@link postingsconsumer} to support pluggable term dictionaries.  this class contains additional hooks to interact with the provided term dictionaries such as {@link blocktreetermswriter}. if you want to re-use an existing implementation and are only interested in customizing the format of the postings list, extend this class instead. @see postingsreaderbase @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) called once after startup, before any terms have been added. implementations typically write a header to the provided {@code termsout}. start a new term. note that a matching call to {@link #finishterm(termstats)} is done, only if the term has at least one document. flush count terms starting at start "backwards", as a block. start is a negative offset from the end of the terms stack, ie bigger start means further back in the stack. finishes the current term. the provided {@link termstats} contains the term's summary statistics. called when the writing switches to another field. todo: find a better name; this defines the api that the terms dict impls use to talk to a postings impl. termsdict + postingsreader/writerbase == postingsconsumer/producer"
org.apache.lucene.codecs.FieldInfosWriter "codec api for writing {@link fieldinfos}. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) writes the provided {@link fieldinfos} to the directory."
org.apache.lucene.codecs.lucene41.Lucene41PostingsBaseFormat "provides a {@link postingsreaderbase} and {@link postingswriterbase}. @lucene.experimental sole constructor. todo: should these also be named / looked up via spi?"
org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat "lucene 4.1 stored fields format. principle this {@link storedfieldsformat} compresses blocks of 16kb of documents in order to improve the compression ratio compared to document-level compression. it uses the lz4 compression algorithm, which is fast to compress and very fast to decompress data. although the compression method that is used focuses more on speed than on compression ratio, it should provide interesting compression ratios for redundant inputs (such as log files, html or plain text). file formats stored fields are represented by two files:   a fields data file (extension .fdt). this file stores a compact representation of documents in compressed blocks of 16kb or more. when writing a segment, documents are appended to an in-memory byte[] buffer. when its size reaches 16kb or more, some metadata about the documents is flushed to disk, immediately followed by a compressed representation of the buffer using the lz4 compression format. here is a more detailed description of the field data file format:  fielddata (.fdt) --&gt; &lt;header&gt;, packedintsversion, compressionformat, &lt;chunk&gt;chunkcount header --&gt; {@link codecutil#writeheader codecheader} packedintsversion --&gt; {@link packedints#version_current} as a {@link dataoutput#writevint vint} chunkcount is not known in advance and is the number of chunks necessary to store all document of the segment chunk --&gt; docbase, chunkdocs, docfieldcounts, doclengths, &lt;compresseddocs&gt; docbase --&gt; the id of the first document of the chunk as a {@link dataoutput#writevint vint} chunkdocs --&gt; the number of documents in the chunk as a {@link dataoutput#writevint vint} docfieldcounts --&gt; the number of stored fields of every document in the chunk, encoded as followed: if chunkdocs=1, the unique value is encoded as a {@link dataoutput#writevint vint} else read a {@link dataoutput#writevint vint} (let's call it bitsrequired) if bitsrequired is 0 then all values are equal, and the common value is the following {@link dataoutput#writevint vint} else bitsrequired is the number of bits required to store any value, and values are stored in a {@link packedints packed} array where every value is stored on exactly bitsrequired bits   doclengths --&gt; the lengths of all documents in the chunk, encoded with the same method as docfieldcounts compresseddocs --&gt; a compressed representation of &lt;docs&gt; using the lz4 compression format docs --&gt; &lt;doc&gt;chunkdocs doc --&gt; &lt;fieldnumandtype, value&gt;docfieldcount fieldnumandtype --&gt; a {@link dataoutput#writevlong vlong}, whose 3 last bits are type and other bits are fieldnum type --&gt; 0: value is string 1: value is binaryvalue 2: value is int 3: value is float 4: value is long 5: value is double 6, 7: unused  fieldnum --&gt; an id of the field value --&gt; {@link dataoutput#writestring(string) string} | binaryvalue | int | float | long | double depending on type binaryvalue --&gt; valuelength &lt;byte&gt;valuelength  notes  if documents are larger than 16kb then chunks will likely contain only one document. however, documents can never spread across several chunks (all fields of a single document are in the same chunk). given that the original lengths are written in the metadata of the chunk, the decompressor can leverage this information to stop decoding as soon as enough data has been decompressed. in case documents are incompressible, compresseddocs will be less than 0.5% larger than docs.    a fields index file (extension .fdx). the data stored in this file is read to load an in-memory data-structure that can be used to locate the start offset of a block containing any document in the fields data file. in order to have a compact in-memory representation, for every block of 1024 chunks, this stored fields index computes the average number of bytes per chunk and for every chunk, only stores the difference between ${chunk number} ${average length of a chunk} and the actual start offset of the chunk data is written as follows:  fieldsindex (.fdx) --&gt; &lt;header&gt;, fieldsindex, packedintsversion, &lt;block&gt;blockcount, blocksendmarker header --&gt; {@link codecutil#writeheader codecheader} packedintsversion --&gt; {@link packedints#version_current} as a {@link dataoutput#writevint vint} blocksendmarker --&gt; 0 as a {@link dataoutput#writevint vint}, this marks the end of blocks since blocks are not allowed to start with 0 block --&gt; blockchunks, &lt;docbases&gt;, &lt;startpointers&gt; blockchunks --&gt; a {@link dataoutput#writevint vint} which is the number of chunks encoded in the block docbases --&gt; docbase, avgchunkdocs, bitsperdocbasedelta, docbasedeltas docbase --&gt; first document id of the block of chunks, as a {@link dataoutput#writevint vint} avgchunkdocs --&gt; average number of documents in a single chunk, as a {@link dataoutput#writevint vint} bitsperdocbasedelta --&gt; number of bits required to represent a delta from the average using zigzag encoding docbasedeltas --&gt; {@link packedints packed} array of blockchunks elements of bitsperdocbasedelta bits each, representing the deltas from the average doc base using zigzag encoding. startpointers --&gt; startpointerbase, avgchunksize, bitsperstartpointerdelta, startpointerdeltas startpointerbase --&gt; the first start pointer of the block, as a {@link dataoutput#writevlong vlong} avgchunksize --&gt; the average size of a chunk of compressed documents, as a {@link dataoutput#writevlong vlong} bitsperstartpointerdelta --&gt; number of bits required to represent a delta from the average using zigzag encoding startpointerdeltas --&gt; {@link packedints packed} array of blockchunks elements of bitsperstartpointerdelta bits each, representing the deltas from the average start pointer using zigzag encoding  notes  for any block, the doc base of the n-th chunk can be restored with docbase + avgchunkdocs n + docbasedeltas[n]. for any block, the start pointer of the n-th chunk can be restored with startpointerbase + avgchunksize n + startpointerdeltas[n]. once data is loaded into memory, you can lookup the start pointer of any document by performing two binary searches: a first one based on the values of docbase in order to find the right block, and then inside the block based on docbasedeltas (by reconstructing the doc bases for every chunk).    known limitations this {@link storedfieldsformat} does not support individual documents larger than (231 - 214) bytes. in case this is a problem, you should use another format, such as {@link lucene40storedfieldsformat}. @lucene.experimental sole constructor."
org.apache.lucene.codecs.lucene41.Lucene41Codec "implements the lucene 4.1 index format, with configurable per-field postings formats.  if you want to reuse functionality of this codec in another codec, extend {@link filtercodec}. @see org.apache.lucene.codecs.lucene41 package documentation for file format details. @lucene.experimental sole constructor. returns the postings format that should be used for writing new segments of field. the default implementation always returns "lucene41" note: if we make largish changes in a minor release, easier to just make lucene42codec or whatever if they are backwards compatible or smallish we can probably do the backwards in the postingsreader (it writes a minor version, etc)."
org.apache.lucene.codecs.lucene41.Lucene41SkipWriter "write skip lists with multiple levels, and support skip within block ints. assume that docfreq = 28, skipinterval = blocksize = 12 | block#0 | | block#1 | |vints| d d d d d d d d d d d d d d d d d d d d d d d d d d d d (posting list) ^ ^ (level 0 skip point) note that skipwriter will ignore first document in block#0, since it is useless as a skip point. also, we'll never skip into the vints block, only record skip data at the start its start point(if it exist). for each skip point, we will record: 1. docid in former position, i.e. for position 12, record docid[11], etc. 2. its related file points(position, payload), 3. related numbers or uptos(position, payload). 4. start offset. sets the values for the current skip data. private boolean debug = lucene41postingsreader.debug; if (debug) { system.out.println("writeskipdata level=" + level + " lastdoc=" + curdoc + " delta=" + delta + " curdocpointer=" + curdocpointer); } if (debug) { system.out.println(" curpospointer=" + curpospointer + " curposbufferupto=" + curposbufferupto); }"
org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter "concrete class that writes docid(maybe frq,pos,offset,payloads) list with postings format. postings list for each term will be stored separately. @see lucene41skipwriter for details about skipping setting and postings layout. @lucene.experimental expert: the maximum number of skip levels. smaller values result in slightly smaller indexes, but slower skipping in big posting lists. creates a postings writer with the specified packedints overhead ratio creates a postings writer with packedints.compact add a new position & payload called when we are done adding docs to this term increment version to change it how current field indexes postings: holds starting file pointers for each term: todo: does this ctor even make sense? todo: should we try skipping every 2/4 blocks...? if (debug) { system.out.println("fpw.startterm startfp=" + doctermstartfp); } if (debug) { system.out.println("fpw.startdoc docid["+docbufferupto+"]=" + docid); } have collected a block of docs, and get a new doc. should write skip data as well as postings list for current block. if (debug) { system.out.println(" bufferskip at writeblock: lastdocid=" + lastblockdocid + " doccount=" + (doccount-1)); } if (debug) { system.out.println(" docdeltabuffer[" + docbufferupto + "]=" + docdelta); } if (debug) { system.out.println(" write docdelta block @ fp=" + docout.getfilepointer()); } if (debug) { system.out.println(" write freq block @ fp=" + docout.getfilepointer()); } note: don't set docbufferupto back to 0 here; finishdoc will do so (because it needs to see that the block was filled so it can save skip data) if (debug) { system.out.println("fpw.addposition pos=" + position + " posbufferupto=" + posbufferupto + (fieldhaspayloads ? " payloadbyteupto=" + payloadbyteupto: "")); } no payload if (debug) { system.out.println(" write pos bulk block @ fp=" + posout.getfilepointer()); } since we don't know df for current term, we had to buffer those skip data for each block, and when a new doc comes, write them to skip file. if (debug) { system.out.println(" docbufferupto="+docbufferupto+" now get lastblockdocid="+lastblockdocid+" lastblockposfp=" + lastblockposfp + " lastblockposbufferupto=" + lastblockposbufferupto + " lastblockpayloadbyteupto=" + lastblockpayloadbyteupto); } todo: wasteful we are counting this (counting # docs for this term) in two places? if (debug) { system.out.println("fpw.finishterm docfreq=" + stats.docfreq); } if (debug) { if (docbufferupto > 0) { system.out.println(" write doc/freq vint block (count=" + docbufferupto + ") at fp=" + docout.getfilepointer() + " doctermstartfp=" + doctermstartfp); } } docfreq == 1, don't write the single docid/freq to a separate file along with a pointer to it. pulse the singleton docid into the term dictionary, freq is implicitly totaltermfreq vint encode the remaining doc deltas and freqs: if (debug) { if (posbufferupto > 0) { system.out.println(" write pos vint block (count=" + posbufferupto + ") at fp=" + posout.getfilepointer() + " postermstartfp=" + postermstartfp + " haspayloads=" + fieldhaspayloads + " hasoffsets=" + fieldhasoffsets); } } totaltermfreq is just total number of positions(or payloads, or offsets) associated with current term. record file offset for last pos in last block todo: should we send offsets/payloads to .pay...? seems wasteful (have to store extra vlong for low (< block_size) df terms = vast vast majority) vint encode the remaining positions/payloads/offsets: force first payload length to be written force first offset length to be written if (debug) { system.out.println(" i=" + i + " payloadlen=" + payloadlength); } if (debug) { system.out.println(" write payload @ pos.fp=" + posout.getfilepointer()); } if (debug) { system.out.println(" write offset @ pos.fp=" + posout.getfilepointer()); } if (debug) { system.out.println(" totaltermfreq=" + stats.totaltermfreq + " lastposblockoffset=" + lastposblockoffset); } if (debug) { system.out.println("skip packet " + (docout.getfilepointer() - (doctermstartfp + skipoffset)) + " bytes"); } if (debug) { system.out.println(" no skip: doccount=" + doccount); } if (debug) { system.out.println(" paystartfp=" + paystartfp); } remove the terms we just wrote:"
org.apache.lucene.codecs.lucene41.Lucene41SkipReader "implements the skip list reader for block postings format that stores positions and payloads. although this skipper uses multilevelskiplistreader as an interface, its definition of skip position will be a little different. for example, when skipinterval = blocksize = 3, df = 2skipinterval = 6, 0 1 2 3 4 5 d d d d d d (posting list) ^ ^ (skip point in multileveskipwriter) ^ (skip point in lucene41skipwriter) in this case, multilevelskiplistreader will use the last document as a skip point, while lucene41skipreader should assume no skip point will comes. if we use the interface directly in lucene41skipreader, it may silly try to read another skip data after the only skip point is loaded. to illustrate this, we can call skipto(d[5]), since skip point d[3] has smaller docid, and numskipped+blocksize== df, the multilevelskiplistreader will assume the skip list isn't exhausted yet, and try to load a non-existed skip point therefore, we'll trim df before passing it to the interface. see trim(int) trim original docfreq to tell skipreader read proper number of skip points. since our definition in lucene41skip is a little different from multilevelskip this trimmed docfreq will prevent skipreader from: 1. silly reading a non-existed skip point after the last block boundary 2. moving into the vint block returns the doc pointer of the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. private boolean debug = lucene41postingsreader.debug; if (debug) { system.out.println("seekchild level=" + level); } if (debug) { system.out.println("setlastskipdata level=" + level); system.out.println(" lastdocpointer=" + lastdocpointer); } if (debug) { system.out.println(" lastpospointer=" + lastpospointer + " lastposbufferupto=" + lastposbufferupto); } if (debug) { system.out.println("readskipdata level=" + level); } if (debug) { system.out.println(" delta=" + delta); } if (debug) { system.out.println(" docfp=" + docpointer[level]); } if (debug) { system.out.println(" posfp=" + pospointer[level]); } if (debug) { system.out.println(" posbufferupto=" + posbufferupto[level]); }"
org.apache.lucene.codecs.lucene41.Lucene41PostingsReader "concrete class that reads docid(maybe frq,pos,offset,payloads) list with postings format. @see lucene41skipreader for details @lucene.experimental sole constructor. read values that have been written using variable-length encoding instead of bit-packing. reads but does not decode the byte[] blob holding metadata for the current terms block public static boolean debug = false; make sure we are talking to the matching postings writer must keep final because we do non-standard clone docid when there is a single pulsed posting, otherwise -1 freq is always implicitly totaltermfreq in this case. only used by the "primary" termstate -- clones don't copy this (basically they are "transient"): todo: should this not be in the termstate...? do not copy bytes, bytesreader (else termstate is very heavy, ie drags around the entire block's byte[]). on seek back, if next() is in fact used (rare!), they will be re-read from disk. todo: specialize to livedocs vs not number of docs in this posting list sum of freqs in this posting list (or docfreq when omitted) how many docs we've read doc we last read accumulator for doc deltas freq we last read where this term's postings start in the .doc file: where this term's skip data starts (after doctermstartfp) in the .doc file (or -1 if there is no skip data for this term): docid for next skip point, we won't use skipper if target docid is not larger than this true if the caller actually needs frequencies docid when there is a single pulsed posting, otherwise -1 if (debug) { system.out.println(" fpr.reset: termstate=" + termstate); } lazy init we won't skip if target is found in first block if (debug) { system.out.println(" fill doc block from fp=" + docin.getfilepointer()); } if (debug) { system.out.println(" fill freq block from fp=" + docin.getfilepointer()); } skip over freqs read vints: if (debug) { system.out.println(" fill last vint block from fp=" + docin.getfilepointer()); } if (debug) { system.out.println("\nfpr.nextdoc"); } if (debug) { system.out.println(" docupto=" + docupto + " (of df=" + docfreq + ") docbufferupto=" + docbufferupto); } if (debug) { system.out.println(" return doc=end"); } if (debug) { system.out.println(" accum=" + accum + " docdeltabuffer[" + docbufferupto + "]=" + docdeltabuffer[docbufferupto]); } if (debug) { system.out.println(" return doc=" + doc + " freq=" + freq); } if (debug) { system.out.println(" doc=" + accum + " is deleted; try next doc"); } todo: make frq block load lazy/skippable if (debug) { system.out.println(" fpr.advance target=" + target); } current skip docid < docids generated from current buffer <= next skip docid we don't need to skip if target is buffered already if (debug) { system.out.println("load skipper"); } lazy init: first time this enum has ever been used for skipping this is the first time this enum has skipped since reset() was called; load the skip data: always plus one to fix the result, since skip position in lucene41skipreader is a little different from multilevelskiplistreader skipper moved if (debug) { system.out.println("skipper moved to docupto=" + newdocupto + " vs current=" + docupto + "; docid=" + skipper.getdoc() + " fp=" + skipper.getdocpointer()); } force to read next block actually, this is just lastskipentry now point to the block we want to search next time we call advance, this is used to foresee whether skipper is necessary. now scan... this is an inlined/pared down version of nextdoc(): if (debug) { system.out.println(" scan doc=" + accum + " docbufferupto=" + docbufferupto); } if (debug) { system.out.println(" return doc=" + accum); } if (debug) { system.out.println(" now do nextdoc()"); } number of docs in this posting list number of positions in this posting list how many docs we've read doc we last read accumulator for doc deltas freq we last read current position how many positions "behind" we are; nextposition must skip these to "catch up": lazy pos seek: if != -1 then we must seek to this fp before reading positions: where this term's postings start in the .doc file: where this term's postings start in the .pos file: where this term's payloads/offsets start in the .pay file: file pointer where the last (vint encoded) pos delta block is. we need this to know whether to bulk decode vs vint decode the block: where this term's skip data starts (after doctermstartfp) in the .doc file (or -1 if there is no skip data for this term): docid when there is a single pulsed posting, otherwise -1 if (debug) { system.out.println(" fpr.reset: termstate=" + termstate); } lazy init if (debug) { system.out.println(" fill doc block from fp=" + docin.getfilepointer()); } if (debug) { system.out.println(" fill freq block from fp=" + docin.getfilepointer()); } read vints: if (debug) { system.out.println(" fill last vint doc block from fp=" + docin.getfilepointer()); } if (debug) { system.out.println(" refillpositions"); } if (debug) { system.out.println(" vint pos block @ fp=" + posin.getfilepointer() + " haspayloads=" + indexhaspayloads + " hasoffsets=" + indexhasoffsets); } offset length changed if (debug) { system.out.println(" bulk pos block @ fp=" + posin.getfilepointer()); } if (debug) { system.out.println(" fpr.nextdoc"); } if (debug) { system.out.println(" docupto=" + docupto + " (of df=" + docfreq + ") docbufferupto=" + docbufferupto); } if (debug) { system.out.println(" accum=" + accum + " docdeltabuffer[" + docbufferupto + "]=" + docdeltabuffer[docbufferupto]); } if (debug) { system.out.println(" return doc=" + doc + " freq=" + freq + " pospendingcount=" + pospendingcount); } if (debug) { system.out.println(" doc=" + accum + " is deleted; try next doc"); } todo: make frq block load lazy/skippable if (debug) { system.out.println(" fpr.advance target=" + target); } if (debug) { system.out.println(" try skipper"); } lazy init: first time this enum has ever been used for skipping if (debug) { system.out.println(" create skipper"); } this is the first time this enum has skipped since reset() was called; load the skip data: if (debug) { system.out.println(" init skipper"); } skipper moved if (debug) { system.out.println(" skipper moved to docupto=" + newdocupto + " vs current=" + docupto + "; docid=" + skipper.getdoc() + " fp=" + skipper.getdocpointer() + " pos.fp=" + skipper.getpospointer() + " pos.bufferupto=" + skipper.getposbufferupto()); } force to read next block now scan... this is an inlined/pared down version of nextdoc(): if (debug) { system.out.println(" scan doc=" + accum + " docbufferupto=" + docbufferupto); } if (debug) { system.out.println(" return doc=" + accum); } if (debug) { system.out.println(" now do nextdoc()"); } todo: in theory we could avoid loading frq block when not needed, ie, use skip data to load how far to seek the pos pointer ... instead of having to load frq blocks only to sum up how many positions to skip skip positions now: if (debug) { system.out.println(" fpr.skippositions: toskip=" + toskip); } if (debug) { system.out.println(" skip w/in block to posbufferupto=" + posbufferupto); } if (debug) { system.out.println(" skip whole block @ fp=" + posin.getfilepointer()); } if (debug) { system.out.println(" skip w/in block to posbufferupto=" + posbufferupto); } if (debug) { system.out.println(" fpr.nextposition pospendingcount=" + pospendingcount + " posbufferupto=" + posbufferupto); } if (debug) { system.out.println(" seek to pendingfp=" + pospendingfp); } force buffer refill: if (debug) { system.out.println(" return pos=" + position); } also handles payloads + offsets number of docs in this posting list number of positions in this posting list how many docs we've read doc we last read accumulator for doc deltas freq we last read current position how many positions "behind" we are; nextposition must skip these to "catch up": lazy pos seek: if != -1 then we must seek to this fp before reading positions: lazy pay seek: if != -1 then we must seek to this fp before reading payloads/offsets: where this term's postings start in the .doc file: where this term's postings start in the .pos file: where this term's payloads/offsets start in the .pay file: file pointer where the last (vint encoded) pos delta block is. we need this to know whether to bulk decode vs vint decode the block: where this term's skip data starts (after doctermstartfp) in the .doc file (or -1 if there is no skip data for this term): true if we actually need offsets true if we actually need payloads docid when there is a single pulsed posting, otherwise -1 if (debug) { system.out.println(" fpr.reset: termstate=" + termstate); } lazy init if (debug) { system.out.println(" fill doc block from fp=" + docin.getfilepointer()); } if (debug) { system.out.println(" fill freq block from fp=" + docin.getfilepointer()); } if (debug) { system.out.println(" fill last vint doc block from fp=" + docin.getfilepointer()); } if (debug) { system.out.println(" refillpositions"); } if (debug) { system.out.println(" vint pos block @ fp=" + posin.getfilepointer() + " haspayloads=" + indexhaspayloads + " hasoffsets=" + indexhasoffsets); } if (debug) { system.out.println(" i=" + i + " payloadlen=" + payloadlength); } system.out.println(" read payload @ pos.fp=" + posin.getfilepointer()); if (debug) { system.out.println(" i=" + i + " read offsets from posin.fp=" + posin.getfilepointer()); } if (debug) { system.out.println(" startoffdelta=" + offsetstartdeltabuffer[i] + " offsetlen=" + offsetlengthbuffer[i]); } if (debug) { system.out.println(" bulk pos block @ fp=" + posin.getfilepointer()); } if (debug) { system.out.println(" bulk payload block @ pay.fp=" + payin.getfilepointer()); } if (debug) { system.out.println(" " + numbytes + " payload bytes @ pay.fp=" + payin.getfilepointer()); } this works, because when writing a vint block we always force the first length to be written skip over lengths read length of payloadbytes skip over payloadbytes if (debug) { system.out.println(" bulk offset block @ pay.fp=" + payin.getfilepointer()); } this works, because when writing a vint block we always force the first length to be written skip over starts skip over lengths if (debug) { system.out.println(" fpr.nextdoc"); } if (debug) { system.out.println(" docupto=" + docupto + " (of df=" + docfreq + ") docbufferupto=" + docbufferupto); } if (debug) { system.out.println(" accum=" + accum + " docdeltabuffer[" + docbufferupto + "]=" + docdeltabuffer[docbufferupto]); } if (debug) { system.out.println(" return doc=" + doc + " freq=" + freq + " pospendingcount=" + pospendingcount); } if (debug) { system.out.println(" doc=" + accum + " is deleted; try next doc"); } todo: make frq block load lazy/skippable if (debug) { system.out.println(" fpr.advance target=" + target); } if (debug) { system.out.println(" try skipper"); } lazy init: first time this enum has ever been used for skipping if (debug) { system.out.println(" create skipper"); } this is the first time this enum has skipped since reset() was called; load the skip data: if (debug) { system.out.println(" init skipper"); } skipper moved if (debug) { system.out.println(" skipper moved to docupto=" + newdocupto + " vs current=" + docupto + "; docid=" + skipper.getdoc() + " fp=" + skipper.getdocpointer() + " pos.fp=" + skipper.getpospointer() + " pos.bufferupto=" + skipper.getposbufferupto() + " pay.fp=" + skipper.getpaypointer() + " laststartoffset=" + laststartoffset); } force to read next block new document now scan: if (debug) { system.out.println(" scan doc=" + accum + " docbufferupto=" + docbufferupto); } if (debug) { system.out.println(" return doc=" + accum); } if (debug) { system.out.println(" now do nextdoc()"); } todo: in theory we could avoid loading frq block when not needed, ie, use skip data to load how far to seek the pos pointer ... instead of having to load frq blocks only to sum up how many positions to skip skip positions now: if (debug) { system.out.println(" fpr.skippositions: toskip=" + toskip); } if (debug) { system.out.println(" skip w/in block to posbufferupto=" + posbufferupto); } if (debug) { system.out.println(" skip whole block @ fp=" + posin.getfilepointer()); } skip payloadlength block: skip payloadbytes block: if (debug) { system.out.println(" skip w/in block to posbufferupto=" + posbufferupto); } if (debug) { system.out.println(" fpr.nextposition pospendingcount=" + pospendingcount + " posbufferupto=" + posbufferupto + " payloadbyteupto=" + payloadbyteupto)// ; } if (debug) { system.out.println(" seek pos to pendingfp=" + pospendingfp); } if (debug) { system.out.println(" seek pay to pendingfp=" + paypendingfp); } force buffer refill: if (debug) { system.out.println(" return pos=" + position); } if (debug) { system.out.println(" fpr.getpayload payloadlength=" + payloadlength + " payloadbyteupto=" + payloadbyteupto); }"
org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat "lucene 4.1 postings format, which encodes postings in packed integer blocks for fast decode. note: this format is still experimental and subject to change without backwards compatibility.  basic idea:   packed blocks and vint blocks: in packed blocks, integers are encoded with the same bit width ({@link packedints packed format}): the block size (i.e. number of integers inside block) is fixed (currently 128). additionally blocks that are all the same value are encoded in an optimized way. in vint blocks, integers are encoded as {@link dataoutput#writevint vint}: the block size is variable.   block structure: when the postings are long enough, lucene41postingsformat will try to encode most integer data as a packed block. take a term with 259 documents as an example, the first 256 document ids are encoded as two packed blocks, while the remaining 3 are encoded as one vint block.  different kinds of data are always encoded separately into different packed blocks, but may possibly be interleaved into the same vint block.  this strategy is applied to pairs: &lt;document number, frequency&gt;, &lt;position, payload length&gt;, &lt;position, offset start, offset length&gt;, and &lt;position, payload length, offsetstart, offset length&gt;.   skipdata settings: the structure of skip table is quite similar to previous version of lucene. skip interval is the same as block size, and each skip entry points to the beginning of each block. however, for the first block, skip data is omitted.   positions, payloads, and offsets: a position is an integer indicating where the term occurs within one document. a payload is a blob of metadata associated with current position. an offset is a pair of integers indicating the tokenized start/end offsets for given term in current position: it is essentially a specialized payload.  when payloads and offsets are not omitted, numpositions==numpayloads==numoffsets (assuming a null payload contributes one count). as mentioned in block structure, it is possible to encode these three either combined or separately. in all cases, payloads and offsets are stored together. when encoded as a packed block, position data is separated out as .pos, while payloads and offsets are encoded in .pay (payload metadata will also be stored directly in .pay). when encoded as vint blocks, all these three are stored interleaved into the .pos (so is payload metadata). with this strategy, the majority of payload and offset data will be outside .pos file. so for queries that require only position data, running on a full index with payloads and offsets, this reduces disk pre-fetches.     files and detailed format:  .tim: term dictionary .tip: term index .doc: frequencies and skip data .pos: positions .pay: payloads and offsets      term dictionary the .tim file contains the list of terms in each field along with per-term statistics (such as docfreq) and pointers to the frequencies, positions, payload and skip data in the .doc, .pos, and .pay files. see {@link blocktreetermswriter} for more details on the format.  note: the term dictionary can plug into different postings implementations: the postings writer/reader are actually responsible for encoding and decoding the postings metadata and term metadata sections described here:  postings metadata --&gt; header, packedblocksize term metadata --&gt; (docfpdelta|singletondocid), posfpdelta?, posvintblockfpdelta?, payfpdelta?, skipfpdelta? header, --&gt; {@link codecutil#writeheader codecheader} packedblocksize, singletondocid --&gt; {@link dataoutput#writevint vint} docfpdelta, posfpdelta, payfpdelta, posvintblockfpdelta, skipfpdelta --&gt; {@link dataoutput#writevlong vlong}  notes:  header is a {@link codecutil#writeheader codecheader} storing the version information for the postings. packedblocksize is the fixed block size for packed blocks. in packed block, bit width is determined by the largest integer. smaller block size result in smaller variance among width of integers hence smaller indexes. larger block size result in more efficient bulk i/o hence better acceleration. this value should always be a multiple of 64, currently fixed as 128 as a tradeoff. it is also the skip interval used to accelerate {@link docsenum#advance(int)}. docfpdelta determines the position of this term's termfreqs within the .doc file. in particular, it is the difference of file offset between this term's data and previous term's data (or zero, for the first term in the block).on disk it is stored as the difference from previous value in sequence.  posfpdelta determines the position of this term's termpositions within the .pos file. while payfpdelta determines the position of this term's &lt;termpayloads, termoffsets?&gt; within the .pay file. similar to docfpdelta, it is the difference between two file positions (or neglected, for fields that omit payloads and offsets). posvintblockfpdelta determines the position of this term's last termposition in last pos packed block within the .pos file. it is synonym for payvintblockfpdelta or offsetvintblockfpdelta. this is actually used to indicate whether it is necessary to load following payloads and offsets from .pos instead of .pay. every time a new block of positions are to be loaded, the postingsreader will use this value to check whether current block is packed format or vint. when packed format, payloads and offsets are fetched from .pay, otherwise from .pos. (this value is neglected when total number of positions i.e. totaltermfreq is less or equal to packedblocksize). skipfpdelta determines the position of this term's skipdata within the .doc file. in particular, it is the length of the termfreq data. skipdelta is only stored if docfreq is not smaller than skipminimum (i.e. 8 in lucene41postingsformat). singletondocid is an optimization when a term only appears in one document. in this case, instead of writing a file pointer to the .doc file (docfpdelta), and then a vintblock at that location, the single document id is written to the term dictionary.       term index the .tip file contains an index into the term dictionary, so that it can be accessed randomly. see {@link blocktreetermswriter} for more details on the format.      frequencies and skip data the .doc file contains the lists of documents which contain each term, along with the frequency of the term in that document (except when frequencies are omitted: {@link indexoptions#docs_only}). it also saves skip data to the beginning of each packed or vint block, when the length of document list is larger than packed block size.  docfile(.doc) --&gt; header, &lt;termfreqs, skipdata?&gt;termcount header --&gt; {@link codecutil#writeheader codecheader} termfreqs --&gt; &lt;packedblock&gt; packeddocblocknum, vintblock?  packedblock --&gt; packeddocdeltablock, packedfreqblock? vintblock --&gt; &lt;docdelta[, freq?]&gt;docfreq-packedblocksizepackeddocblocknum skipdata --&gt; &lt;&lt;skiplevellength, skiplevel&gt; numskiplevels-1, skiplevel&gt;, skipdatum? skiplevel --&gt; &lt;skipdatum&gt; trimmeddocfreq/(packedblocksize^(level + 1)) skipdatum --&gt; docskip, docfpskip, &lt;posfpskip, posblockoffset, paylength?, payfpskip?&gt;?, skipchildlevelpointer? packeddocdeltablock, packedfreqblock --&gt; {@link packedints packedints} docdelta, freq, docskip, docfpskip, posfpskip, posblockoffset, paybyteupto, payfpskip --&gt; {@link dataoutput#writevint vint} skipchildlevelpointer --&gt; {@link dataoutput#writevlong vlong}  notes:  packeddocdeltablock is theoretically generated from two steps:  calculate the difference between each document number and previous one, and get a d-gaps list (for the first document, use absolute value);  for those d-gaps from first one to packeddocblocknumpackedblocksizeth, separately encode as packed blocks.  if frequencies are not omitted, packedfreqblock will be generated without d-gap step.  vintblock stores remaining d-gaps (along with frequencies when possible) with a format that encodes docdelta and freq: docdelta: if frequencies are indexed, this determines both the document number and the frequency. in particular, docdelta/2 is the difference between this document number and the previous document number (or zero when this is the first document in a termfreqs). when docdelta is odd, the frequency is one. when docdelta is even, the frequency is read as another vint. if frequencies are omitted, docdelta contains the gap (not multiplied by 2) between document numbers and no frequency information is stored. for example, the termfreqs for a term which occurs once in document seven and three times in document eleven, with frequencies indexed, would be the following sequence of vints: 15, 8, 3 if frequencies were omitted ({@link indexoptions#docs_only}) it would be this sequence of vints instead: 7,4  packeddocblocknum is the number of packed blocks for current term's docids or frequencies. in particular, packeddocblocknum = floor(docfreq/packedblocksize)  trimmeddocfreq = docfreq % packedblocksize == 0 ? docfreq - 1 : docfreq. we use this trick since the definition of skip entry is a little different from base interface. in {@link multilevelskiplistwriter}, skip data is assumed to be saved for skipintervalth, 2skipintervalth ... posting in the list. however, in lucene41postingsformat, the skip data is saved for skipinterval+1th, 2skipinterval+1th ... posting (skipinterval==packedblocksize in this case). when docfreq is multiple of packedblocksize, multilevelskiplistwriter will expect one more skip data than lucene41skipwriter.  skipdatum is the metadata of one skip entry. for the first block (no matter packed or vint), it is omitted. docskip records the document number of every packedblocksizeth document number in the postings (i.e. last document number in each packed block). on disk it is stored as the difference from previous value in the sequence.  docfpskip records the file offsets of each block (excluding )posting at packedblocksize+1th, 2packedblocksize+1th ... , in docfile. the file offsets are relative to the start of current term's termfreqs. on disk it is also stored as the difference from previous skipdatum in the sequence. since positions and payloads are also block encoded, the skip should skip to related block first, then fetch the values according to in-block offset. posfpskip and payfpskip record the file offsets of related block in .pos and .pay, respectively. while posblockoffset indicates which value to fetch inside the related block (payblockoffset is unnecessary since it is always equal to posblockoffset). same as docfpskip, the file offsets are relative to the start of current term's termfreqs, and stored as a difference sequence. paybyteupto indicates the start offset of the current payload. it is equivalent to the sum of the payload lengths in the current block up to posblockoffset       positions the .pos file contains the lists of positions that each term occurs at within documents. it also sometimes stores part of payloads and offsets for speedup.  posfile(.pos) --&gt; header, &lt;termpositions&gt; termcount header --&gt; {@link codecutil#writeheader codecheader} termpositions --&gt; &lt;packedposdeltablock&gt; packedposblocknum, vintblock?  vintblock --&gt; &lt;positiondelta[, payloadlength?], payloaddata?, offsetdelta?, offsetlength?&gt;posvintcount packedposdeltablock --&gt; {@link packedints packedints} positiondelta, offsetdelta, offsetlength --&gt; {@link dataoutput#writevint vint} payloaddata --&gt; {@link dataoutput#writebyte byte}paylength  notes:  termpositions are order by term (terms are implicit, from the term dictionary), and position values for each term document pair are incremental, and ordered by document number. packedposblocknum is the number of packed blocks for current term's positions, payloads or offsets. in particular, packedposblocknum = floor(totaltermfreq/packedblocksize)  posvintcount is the number of positions encoded as vint format. in particular, posvintcount = totaltermfreq - packedposblocknumpackedblocksize the procedure how packedposdeltablock is generated is the same as packeddocdeltablock in chapter frequencies and skip data. positiondelta is, if payloads are disabled for the term's field, the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first occurrence in this document). if payloads are enabled for the term's field, then positiondelta/2 is the difference between the current and the previous position. if payloads are enabled and positiondelta is odd, then payloadlength is stored, indicating the length of the payload at the current term position. for example, the termpositions for a term which occurs as the fourth term in one document, and as the fifth and ninth term in a subsequent document, would be the following sequence of vints (payloads disabled): 4, 5, 4 payloaddata is metadata associated with the current term position. if payloadlength is stored at the current position, then it indicates the length of this payload. if payloadlength is not stored, then this payload has the same length as the payload at the previous position. offsetdelta/2 is the difference between this position's startoffset from the previous occurrence (or zero, if this is the first occurrence in this document). if offsetdelta is odd, then the length (endoffset-startoffset) differs from the previous occurrence and an offsetlength follows. offset data is only written for {@link indexoptions#docs_and_freqs_and_positions_and_offsets}.       payloads and offsets the .pay file will store payloads and offsets associated with certain term-document positions. some payloads and offsets will be separated out into .pos file, for performance reasons.  payfile(.pay): --&gt; header, &lt;termpayloads, termoffsets?&gt; termcount header --&gt; {@link codecutil#writeheader codecheader} termpayloads --&gt; &lt;packedpaylengthblock, sumpaylength, paydata&gt; packedpayblocknum termoffsets --&gt; &lt;packedoffsetstartdeltablock, packedoffsetlengthblock&gt; packedpayblocknum packedpaylengthblock, packedoffsetstartdeltablock, packedoffsetlengthblock --&gt; {@link packedints packedints} sumpaylength --&gt; {@link dataoutput#writevint vint} paydata --&gt; {@link dataoutput#writebyte byte}sumpaylength  notes:  the order of termpayloads/termoffsets will be the same as termpositions, note that part of payload/offsets are stored in .pos. the procedure how packedpaylengthblock and packedoffsetlengthblock are generated is the same as packedfreqblock in chapter frequencies and skip data. while packedstartdeltablock follows a same procedure as packeddocdeltablock. packedpayblocknum is always equal to packedposblocknum, for the same term. it is also synonym for packedoffsetblocknum. sumpaylength is the total length of payloads written within one block, should be the sum of paylengths in one packed block. paylength in packedpaylengthblock is the length of each payload associated with the current position.     @lucene.experimental filename extension for document number, frequencies, and skip data. see chapter: frequencies and skip data filename extension for positions. see chapter: positions filename extension for payloads and offsets. see chapter: payloads and offsets fixed packed block size, number of integers encoded in a single packed block. creates {@code lucene41postingsformat} with default settings. creates {@code lucene41postingsformat} with custom values for {@code minblocksize} and {@code maxblocksize} passed to block terms dictionary. @see blocktreetermswriter#blocktreetermswriter(segmentwritestate,postingswriterbase,int,int) note: must be multiple of 64 because of packedints long-aligned encoding/decoding"
org.apache.lucene.codecs.lucene41.ForUtil "encode all values in normal area with fixed bit width, which is determined by the max value in this block. special number of bits per value used whenever all values to encode are equal. upper limit of the number of bytes that might be required to stored block_size encoded values. upper limit of the number of values that might be decoded in a single call to {@link #readblock(indexinput, byte[], int[])}. although values after block_size are garbage, it is necessary to allocate value buffers whose size is >= max_data_size to avoid {@link arrayindexoutofboundsexception}s. compute the number of iterations required to decode block_size values with the provided {@link decoder}. compute the number of bytes required to encode a block of values that require bitspervalue bits per value with format format. create a new {@link forutil} instance and save state into out. restore a {@link forutil} from a {@link datainput}. write a block of data (for format). the data to write a buffer to use to encode data the destination output @throws ioexception if there is a low-level i/o error read the next block of data (for format). the input to use to read data a buffer that can be used to store encoded data where to write decoded data @throws ioexception if there is a low-level i/o error skip the next block of data. the input where to read data @throws ioexception if there is a low-level i/o error compute the number of bits required to serialize any of the longs in data."
org.apache.lucene.codecs.lucene3x.Lucene3xStoredFieldsReader "class responsible for access to stored document fields.  it uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files. @deprecated only for reading existing 3.x indexes extension of stored fields file extension of stored fields index file returns a cloned fieldsreader that shares open indexinputs with the original one. it is the caller's job not to close the original fieldsreader until all clones are called (eg, currently segmentreader manages this logic). verifies that the code version which wrote the segment is supported. @throws alreadyclosedexception if this fieldsreader is closed closes the underlying {@link org.apache.lucene.store.indexinput} streams. this means that the fields values will not be accessible. @throws ioexception if there is a low-level i/o error. lucene 3.0: removal of compressed fields lucene 3.2: numericfields are stored in binary format note: if you introduce a new format, make it 1 higher than the current one, and always change this if you switch to a new format! when removing support for old versions, leave the last supported version here note: bit 0 is free here! you can steal it! the old bit 1 << 2 was compressed, is now left out the docid offset where our docs begin in the index file. this will be 0 if we have our own private file. when we are inside a compound share doc store (cfx), (lucene 3.0 indexes only), we privately open our own fd. used only by clone we read only a slice out of this shared fields file verify the file is long enough to hold all of our docs verify two sources of "maxdoc" agree: with lock-less commits, it's entirely possible (and fine) to hit a filenotfound exception above. in this case, we want to explicitly close any subset of things that were opened so that we don't have to wait for a gc to do so. keep our original exception"
org.apache.lucene.codecs.lucene3x.TermInfosReaderIndex "this stores a monotonically increasing set of  pairs in an index segment. pairs are accessed either by term or by ordinal position the set. the terms and terminfo are actually serialized and stored into a byte array and pointers to the position of each are stored in a int array. @deprecated only for reading existing 3.x indexes loads the segment information at segment load time. the term enum. the index divisor. the size of the tii file, used to approximate the size of the buffer. the total index interval. binary search for the given term. the term to locate. @throws ioexception if there is a low-level i/o error. gets the term at the given position. for testing. the position to read the term from the index. term. @throws ioexception if there is a low-level i/o error. returns the number of terms. . the compares the given term against the term in the index specified by the term index. ie it returns negative n when term is less than index term; the given term. the index of the of term to compare. . @throws ioexception if there is a low-level i/o error. compare the fields of the terms first, and if not equals return from compare. if equal compare terms. the term to compare. the position of the term in the input to compare the input buffer. . @throws ioexception if there is a low-level i/o error. compares the fields before checking the text of the terms. the given term. the term that exists in the data block. the data block. . @throws ioexception if there is a low-level i/o error. 256 kb block this is only an inital size, it will be gced once the build is complete read the term read the terminfo perform the seek read the term if term field does not equal mid's field index, then compare fields else if they are equal, compare term's string values..."
org.apache.lucene.codecs.lucene3x.Lucene3xCodec "supports the lucene 3.x index format (readonly) @deprecated only for reading existing 3.x indexes extension of compound file for doc store files returns file names for shared doc stores, if any, else null. todo: this should really be a different impl 3.x doesn't support docvalues"
org.apache.lucene.codecs.lucene3x.Lucene3xTermVectorsReader "@deprecated only for reading existing 3.x indexes extension of vectors fields file extension of vectors documents file extension of vectors index file number of documents in the reader ignored note: if you make a new format, it must be larger than the current format changed strings to utf8 with length-in-bytes not length-in-chars note: always change this if you switch to a new format! whenever you add a new format, make it 1 larger (positive version logic)! when removing support for old versions, leave the last supported version here the size in bytes that the format_version will take up at the beginning of each file the docid offset where our docs begin in the index file. this will be 0 if we have our own private file. when we are inside a compound share doc store (cfx), (lucene 3.0 indexes only), we privately open our own fd. todo: if we are worried, maybe we could eliminate the extra fd somehow when you also have vectors... used by clone verify the file is long enough to hold all of our docs with lock-less commits, it's entirely possible (and fine) to hit a filenotfound exception above. in this case, we want to explicitly close any subset of things that were opened so that we don't have to wait for a gc to do so. keep our original exception not private to avoid synthetic access$nnn methods todo: we can improve writer here, eg write 0 into tvx file, so we know on first read from tvx that this doc has no tvs no such field term vectors were not indexed for this field every term occurs in just one doc: note: tvf is pre-positioned by caller lucene-1542 correction note: slow! (linear scan) note: sort of a silly class, since you can get the freq() already by termsenum.totaltermfreq todo: we can improve writer here, eg write 0 into tvx file, so we know on first read from tvx that this doc has no tvs these are null when a termvectorsreader was on a segment that did not have term vectors saved if this returns, we do the surrogates shuffle so that the terms are sorted by unicode sort order. this should be true when segments are used for "normal" searching; it's only false during testing, to create a pre-flex index, using the test-only preflexrw."
org.apache.lucene.codecs.lucene3x.Lucene3xNormsFormat "lucene3x readonly normsformat implementation @deprecated (4.0) this is only used to read indexes"
org.apache.lucene.codecs.lucene3x.SegmentTermPositions "@lucene.experimental @deprecated (4.0) segmenttermpositions(segmentreader p) { super(p); this.proxstream = null; // the proxstream will be cloned lazily when nextposition() is called for the first time } called by super.skipto(). the current payload length indicates whether the payload of the current position has been read from the proxstream yet these variables are being used to remember information for a lazy skip the proxstream will be cloned lazily when nextposition() is called for the first time the proxstream will be cloned lazily when nextposition() is called for the first time this field does not store positions, payloads perform lazy skips if necessary if the current field stores payloads then the position delta is shifted one bit to the left. if the lsb is set, then we have to read the current payload length lucene-1542 correction we remember to skip a document lazily we remember to skip the remaining positions of the current document lazily run super note frequency reset position we save the pointer, we might have to skip there lazily skip unread positions it is not always necessary to move the prox pointer to a new document after the freq pointer has been moved. consider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. but only if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. so we move the prox pointer lazily to the document as soon as positions are requested. clone lazily we might have to skip the current payload if it was not read yet no payload read payloads lazily"
org.apache.lucene.codecs.lucene3x.Lucene3xTermVectorsFormat "lucene3x readonly termvectorsformat implementation @deprecated (4.0) this is only used to read indexes unfortunately, for 3.x indices, each segment's fieldinfos can lie about hasvectors (claim it's true when really it's false).... so we have to carefully check if the files really exist before trying to open them (4.x has fixed this): 3x's fieldinfos sometimes lies and claims a segment has vectors when it doesn't:"
org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat "codec that reads the pre-flex-indexing postings format. it does not provide a writer because newly written segments should use the codec configured on indexwriter. @deprecated (4.0) this is only used to read indexes extension of terms file extension of terms index file extension of freq postings file extension of prox postings file"
org.apache.lucene.codecs.lucene3x.Lucene3xStoredFieldsFormat "@deprecated only for reading existing 3.x indexes"
org.apache.lucene.codecs.lucene3x.Lucene3xFields "exposes flex api on a pre-flex index, as a codec. @lucene.experimental @deprecated (4.0) note: we must always load terms index, even for "sequential" scan during merging, because what is sequential to merger may not be to terminfosreader since we do the surrogates dance: make sure that all index files have been read or are kept open so that if an index update removes them we'll still have them with lock-less commits, it's entirely possible (and fine) to hit a filenotfound exception above. in this case, we want to explicitly close any subset of things that were opened so that we don't have to wait for a gc to do so. if this returns, we do the surrogates dance so that the terms are sorted by unicode sort order. this should be true when segments are used for "normal" searching; it's only false during testing, to create a pre-flex index, using the test-only preflexrw. pre-flex indexes always sorted in utf16 order, but we remap on-the-fly to unicode order preflex doesn't support this returns true if the unicode char is "after" the surrogates in utf16, ie >= u+e000 and = pos + 3: "term.length=" + term.length + " pos+3=" + (pos+3) + " byte=" + integer.tohexstring(term.bytes[pos]) + " term=" + term.tostring(); save the bytes && length, since we need to restore this if seek "back" finds no matching terms seek "back": test if the term we seek'd to in fact found a surrogate pair at the same position as the e: cannot be null (or move to next field) because at "worst" it'd seek to the same term we are on now, unless we are being called from seek now test if prefix is identical and we found a non-bmp char at the same position: restore term: seek type 2 "continue" (back to the start of the surrogates): scan the stripped suffix from the prior term, backwards. if there was an e in that part, then we try to seek back to s. if that seek finds a matching term, we go there. todo: more efficient seek? newsuffixstart = downto+4; shorten prevterm in place so that we don't redo this loop if we come back here: look for seek type 3 ("pop"): if the delta from prev -> current was replacing an s with an e, we must now seek to beyond that e. this seek "finishes" the dance at this character position. seek type 2 -- put 0xff at this position: todo: more efficient seek? can we simply swap the enums? we could hit eof or different field since this was a seek "forward": set newsuffixstart -- we can't use termenum's since the above seek may have done no scanning (eg, term was precisely and index term, or, was in the term seek cache): pre-flex indices store terms in utf16 sort order, but certain queries require unicode codepoint order; this method carefully seeks around surrogates to handle this impedance mismatch we are invoked after tis.next() (by utf16 order) to possibly seek to a different "next" (by unicode order) term. we scan only the "delta" from the last term to the current term, in utf8 bytes. we look at 1) the bytes stripped from the prior term, and then 2) the bytes appended to that prior term's prefix. we don't care about specific utf8 sequences, just the "category" of the utf16 character. category s is a high/low surrogate pair (it non-bmp). category e is any bmp char > uni_sur_low_end (and   look for seek type 1 ("push"): if the newly added suffix contains any s, we must try to seek to the corresponding e. if we find a match, we go there; else we keep looking for additional s's in the new suffix. this "starts" the dance, at this character position: a non-bmp char (4 bytes utf8) starts here: seek "forward": todo: more efficient seek? did we find a match? since this was a seek "forward", we could hit eof or a different field: ok seek "back" todo: more efficient seek? +3 because we don't need to check the char at upto: we know it's > bmp note: we keep iterating, now, since this can easily "recurse". ie, after seeking forward at a certain char position, we may find another surrogate in our [new] suffix and must then do another seek (recurse) system.out.println("pff.reset te=" + termenum); system.out.println(" term=" + termenum.term()); pre-flex indexes always sorted in utf16 order, but we remap on-the-fly to unicode order if we found an exact match, no need to do the surrogate dance todo: maybe we can handle this like the next() into null? set term as prevterm then dance? we hit eof; try end-case surrogate dance: if we find an e, try swapping in s, backwards: found a match todo: faster seek? we found a non-exact but non-null term; this one is fun -- just treat it like next, by pretending requested term was prev: preflex codec interns field names; verify: preflex codec interns field names: todo: can we use ste's prevbuffer here? preflex codec interns field names; verify: this field is exhausted, but we have to give surrogatedance a chance to seek back: newsuffixstart = prevterm.length; preflex codec interns field names; verify:"
org.apache.lucene.codecs.lucene3x.Lucene3xSkipListReader "@deprecated (4.0) this is only used to read indexes returns the freq pointer of the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. returns the prox pointer of the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. returns the payload length of the payload stored just before the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. the current field stores payloads. if the doc delta is odd then we have to read the current payload length because it differs from the length of the previous payload"
org.apache.lucene.codecs.lucene3x.Lucene3xFieldInfosReader "@lucene.experimental @deprecated only for reading existing 3.x indexes extension of field infos first used in 2.9; prior to 2.9 there was no format header first used in 3.4: omit only positional information read in the size lucene-3027: past indices were able to write storepayloads=true when omittfap is also true, which is invalid. we correct that, here:"
org.apache.lucene.codecs.lucene3x.TermInfo "a terminfo is the record of information stored for a term @deprecated (4.0) this class is no longer used in flexible indexing. the number of documents which contain the term."
org.apache.lucene.codecs.lucene3x.Lucene3xNormsProducer "reads lucene 3.x norms format and exposes it via docvalues api @lucene.experimental @deprecated only for reading existing 3.x indexes norms header placeholder extension of norms file extension of separate norms file any .nrm or .snn files we have open at any time. todo: just a list, and double-close() separate norms files? points to a singlenormfile note: just like segmentreader in 3.x, we open up all the files here (including separate norms) up front. but we just don't do any seeks or reading yet. separate norms are never inside cfs skip header (header unused for now) singlenormfile means multiple norms share this file all norms in the .nrm file can share a single indexinput since they are only used in a synchronized context. if this were to change in the future, a clone could be done here. if the segment was and don't need to do the sketchy file size check. otherwise, we check if the size is exactly equal to maxdoc to detect a headerless file. note: remove this check in lucene 5.0! increment also if some norms are separate todo: change to a real check? see lucene-3619 single file for all norms some norms share fds we are done with this file"
org.apache.lucene.codecs.lucene3x.Lucene3xFieldInfosFormat "lucene3x readonly fieldinfosfromat implementation @deprecated (4.0) this is only used to read indexes"
org.apache.lucene.codecs.lucene3x.TermInfosReader "this stores a monotonically increasing set of  pairs in a directory. pairs are accessed either by term or by ordinal position the set @deprecated (4.0) this class has been replaced by formatpostingstermsdictreader, except for reading old segments. @lucene.experimental per-thread resources managed by threadlocal returns the number of term/value pairs in the set. returns the terminfo for a term in the set, or null. returns the terminfo for a term in the set, or null. returns the position of a term in the set or -1. returns an enumeration of all the terms and terminfos in the set. returns an enumeration of terms starting at or after the named term. just adds term's ord to terminfo load terms index do not load terms index: with lock-less commits, it's entirely possible (and fine) to hit a filenotfound exception above. in this case, we want to explicitly close any subset of things that were opened so that we don't have to wait for a gc to do so. optimize sequential access: first try scanning cached enum w/o seeking term is at or past current but before end of block no need to seek we only want to put this terminfo into the cache if scanenum skipped more than one dictionary entry. this prevents rangequeries or wildcardqueries to wipe out the cache when they iterate over a large numbers of terms in order random-access: must seek must do binary search: called only from asserts skipoffset is only valid when docfreq >= skipinterval:"
org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoReader "lucene 3x implementation of {@link segmentinforeader}. @lucene.experimental @deprecated only for reading existing 3.x indexes reads from legacy 3.x segments_n read version read counter read segmentinfos could be a 3.0 - try to open the doc stores - if it fails, it's a 2.x segment, and an indexformattoooldexception will be thrown, which is what we want. if we opened the directory, close it above call succeeded, so it's a 3.0 segment. upgrade it so the next time the segment is read, its version won't be null and we won't need to open fieldsreader every time for each such segment. if it's a 3x index touched by 3.1+ code, then segments record their version, whether they are 2.x ones or not. we detect that and throw appropriate exception. note: this is not how 3.x is really written... check that it is a format we can understand parse the docstore stuff and shove it into attributes pre-4.0 indexes write a byte if there is a single norms file system.out.println("version=" + version + " name=" + name + " doccount=" + doccount + " delgen=" + delgen + " dso=" + docstoreoffset + " dss=" + docstoresegment + " dsscfs=" + docstoreiscompoundfile + " b=" + b + " format=" + format); note: unused replicate logic from 3.x's segmentinfo.files(): parse the normgen stuff and shove it into attributes definitely a separate norm file, with generation: no separate norm we should have already hit indexformat too old exception"
org.apache.lucene.codecs.lucene3x.SegmentTermEnum "@deprecated (4.0) no longer used with flex indexing, except for reading old segments @lucene.experimental increments the enumeration to the next element. true if one exists. optimized scan, without allocating new terms. return number of invocations to next(). note: lucene-3183: if you pass term("", "") here then this will incorrectly return before positioning the enum, and position will be -1; caller must detect this. returns the current term in the enumeration. initially invalid, valid after next() called for the first time. returns the previous term enumerated. initially null. returns the current terminfo in the enumeration. initially invalid, valid after next() called for the first time. sets the argument to the current terminfo in the enumeration. initially invalid, valid after next() called for the first time. returns the docfreq from the current terminfo in the enumeration. initially invalid, valid after next() called for the first time. returns the freqpointer from the current terminfo in the enumeration. initially invalid, valid after next() called for the first time. returns the proxpointer from the current terminfo in the enumeration. initially invalid, valid after next() called for the first time. closes the enumeration to further activity, freeing resources. changed strings to true utf8 with length-in-bytes not length-in-chars note: always change this if you switch to a new format! whenever you add a new format, make it 1 smaller (negative version logic)! when removing support for old versions, leave the last supported version here used for scanning use single-level skip lists for formats > -3 original-format file, without explicit format version number back-compatible settings switch off skipto optimization we have a format version number check that it is a format we can understand read the size system.out.println(" ste doseek prev=" + prevbuffer.toterm() + " this=" + this); system.out.println(" ste setprev=" + prev() + " this=" + this); system.out.println(" eof"); read doc freq read freq pointer read prox pointer read index pointer system.out.println(" ste ret term=" + term()); always force initial next() in case term is term("", "")"
org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoFormat "lucene3x readonly segmentinfoformat implementation @deprecated (4.0) this is only used to read indexes this format adds optional per-segment string diagnostics storage, and switches each segment records whether it has term vectors each segment records the lucene version that extension used for saving each segmentinfo, once a 3.x index is first committed to with 4.0. this segment shares stored fields & vectors, this offset is where in that file this segment's docs begin used to derive fields/vectors file we share with other segments doc store files are stored in compound file (.cfx) only for backwards compat"
org.apache.lucene.codecs.lucene3x.TermBuffer "@lucene.experimental @deprecated (4.0) cached cannot be -1 since (strangely) we write that fieldnumber into index for first indexed term: only valid right after .read is called fields are interned (only by preflex codec) invalidate cache note: too much sneakiness here, seriously this is a negative vint?! dangerous to copy term over, since the underlying bytesref could subsequently be modified: unset"
org.apache.lucene.codecs.lucene3x.SegmentTermDocs "@deprecated (4.0) @lucene.experimental optimized implementation. overridden by segmenttermpositions to skip in prox stream. optimized implementation. protected segmentreader parent; use comparison of fieldinfos to verify that termenum belongs to the same segment as this segmenttermdocs optimized case punt case shift off low bit if low bit is set freq is one else read freq manually inlined call to next() for speed shift off low bit if low bit is set freq is one else read freq manually inlined call to next() for speed hardware freq to 1 when term freqs were not stored in the index don't skip if the target is close (within skipinterval docs away) optimized case lazily clone lazily initialize skip stream done skipping, now just scan"
org.apache.lucene.codecs.MultiLevelSkipListWriter "this abstract class writes skip lists with multiple levels.  example for skipinterval = 3: c (skip level 2) c c c (skip level 1) x x x x x x x x x x (skip level 0) d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d (posting list) 3 6 9 12 15 18 21 24 27 30 (df) d - document x - skip data c - skip data with child pointer skip level i contains every skipinterval-th entry from skip level i-1. therefore the number of entries on level i is: floor(df / ((skipinterval ^ (i + 1))). each skip entry on a level i>0 contains a pointer to the corresponding skip entry in list i-1. this guarantees a logarithmic amount of skips to find the target document. while this class takes care of writing the different skip levels, subclasses must define the actual format of the skip data.  @lucene.experimental number of levels in this skip list the skip interval in the list with level = 0 skipinterval used for level &gt; 0 for every skip level a different buffer is used creates a {@code multilevelskiplistwriter}. creates a {@code multilevelskiplistwriter}, where {@code skipinterval} and {@code skipmultiplier} are the same. allocates internal skip buffers. creates new buffers or empties the existing ones subclasses must implement the actual skip data encoding in this method. the level skip data shall be writing for the skip buffer to write to writes the current skip data to the buffers. the current document frequency determines the max level is skip data is to be written to. the current document frequency @throws ioexception if an i/o error occurs writes the buffered skip lists to the given output. the indexoutput the skip lists shall be written to pointer the skip list starts calculate the maximum number of skip levels for this document frequency make sure it does not exceed maxskiplevels determine max level store child pointers for all levels except the lowest remember the childpointer for the next level system.out.println("skipper.writeskip fp=" + skippointer);"
org.apache.lucene.codecs.lucene40.Lucene40PostingsReader "concrete class that reads the 4.0 frq/prox postings format. @see lucene40postingsformat @deprecated only for reading old 4.0 segments sole constructor. reads but does not decode the byte[] blob holding metadata for the current terms block if (debug) { system.out.println(" df=" + termstate.docfreq); system.out.println(" freqfp=" + termstate.freqoffset); } returns the payload at this position, or null if no payload was indexed. returns the payload at this position, or null if no payload was indexed. private static boolean debug = blocktreetermswriter.debug; increment version to change it: public static boolean debug = blocktreetermswriter.debug; private string segment; todo: hasprox should (somehow!) become codec private, but it's tricky because 1) fis.hasprox is global (it could be all fields that have prox are written by a different codec), 2) the field may have had prox in the past but all docs w/ that field were deleted. really we'd need to init prxout lazily on write, and then somewhere record that we actually wrote it so we know whether to open on read: make sure we are talking to the matching past writer must keep final because we do non-standard clone only used by the "primary" termstate -- clones don't copy this (basically they are "transient"): todo: should this not be in the termstate...? do not copy bytes, bytesreader (else termstate is very heavy, ie drags around the entire block's byte[]). on seek back, if next() is in fact used (rare!), they will be re-read from disk. if (debug) system.out.println(" spr.readtermsblock bytes=" + len + " ts=" + _termstate); if (debug) system.out.println("spr: nextterm seg=" + segment + " tbord=" + termstate.termblockord + " bytesreader.fp=" + termstate.bytesreader.getposition()); if (debug) system.out.println(" skipoffset=" + termstate.skipoffset + " vs freqin.length=" + freqin.length()); undefined if (debug) system.out.println(" proxfp=" + termstate.proxoffset); if (debug) system.out.println("spr.docs ts=" + termstate); if you are using parellelreader, and pass in a reused docsenum, it could have come from another reader also using standard codec we only reuse if the the actual the incoming enum has the same livedocs as the given livedocs todo: can we optimize if flag_payloads / flag_offsets isn't passed? todo: refactor if you are using parellelreader, and pass in a reused docsenum, it could have come from another reader also using standard codec if you are using parellelreader, and pass in a reused docsenum, it could have come from another reader also using standard codec reuse reuse reuse - lazy loaded does current field omit term freq? does current field store payloads? does current field store offsets? number of docs in this posting how many docs we've read doc we last read accumulator for doc deltas freq we last read todo: for full enum case (eg segment merging) this seek is unnecessary; maybe we can avoid in such cases if (debug) system.out.println(" sde limit=" + limit + " freqfp=" + freqoffset); last doc in our buffer is >= target, binary search + next() 32 seemed to be a sweetspot here so use binsearch if the pending results are a lot buffer is consumed if low bit is set freq is one else read freq shift off low bit there are enough docs in the posting to have skip data, and it isn't too close. this is the first time this enum has ever been used for skipping -- do lazy init this is the first time this posting has skipped since reset() was called, so now we load the skip data for this posting skipper moved shift off low bit shift off low bit shift off low bit shift off low bit todo specialize docsandposenum too decodes docs & positions. payloads nor offsets are present. number of docs in this posting how many docs we've read doc we last read accumulator for doc deltas freq we last read todo: for full enum case (eg segment merging) this seek is unnecessary; maybe we can avoid in such cases if (debug) system.out.println("standardr.d&pe reset seg=" + segment + " limit=" + limit + " freqfp=" + freqoffset + " proxfp=" + proxoffset); if (debug) system.out.println("spr.nextdoc seg=" + segment + " freqin.fp=" + freqin.getfilepointer()); if (debug) system.out.println(" return end"); decode next doc/freq pair shift off low bit if low bit is set freq is one else read freq if (debug) system.out.println(" return doc=" + doc); system.out.println("standardr.d&pe advance target=" + target); there are enough docs in the posting to have skip data, and it isn't too close this is the first time this enum has ever been used for skipping -- do lazy init this is the first time this posting has skipped, since reset() was called, so now we load the skip data for this posting skipper moved now, linear scan for the rest: scan over any docs that were iterated without their positions decodes docs & positions & (payloads and/or offsets) number of docs in this posting how many docs we've read doc we last read accumulator for doc deltas freq we last read todo: for full enum case (eg segment merging) this seek is unnecessary; maybe we can avoid in such cases system.out.println("standardr.d&pe reset seg=" + segment + " limit=" + limit + " freqfp=" + freqoffset + " proxfp=" + proxoffset + " this=" + this); system.out.println("standardr.d&pe seg=" + segment + " nextdoc return doc=end"); decode next doc/freq pair shift off low bit if low bit is set freq is one else read freq system.out.println("standardr.d&pe nextdoc seg=" + segment + " return doc=" + doc); system.out.println("standardr.d&pe advance seg=" + segment + " target=" + target + " this=" + this); there are enough docs in the posting to have skip data, and it isn't too close this is the first time this enum has ever been used for skipping -- do lazy init this is the first time this posting has skipped, since reset() was called, so now we load the skip data for this posting system.out.println(" init skipper freqoffset=" + freqoffset + " skipoffset=" + skipoffset + " vs len=" + freqin.length()); skipper moved now, linear scan for the rest: payload of last position was never retrieved -- skip it scan over any docs that were iterated without their positions new payload length new offset length system.out.println("standardr.d&pe skippos"); read next position payload wasn't retrieved for last position new payload length new offset length system.out.println("standardr.d&pe nextpos return pos=" + position);"
org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader "lucene 4.0 fieldinfos reader. @lucene.experimental @see lucene40fieldinfosformat sole constructor. read in the size lucene-3027: past indices were able to write storepayloads=true when omittfap is also true, which is invalid. we correct that, here: dv types are packed in one byte"
org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat "lucene 4.0 term vectors format. term vector support is an optional on a field by field basis. it consists of 3 files.   the document index or .tvx file. for each document, this stores the offset into the document data (.tvd) and field data (.tvf) files. documentindex (.tvx) --&gt; header,&lt;documentposition,fieldposition&gt; numdocs  header --&gt; {@link codecutil#writeheader codecheader} documentposition --&gt; {@link dataoutput#writelong uint64} (offset in the .tvd file) fieldposition --&gt; {@link dataoutput#writelong uint64} (offset in the .tvf file)    the document or .tvd file. this contains, for each document, the number of fields, a list of the fields with term vector info and finally a list of pointers to the field information in the .tvf (term vector fields) file. the .tvd file is used to map out the fields that have term vectors stored and where the field information is in the .tvf file. document (.tvd) --&gt; header,&lt;numfields, fieldnums, fieldpositions&gt; numdocs  header --&gt; {@link codecutil#writeheader codecheader} numfields --&gt; {@link dataoutput#writevint vint} fieldnums --&gt; &lt;fieldnumdelta&gt; numfields fieldnumdelta --&gt; {@link dataoutput#writevint vint} fieldpositions --&gt; &lt;fieldpositiondelta&gt; numfields-1 fieldpositiondelta --&gt; {@link dataoutput#writevlong vlong}    the field or .tvf file. this file contains, for each field that has a term vector stored, a list of the terms, their frequencies and, optionally, position, offset, and payload information. field (.tvf) --&gt; header,&lt;numterms, flags, termfreqs&gt; numfields  header --&gt; {@link codecutil#writeheader codecheader} numterms --&gt; {@link dataoutput#writevint vint} flags --&gt; {@link dataoutput#writebyte byte} termfreqs --&gt; &lt;termtext, termfreq, positions?, payloaddata?, offsets?&gt; numterms termtext --&gt; &lt;prefixlength, suffix&gt; prefixlength --&gt; {@link dataoutput#writevint vint} suffix --&gt; {@link dataoutput#writestring string} termfreq --&gt; {@link dataoutput#writevint vint} positions --&gt; &lt;positiondelta payloadlength?&gt;termfreq positiondelta --&gt; {@link dataoutput#writevint vint} payloadlength --&gt; {@link dataoutput#writevint vint} payloaddata --&gt; {@link dataoutput#writebyte byte}numpayloadbytes offsets --&gt; &lt;{@link dataoutput#writevint vint}, {@link dataoutput#writevint vint}&gt;termfreq  notes:  flags byte stores whether this term vector has position, offset, payload. information stored. term byte prefixes are shared. the prefixlength is the number of initial bytes from the previous term which must be pre-pended to a term's suffix in order to form the term's bytes. thus, if the previous term's text was "bone" and the term is "boy", the prefixlength is two and the suffix is "y". positiondelta is, if payloads are disabled for the term's field, the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first occurrence in this document). if payloads are enabled for the term's field, then positiondelta/2 is the difference between the current and the previous position. if payloads are enabled and positiondelta is odd, then payloadlength is stored, indicating the length of the payload at the current term position. payloaddata is metadata associated with a term position. if payloadlength is stored at the current position, then it indicates the length of this payload. if payloadlength is not stored, then this payload has the same length as the payload at the previous position. payloaddata encodes the concatenated bytes for all of a terms occurrences. offsets are stored as delta encoded vints. the first vint is the startoffset, the second is the endoffset.    sole constructor. javadocs"
org.apache.lucene.codecs.lucene40.Lucene40SkipListReader "implements the skip list reader for the 4.0 posting list format that stores positions and payloads. @see lucene40postingsformat @deprecated only for reading old 4.0 segments sole constructor. per-term initialization. returns the freq pointer of the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. returns the prox pointer of the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. returns the payload length of the payload stored just before the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. returns the offset length (endoffset-startoffset) of the position stored just before the doc to which the last call of {@link multilevelskiplistreader#skipto(int)} has skipped. the current field stores payloads and/or offsets. if the doc delta is odd then we have to read the current payload/offset lengths because it differs from the lengths of the previous payload/offset"
org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat "lucene 4.0 postings format.  files:  .tim: term dictionary .tip: term index .frq: frequencies .prx: positions     term dictionary the .tim file contains the list of terms in each field along with per-term statistics (such as docfreq) and pointers to the frequencies, positions and skip data in the .frq and .prx files. see {@link blocktreetermswriter} for more details on the format.  note: the term dictionary can plug into different postings implementations: the postings writer/reader are actually responsible for encoding and decoding the postings metadata and term metadata sections described here:  postings metadata --&gt; header, skipinterval, maxskiplevels, skipminimum term metadata --&gt; freqdelta, skipdelta?, proxdelta? header --&gt; {@link codecutil#writeheader codecheader} skipinterval,maxskiplevels,skipminimum --&gt; {@link dataoutput#writeint uint32} skipdelta,freqdelta,proxdelta --&gt; {@link dataoutput#writevlong vlong}  notes:  header is a {@link codecutil#writeheader codecheader} storing the version information for the postings. skipinterval is the fraction of termdocs stored in skip tables. it is used to accelerate {@link docsenum#advance(int)}. larger values result in smaller indexes, greater acceleration, but fewer accelerable cases, while smaller values result in bigger indexes, less acceleration (in case of a small value for maxskiplevels) and more accelerable cases.  maxskiplevels is the max. number of skip levels stored for each term in the .frq file. a low value results in smaller indexes but less acceleration, a larger value results in slightly larger indexes but greater acceleration. see format of .frq file for more information about skip levels. skipminimum is the minimum document frequency a term must have in order to write any skip data at all. freqdelta determines the position of this term's termfreqs within the .frq file. in particular, it is the difference between the position of this term's data in that file and the position of the previous term's data (or zero, for the first term in the block). proxdelta determines the position of this term's termpositions within the .prx file. in particular, it is the difference between the position of this term's data in that file and the position of the previous term's data (or zero, for the first term in the block. for fields that omit position data, this will be 0 since prox information is not stored. skipdelta determines the position of this term's skipdata within the .frq file. in particular, it is the number of bytes after termfreqs that the skipdata starts. in other words, it is the length of the termfreq data. skipdelta is only stored if docfreq is not smaller than skipminimum.   term index the .tip file contains an index into the term dictionary, so that it can be accessed randomly. see {@link blocktreetermswriter} for more details on the format.  frequencies the .frq file contains the lists of documents which contain each term, along with the frequency of the term in that document (except when frequencies are omitted: {@link indexoptions#docs_only}).  freqfile (.frq) --&gt; header, &lt;termfreqs, skipdata?&gt; termcount header --&gt; {@link codecutil#writeheader codecheader} termfreqs --&gt; &lt;termfreq&gt; docfreq termfreq --&gt; docdelta[, freq?] skipdata --&gt; &lt;&lt;skiplevellength, skiplevel&gt; numskiplevels-1, skiplevel&gt; &lt;skipdatum&gt; skiplevel --&gt; &lt;skipdatum&gt; docfreq/(skipinterval^(level + 1)) skipdatum --&gt; docskip,payloadlength?,offsetlength?,freqskip,proxskip,skipchildlevelpointer? docdelta,freq,docskip,payloadlength,offsetlength,freqskip,proxskip --&gt; {@link dataoutput#writevint vint} skipchildlevelpointer --&gt; {@link dataoutput#writevlong vlong}  termfreqs are ordered by term (the term is implicit, from the term dictionary). termfreq entries are ordered by increasing document number. docdelta: if frequencies are indexed, this determines both the document number and the frequency. in particular, docdelta/2 is the difference between this document number and the previous document number (or zero when this is the first document in a termfreqs). when docdelta is odd, the frequency is one. when docdelta is even, the frequency is read as another vint. if frequencies are omitted, docdelta contains the gap (not multiplied by 2) between document numbers and no frequency information is stored. for example, the termfreqs for a term which occurs once in document seven and three times in document eleven, with frequencies indexed, would be the following sequence of vints: 15, 8, 3 if frequencies were omitted ({@link indexoptions#docs_only}) it would be this sequence of vints instead: 7,4 docskip records the document number before every skipinterval th document in termfreqs. if payloads and offsets are disabled for the term's field, then docskip represents the difference from the previous value in the sequence. if payloads and/or offsets are enabled for the term's field, then docskip/2 represents the difference from the previous value in the sequence. in this case when docskip is odd, then payloadlength and/or offsetlength are stored indicating the length of the last payload/offset before the skipintervalth document in termpositions. payloadlength indicates the length of the last payload. offsetlength indicates the length of the last offset (endoffset-startoffset).  freqskip and proxskip record the position of every skipinterval th entry in freqfile and proxfile, respectively. file positions are relative to the start of termfreqs and positions, to the previous skipdatum in the sequence. for example, if docfreq=35 and skipinterval=16, then there are two skipdata entries, containing the 15 th and 31 st document numbers in termfreqs. the first freqskip names the number of bytes after the beginning of termfreqs that the 16 th skipdatum starts, and the second the number of bytes after that that the 32 nd starts. the first proxskip names the number of bytes after the beginning of positions that the 16 th skipdatum starts, and the second the number of bytes after that that the 32 nd starts. each term can have multiple skip levels. the amount of skip levels for a term is numskiplevels = min(maxskiplevels, floor(log(docfreq/log(skipinterval)))). the number of skipdata entries for a skip level is docfreq/(skipinterval^(level + 1)), whereas the lowest skip level is level=0. example: skipinterval = 4, maxskiplevels = 2, docfreq = 35. then skip level 0 has 8 skipdata entries, containing the 3rd, 7th, 11th, 15th, 19th, 23rd, 27th, and 31st document numbers in termfreqs. skip level 1 has 2 skipdata entries, containing the 15th and 31st document numbers in termfreqs. the skipdata entries on all upper levels &gt; 0 contain a skipchildlevelpointer referencing the corresponding skipdata entry in level-1. in the example has entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a pointer to entry 31 on level 0.   positions the .prx file contains the lists of positions that each term occurs at within documents. note that fields omitting positional data do not store anything into this file, and if all fields in the index omit positional data then the .prx file will not exist.  proxfile (.prx) --&gt; header, &lt;termpositions&gt; termcount header --&gt; {@link codecutil#writeheader codecheader} termpositions --&gt; &lt;positions&gt; docfreq positions --&gt; &lt;positiondelta,payloadlength?,offsetdelta?,offsetlength?,payloaddata?&gt; freq positiondelta,offsetdelta,offsetlength,payloadlength --&gt; {@link dataoutput#writevint vint} payloaddata --&gt; {@link dataoutput#writebyte byte}payloadlength  termpositions are ordered by term (the term is implicit, from the term dictionary). positions entries are ordered by increasing document number (the document number is implicit from the .frq file). positiondelta is, if payloads are disabled for the term's field, the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first occurrence in this document). if payloads are enabled for the term's field, then positiondelta/2 is the difference between the current and the previous position. if payloads are enabled and positiondelta is odd, then payloadlength is stored, indicating the length of the payload at the current term position. for example, the termpositions for a term which occurs as the fourth term in one document, and as the fifth and ninth term in a subsequent document, would be the following sequence of vints (payloads disabled): 4, 5, 4 payloaddata is metadata associated with the current term position. if payloadlength is stored at the current position, then it indicates the length of this payload. if payloadlength is not stored, then this payload has the same length as the payload at the previous position. offsetdelta/2 is the difference between this position's startoffset from the previous occurrence (or zero, if this is the first occurrence in this document). if offsetdelta is odd, then the length (endoffset-startoffset) differs from the previous occurrence and an offsetlength follows. offset data is only written for {@link indexoptions#docs_and_freqs_and_positions_and_offsets}. @deprecated only for reading old 4.0 segments minimum items (terms or sub-blocks) per block for blocktree maximum items (terms or sub-blocks) per block for blocktree creates {@code lucene40postingsformat} with default settings. creates {@code lucene40postingsformat} with custom values for {@code minblocksize} and {@code maxblocksize} passed to block terms dictionary. @see blocktreetermswriter#blocktreetermswriter(segmentwritestate,postingswriterbase,int,int) extension of freq postings file extension of prox postings file javadocs javadocs javadocs javadocs javadocs todo: this class could be blocktreetermsdict around lucene40postingsbaseformat; ie we should not duplicate the code from that class here:"
org.apache.lucene.codecs.lucene40.Lucene40DocValuesProducer "lucene 4.0 perdocproducer implementation that uses compound file. @see lucene40docvaluesformat @lucene.experimental maps field name to {@link docvalues} instance. creates a new {@link lucene40docvaluesproducer} instance and loads all {@link docvalues} instances for this segment and codec."
org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat "lucene 4.0 segment info format.  files:  .si: header, segversion, segsize, iscompoundfile, diagnostics, attributes, files   data types:   header --&gt; {@link codecutil#writeheader codecheader} segsize --&gt; {@link dataoutput#writeint int32} segversion --&gt; {@link dataoutput#writestring string} files --&gt; {@link dataoutput#writestringset set&lt;string&gt;} diagnostics, attributes --&gt; {@link dataoutput#writestringstringmap map&lt;string,string&gt;} iscompoundfile --&gt; {@link dataoutput#writebyte int8}   field descriptions:   segversion is the code version that sole constructor. file extension used to store {@link segmentinfo}. javadocs javadocs javadocs javadocs"
org.apache.lucene.codecs.lucene40.Lucene40TermVectorsWriter "lucene 4.0 term vectors writer.  it writes .tvd, .tvf, and .tvx files. @see lucene40termvectorsformat sole constructor. do a bulk copy of numdocs documents from reader to our streams. this is used to expedite merging, if the field numbers are congruent. maximum number of contiguous documents to bulk-copy when merging term vectors close all streams. todo: make a new 4.0 tv format that encodes better - use startoffset (not endoffset) as base for delta on next startoffset because today for syns or ngrams or wdf or shingles etc. we are encoding negative vints (= slow, 5 bytes per) - if doc has no term vectors, write 0 into the tvx file; saves a seek to tvd only to read a 0 vint (and saves a byte in tvd) open files for termvector storage pointers to the tvf before writing each field number of fields we have written so far for this document total number of fields we will write for this document force first payload to write its length note: we override addprox, so we don't need to buffer when indexing. we also don't buffer during bulk merges. we might need to buffer if its a non-bulk merge force first payload to write its length used only by this optimized flush below todo, maybe overkill and just call super.addprox() in this case? we do avoid buffering the offsets in ram though. pure positions, no payloads write position delta buffer offsets write position delta write offset deltas dump buffer we overflowed the payload buffer, just throw uoe having > integer.max_value bytes of payload for a single term in a single doc is nuts. used for bulk-reading raw bytes for term vectors we can bulk-copy because the fieldinfos are "congruent" skip deleted docs we can optimize this case (doing a bulk byte copy) since the field numbers are identical skip deleted docs note: it's very important to first assign to vectors then pass it to termvectorswriter.addalldocvectors; see lucene-1282 we can bulk-copy because the fieldinfos are "congruent" note: it's very important to first assign to vectors then pass it to termvectorswriter.addalldocvectors; see lucene-1282 this is most likely a bug in sun jre 1.6.0_04/_05; we detect that the bug has struck, here, and throw an exception to prevent the corruption from entering the index. see lucene-1282 for details. make an effort to close all streams we can but remember and re-throw the first exception encountered in this process"
org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat "lucene 4.0 live documents format.  the .del file is optional, and only exists when a segment contains deletions. although per-segment, this file is maintained exterior to compound segment files. deletions (.del) --&gt; format,header,bytecount,bitcount, bits | dgaps (depending on format)  format,bytesize,bitcount --&gt; {@link dataoutput#writeint uint32} bits --&gt; &lt;{@link dataoutput#writebyte byte}&gt; bytecount dgaps --&gt; &lt;dgap,nononesbyte&gt; nonzerobytescount dgap --&gt; {@link dataoutput#writevint vint} nononesbyte --&gt; {@link dataoutput#writebyte byte} header --&gt; {@link codecutil#writeheader codecheader}  format is 1: indicates cleared dgaps. bytecount indicates the number of bytes in bits. it is typically (segsize/8)+1. bitcount indicates the number of bits that are currently set in bits. bits contains one bit for each document indexed. when the bit corresponding to a document number is cleared, that document is marked as deleted. bit ordering is from least to most significant. thus, if bits contains two bytes, 0x00 and 0x02, then document 9 is marked as alive (not deleted). dgaps represents sparse bit-vectors more efficiently than bits. it is made of dgaps on indexes of nonones bytes in bits, and the nonones bytes themselves. the number of nonones bytes in bits (nononesbytescount) is not stored. for example, if there are 8000 bits and only bits 10,12,32 are cleared, dgaps would be used: (vint) 1 , (byte) 20 , (vint) 3 , (byte) 1 extension of deletes sole constructor. javadocs"
org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter "class responsible for writing stored document fields.  it uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files. @see lucene40storedfieldsformat @lucene.experimental extension of stored fields file extension of stored fields index file sole constructor. bulk write a contiguous series of documents. the lengths array is the length (in bytes) of each raw document. the stream indexinput is the fieldsstream from which we should bulk-copy all bytes. maximum number of contiguous documents to bulk-copy when merging stored fields note: bit 0 is free here! you can steal it! the old bit 1 << 2 was compressed, is now left out the next possible bits are: 1 << 6; 1 << 7 currently unused: static final int field_is_numeric_short = 5 << _numeric_bit_shift; currently unused: static final int field_is_numeric_byte = 6 << _numeric_bit_shift; writes the contents of buffer into the fields stream and adds a new entry for this document into the index stream. this assumes the buffer was already written in the correct fields format. todo: maybe a field should serialize itself? this way we don't bake into indexer all these specific encodings for different fields? and apps can customize... this is most likely a bug in sun jre 1.6.0_04/_05; we detect that the bug has struck, here, and throw an exception to prevent the corruption from entering the index. see lucene-1282 for details. used for bulk-reading raw bytes for stored fields we can only bulk-copy if the matching reader is also a lucene40fieldsreader we can bulk-copy because the fieldinfos are "congruent" skip deleted docs we can optimize this case (doing a bulk byte copy) since the field numbers are identical skip deleted docs todo: this could be more efficient using fieldvisitor instead of loading/writing entire doc; ie we just have to renumber the field number on the fly? note: it's very important to first assign to doc then pass it to fieldswriter.adddocument; see lucene-1282 we can bulk-copy because the fieldinfos are "congruent" note: it's very important to first assign to doc then pass it to fieldswriter.adddocument; see lucene-1282"
org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoWriter "lucene 4.0 implementation of {@link segmentinfowriter}. @see lucene40segmentinfoformat @lucene.experimental sole constructor. save a single segment's info. write the lucene version that"
org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat "lucene 4.0 stored fields format. stored fields are represented by two files:   the field index, or .fdx file. this is used to find the location within the field data file of the fields of a particular document. because it contains fixed-length data, this file may be easily randomly accessed. the position of document n 's field data is the {@link dataoutput#writelong uint64} at n8 in this file. this contains, for each document, a pointer to its field data, as follows:  fieldindex (.fdx) --&gt; &lt;header&gt;, &lt;fieldvaluesposition&gt; segsize header --&gt; {@link codecutil#writeheader codecheader} fieldvaluesposition --&gt; {@link dataoutput#writelong uint64}    the field data, or .fdt file. this contains the stored fields of each document, as follows:  fielddata (.fdt) --&gt; &lt;header&gt;, &lt;docfielddata&gt; segsize header --&gt; {@link codecutil#writeheader codecheader} docfielddata --&gt; fieldcount, &lt;fieldnum, bits, value&gt; fieldcount fieldcount --&gt; {@link dataoutput#writevint vint} fieldnum --&gt; {@link dataoutput#writevint vint} bits --&gt; {@link dataoutput#writebyte byte}  low order bit reserved. second bit is one for fields containing binary data third bit reserved. 4th to 6th bit (mask: 0x7&lt;&lt;3) define the type of a numeric field:  all bits in mask are cleared if no numeric field at all 1&lt;&lt;3: value is int 2&lt;&lt;3: value is long 3&lt;&lt;3: value is int as float (as of {@link float#intbitstofloat(int)} 4&lt;&lt;3: value is long as double (as of {@link double#longbitstodouble(long)}    value --&gt; string | binaryvalue | int | long (depending on bits) binaryvalue --&gt; valuesize, &lt;{@link dataoutput#writebyte byte}&gt;^valuesize valuesize --&gt; {@link dataoutput#writevint vint}    @lucene.experimental sole constructor. javadocs"
org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoReader "lucene 4.0 implementation of {@link segmentinforeader}. @see lucene40segmentinfoformat @lucene.experimental sole constructor."
org.apache.lucene.codecs.lucene40.Lucene40NormsFormat "lucene 4.0 norms format.  files:  .nrm.cfs: {@link compoundfiledirectory compound container} .nrm.cfe: {@link compoundfiledirectory compound entries}  norms are implemented as docvalues, so other than file extension, norms are written exactly the same way as {@link lucene40docvaluesformat docvalues}. @see lucene40docvaluesformat @lucene.experimental sole constructor. lucene 4.0 perdocproducer implementation that uses compound file. @see lucene40docvaluesformat sole constructor. lucene 4.0 perdocconsumer implementation that uses compound file. @see lucene40docvaluesformat @lucene.experimental sole constructor. javadocs"
org.apache.lucene.codecs.lucene40.Lucene40FieldInfosWriter "lucene 4.0 fieldinfos writer. @see lucene40fieldinfosformat @lucene.experimental extension of field infos sole constructor. returns the byte used to encode the {@link type} for each field. pack the dv types in one byte"
org.apache.lucene.codecs.lucene40.values.Ints "stores ints packed and fixed with fixed-bit precision. @lucene.experimental codec name, written in the header. initial version. current version. sole constructor. creates and returns a {@link docvaluesconsumer} to write int values. creates and returns a {@link docvalues} to read previously written int values. only bulk merge if value type is the same otherwise size differs"
org.apache.lucene.codecs.lucene40.values.FixedStraightBytesImpl "@lucene.experimental simplest storage: stores fixed length byte[] per document, with no dedup and no sorting. start at -1 if the first added value is > 0 todo should we add a transfer to api to each reader? fills up to but not including this docid indexing path - no disk io until here merge path - datout should be initialized specialized version for single bytes"
org.apache.lucene.codecs.lucene40.values.FixedSortedBytesImpl "@lucene.experimental stores fixed-length byte[] by deref, ie when two docs have the same value, they store only 1 byte[] important that we get doccount, in case there were some last docs that we didn't see first dump bytes data, recording address as we go"
org.apache.lucene.codecs.lucene40.values.VarSortedBytesImpl "@lucene.experimental stores variable-length byte[] by deref, ie when two docs have the same value, they store only 1 byte[] and both docs reference that single source allow var bytes sizes important that we get doccount, in case there were some last docs that we didn't see total bytes of data first dump bytes data, recording index & write offset as we go todo: we could prefix code... write sentinel write index the last value here is just a dummy value to get the length of the last value the last value here is just a dummy value to get the length of the last value advance this iterator to the end and clone the stream once it points to the doctoordindex header read the ords in to prevent too many random disk seeks 1+ord is safe because we write a sentinel at the end"
org.apache.lucene.codecs.lucene40.values.VarDerefBytesImpl "@lucene.experimental todo: if impls like this are merged we are bound to the amount of memory we can store into a bytesrefhash and therefore how much memory a byteblockpool can address. this is currently limited to 2gb. while we could extend that and use 64bit for addressing this still limits us to the existing main memory as all distinct bytes will be loaded up into main memory. we could move the byte[] writing to #finish(int) and store the bytes in sorted order and merge them in a streamed fashion. stores variable-length byte[] by deref, ie when two docs have the same value, they store only 1 byte[] and both docs reference that single source allow var bytes sizes important that we get doccount, in case there were some last docs that we didn't see write the max address to read directly on source load length is 1 byte"
org.apache.lucene.codecs.lucene40.values.DirectSource "base class for disk resident source implementations @lucene.internal"
org.apache.lucene.codecs.lucene40.values.Bytes "base class for specific bytes reader/writer implementations provides concrete writer/reader implementations for byte[] value per document. there are 6 package-private default implementations of this, for all combinations of {@link mode#deref}/{@link mode#straight} x fixed-length/variable-length.  note: currently the total amount of byte[] data stored (across a single segment) cannot exceed 2gb.   note: each byte[] must be  @lucene.experimental don't instantiate! defines the {@link writer}s store mode. the writer will either store the bytes sequentially ({@link #straight}, dereferenced ({@link #deref}) or sorted ({@link #sorted}) @lucene.experimental mode for sequentially stored bytes mode for dereferenced stored bytes mode for sorted stored bytes creates a new byte[] {@link writer} instances for the given directory. the directory to write the values to the id used to create a unique file name. usually composed out of the segment name and a unique id per segment. the writers store mode true if all bytes subsequently passed to the {@link writer} will have the same length {@link bytesref} comparator used by sorted variants. if null {@link bytesref#getutf8sortedasunicodecomparator()} is used instead an {@link atomiclong} instance to track the used bytes within the {@link writer}. a call to {@link writer#finish(int)} will release all internally used resources and frees the memory tracking reference. how to trade space for speed. this option is only applicable for docvalues of type {@link type#bytes_fixed_sorted} and {@link type#bytes_var_sorted}. i/o context new {@link writer} instance @see packedints#getreader(org.apache.lucene.store.datainput) creates a new {@link docvalues} instance that provides either memory resident or iterative access to a per-document stored byte[] value. the returned {@link docvalues} instance will be initialized without consuming a significant amount of memory. the directory to load the {@link docvalues} from. the file id in the {@link directory} to load the values from. the mode used to store the values true iff the values are stored with fixed-size, otherwise false the number of document values stored for the given id {@link bytesref} comparator used by sorted variants. if null {@link bytesref#getutf8sortedasunicodecomparator()} is used instead initialized {@link docvalues} instance. @throws ioexception if an {@link ioexception} occurs opens all necessary files, but does not read any data in until you call {@link #loadsource}. clones and returns the data {@link indexinput} clones and returns the indexing {@link indexinput} todo - add bulk copy where possible todo -- i shouldn't have to specify fixed? can track itself & do the write thing at write time? todo -- i can peek @ header to determing fixed/mode? todo open up this api? todo: open up this api?! default value - skip it important that we get doccount, in case there were some last docs that we didn't see"
org.apache.lucene.codecs.lucene40.values.VarStraightBytesImpl "@lucene.experimental variable length byte[] per document, no sharing start at -1 if the first added value is > 0 init fills up to but not including this docid default bulk merge since we don't have any deletes this is the address after all addr pointers are updated default header is already written in getdataout() doccount+1 so we write sentinel write sentinel safe to do 1+docid because we write sentinel at the end:"
org.apache.lucene.codecs.lucene40.values.Writer "abstract api for per-document stored primitive values of type byte[] , long or double. the api accepts a single value for each document. the underlying storage mechanism, file formats, data-structures and representations depend on the actual implementation.  document ids passed to this api must always be increasing unless stated otherwise.  @lucene.experimental creates a new {@link writer}. bytes-usage tracking reference used by implementation to track internally allocated memory. all tracked bytes must be released once {@link #finish(int)} has been called. factory method to create a {@link writer} instance for a given type. this method returns default implementations for each of the different types defined in the {@link type} enumeration. the {@link type} to create the {@link writer} for the file name id used to create files within the writer. the {@link directory} to create the files from. a byte-usage tracking reference how to trade space for speed. this option is only applicable for docvalues of type {@link type#bytes_fixed_sorted} and {@link type#bytes_var_sorted}. new {@link writer} instance for the given {@link type} @see packedints#getreader(org.apache.lucene.store.datainput)"
org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase "abstract base class for perdocconsumer implementations @lucene.experimental segment name to use when writing files. {@link iocontext} to use when writing files. filename extension for index files filename extension for data files. creates {@code docvalueswriterbase}, using {@link packedints#fast}. the state to initiate a {@link perdocconsumer} instance creates {@code docvalueswriterbase}. the state to initiate a {@link perdocconsumer} instance how to trade space for speed. this option is only applicable for docvalues of type {@link type#bytes_fixed_sorted} and {@link type#bytes_var_sorted}. @see packedints#getreader(org.apache.lucene.store.datainput) returns the {@link directory} that files should be written to. returns the comparator used to sort {@link bytesref} values. javadoc"
org.apache.lucene.codecs.lucene40.values.Floats "exposes {@link writer} and reader ({@link source}) for 32 bit and 64 bit floating point values.  current implementations store either 4 byte or 8 byte floating points with full precision without any compression. @lucene.experimental codec name, written in the header. initial version. current version. sole constructor. creates and returns a {@link docvaluesconsumer} to write float values. creates and returns a {@link docvalues} to read previously written float values. only bulk merge if value type is the same otherwise size differs"
org.apache.lucene.codecs.lucene40.values.FixedDerefBytesImpl "@lucene.experimental stores fixed-length byte[] by deref, ie when two docs have the same value, they store only 1 byte[]"
org.apache.lucene.codecs.lucene40.values.PackedIntValues "stores integers using {@link packedints} @lucene.experimental opens all necessary files, but does not read any data in until you call {@link #loadsource}. loads the actual values. you may call this more than once, eg if you already previously loaded but then discarded the source. if we exceed the range of positive longs we must switch to fixed ints done write a default value to recognize docs without a value for that field todo -- can we somehow avoid 2x method calls on each get? must push minvalue down, and make packedints implement ints.source"
org.apache.lucene.codecs.lucene40.Lucene40Codec "implements the lucene 4.0 index format, with configurable per-field postings formats.  if you want to reuse functionality of this codec in another codec, extend {@link filtercodec}. @see org.apache.lucene.codecs.lucene40 package documentation for file format details. @deprecated only for reading old 4.0 segments sole constructor. returns the postings format that should be used for writing new segments of field. the default implementation always returns "lucene40" note: if we make largish changes in a minor release, easier to just make lucene42codec or whatever if they are backwards compatible or smallish we can probably do the backwards in the postingsreader (it writes a minor version, etc)."
org.apache.lucene.codecs.lucene40.Lucene40TermVectorsReader "lucene 4.0 term vectors reader.  it reads .tvd, .tvf, and .tvx files. @see lucene40termvectorsformat extension of vectors fields file extension of vectors documents file extension of vectors index file used by clone. sole constructor. retrieve the length (in bytes) of the tvd and tvf entries for the next numdocs starting with startdocid. this is used for bulk copying when merging segments, if the field numbers are congruent. once this returns, the tvf & tvd streams are seeked to the startdocid. number of documents in the reader no payloads ignored with lock-less commits, it's entirely possible (and fine) to hit a filenotfound exception above. in this case, we want to explicitly close any subset of things that were opened so that we don't have to wait for a gc to do so. ensure we throw our original exception used for bulk copy when merging used for bulk copy when merging not private to avoid synthetic access$nnn methods todo: we can improve writer here, eg write 0 into tvx file, so we know on first read from tvx that this doc has no tvs no such field term vectors were not indexed for this field every term occurs in just one doc: todo: really indexer hardwires this...? i guess codec could buffer and re-sort... one shared byte[] for any term's payloads note: tvf is pre-positioned by caller note: slow! (linear scan) length change todo: we could maybe reuse last array, if we can somehow be careful about consumer never using two d&penums at once... note: sort of a silly class, since you can get the freq() already by termsenum.totaltermfreq todo: we can improve writer here, eg write 0 into tvx file, so we know on first read from tvx that this doc has no tvs these are null when a termvectorsreader was on a segment that did not have term vectors saved"
org.apache.lucene.codecs.lucene40.Lucene40PostingsBaseFormat "provides a {@link postingsreaderbase} and {@link postingswriterbase}. @deprecated only for reading old 4.0 segments sole constructor. todo: should these also be named / looked up via spi?"
org.apache.lucene.codecs.lucene40.BitVector "optimized implementation of a vector of bits. this is more-or-less like java.util.bitset, but also includes the following:  a count() method, which efficiently computes the number of one bits; optimized read from and write to disk; inlinable get() method; store and load, as bit set or d-gaps, depending on sparseness;  @lucene.internal constructs a vector capable of holding n bits. sets the value of bit to one. sets the value of bit to true, and returns true if bit was already set sets the value of bit to zero. returns true if bit is one and false if it is zero. returns the number of bits in this vector. this is also one greater than the number of the largest valid bit number. returns the total number of one bits in this vector. this is efficiently computed and cached, so that, if the vector is not changed, no recomputation is done for repeated calls. for testing writes this vector to the file name in directory d, in a format that can be read by the constructor {@link #bitvector(directory, string, iocontext)}. invert all bits set all bits write as a bit set write as a d-gaps list indicates if the bit vector is sparse and should be saved as a d-gaps list, or dense, and should be saved as a bit set. constructs a bit vector from the file name in directory d, as written by the {@link #write} method. read as a bit set read as a d-gaps list read as a d-gaps cleared bits list pkg-private: if this thing is generally useful then it can go back in .util, but the serialization must be here underneath the codec. if the vector has been modified sum bits per byte sum bits per byte table of bits/byte version before version tracking was added: first version: changed dgaps to encode gaps between cleared bits, not set: increment version to change it: sparse bit-set more efficiently saved as d-gaps. take care not to invert the "unused" bits in the last byte: write size write count mark using d-gaps write size write count expected number of bytes for vint encoding of each gap +1 because we write the byte itself that contains the set bit note: adding 32 because we start with ((int) -1) to indicate d-gaps format. note: factor is for read/write of byte-arrays being faster than vints. new format, with full header & version: asserts only read count allocate bits (re)read size read count allocate bits (re)read size read count allocate bits"
org.apache.lucene.codecs.lucene40.Lucene40DocValuesConsumer "lucene 4.0 perdocconsumer implementation that uses compound file. @see lucene40docvaluesformat @lucene.experimental segment suffix used when writing doc values index files. sole constructor. todo maybe we should enable a global cfs that all codecs can pull on demand to further reduce the number of files? lazy init ignore todo: why the inconsistency here? we do this, but not simpletext (which says ifd will do it). todo: check that ifd really does this always, even if codec abort() throws a runtimeexception (e.g. threadinterruptedexception)"
org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat "lucene 4.0 docvalues format.  files:  .dv.cfs: {@link compoundfiledirectory compound container} .dv.cfe: {@link compoundfiledirectory compound entries}  entries within the compound file:  &lt;segment&gt;_&lt;fieldnumber&gt;.dat: data values &lt;segment&gt;_&lt;fieldnumber&gt;.idx: index into the .dat for deref types   there are several many types of {@link docvalues} with different encodings. from the perspective of filenames, all types store their values in .dat entries within the compound file. in the case of dereferenced/sorted types, the .dat actually contains only the unique values, and an additional .idx file contains pointers to these unique values.  formats:  {@link type#var_ints var_ints} .dat --&gt; header, packedtype, minvalue, defaultvalue, packedstream {@link type#fixed_ints_8 fixed_ints_8} .dat --&gt; header, valuesize, {@link dataoutput#writebyte byte}maxdoc {@link type#fixed_ints_16 fixed_ints_16} .dat --&gt; header, valuesize, {@link dataoutput#writeshort short}maxdoc {@link type#fixed_ints_32 fixed_ints_32} .dat --&gt; header, valuesize, {@link dataoutput#writeint int32}maxdoc {@link type#fixed_ints_64 fixed_ints_64} .dat --&gt; header, valuesize, {@link dataoutput#writelong int64}maxdoc {@link type#float_32 float_32} .dat --&gt; header, valuesize, float32maxdoc {@link type#float_64 float_64} .dat --&gt; header, valuesize, float64maxdoc {@link type#bytes_fixed_straight bytes_fixed_straight} .dat --&gt; header, valuesize, ({@link dataoutput#writebyte byte} valuesize)maxdoc {@link type#bytes_var_straight bytes_var_straight} .idx --&gt; header, maxaddress, addresses {@link type#bytes_var_straight bytes_var_straight} .dat --&gt; header, totalbytes, addresses, ({@link dataoutput#writebyte byte} variable valuesize)maxdoc {@link type#bytes_fixed_deref bytes_fixed_deref} .idx --&gt; header, numvalues, addresses {@link type#bytes_fixed_deref bytes_fixed_deref} .dat --&gt; header, valuesize, ({@link dataoutput#writebyte byte} valuesize)numvalues {@link type#bytes_var_deref bytes_var_deref} .idx --&gt; header, totalvarbytes, addresses {@link type#bytes_var_deref bytes_var_deref} .dat --&gt; header, (lengthprefix + {@link dataoutput#writebyte byte} variable valuesize)numvalues {@link type#bytes_fixed_sorted bytes_fixed_sorted} .idx --&gt; header, numvalues, ordinals {@link type#bytes_fixed_sorted bytes_fixed_sorted} .dat --&gt; header, valuesize, ({@link dataoutput#writebyte byte} valuesize)numvalues {@link type#bytes_var_sorted bytes_var_sorted} .idx --&gt; header, totalvarbytes, addresses, ordinals {@link type#bytes_var_sorted bytes_var_sorted} .dat --&gt; header, ({@link dataoutput#writebyte byte} variable valuesize)numvalues  data types:  header --&gt; {@link codecutil#writeheader codecheader} packedtype --&gt; {@link dataoutput#writebyte byte} maxaddress, minvalue, defaultvalue --&gt; {@link dataoutput#writelong int64} packedstream, addresses, ordinals --&gt; {@link packedints} valuesize, numvalues --&gt; {@link dataoutput#writeint int32} float32 --&gt; 32-bit float encoded with {@link float#floattorawintbits(float)} then written as {@link dataoutput#writeint int32} float64 --&gt; 64-bit float encoded with {@link double#doubletorawlongbits(double)} then written as {@link dataoutput#writelong int64} totalbytes --&gt; {@link dataoutput#writevlong vlong} totalvarbytes --&gt; {@link dataoutput#writelong int64} lengthprefix --&gt; length of the data value as {@link dataoutput#writevint vint} (maximum of 2 bytes)  notes:  packedtype is a 0 when compressed, 1 when the stream is written as 64-bit integers. addresses stores pointers to the actual byte location (indexed by docid). in the var_straight case, each entry can have a different length, so to determine the length, docid+1 is retrieved. a sentinel address is written at the end for the var_straight case, so the addresses stream contains maxdoc+1 indices. for the deduplicated var_deref case, each length is encoded as a prefix to the data itself as a {@link dataoutput#writevint vint} (maximum of 2 bytes). ordinals stores the term id in sorted order (indexed by docid). in the fixed_sorted case, the address into the .dat can be computed from the ordinal as header+valuesize+(ordinalvaluesize) because the byte length is fixed. in the var_sorted case, there is double indirection (docid -> ordinal -> address), but an additional sentinel ordinal+address is always written (so there are numvalues+1 ordinals). to determine the length, ord+1's address is looked up as well. {@link type#bytes_var_straight bytes_var_straight} in contrast to other straight variants uses a .idx file to improve lookup perfromance. in contrast to {@link type#bytes_var_deref bytes_var_deref} it doesn't apply deduplication of the document values.   sole constructor. javadocs javadocs javadocs javadocs javadocs"
org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsReader "class responsible for access to stored document fields.  it uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files. @see lucene40storedfieldsformat @lucene.internal returns a cloned fieldsreader that shares open indexinputs with the original one. it is the caller's job not to close the original fieldsreader until all clones are called (eg, currently segmentreader manages this logic). used only by clone. sole constructor. @throws alreadyclosedexception if this fieldsreader is closed closes the underlying {@link org.apache.lucene.store.indexinput} streams. this means that the fields values will not be accessible. @throws ioexception if an i/o error occurs returns number of documents. returns the length in bytes of each raw document in a contiguous range of length numdocs starting with startdocid. returns the indexinput (the fieldstream), already seeked to the starting point for startdocid. verify two sources of "maxdoc" agree: with lock-less commits, it's entirely possible (and fine) to hit a filenotfound exception above. in this case, we want to explicitly close any subset of things that were opened so that we don't have to wait for a gc to do so. ensure we throw our original exception"
org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat "lucene 4.0 field infos format.  field names are stored in the field info file, with suffix .fnm. fieldinfos (.fnm) --&gt; header,fieldscount, &lt;fieldname,fieldnumber, fieldbits,docvaluesbits,attributes&gt; fieldscount data types:  header --&gt; {@link codecutil#checkheader codecheader} fieldscount --&gt; {@link dataoutput#writevint vint} fieldname --&gt; {@link dataoutput#writestring string} fieldbits, docvaluesbits --&gt; {@link dataoutput#writebyte byte} fieldnumber --&gt; {@link dataoutput#writeint vint} attributes --&gt; {@link dataoutput#writestringstringmap map&lt;string,string&gt;}   field descriptions:  fieldscount: the number of fields in this file. fieldname: name of the field as a utf-8 string. fieldnumber: the field's number. note that unlike previous versions of lucene, the fields are not numbered implicitly by their order in the file, instead explicitly. fieldbits: a byte containing field options.  the low-order bit is one for indexed fields, and zero for non-indexed fields. the second lowest-order bit is one for fields that have term vectors stored, and zero for fields without term vectors. if the third lowest order-bit is set (0x4), offsets are stored into the postings list in addition to positions. fourth bit is unused. if the fifth lowest-order bit is set (0x10), norms are omitted for the indexed field. if the sixth lowest-order bit is set (0x20), payloads are stored for the indexed field. if the seventh lowest-order bit is set (0x40), term frequencies and positions omitted for the indexed field. if the eighth lowest-order bit is set (0x80), positions are omitted for the indexed field.   docvaluesbits: a byte containing per-document value types. the type recorded as two four-bit integers, with the high-order bits representing norms options, and the low-order bits representing {@link docvalues} options. each four-bit integer can be decoded as such:  0: no docvalues for this field. 1: variable-width signed integers. ({@link type#var_ints var_ints}) 2: 32-bit floating point values. ({@link type#float_32 float_32}) 3: 64-bit floating point values. ({@link type#float_64 float_64}) 4: fixed-length byte array values. ({@link type#bytes_fixed_straight bytes_fixed_straight}) 5: fixed-length dereferenced byte array values. ({@link type#bytes_fixed_deref bytes_fixed_deref}) 6: variable-length byte array values. ({@link type#bytes_var_straight bytes_var_straight}) 7: variable-length dereferenced byte array values. ({@link type#bytes_var_deref bytes_var_deref}) 8: 16-bit signed integers. ({@link type#fixed_ints_16 fixed_ints_16}) 9: 32-bit signed integers. ({@link type#fixed_ints_32 fixed_ints_32}) 10: 64-bit signed integers. ({@link type#fixed_ints_64 fixed_ints_64}) 11: 8-bit signed integers. ({@link type#fixed_ints_8 fixed_ints_8}) 12: fixed-length sorted byte array values. ({@link type#bytes_fixed_sorted bytes_fixed_sorted}) 13: variable-length sorted byte array values. ({@link type#bytes_var_sorted bytes_var_sorted})   attributes: a key-value map of codec-private attributes.  @lucene.experimental sole constructor. javadoc javadoc javadoc"
org.apache.lucene.codecs.TermVectorsReader "codec api for reading term vectors: @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns term vectors for this document, or null if term vectors were not indexed. if offsets are available they are in an {@link offsetattribute} available from the {@link docsandpositionsenum}. create a clone that one caller at a time may use to read term vectors. javadocs javadocs"
org.apache.lucene.codecs.PerDocConsumer "abstract api that consumes per document values. concrete implementations of this convert field values into a codec specific format during indexing.  the {@link perdocconsumer} api is accessible through the {@link postingsformat} - api providing per field consumers and producers for inverted data (terms, postings) as well as per-document data. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) adds a new docvaluesfield consumes and merges the given {@link perdocproducer} producer into this consumers format. returns a {@link docvalues} instance for merging from the given reader for the given {@link fieldinfo}. this method is used for merging and uses {@link atomicreader#docvalues(string)} by default.  to enable {@link docvalues} merging for different {@link docvalues} than the default override this method accordingly.  returns true iff the given field can be merged ie. has {@link docvalues}. by default this method uses {@link fieldinfo#hasdocvalues()}.  to enable {@link docvalues} merging for different {@link docvalues} than the default override this method accordingly.  returns the {@link docvalues} {@link type} for the given {@link fieldinfo}. by default this method uses {@link fieldinfo#getdocvaluestype()}.  to enable {@link docvalues} merging for different {@link docvalues} than the default override this method accordingly.  called during indexing if the indexing session is aborted due to a unrecoverable exception. this method should cleanup all resources. set the field we are merging"
org.apache.lucene.codecs.StoredFieldsWriter "codec api for writing stored fields:   for every document, {@link #startdocument(int)} is called, informing the codec how many fields will be written. {@link #writefield(fieldinfo, indexablefield)} is called for each field in the document. after all documents have been written, {@link #finish(fieldinfos, int)} is called for verification/sanity-checks. finally the writer is closed ({@link #close()})  @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) called before writing the stored fields of the document. {@link #writefield(fieldinfo, indexablefield)} will be called numstoredfields times. note that this is called even if the document has no stored fields, in this case numstoredfields will be zero. called when a document and all its fields have been added. writes a single stored field. aborts writing entirely, implementation should remove any partially-written files, etc. called before {@link #close()}, passing in the number of documents that were written. note that this is intentionally redundant (equivalent to the number of calls to {@link #startdocument(int)}, but a codec should check that this is the case to detect the jre bug described in lucene-1282. merges in the stored fields from the readers in mergestate. the default implementation skips over deleted documents, and uses {@link #startdocument(int)}, {@link #writefield(fieldinfo, indexablefield)}, and {@link #finish(fieldinfos, int)}, returning the number of documents that were written. implementations can override this method for more sophisticated merging (bulk-byte copying, etc). sugar method for startdocument() + writefield() for every stored field in the document skip deleted docs todo: this could be more efficient using fieldvisitor instead of loading/writing entire doc; ie we just have to renumber the field number on the fly? note: it's very important to first assign to doc then pass it to fieldswriter.adddocument; see lucene-1282"
org.apache.lucene.codecs.SegmentInfoFormat "expert: controls the format of the {@link segmentinfo} (segment metadata file).  @see segmentinfo @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns the {@link segmentinforeader} for reading {@link segmentinfo} instances. returns the {@link segmentinfowriter} for writing {@link segmentinfo} instances."
org.apache.lucene.codecs.TermsConsumer "abstract api that consumes terms for an individual field.  the lifecycle is:  termsconsumer is returned for each field by {@link fieldsconsumer#addfield(fieldinfo)}. termsconsumer returns a {@link postingsconsumer} for each term in {@link #startterm(bytesref)}. when the producer (e.g. indexwriter) is done adding documents for the term, it calls {@link #finishterm(bytesref, termstats)}, passing in the accumulated term statistics. producer calls {@link #finish(long, long, int)} with the accumulated collection statistics when it is finished adding terms to the field.  @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) starts a new term in this field; this may be called with no corresponding call to finish if the term had no docs. finishes the current term; numdocs must be > 0. stats.totaltermfreq will be -1 when term frequencies are omitted for the field. called when we are done adding terms to this field. sumtotaltermfreq will be -1 when term frequencies are omitted for the field. return the bytesref comparator used to sort terms before feeding to this api. default merge impl javadocs we can pass null for livedocs, because the mapping enum will skip the non-live docs: we can pass null for livedocs, because the mapping enum will skip the non-live docs: we can pass null for livedocs, because the mapping enum will skip the non-live docs: we can pass null for livedocs, because the mapping enum will skip the non-live docs:"
org.apache.lucene.codecs.PerDocProducer "abstract api that provides access to one or more per-document storage features. the concrete implementations provide access to the underlying storage on a per-document basis corresponding to their actual {@link perdocconsumer} counterpart.  the {@link perdocproducer} api is accessible through the {@link postingsformat} - api providing per field consumers and producers for inverted data (terms, postings) as well as per-document data. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns {@link docvalues} for the current field. the field name {@link docvalues} for this field or null if not applicable. @throws ioexception if an i/o error occurs"
org.apache.lucene.codecs.SegmentInfoReader "specifies an api for classes that can read {@link segmentinfo} information. @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) read {@link segmentinfo} data from a directory. directory to read from name of the segment to read instance to be populated with data @throws ioexception if an i/o error occurs"
org.apache.lucene.codecs.PostingsFormat "encodes/decodes terms, postings, and proximity data.  note, when extending this class, the name ({@link #getname}) may written into the index in certain configurations. in order for the segment to be read, the name must resolve to your implementation via {@link #forname(string)}. this method uses java's {@link serviceloader service provider interface} (spi) to resolve format names.  if you implement your own format, make sure that it has a no-arg constructor so spi can load it. @see serviceloader @lucene.experimental zero-length {@code postingsformat} array. unique name that's used to retrieve this format when reading the index. creates a new postings format.  the provided name will be written into the index segment in some configurations (such as when using {@link perfieldpostingsformat}): in such configurations, for the segment to be read this class should be registered with java's spi mechanism (registered in meta-inf/ of your jar file, etc). must be all ascii alphanumeric, and less than 128 characters in length. returns this posting format's name writes a new segment reads a segment. note: by the time this call returns, it must hold open any files it will need to use; else, those files may be deleted. additionally, required files may be deleted during the execution of this call before there is a chance to open them. under these circumstances an ioexception should be thrown by the implementation. ioexceptions are expected and will automatically cause a retry of the segment opening logic with the newly revised segments. looks up a format by name returns a list of all available format names reloads the postings format list from the given {@link classloader}. changes to the postings formats are visible after the method ends, all iterators ({@link #availablepostingsformats()},...) stay consistent. note: only new postings formats are added, existing ones are never removed or replaced. this method is expensive and should only be called for discovery of new postings formats on the given classpath/classloader! javadocs"
org.apache.lucene.codecs.PostingsConsumer "abstract api that consumes postings for an individual term.  the lifecycle is:  postingsconsumer is returned for each term by {@link termsconsumer#startterm(bytesref)}. {@link #startdoc(int, int)} is called for each document where the term occurs, specifying id and term frequency for that document. if positions are enabled for the field, then {@link #addposition(int, bytesref, int, int)} will be called for each occurrence in the document. {@link #finishdoc()} is called when the producer is done adding positions to the document.  @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) adds a new doc in this term. freq will be -1 when term frequencies are omitted for the field. add a new position & payload, and start/end offset. a null payload means no payload; a non-null payload with zero length also means no payload. caller may reuse the {@link bytesref} for the payload between calls (method must fully consume the payload). startoffset and endoffset will be -1 when offsets are not indexed. called when we are done adding positions & payloads for each doc. default merge impl: append documents, mapping around deletes"
org.apache.lucene.codecs.FieldInfosFormat "encodes/decodes {@link fieldinfos} @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) returns a {@link fieldinfosreader} to read field infos from the index returns a {@link fieldinfoswriter} to write field infos to the index javadocs"
org.apache.lucene.codecs.StoredFieldsReader "codec api for reading stored fields.  you need to implement {@link #visitdocument(int, storedfieldvisitor)} to read the stored fields for a document, implement {@link #clone()} (creating clones of any indexinputs used, etc), and {@link #close()} @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) visit the stored fields for document n"
org.apache.lucene.codecs.perfield.PerFieldPostingsFormat "enables per field format support.  note, when extending this class, the name ({@link #getname}) is written into the index. in order for the field to be read, the name must resolve to your implementation via {@link #forname(string)}. this method uses java's {@link serviceloader service provider interface} to resolve format names.  files written by each posting format have an additional suffix containing the format name. for example, in a per-field configuration instead of _1.prx filenames would look like _1_lucene40_0.prx. @see serviceloader @lucene.experimental name of this {@link postingsformat}. {@link fieldinfo} attribute name used to store the format name for each field. {@link fieldinfo} attribute name used to store the segment suffix name for each field. sole constructor. returns the postings format that should be used for writing new segments of field.  the field to format mapping is written to the index, so this method is only invoked when writing, not when reading. javadocs first time we are seeing this format; create a new instance bump the suffix we've already seen this format, so just grab its suffix todo: we should only provide the "slice" of fis that this pf actually sees ... then stuff like .hasprox could work correctly? note: .hasprox is already broken in the same way for the non-perfield case, if there is a fieldinfo with prox that has no postings, you get a 0 byte file. close all subs todo: support embedding; i think it should work but we need a test confirm to confirm return outersegmentsuffix + "_" + segmentsuffix; read _x.per and init each format: read field name -> format name null formatname means the field is in fieldinfos, but has no postings!"
org.apache.lucene.codecs.DocValuesFormat "encodes/decodes {@link docvalues} @lucene.experimental sole constructor. (for invocation by subclass constructors, typically implicit.) consumes (writes) doc values during indexing. produces (reads) doc values during reading/searching. javadocs"
org.apache.lucene.codecs.NormsFormat "format for normalization factors sole constructor. (for invocation by subclass constructors, typically implicit.) returns a {@link perdocconsumer} to write norms to the index. returns a {@link perdocproducer} to read norms from the index."
org.apache.lucene.codecs.MappingMultiDocsAndPositionsEnum "exposes flex api, merged from flex api of sub-segments, remapping docids (this is used for segment merging). @lucene.experimental sole constructor. sets the {@link mergestate}, which is used to re-map document ids. how many sub-readers we are merging. @see #getsubs returns sub-readers we are merging. compact deletions"
org.apache.lucene.util.NamedSPILoader "helper class for loading named spis from classpath (e.g. codec, postingsformat). @lucene.internal reloads the internal spi list from the given {@link classloader}. changes to the service list are visible after the method ends, all iterators ({@link #iterator()},...) stay consistent. note: only new service providers are added, existing ones are never removed or replaced. this method is expensive and should only be called for discovery of new service providers on the given classpath/classloader! validates that a service name meets the requirements of {@link namedspi} checks whether a character is a letter or digit (ascii) which are defined in the spec. interface to support {@link namedspiloader#lookup(string)} by name.  names must be all ascii alphanumeric, and less than 128 characters in length. only add the first one for each name, later services will be ignored this allows to place services before others in classpath to make them used instead of others based on harmony charset.java"
org.apache.lucene.util.FixedBitSet "bitset of fixed length (numbits), backed by accessible ({@link #getbits}) long[], accessed with an int index, implementing bits and docidset. unlike {@link openbitset} this bit set does not auto-expand, cannot handle long index, and does not have fastxx/xx variants (just x). @lucene.internal returns the number of 64 bit words it would take to hold numbits makes full copy. this docidset implementation is cacheable. expert. returns number of set bits. note: this visits every long in the backing bits array, and the result is not internally cached! returns the index of the first set bit starting at the index specified. -1 is returned if there are no more set bits. returns the index of the last set bit before or on the index specified. -1 is returned if there are no more set bits. does in-place or of the bits provided by the iterator. this = this or other does in-place and of the bits provided by the iterator. this = this and other does in-place and not of the bits provided by the iterator. this = this and not other flips a range of bits lower index one-past the last bit to flip grrr, java shifting wraps around so -1l>>>64 == -1 for that reason, make sure not to use endmask if the bits to flip will be zero in the last word (redefine endword to be the last changed...) long startmask = -1l >> (64-(endindex & 0x3f)); // example: 00111...111111 sets a range of bits lower index one-past the last bit to set clears a range of bits. lower index one-past the last bit to clear returns true if both sets have the same bits set todo: maybe merge with bitvector? problem is bitvector caches its cardinality... div 64 signed shift will keep a negative index and force an array-index-out-of-bounds-exception, removing the need for an explicit check. mod 64 div 64 mod 64 div 64 mod 64 div 64 mod 64 index within the word skip all the bits to the right of index index within the word skip all the bits to the left of index see lucene-3197 advance after last doc that would be accepted if standard iteration is used (to exhaust it): advance after last doc that would be accepted if standard iteration is used (to exhaust it): advance after last doc that would be accepted if standard iteration is used (to exhaust it): note: no .isempty() here because that's trappy (ie, typically isempty is low cost, but this one wouldn't be) example: 11111...111000 example: 00111...111111 64-(endindex&0x3f) is the same as -endindex due to wrap 64-(endindex&0x3f) is the same as -endindex due to wrap 64-(endindex&0x3f) is the same as -endindex due to wrap invert masks since we are clearing rotate left fold leftmost bits into right and add a constant to prevent empty sets from returning 0, which is too common."
org.apache.lucene.util.VirtualMethod "a utility for keeping backwards compatibility on previously abstract methods (or similar replacements). before the replacement method can be made abstract, the old method must kept deprecated. if somebody still overrides the deprecated method in a non-final class, you must keep track, of this and maybe delegate to the old method in the subclass. the cost of reflection is minimized by the following usage of this class: define static final fields in the base class ({@code baseclass}), where the old and new method are declared:  static final virtualmethod&lt;baseclass&gt; newmethod = new virtualmethod&lt;baseclass&gt;(baseclass.class, "newname", parameters...); static final virtualmethod&lt;baseclass&gt; oldmethod = new virtualmethod&lt;baseclass&gt;(baseclass.class, "oldname", parameters...);  this enforces the singleton status of these objects, as the maintenance of the cache would be too costly else. if you try to create a second instance of for the same method/{@code baseclass} combination, an exception is thrown. to detect if e.g. the old method was overridden by a more far subclass on the inheritance path to the current instance's class, use a non-static field:  final boolean isdeprecatedmethodoverridden = oldmethod.getimplementationdistance(this.getclass()) > newmethod.getimplementationdistance(this.getclass()); // alternatively (more readable): final boolean isdeprecatedmethodoverridden = virtualmethod.compareimplementationdistance(this.getclass(), oldmethod, newmethod) > 0  {@link #getimplementationdistance} returns the distance of the subclass that overrides this method. the one with the larger distance should be used preferable. this way also more complicated method rename scenarios can be handled (think of 2.9 {@code tokenstream} deprecations). @lucene.internal creates a new instance for the given {@code baseclass} and method declaration. @throws unsupportedoperationexception if you create a second instance of the same {@code baseclass} and method declaration combination. this enforces the singleton status. @throws illegalargumentexception if {@code baseclass} does not declare the given method. returns the distance from the {@code baseclass} in which this method is overridden/implemented in the inheritance path between {@code baseclass} and the given subclass {@code subclazz}. 0 iff not overridden, else the distance to the base class returns, if this method is overridden/implemented in the inheritance path between {@code baseclass} and the given subclass {@code subclazz}. you can use this method to detect if a method that should normally be final was overridden by the given instance's class. {@code false} iff not overridden utility method that compares the implementation/override distance of two methods.  &gt; 1, iff {@code m1} is overridden/implemented in a subclass of the class overriding/declaring {@code m2} &lt; 1, iff {@code m2} is overridden in a subclass of the class overriding/declaring {@code m1} 0, iff both methods are overridden in the same class (or are not overridden at all)  we have the slight chance that another thread may do the same, but who cares? lookup method, if success mark as overridden increment distance if overridden"
org.apache.lucene.util.AttributeReflector "this interface is used to reflect contents of {@link attributesource} or {@link attributeimpl}. this method gets called for every property in an {@link attributeimpl}/{@link attributesource} passing the class name of the {@link attribute}, a key and the actual value. e.g., an invocation of {@link org.apache.lucene.analysis.tokenattributes.chartermattributeimpl#reflectwith} would call this method once using {@code org.apache.lucene.analysis.tokenattributes.chartermattribute.class} as attribute class, {@code "term"} as key and the actual value as a string."
org.apache.lucene.util.ByteBlockPool "class that posting and postingvector use to write byte streams into shared fixed-size byte[] arrays. the idea is to allocate slices of increasing lengths for example, the first slice is 5 bytes, the next slice is 14, etc. we start by writing our bytes into the first 5 bytes. when we hit the end of the slice, we allocate the next slice and then write the address of the new slice into the last 4 bytes of the previous slice (the "forwarding address"). each slice is filled with 0's initially, and we mark the end with a non-zero byte. this way the methods that are writing into the slice don't need to record its length and instead allocate a new slice once they hit a non-zero byte. @lucene.internal abstract class for allocating and freeing byte blocks. a simple {@link allocator} that never recycles. a simple {@link allocator} that never recycles, but tracks how much total ram is in use. array of buffers currently used in the pool. buffers are allocated if needed don't modify this outside of this class. index into the buffers array pointing to the current buffer used as the head where we are in head buffer current head buffer current head offset resets the pool to its initial state reusing the first buffer and fills all buffers with 0 bytes before they reused or passed to {@link allocator#recyclebyteblocks(byte[][], int, int)}. calling {@link byteblockpool#nextbuffer()} is not needed after reset. expert: resets the pool to its initial state reusing the first buffer. calling {@link byteblockpool#nextbuffer()} is not needed after reset. if true the buffers are filled with 0. this should be set to true if this pool is used with slices. if true the first buffer will be reused and calling {@link byteblockpool#nextbuffer()} is not needed after reset iff the block pool was used before ie. {@link byteblockpool#nextbuffer()} was called before. advances the pool to its next buffer. this method should be called once after the constructor to initialize the pool. in contrast to the constructor a {@link byteblockpool#reset()} call will advance the pool to its first buffer immediately. allocates a new slice with the given size. @see byteblockpool#first_level_size an array holding the offset into the {@link byteblockpool#level_size_array} to quickly navigate to the next slice level. an array holding the level sizes for byte slices. the first level size for new slices @see byteblockpool#newslice(int) creates a new byte slice with the given starting size and returns the slices offset in the pool. dereferences the byte block according to {@link bytesref} offset. the offset is interpreted as the absolute offset into the {@link byteblockpool}. copies the given {@link bytesref} at the current positions ( {@link #byteupto} across buffer boundaries copies bytes from the pool starting at the given offset with the given length into the given {@link bytesref} at offset 0 and returns it. note: this method allows to copy across block boundaries. writes the pools content to the given {@link dataoutput} which buffer we are upto we allocated at least one buffer fully zero fill buffers that we fully used partial zero fill the final buffer recycle all but the first buffer re-use the first buffer size of each slice. these arrays should be at most 16 elements (index is encoded with 4 bits). first array is just a compact way to encode x+1 with a max. second array is the length of each slice, ie first slice is 5 bytes, next slice is 14 bytes, etc. maybe allocate another block copy forward the past 3 bytes (which we are about to overwrite with the forwarding address): write forwarding address at end of last slice: write new level: fill in a bytesref from term's length & bytes encoded in byte block length is 1 byte length is 2 bytes"
org.apache.lucene.util.ToStringUtils "helper methods to ease implementing {@link object#tostring()}. for printing boost only if not 1.0 no instance"
org.apache.lucene.util.BytesRefHash "{@link bytesrefhash} is a special purpose hash-map like data-structure optimized for {@link bytesref} instances. bytesrefhash maintains mappings of byte arrays to ordinal (map&lt;bytesref,int&gt;) storing the hashed bytes efficiently in continuous storage. the mapping to the ordinal is encapsulated inside {@link bytesrefhash} and is guaranteed to be increased for each added {@link bytesref}.  note: the maximum capacity {@link bytesref} instance passed to {@link #add(bytesref)} must not be longer than {@link byteblockpool#byte_block_size}-2. the internal storage is limited to 2gb total byte storage.  @lucene.internal creates a new {@link bytesrefhash} with a {@link byteblockpool} using a {@link directallocator}. creates a new {@link bytesrefhash} creates a new {@link bytesrefhash} returns the number of {@link bytesref} values in this {@link bytesrefhash}. number of {@link bytesref} values in this {@link bytesrefhash}. populates and returns a {@link bytesref} with the bytes for the given ord.  note: the given ord must be a positive integer less that the current size ( {@link #size()})  the ord the {@link bytesref} to populate given bytesref instance populated with the bytes for the given ord returns the ords array in arbitrary order. valid ords start at offset of 0 and end at a limit of {@link #size()} - 1  note: this is a destructive operation. {@link #clear()} must be called in order to reuse this {@link bytesrefhash} instance.  returns the values array sorted by the referenced byte values.  note: this is a destructive operation. {@link #clear()} must be called in order to reuse this {@link bytesrefhash} instance.  the {@link comparator} used for sorting clears the {@link bytesref} which maps to the given {@link bytesref} closes the bytesrefhash and releases all internally used memory adds a new {@link bytesref} the bytes to hash ord the given bytes are hashed if there was no mapping for the given bytes, otherwise (-(ord)-1). this guarantees that the return value will always be &gt;= 0 if the given bytes haven't been hashed before. @throws maxbyteslengthexceededexception if the given bytes are > 2 + {@link byteblockpool#byte_block_size} adds a new {@link bytesref} with a pre-calculated hash code. the bytes to hash the bytes hash code  hashcode is defined as:  int hash = 0; for (int i = offset; i &lt; offset + length; i++) { hash = 31 hash + bytes[i]; }  ord the given bytes are hashed if there was no mapping for the given bytes, otherwise (-(ord)-1). this guarantees that the return value will always be &gt;= 0 if the given bytes haven't been hashed before. @throws maxbyteslengthexceededexception if the given bytes are > {@link byteblockpool#byte_block_size} - 2 called when hash is too small (> 50% occupied) or too large (null a {@link counter} reference holding the number of bytes used by this {@link bytesstartarray}. the {@link bytesrefhash} uses this reference to track it memory usage {@link atomiclong} reference holding the number of bytes used by this {@link bytesstartarray}. a simple {@link bytesstartarray} that tracks all memory allocation using a shared {@link counter} instance. a simple {@link bytesstartarray} that tracks memory allocation using a private {@link atomiclong} instance. the following fields are needed by comparator, so package private to prevent access$-methods: cannot use arrayutil.shrink because we require power of 2: we don't need to 0-fill the buffers shrink clears the hash entries final position conflict: keep searching different locations in the hash table. new entry we first encode the length, followed by the bytes. length is encoded as vint, but will consume 1 or 2 bytes at most (we reject too-long terms, above). 1 byte to store length 2 byte to store length final position conflict: keep searching different locations in the hash table. new entry length is 1 byte todo: can't we just merge this w/ trackingdirectbytesstartarray...? just add a ctor that makes a private bytesused?"
org.apache.lucene.util.Attribute "base interface for attributes."
org.apache.lucene.util.SorterTemplate "this class was inspired by cglib, but provides a better quicksort algorithm without additional insertionsort at the end. to use, subclass and override the four abstract methods which compare and modify your data. allows custom swap so that two arrays can be sorted at the same time. @lucene.internal implement this method, that swaps slots {@code i} and {@code j} in your data compares slots {@code i} and {@code j} of you data. should be implemented like valueof(i).compareto(valueof(j)) implement this method, that stores the value of slot {@code i} as pivot value implements the compare function for the previously stored pivot value. should be implemented like pivot.compareto(valueof(j)) sorts via stable in-place insertionsort algorithm (ideal for small collections which are mostly presorted). sorts via in-place, but unstable, quicksort algorithm. for small collections falls back to {@link #insertionsort(int,int)}. sorts via stable in-place mergesort algorithm for small collections falls back to {@link #insertionsort(int,int)}. from integer's javadocs: ceil(log2(x)) = 32 - numberofleadingzeros(x - 1) fall back to insertion when array has short length fall back to merge sort when recursion depth gets too big"
org.apache.lucene.util.AttributeImpl "base class for attributes that can be added to a {@link org.apache.lucene.util.attributesource}.  attributes are used to add data in a dynamic, yet type-safe way to a source of usually streamed objects, e. g. a {@link org.apache.lucene.analysis.tokenstream}. clears the values in this attributeimpl and resets it to its default value. if this implementation implements more than one attribute interface it clears all. this method returns the current attribute values as a string in the following format by calling the {@link #reflectwith(attributereflector)} method:  iff {@code prependattclass=true}: {@code "attributeclass#key=value,attributeclass#key=value"} iff {@code prependattclass=false}: {@code "key=value,key=value"}  @see #reflectwith(attributereflector) this method is for introspection of attributes, it should simply add the key/values this attribute holds to the given {@link attributereflector}. the default implementation calls {@link attributereflector#reflect} for all non-static fields from the implementing class, using the field name as key and the field value as value. the attribute class is also determined by reflection. please note that the default implementation can only handle single-attribute implementations. custom implementations look like this (e.g. for a combined attribute implementation):  public void reflectwith(attributereflector reflector) { reflector.reflect(chartermattribute.class, "term", term()); reflector.reflect(positionincrementattribute.class, "positionincrement", getpositionincrement()); }  if you implement this method, make sure that for each invocation, the same set of {@link attribute} interfaces and keys are passed to {@link attributereflector#reflect} in the same order, but possibly different values. so don't automatically exclude e.g. {@code null} properties! @see #reflectasstring(boolean) copies the values from this attribute into the passed-in target attribute. the target implementation must support all the attributes this implementation supports. shallow clone. subclasses must override this if they need to clone any members deeply, this should never happen, because we're just accessing fields from 'this' shouldn't happen"
org.apache.lucene.util.PrintStreamInfoStream "infostream implementation over a {@link printstream} such as system.out. @lucene.internal used for printing messages"
org.apache.lucene.util.RollingBuffer "acts like forever growing t[], but internally uses a circular buffer to reuse instances of t. @lucene.internal implement to reset an instance get t instance for this absolute position; this is allowed to be arbitrarily far "in the future" but cannot be before the last freebefore. returns the maximum position looked up, or -1 if no position has been looked up sinc reset/init. todo: probably move this to core at some point (eg, cutover kuromoji, synfilter, lookaheadtokenfilter) next array index to write to: next position to write: how many valid position are held in the array: for assert: system.out.println("ra.get pos=" + pos + " nextpos=" + nextpos + " nextwrite=" + nextwrite + " count=" + count); system.out.println(" grow length=" + newbuffer.length); should have already been reset: system.out.println(" pos=" + pos + " nextpos=" + nextpos + " -> index=" + index); assert buffer[index].pos == pos; system.out.println(" fb idx=" + index);"
org.apache.lucene.util.SetOnce "a convenient class which offers a semi-immutable object wrapper implementation which allows one to set the value of an object exactly once, and retrieve it many times. if {@link #set(object)} is called more than once, {@link alreadysetexception} is thrown and the operation will fail. @lucene.experimental thrown when {@link setonce#set(object)} is called more than once. a default constructor which does not set the internal object, and allows setting it by calling {@link #set(object)}. creates a new instance with the internal object set to the given object. note that any calls to {@link #set(object)} afterwards will result in {@link alreadysetexception} @throws alreadysetexception if called more than once @see #set(object) sets the given object. if the object has already been set, an exception is thrown. returns the object set by {@link #set(object)}."
org.apache.lucene.util.ArrayUtil "methods for manipulating arrays. @lucene.internal begin apache harmony code revision taken on friday, june 12. https://svn.apache.org/repos/asf/harmony/enhanced/classlib/archive/java6/modules/luni/src/main/java/java/lang/integer.java parses the string argument as if it was an int value and returns the result. throws numberformatexception if the string does not represent an int quantity. a string representation of an int quantity. the value represented by the argument @throws numberformatexception if the argument could not be parsed as an int quantity. parses a char array into an int. the character array the offset into the array the length int @throws numberformatexception if it can't parse parses the string argument as if it was an int value and returns the result. throws numberformatexception if the string does not represent an int quantity. the second argument specifies the radix to use when parsing the value. a string representation of an int quantity. the base to use for conversion. the value represented by the argument @throws numberformatexception if the argument could not be parsed as an int quantity. while (offset = mintargetsize, generally over-allocating exponentially to achieve amortized linear-time cost as the array grows. note: this was originally borrowed from python 2.4.2 listobject.c sources (attribution in license.txt), but has now been substantially changed based on discussions from java-dev thread with subject "dynamic array reallocation algorithms", started on jan 12 2010. minimum required value to be returned. bytes used by each element of the array. see constants in {@link ramusageestimator}. @lucene.internal returns hash of chars in range start (inclusive) to end (inclusive) returns hash of bytes in range start (inclusive) to end (inclusive) see if two array slices are the same. the left array to compare the offset into the array. must be positive the right array to compare the offset into the right array. must be positive the length of the section of the array to compare if the two arrays, starting at their respective offsets, are equal @see java.util.arrays#equals(char[], char[]) see if two array slices are the same. the left array to compare the offset into the array. must be positive the right array to compare the offset into the right array. must be positive the length of the section of the array to compare if the two arrays, starting at their respective offsets, are equal @see java.util.arrays#equals(byte[], byte[]) disable this for now: this has performance problems until java creates intrinsics for class#getcomponenttype() and array.newinstance() public static  t[] grow(t[] array, int minsize) { assert minsize >= 0: "size must be positive (got " + minsize + "): likely integer overflow?"; if (array.length  t[] grow(t[] array) { return grow(array, 1 + array.length); } public static  t[] shrink(t[] array, int targetsize) { assert targetsize >= 0: "size must be positive (got " + targetsize + "): likely integer overflow?"; final int newsize = getshrinksize(array.length, targetsize, ramusageestimator.num_bytes_object_ref); if (newsize != array.length) { @suppresswarnings("unchecked") final t[] newarray = (t[]) array.newinstance(array.getclass().getcomponenttype(), newsize); system.arraycopy(array, 0, newarray, 0, newsize); return newarray; } else return array; } see if two array slices are the same. the left array to compare the offset into the array. must be positive the right array to compare the offset into the right array. must be positive the length of the section of the array to compare if the two arrays, starting at their respective offsets, are equal @see java.util.arrays#equals(char[], char[]) sortertemplate with custom {@link comparator} natural sortertemplate sorts the given array slice using the {@link comparator}. this method uses the quick sort algorithm, but falls back to insertion sort for small arrays. start index (inclusive) end index (exclusive) sorts the given array using the {@link comparator}. this method uses the quick sort algorithm, but falls back to insertion sort for small arrays. sorts the given array slice in natural order. this method uses the quick sort algorithm, but falls back to insertion sort for small arrays. start index (inclusive) end index (exclusive) sorts the given array in natural order. this method uses the quick sort algorithm, but falls back to insertion sort for small arrays. sorts the given array slice using the {@link comparator}. this method uses the merge sort algorithm, but falls back to insertion sort for small arrays. start index (inclusive) end index (exclusive) sorts the given array using the {@link comparator}. this method uses the merge sort algorithm, but falls back to insertion sort for small arrays. sorts the given array slice in natural order. this method uses the merge sort algorithm, but falls back to insertion sort for small arrays. start index (inclusive) end index (exclusive) sorts the given array in natural order. this method uses the merge sort algorithm, but falls back to insertion sort for small arrays. sorts the given array slice using the {@link comparator}. this method uses the insertion sort algorithm. it is only recommended to use this algorithm for partially sorted small arrays! start index (inclusive) end index (exclusive) sorts the given array using the {@link comparator}. this method uses the insertion sort algorithm. it is only recommended to use this algorithm for partially sorted small arrays! sorts the given array slice in natural order. this method uses the insertion sort algorithm. it is only recommended to use this algorithm for partially sorted small arrays! start index (inclusive) end index (exclusive) sorts the given array in natural order. this method uses the insertion sort algorithm. it is only recommended to use this algorithm for partially sorted small arrays! no instance catch usage that accidentally overflows int wait until at least one element is requested asymptotic exponential growth by 1/8th, favors spending a bit more cpu to not tie up too much wasted ram: for very small arrays, where constant overhead of realloc is presumably relatively high, we grow faster add 7 to allow for worst case byte alignment addition below: int overflowed -- return max allowed array size round up to 8 byte alignment in 64bit env round up to multiple of 2 round up to multiple of 4 round up to multiple of 8 no rounding odd (invalid?) size round up to 4 byte alignment in 64bit env round up to multiple of 2 round up to multiple of 4 no rounding odd (invalid?) size only reallocate if we are "substantially" smaller. this saves us from "running hot" (constantly making a bit bigger then a bit smaller, over and over): since arrays.equals doesn't implement offsets for equals since arrays.equals doesn't implement offsets for equals since arrays.equals doesn't implement offsets for equals paranoia: quicksorts (endindex is exclusive!): mergesorts: system.out.println("sort: " + (toindex-fromindex)); insertionsorts:"
org.apache.lucene.util.mutable.MutableValueInt "{@link mutablevalue} implementation of type int. todo: if used in hashmap, it already mixes the value... maybe use a straight value?"
org.apache.lucene.util.mutable.MutableValueBool "{@link mutablevalue} implementation of type boolean."
org.apache.lucene.util.mutable.MutableValue "base class for all mutable values. @lucene.internal"
org.apache.lucene.util.mutable.MutableValueLong "{@link mutablevalue} implementation of type long."
org.apache.lucene.util.mutable.MutableValueDate "{@link mutablevalue} implementation of type {@link date}."
org.apache.lucene.util.mutable.MutableValueDouble "{@link mutablevalue} implementation of type double."
org.apache.lucene.util.mutable.MutableValueStr "{@link mutablevalue} implementation of type {@link string}."
org.apache.lucene.util.mutable.MutableValueFloat "{@link mutablevalue} implementation of type float."
org.apache.lucene.util.BitUtil "a variety of high efficiency bit twiddling routines. @lucene.internal returns the number of set bits in an array of longs. returns the popcount or cardinality of the two sets after an intersection. neither array is modified. returns the popcount or cardinality of the union of two sets. neither array is modified. returns the popcount or cardinality of a & ~b. neither array is modified. returns the popcount or cardinality of a ^ b neither array is modified. returns the next highest power of two, or the current value if it's already a power of two or zero returns the next highest power of two, or the current value if it's already a power of two or zero from org.apache.solr.util rev 555343 no instance the pop methods used to rely on bit-manipulation tricks for speed but it turns out that it is faster to use the long.bitcount method (which is an intrinsic since java 6u18) in a naive loop, see lucene-2221"
org.apache.lucene.util.CommandLineUtil "class containing some useful methods used by command line tools creates a specific fsdirectory instance starting from its class name the name of the fsdirectory class to load the file to be used as parameter constructor new fsdirectory instance loads a specific directory implementation the name of the directory class to load directory class loaded @throws classnotfoundexception if the specified class cannot be found. loads a specific fsdirectory implementation the name of the fsdirectory class to load fsdirectory class loaded @throws classnotfoundexception if the specified class cannot be found. creates a new specific fsdirectory instance the class of the object to be assuming every fsdirectory has a ctor(file):"
org.apache.lucene.util.BytesRefIterator "a simple iterator interface for {@link bytesref} iteration. increments the iteration to the next {@link bytesref} in the iterator. returns the resulting {@link bytesref} or null if the end of the iterator is reached. the returned bytesref may be re-used across calls to next. after this method returns null, do not call it again: the results are undefined. next {@link bytesref} in the iterator or null if the end of the iterator is reached. @throws ioexception if there is a low-level i/o error. return the {@link bytesref} comparator used to sort terms provided by the iterator. this may return null if there are no items or the iterator is not sorted. callers may invoke this method many times, so it's best to cache a single instance & reuse it. singleton bytesrefiterator that iterates over 0 bytesrefs."
org.apache.lucene.util.OpenBitSetDISI "openbitset with added methods to bulk-update the bits from a {@link docidsetiterator}. construct an openbitsetdisi with its bits set from the doc ids of the given docidsetiterator. also give a maximum size one larger than the largest doc id for which a bit may ever be set on this openbitsetdisi. construct an openbitsetdisi with no bits set, and a given maximum size one larger than the largest doc id for which a bit may ever be set on this openbitsetdisi. perform an inplace or with the doc ids from a given docidsetiterator, setting the bit for each such doc id. these doc ids should be smaller than the maximum size passed to the constructor. perform an inplace and with the doc ids from a given docidsetiterator, leaving only the bits set for which the doc ids are in common. these doc ids should be smaller than the maximum size passed to the constructor. perform an inplace not with the doc ids from a given docidsetiterator, clearing all the bits for each such doc id. these doc ids should be smaller than the maximum size passed to the constructor. perform an inplace xor with the doc ids from a given docidsetiterator, flipping all the bits for each such doc id. these doc ids should be smaller than the maximum size passed to the constructor."
org.apache.lucene.util.PriorityQueue "a priorityqueue maintains a partial ordering of its elements such that the least element can always be found in constant time. put()'s and pop()'s require log(size) time. note: this class will pre-allocate a full array of length maxsize+1 if instantiated via the {@link #priorityqueue(int,boolean)} constructor with prepopulate set to true. @lucene.internal determines the ordering of objects in this priority queue. subclasses must define this one method. true iff parameter a is less than parameter b. this method can be overridden by extending classes to return a sentinel object which will be used by the {@link priorityqueue#priorityqueue(int,boolean)} constructor to fill the queue, so that the code which uses that queue can always assume it's full and only change the top without attempting to insert any new object. those sentinel values should always compare worse than any non-sentinel value (i.e., {@link #lessthan} should always favor the non-sentinel values). by default, this method returns false, which means the queue will not be filled with sentinel values. otherwise, the value returned will be used to pre-populate the queue. adds sentinel values to the queue. if this method is extended to return a non-null value, then the following usage pattern is recommended:  // extends getsentinelobject() to return a non-null value. priorityqueue&lt;myobject&gt; pq = new myqueue&lt;myobject&gt;(numhits); // save the 'top' element, which is guaranteed to not be null. myobject pqtop = pq.top(); &lt;...&gt; // now in order to add a new element, which is 'better' than top (after // you've verified it is better), it is as simple as: pqtop.change(). pqtop = pq.updatetop();  note: if this method returns a non-null value, it will be called by the {@link priorityqueue#priorityqueue(int,boolean)} constructor {@link #size()} times, relying on a new object to be returned and will not check if it's null again. therefore you should ensure any call to this method creates a new instance and behaves consistently, e.g., it cannot return null if it previously returned non-null. sentinel object to use to pre-populate the queue, or null if sentinel objects are not supported. adds an object to a priorityqueue in log(size) time. if one tries to add more objects than maxsize from initialize an {@link arrayindexoutofboundsexception} is thrown. new 'top' element in the queue. adds an object to a priorityqueue in log(size) time. it returns the object (if any) that was dropped off the heap because it was full. this can be the given parameter (in case it is smaller than the full heap's minimum, and couldn't be added), or another object that was previously the smallest value in the heap and now has been replaced by a larger one, or null if the queue wasn't yet full with maxsize elements. returns the least element of the priorityqueue in constant time. removes and returns the least element of the priorityqueue in log(size) time. should be called when the object at top changes values. still log(n) worst case, but it's at least twice as fast to  pq.top().change(); pq.updatetop();  instead of  o = pq.pop(); o.change(); pq.push(o);  new 'top' element. returns the number of elements currently stored in the priorityqueue. removes all entries from the priorityqueue. this method returns the internal heap array as object[]. @lucene.internal we allocate 1 extra to avoid if statement in top() don't wrap heapsize to -1, in this case, which causes a confusing negativearraysizeexception. note that very likely this will simply then hit an oome, but at least that's more indicative to caller that this values is too big. we don't +1 in this case, but it's very unlikely in practice one will actually insert this many objects into the pq: note: we add +1 because all access to heap is 1-based not 0-based. heap[0] is unused. t is unbounded type, so this unchecked cast works always if sentinel objects are supported, populate the queue with them extends getsentinelobject() to return a non-null value. save the 'top' element, which is guaranteed to not be null. now in order to add a new element, which is 'better' than top (after you've verified it is better), it is as simple as: we don't need to check size here: if maxsize is 0, then heap is length 2 array with both entries null. if size is 0 then heap[1] is already null. save first value move last to first permit gc of objects adjust heap save bottom node shift parents down install saved node save top node find smaller child shift up child install saved node"
org.apache.lucene.util.automaton.UTF32ToUTF8 "converts utf-32 automata to the equivalent utf-8 representation. @lucene.internal converts an incoming utf32 automaton to an equivalent utf8 one. the incoming automaton need not be deterministic. note that the returned automaton will not in general be deterministic, so you must determinize it if that's needed. todo - do we really need the .bits...? if not we can make util in unicodeutil to convert 1 char into a bytesref unicode boundaries for utf8 bytes 1,2,3,4 represents one of the n utf8 bytes that (in sequence) define a code point. value is the byte value; bits is how many bits are "used" by utf8 at that byte todo: change to byte holds a single code point, as a sequence of 1-4 utf8 bytes: todo: maybe move to unicodeutil? 0xxxxxxx 110yyyxx 10xxxxxx 1110yyyy 10yyyyxx 10xxxxxx 11110zzz 10zzyyyy 10yyyyxx 10xxxxxx builds necessary utf8 edges between start & end system.out.println("start = " + startutf8); system.out.println(" end = " + endutf8); break into start, middle, end: degen case: lead with the same byte: super degen: just single edge, one utf8 byte: single value leading edge type=single recurse for the rest type=startend there is a middle start possibly middle, spanning multiple num bytes wasteful: we only need first byte, and, we should statically encode this first byte: end done recursing type=start type=start done recursing type=end special case -- avoid doesn't accept certain byte sequences) -- there are other cases we could optimize too: type=end type=all type=all type=all type = all"
org.apache.lucene.util.automaton.Lev1TParametricDescription "parametric description for generating a levenshtein automaton of degree 1, with transpositions as primitive edits 2 bits per value 1 bits per value 2 bits per value 1 bits per value 3 bits per value 2 bits per value 3 bits per value 2 bits per value the following code was generated with the moman/finenight pkg this package is available under the mit license, see notice.txt for more details. null absstate should never be passed in decode absstate -> state, offset null state translate back to abs 1 vectors; 2 states per vector; array length = 2 2 vectors; 3 states per vector; array length = 6 4 vectors; 6 states per vector; array length = 24 8 vectors; 6 states per vector; array length = 48 state map 0 -> [(0, 0)] 1 -> [(0, 1)] 2 -> [(0, 1), (1, 1)] 3 -> [(0, 1), (2, 1)] 4 -> [t(0, 1), (0, 1), (1, 1), (2, 1)] 5 -> [(0, 1), (1, 1), (2, 1)]"
org.apache.lucene.util.automaton.MinimizationOperations "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the operations for minimizing automata. @lucene.experimental minimizes (and determinizes if not already deterministic) the given automaton. @see automaton#setminimization(int) minimizes the given automaton using hopcroft's algorithm. recompute hash code a.hash_code = 1a.getnumberofstates() 3 + a.getnumberoftransitions() 2; if (a.hash_code == 0) a.hash_code = 1; initialize data structures find initial partition and reverse edges initialize active sets initialize pending process pending until fixed point find states that need to be split off their blocks refine blocks update pending make a new state for each equivalence class, set initial state select representative build transitions and set acceptance"
org.apache.lucene.util.automaton.State "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the automaton state. @lucene.experimental constructs a new state. initially, the new state is a reject state. resets transition set. returns the set of outgoing transitions. subsequent changes are reflected in the automaton. set adds an outgoing transition. transition sets acceptance for this state. if true, this state is an accept state returns acceptance status. is this is an accept state performs lookup in transitions, assuming determinism. codepoint to look up state, null if no matching outgoing transition @see #step(int, collection) performs lookup in transitions, allowing nondeterminism. codepoint to look up collection where destination states are stored @see #step(int) virtually adds an epsilon transition to the target {@code to} state. this is implemented by copying all transitions from {@code to} to this state, and if {@code to} is an accept state then set accept for this state. downsizes transitionarray to numtransitions reduces this state. a state is "reduced" by combining overlapping and adjacent edge intervals with same destination. returns sorted list of outgoing transitions. _first if true, order by (to, min, reverse max); otherwise (min, reverse max, to) list sorts transitions array in-place. return this state's number.  expert: will be useless unless {@link automaton#getnumberedstates} has been called first to number the states. number returns string describing this state. normally invoked via {@link automaton#tostring()}. compares this object with the specified object for order. states are ordered by the time of construction. mergesort seems to perform better on already sorted arrays:"
org.apache.lucene.util.automaton.Lev2TParametricDescription "parametric description for generating a levenshtein automaton of degree 2, with transpositions as primitive edits 2 bits per value 1 bits per value 3 bits per value 1 bits per value 4 bits per value 2 bits per value 5 bits per value 2 bits per value 6 bits per value 3 bits per value 6 bits per value 3 bits per value the following code was generated with the moman/finenight pkg this package is available under the mit license, see notice.txt for more details. null absstate should never be passed in decode absstate -> state, offset null state translate back to abs 1 vectors; 3 states per vector; array length = 3 2 vectors; 5 states per vector; array length = 10 4 vectors; 13 states per vector; array length = 52 8 vectors; 28 states per vector; array length = 224 16 vectors; 45 states per vector; array length = 720 32 vectors; 45 states per vector; array length = 1440 state map 0 -> [(0, 0)] 1 -> [(0, 2)] 2 -> [(0, 1)] 3 -> [(0, 1), (1, 1)] 4 -> [(0, 2), (1, 2)] 5 -> [t(0, 2), (0, 2), (1, 2), (2, 2)] 6 -> [(0, 2), (2, 1)] 7 -> [(0, 1), (2, 2)] 8 -> [(0, 2), (2, 2)] 9 -> [(0, 1), (1, 1), (2, 1)] 10 -> [(0, 2), (1, 2), (2, 2)] 11 -> [(0, 1), (2, 1)] 12 -> [t(0, 1), (0, 1), (1, 1), (2, 1)] 13 -> [(0, 2), (1, 2), (2, 2), (3, 2)] 14 -> [t(0, 2), (0, 2), (1, 2), (2, 2), (3, 2)] 15 -> [(0, 2), t(1, 2), (1, 2), (2, 2), (3, 2)] 16 -> [(0, 2), (2, 1), (3, 1)] 17 -> [(0, 1), t(1, 2), (2, 2), (3, 2)] 18 -> [(0, 2), (3, 2)] 19 -> [(0, 2), (1, 2), t(1, 2), (2, 2), (3, 2)] 20 -> [t(0, 2), (0, 2), (1, 2), (3, 1)] 21 -> [(0, 1), (1, 1), (3, 2)] 22 -> [(0, 2), (2, 2), (3, 2)] 23 -> [(0, 2), (1, 2), (3, 1)] 24 -> [(0, 2), (1, 2), (3, 2)] 25 -> [(0, 1), (2, 2), (3, 2)] 26 -> [(0, 2), (3, 1)] 27 -> [(0, 1), (3, 2)] 28 -> [(0, 2), (2, 1), (4, 2)] 29 -> [(0, 2), t(1, 2), (1, 2), (2, 2), (3, 2), (4, 2)] 30 -> [(0, 2), (1, 2), (4, 2)] 31 -> [(0, 2), (1, 2), (3, 2), (4, 2)] 32 -> [(0, 2), (2, 2), (3, 2), (4, 2)] 33 -> [(0, 2), (1, 2), t(2, 2), (2, 2), (3, 2), (4, 2)] 34 -> [(0, 2), (1, 2), (2, 2), t(2, 2), (3, 2), (4, 2)] 35 -> [(0, 2), (3, 2), (4, 2)] 36 -> [(0, 2), t(2, 2), (2, 2), (3, 2), (4, 2)] 37 -> [t(0, 2), (0, 2), (1, 2), (2, 2), (4, 2)] 38 -> [(0, 2), (1, 2), (2, 2), (4, 2)] 39 -> [t(0, 2), (0, 2), (1, 2), (2, 2), (3, 2), (4, 2)] 40 -> [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2)] 41 -> [(0, 2), (4, 2)] 42 -> [t(0, 2), (0, 2), (1, 2), (2, 2), t(2, 2), (3, 2), (4, 2)] 43 -> [(0, 2), (2, 2), (4, 2)] 44 -> [(0, 2), (1, 2), t(1, 2), (2, 2), (3, 2), (4, 2)]"
org.apache.lucene.util.automaton.SpecialOperations "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the special automata operations. @lucene.experimental finds the largest entry whose value is less than or equal to c, or 0 if there is no such entry. returns true if the language of this automaton is finite. checks whether there is a loop containing s. (this is sufficient since there are never transitions to dead states.) returns the longest string that is a prefix of all accepted strings and visits each state at most once. prefix returns the longest string that is a suffix of all accepted strings and visits each state at most once. suffix reverses the language of the given (non-singleton) automaton while returning the set of new initial states. returns the set of accepted strings, assuming that at most limit strings are accepted. if more than limit strings are accepted, null is returned. if limit&lt;0, then the limit is infinite. returns the strings that can be produced from the given state, or false if more than limit strings are found. limit&lt;0 means "infinite". todo: not great that this is recursive... in theory a large automata could exceed java's stack todo: this currently requites a determinized machine, but it need not -- we can speed it up by walking the nfa instead. it'd still be fail fast. if singleton, the suffix is the string itself. reverse the language of the automaton, then reverse its common prefix. if singleton, the suffix is the string itself. reverse the language of the automaton, then reverse its common prefix. reverse all edges make new initial+final states ensures that all initial states are reachable todo: this is a dangerous method ... automaton could be huge ... and it's better in general for caller to enumerate & process in a single walk:"
org.apache.lucene.util.automaton.Lev2ParametricDescription "parametric description for generating a levenshtein automaton of degree 2 2 bits per value 1 bits per value 3 bits per value 1 bits per value 4 bits per value 2 bits per value 5 bits per value 2 bits per value 5 bits per value 3 bits per value 5 bits per value 3 bits per value the following code was generated with the moman/finenight pkg this package is available under the mit license, see notice.txt for more details. null absstate should never be passed in decode absstate -> state, offset null state translate back to abs 1 vectors; 3 states per vector; array length = 3 2 vectors; 5 states per vector; array length = 10 4 vectors; 11 states per vector; array length = 44 8 vectors; 21 states per vector; array length = 168 16 vectors; 30 states per vector; array length = 480 32 vectors; 30 states per vector; array length = 960 state map 0 -> [(0, 0)] 1 -> [(0, 2)] 2 -> [(0, 1)] 3 -> [(0, 1), (1, 1)] 4 -> [(0, 2), (1, 2)] 5 -> [(0, 2), (2, 1)] 6 -> [(0, 1), (2, 2)] 7 -> [(0, 2), (2, 2)] 8 -> [(0, 1), (1, 1), (2, 1)] 9 -> [(0, 2), (1, 2), (2, 2)] 10 -> [(0, 1), (2, 1)] 11 -> [(0, 2), (3, 2)] 12 -> [(0, 2), (1, 2), (3, 2)] 13 -> [(0, 2), (1, 2), (2, 2), (3, 2)] 14 -> [(0, 1), (2, 2), (3, 2)] 15 -> [(0, 2), (3, 1)] 16 -> [(0, 1), (3, 2)] 17 -> [(0, 1), (1, 1), (3, 2)] 18 -> [(0, 2), (1, 2), (3, 1)] 19 -> [(0, 2), (2, 2), (3, 2)] 20 -> [(0, 2), (2, 1), (3, 1)] 21 -> [(0, 2), (2, 1), (4, 2)] 22 -> [(0, 2), (1, 2), (4, 2)] 23 -> [(0, 2), (1, 2), (3, 2), (4, 2)] 24 -> [(0, 2), (2, 2), (3, 2), (4, 2)] 25 -> [(0, 2), (3, 2), (4, 2)] 26 -> [(0, 2), (1, 2), (2, 2), (4, 2)] 27 -> [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2)] 28 -> [(0, 2), (4, 2)] 29 -> [(0, 2), (2, 2), (4, 2)]"
org.apache.lucene.util.automaton.BasicAutomata "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the construction of basic automata. @lucene.experimental returns a new (deterministic) automaton with the empty language. returns a new (deterministic) automaton that accepts only the empty string. returns a new (deterministic) automaton that accepts all strings. returns a new (deterministic) automaton that accepts any single codepoint. returns a new (deterministic) automaton that accepts a single codepoint of the given value. returns a new (deterministic) automaton that accepts a single codepoint whose value is in the given interval (including both end points). constructs sub-automaton corresponding to decimal numbers of length x.substring(n).length(). constructs sub-automaton corresponding to decimal numbers of value at least x.substring(n) and length x.substring(n).length(). constructs sub-automaton corresponding to decimal numbers of value at most x.substring(n) and length x.substring(n).length(). constructs sub-automaton corresponding to decimal numbers of value between x.substring(n) and y.substring(n) and of length x.substring(n).length() (which must be equal to y.substring(n).length()). returns a new automaton that accepts strings representing decimal non-negative integers in the given interval. minimal value of interval maximal value of interval (both end points are included in the interval) if >0, use fixed number of digits (strings must be prefixed by 0's to obtain the right length) - otherwise, the number of digits is not fixed @exception illegalargumentexception if min>max or if numbers in the interval cannot be expressed with the given fixed number of digits returns a new (deterministic) automaton that accepts the single given string. returns a new (deterministic and minimal) automaton that accepts the union of the given collection of {@link bytesref}s representing utf-8 encoded strings. 8strings the input strings, utf-8 encoded. the collection must be in sorted order. {@link automaton} accepting all input strings. the resulting automaton is codepoint based (full unicode codepoints on transitions). cx<cy"
org.apache.lucene.util.automaton.AutomatonProvider "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the automaton provider for regexp. {@link regexp#toautomaton(automatonprovider)} @lucene.experimental returns automaton of the given name. automaton name @throws ioexception if errors occur"
org.apache.lucene.util.automaton.BasicOperations "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the basic automata operations. @lucene.experimental returns an automaton that accepts the concatenation of the languages of the given automata.  complexity: linear in number of states. returns an automaton that accepts the concatenation of the languages of the given automata.  complexity: linear in total number of states. returns an automaton that accepts the union of the empty string and the language of the given automaton.  complexity: linear in number of states. returns an automaton that accepts the kleene star (zero or more concatenated repetitions) of the language of the given automaton. never modifies the input automaton language.  complexity: linear in number of states. returns an automaton that accepts min or more concatenated repetitions of the language of the given automaton.  complexity: linear in number of states and in min. returns an automaton that accepts between min and max (including both) concatenated repetitions of the language of the given automaton.  complexity: linear in number of states and in min and max. returns a (deterministic) automaton that accepts the complement of the language of the given automaton.  complexity: linear in number of states (if already deterministic). returns a (deterministic) automaton that accepts the intersection of the language of a1 and the complement of the language of a2. as a side-effect, the automata may be determinized, if not already deterministic.  complexity: quadratic in number of states (if already deterministic). returns an automaton that accepts the intersection of the languages of the given automata. never modifies the input automata languages.  complexity: quadratic in number of states. returns true if these two automata accept exactly the same language. this is a costly computation! note also that a1 and a2 will be determinized as a side effect. returns true if the language of a1 is a subset of the language of a2. as a side-effect, a2 is determinized if not already marked as deterministic.  complexity: quadratic in number of states. returns an automaton that accepts the union of the languages of the given automata.  complexity: linear in number of states. returns an automaton that accepts the union of the languages of the given automata.  complexity: linear in number of states. determinizes the given automaton.  worst case complexity: exponential in number of states. adds epsilon transitions to the given automaton. this method adds extra character interval transitions that are equivalent to the given set of epsilon transitions. collection of {@link statepair} objects representing pairs of source/destination states where epsilon transitions should be added returns true if the given automaton accepts the empty string and nothing else. returns true if the given automaton accepts no strings. returns true if the given automaton accepts all strings. returns true if the given string is accepted by the automaton.  complexity: linear in the length of the string.  note: for full performance, use the {@link runautomaton} class. adding epsilon transitions with the nfa concatenation algorithm in this case always produces a resulting dfa, preventing expensive redundant determinize() calls for this common case. a1.clearhashcode(); b.clearhashcode(); a.clearhashcode(); a.clearhashcode(); b.clearhashcode(); subsetof is faster if the first automaton is a singleton a1.clearhashcode(); a.clearhashcode(); simple custom arraylist holds all transitions that start on this int point, or end at this point-1 1st time we are seeing this point switch to hashmap on the fly mergesort seems to perform better on already sorted arrays: subset construction like set like sortedmap collate all outgoing transitions by min/1+max: no outgoing transitions -- skip it process transitions that end on this point (closes an overlapping interval) process transitions that start on this point (opens a new interval) calculate epsilon closure add transitions a.clearhashcode();"
org.apache.lucene.util.automaton.RegExp "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the regular expression extension to automaton.  regular expressions are built from the following abstract syntax:    regexp ::= unionexp      |      unionexp ::= interexp&nbsp;|&nbsp;unionexp (union)     | interexp     interexp ::= concatexp&nbsp;&amp;&nbsp;interexp (intersection) [optional]    | concatexp     concatexp ::= repeatexp&nbsp;concatexp (concatenation)     | repeatexp     repeatexp ::= repeatexp&nbsp;? (zero or one occurrence)     | repeatexp&nbsp; (zero or more occurrences)     | repeatexp&nbsp;+ (one or more occurrences)     | repeatexp&nbsp;{n} (n occurrences)     | repeatexp&nbsp;{n,} (n or more occurrences)     | repeatexp&nbsp;{n,m} (n to m occurrences, including both)     | complexp     complexp ::= ~&nbsp;complexp (complement) [optional]    | charclassexp     charclassexp ::= [&nbsp;charclasses&nbsp;] (character class)     | [^&nbsp;charclasses&nbsp;] (negated character class)     | simpleexp     charclasses ::= charclass&nbsp;charclasses      | charclass     charclass ::= charexp&nbsp;-&nbsp;charexp (character range, including end-points)     | charexp     simpleexp ::= charexp      | . (any single character)     | # (the empty language) [optional]    | @ (any string) [optional]    | "&nbsp;&lt;unicode string without double-quotes&gt;&nbsp; " (a string)     | (&nbsp;) (the empty string)     | (&nbsp;unionexp&nbsp;) (precedence override)     | &lt;&nbsp;&lt;identifier&gt;&nbsp;&gt; (named automaton) [optional]    | &lt;n-m&gt; (numerical interval) [optional]   charexp ::= &lt;unicode character&gt; (a single non-reserved character)     | \&nbsp;&lt;unicode character&gt;&nbsp; (a single character)     the productions marked [optional] are only allowed if specified by the syntax flags passed to the regexp constructor. the reserved characters used in the (enabled) syntax must be escaped with backslash (\) or double-quotes ("..."). (in contrast to other regexp syntaxes, this is required also in character classes.) be aware that dash (-) has a special meaning in charclass expressions. an identifier is a string not containing right angle bracket (&gt;) or dash (-). numerical intervals are specified by non-negative decimal integers and include both end points, and if n and m have the same number of digits, then the conforming strings must have that length (i.e. prefixed by 0's). @lucene.experimental syntax flag, enables intersection (&amp;). syntax flag, enables complement (~). syntax flag, enables empty language (#). syntax flag, enables anystring (@). syntax flag, enables named automata (&lt;identifier&gt;). syntax flag, enables numerical intervals ( &lt;n-m&gt;). syntax flag, enables all optional regexp syntax. syntax flag, enables no optional regexp syntax. constructs new regexp from a string. same as regexp(s, all). regexp string @exception illegalargumentexception if an error occured while parsing the regular expression constructs new regexp from a string. regexp string _flags boolean 'or' of optional syntax constructs to be enabled @exception illegalargumentexception if an error occured while parsing the regular expression constructs new automaton from this regexp. same as toautomaton(null) (empty automaton map). constructs new automaton from this regexp. the constructed automaton is minimal and deterministic and has no transitions to dead states. _provider provider of automata for named identifiers @exception illegalargumentexception if this regular expression uses a named identifier that is not available from the automaton provider constructs new automaton from this regexp. the constructed automaton is minimal and deterministic and has no transitions to dead states. a map from automaton identifiers to automata (of type automaton). @exception illegalargumentexception if this regular expression uses a named identifier that does not occur in the automaton map sets or resets allow mutate flag. if this flag is set, then automata construction uses mutable automata, which is slightly faster but not thread safe. by default, the flag is not set. if true, the flag is set value of the flag constructs string from parsed regular expression. returns set of automaton identifiers that occur in this regular expression. thread unsafe always clone here (ignore allow_mutate)"
org.apache.lucene.util.automaton.Automaton "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the finite-state automaton with regular expression operations.  class invariants:  an automaton is either represented explicitly (with {@link state} and {@link transition} objects) or with a singleton string (see {@link #getsingleton()} and {@link #expandsingleton()}) in case the automaton is known to accept exactly one string. (implicitly, all states and transitions of an automaton are reachable from its initial state.) automata are always reduced (see {@link #reduce()}) and have no transitions to dead states (see {@link #removedeadtransitions()}). if an automaton is nondeterministic, then {@link #isdeterministic()} returns false (but the converse is not required). automata provided as input to operations are generally assumed to be disjoint.   if the states or transitions are manipulated manually, the {@link #restoreinvariant()} and {@link #setdeterministic(boolean)} methods should be used afterwards to restore representation invariants that are assumed by the built-in automata operations.   note: this class has internal mutable state and is not thread safe. it is the caller's responsibility to ensure any necessary synchronization if you wish to use the same automaton from multiple threads. in general it is instead recommended to use a {@link runautomaton} for multithreaded matching: it is immutable, thread safe, and much faster.  @lucene.experimental minimize using hopcroft's o(n log n) algorithm. this is regarded as one of the most generally efficient algorithms that exist. @see #setminimization(int) selects minimization algorithm (default: minimize_hopcroft). initial state of this automaton. if true, then this automaton is definitely deterministic (i.e., there are no choices for any run, but a run may crash). extra data associated with this automaton. hash code. recomputed by {@link minimizationoperations#minimize(automaton)} singleton string. null if not applicable. minimize always flag. selects whether operations may modify the input automata (default: false). constructs a new automaton that accepts the empty language. using this constructor, automata can be constructed manually from {@link state} and {@link transition} objects. @see state @see transition selects minimization algorithm (default: minimize_hopcroft). minimization algorithm sets or resets minimize always flag. if this flag is set, then {@link minimizationoperations#minimize(automaton)} will automatically be invoked after all operations that otherwise may produce non-minimal automata. by default, the flag is not set. if true, the flag is set sets or resets allow mutate flag. if this flag is set, then all automata operations may modify automata given as input; otherwise, operations will always leave input automata languages unmodified. by default, the flag is not set. if true, the flag is set value of the flag returns the state of the allow mutate flag. if this flag is set, then all automata operations may modify automata given as input; otherwise, operations will always leave input automata languages unmodified. by default, the flag is not set. value of the flag returns the singleton string for this automaton. an automaton that accepts exactly one string may be represented in singleton mode. in that case, this method may be used to obtain the string. , null if this automaton is not in singleton mode. sets initial state. state public void setinitialstate(state s) { initial = s; singleton = null; } gets initial state. returns deterministic flag for this automaton. if the automaton is definitely deterministic, false if the automaton may be nondeterministic sets deterministic flag for this automaton. this method should (only) be used if automata are constructed manually. true if the automaton is definitely deterministic, false if the automaton may be nondeterministic associates extra information with this automaton. extra information returns extra information associated with this automaton. information @see #setinfo(object) returns the set of reachable accept states. of {@link state} objects adds transitions to explicit crash state to ensure that transition function is total. restores representation invariant. this method must be invoked before any built-in automata operation is performed if automaton states or transitions are manipulated manually. @see #setdeterministic(boolean) reduces this automaton. an automaton is "reduced" by combining overlapping and adjacent edge intervals with same destination. returns sorted array of all interval start points. returns the set of live states. a state is "live" if an accept state is reachable from it. of {@link state} objects removes transitions to dead states and calls {@link #reduce()}. (a state is "dead" if no accept state is reachable from it.) returns a sorted array of transitions for each state (and sets state numbers). expands singleton representation to normal representation. does nothing if not in singleton representation. returns the number of states in this automaton. returns the number of transitions in this automaton. this number is counted as the total number of edges, where one edge may be a character interval. must be invoked when the stored hash code may no longer be valid. void clearhashcode() { hash_code = 0; } returns a string representation of this automaton. returns graphviz dot representation of this automaton. returns a clone of this automaton, expands if singleton. returns a clone of this automaton unless allow_mutation is set, expands if singleton. returns a clone of this automaton. returns a clone of this automaton, or this automaton itself if allow_mutation flag is set. see {@link basicoperations#concatenate(automaton, automaton)}. see {@link basicoperations#concatenate(list)}. see {@link basicoperations#optional(automaton)}. see {@link basicoperations#repeat(automaton)}. see {@link basicoperations#repeat(automaton, int)}. see {@link basicoperations#repeat(automaton, int, int)}. see {@link basicoperations#complement(automaton)}. see {@link basicoperations#minus(automaton, automaton)}. see {@link basicoperations#intersection(automaton, automaton)}. see {@link basicoperations#subsetof(automaton, automaton)}. see {@link basicoperations#union(automaton, automaton)}. see {@link basicoperations#union(collection)}. see {@link basicoperations#determinize(automaton)}. see {@link basicoperations#isemptystring(automaton)}. see {@link minimizationoperations#minimize(automaton)}. returns the automaton being given as argument. int hash_code; cached todo: maybe we can eventually allow for oversizing here... map> clearhashcode(); filter out transitions to dead states: sneaky corner case -- if machine accepts no strings"
org.apache.lucene.util.automaton.DaciukMihovAutomatonBuilder "builds a minimal, deterministic {@link automaton} that accepts a set of strings. the algorithm requires sorted input data, but is very fast (nearly linear with the input size). @see #build(collection) @see basicautomata#makestringunion(collection) dfsa state with char labels on transitions. an empty set of labels. an empty set of states. labels of outgoing transitions. indexed identically to {@link #states}. labels must be sorted lexicographically. states reachable from outgoing transitions. indexed identically to {@link #labels}. true if this state corresponds to the end of at least one input sequence. returns the target state of a transition leaving this state and labeled with label. if no such transition exists, returns null. two states are equal if:  they have an identical number of outgoing transitions, labeled with the same labels corresponding outgoing transitions lead to the same states (to states with an identical right-language).  compute the hash code of the current status of this state. compare the right-language of this state using reference-identity of outgoing states. this is possible because states are interned (stored in registry) and traversed in post-order, so any outgoing transitions are already interned. return true if this state has any children (outgoing transitions). create a new outgoing transition labeled label and return the newly return the most recent transitions's target state. return the associated state if the most recent transition is labeled with label. replace the last added outgoing transition's target state with the given state. compare two lists of objects for reference-equality. a "registry" for state interning. root automaton state. previous sequence added to the automaton in {@link #add(charsref)}. a comparator used for enforcing sorted utf8 order, used in assertions only. add another character sequence to this automaton. the sequence must be lexicographically larger or equal compared to any previous sequences added to this automaton (the input must be sorted). finalize the automaton and return the root state. no more strings can be added to the builder after this call. automaton state. internal recursive traversal for conversion. build a minimal, deterministic automaton from a sorted list of {@link bytesref} representing strings in utf-8. these strings must be binary-sorted. copy current into an internal buffer. replace last child of state with an already registered state or stateregistry the last child state. add a suffix of current starting at fromindex (inclusive) to state state. descend in the automaton (find matching prefix). todo, optimize me don't need to copy, once we fix https://issues.apache.org/jira/browse/lucene-3277 still, called only from assert"
org.apache.lucene.util.automaton.ByteRunAutomaton "automaton representation for matching utf-8 byte[]. expert: if utf8 is true, the input is already byte-based returns true if the given byte array is accepted by this automaton"
org.apache.lucene.util.automaton.StatePair "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the pair of states. @lucene.experimental constructs a new state pair. 1 first state 2 second state returns first component of this pair. state returns second component of this pair. state checks for equality. object to compare with if obj represents the same pair of states as this pair returns hash code. code"
org.apache.lucene.util.automaton.Transition "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the automaton transition.  a transition, which belongs to a source state, consists of a unicode codepoint interval and a destination state. @lucene.experimental class invariant: minobj is a transition with same character interval and destination state as this transition. returns hash code. the hash code is based on the character interval (not the destination state). code clones this transition. with same character interval and destination state returns a string describing this state. normally invoked via {@link automaton#tostring()}."
org.apache.lucene.util.automaton.LevenshteinAutomata "class to construct dfas that match a word within some edit distance.  implements the algorithm described in: schulz and mihov: fast string correction with levenshtein automata  @lucene.experimental @lucene.internal input word the automata alphabet. the maximum symbol in the alphabet (e.g. 255 for utf-8 or 10ffff for utf-32) the ranges outside of alphabet create a new levenshteinautomata for some input string. optionally count transpositions as a primitive edit. expert: specify a custom maximum possible symbol (alphamax); default is character.max_code_point. add the final endpoint for n=0, we do not need to go through the trouble compute a dfa that accepts all strings within an edit distance of n.  all automata have the following properties:  they are deterministic (dfa). there are no transitions to dead states. they are not minimal (some transitions could be combined).   get the characteristic vector x(x, v) where v is substring(pos, end) a parametricdescription describes the structure of a levenshtein dfa for some degree n.  there are four components of a parametric description, all parameterized on the length of the word w:  the number of states: {@link #size()} the set of final states: {@link #isaccept(int)} the transition function: {@link #transition(int, int, int)} minimal boundary function: {@link #getposition(int)}  return the number of states needed to compute a levenshtein dfa returns true if the state in any levenshtein dfa is an accept state (final state). returns the position in the input word for a given state. this is the minimal boundary for the state. returns the state number for a transition from the given state, assuming position and characteristic vector vector calculate the alphabet calculate the unicode range intervals that exclude the alphabet these are the ranges for all unicode characters not in the alphabet the number of states is based on the length of the word and n create all states, and mark as accept states if appropriate create transitions from state to state get the characteristic vector at this position wrt ch add transitions for all other chars in unicode by definition, their characteristic vectors are always 0, because they do not exist in the input string. by definition we create some useless unconnected states, and its a net-win overall to remove these, as well as to combine any adjacent transitions (it makes later algorithms more efficient). so, while we could set our numberedstates here, its actually best not to, and instead to force a traversal in reduce, pruning the unconnected states while we combine adjacent transitions. a.setnumberedstates(states); we need not trim transitions to dead states, as they are not a.restoreinvariant(); decode absstate -> state, offset system.out.println("index=" + index + " dataloc=" + dataloc + " bitstart=" + bitstart + " bitsperv=" + bitspervalue); not split split"
org.apache.lucene.util.automaton.Lev1ParametricDescription "parametric description for generating a levenshtein automaton of degree 1 2 bits per value 1 bits per value 2 bits per value 1 bits per value 3 bits per value 2 bits per value 3 bits per value 2 bits per value the following code was generated with the moman/finenight pkg this package is available under the mit license, see notice.txt for more details. null absstate should never be passed in decode absstate -> state, offset null state translate back to abs 1 vectors; 2 states per vector; array length = 2 2 vectors; 3 states per vector; array length = 6 4 vectors; 5 states per vector; array length = 20 8 vectors; 5 states per vector; array length = 40 state map 0 -> [(0, 0)] 1 -> [(0, 1)] 2 -> [(0, 1), (1, 1)] 3 -> [(0, 1), (2, 1)] 4 -> [(0, 1), (1, 1), (2, 1)]"
org.apache.lucene.util.automaton.SortedIntSet "just holds a set of int[] states, plus a corresponding int[] count per state. used by basicoperations.determinize if we hold more than this many states, we switch from o(n^2) linear ops to o(n log(n)) treemap adds this state to the set insert here append removes this state from the set, if count decrs to 0 fall back to simple arrays once we touch zero again"
org.apache.lucene.util.automaton.RunAutomaton "dk.brics.automaton copyright (c) 2001-2009 anders moeller all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. the name of the finite-state automaton with fast run operation. @lucene.experimental returns a string representation of this automaton. returns number of states in automaton. returns acceptance status for given state. returns initial state. returns array of codepoint class interval start points. the array should not be modified by the caller. gets character class of given codepoint constructs a new runautomaton from a deterministic automaton. an automaton set alphabet table for optimal run performance. returns the state obtained by reading the given char from the given state. returns -1 if not obtaining any such state. (if the original automaton had no dead states, -1 is returned here if and only if a dead state is entered in an equivalent automaton with a total transition function.) delta(state,c) = transitions[statepoints.length + getcharclass(c)] char interval start points map from char number to class class"
org.apache.lucene.util.automaton.CharacterRunAutomaton "automaton representation for matching char[]. returns true if the given string is accepted by this automaton. returns true if the given string is accepted by this automaton"
org.apache.lucene.util.automaton.CompiledAutomaton "immutable class holding compiled details for a given automaton. the automaton is deterministic, must not have dead states but is not necessarily minimal. @lucene.experimental automata are compiled into different internal forms for the most efficient execution depending upon the language they accept. automaton that accepts no strings. automaton that accepts all possible strings. automaton that accepts only a single fixed string. automaton that matches all strings with a constant prefix. catch-all for any other automata. for {@link automaton_type#prefix}, this is the prefix term; for {@link automaton_type#single} this is the singleton term. matcher for quickly determining if a byte[] is accepted. only valid for {@link automaton_type#normal}. two dimensional array of transitions, indexed by state number for traversal. the state numbering is consistent with {@link #runautomaton}. only valid for {@link automaton_type#normal}. shared common suffix accepted by the automaton. only valid for {@link automaton_type#normal}, and only when the automaton accepts an infinite language. indicates if the automaton accepts a finite set of strings. null if this was not computed. only valid for {@link automaton_type#normal}. finds largest term accepted by this automaton, that's <= the provided input term. the result is placed in output; it's fine for output and input to point to the same bytesref. the returned result is either the provided output, or null if there is no floor term (ie, the provided input term is before the first term accepted by this automaton). todo: would be nice if these sortedtransitions had "int to;" instead of "state to;" somehow: test whether the automaton is a "simple" form and if so, don't create a runautomaton. note that on a large automaton these tests could be costly: matches nothing matches all possible strings matches a fixed string in singleton or expanded representation matches a constant prefix private static final boolean debug = blocktreetermswriter.debug; find biggest transition that's < label todo: use binary search here append floorlabel if (debug) system.out.println(" add floorlabel=" + (char) floorlabel + " idx=" + idx); push down to last accept state if (debug) system.out.println(" return " + term.utf8tostring()); we are pushing "top" -- so get last label of last transition: if (debug) system.out.println(" push maxlabel=" + (char) lasttransition.max + " idx=" + idx); todo: should this take startterm too? this way terms.intersect could forward to this method if type != normal: todo: this is very likely faster than .intersect, but we should test and maybe cutover unreachable if (debug) system.out.println("ca.floor input=" + input.utf8tostring()); special case empty string: if (debug) system.out.println(" cycle label=" + (char) label + " nextstate=" + nextstate); input string is accepted if (debug) system.out.println(" input is accepted; return term=" + output.utf8tostring()); pop back to a state that has a transition <= our label: if (debug) system.out.println(" return " + output.utf8tostring()); if (debug) system.out.println(" return " + output.utf8tostring()); pop if (debug) system.out.println(" pop ord=" + idx + " return null"); if (debug) system.out.println(" pop ord=" + (idx+1) + " label=" + (char) label + " first trans.min=" + (char) transitions[0].min); if (debug) system.out.println(" stop pop ord=" + idx + " first trans.min=" + (char) transitions[0].min); if (debug) system.out.println(" label=" + (char) label + " idx=" + idx);"
org.apache.lucene.util.FieldCacheSanityChecker "provides methods for sanity checking that entries in the fieldcache are not wasteful or inconsistent.   lucene 2.9 introduced numerous enhancements into how the fieldcache is used by the low levels of lucene searching (for sorting and valuesourcequeries) to improve both the speed for sorting, as well as reopening of indexreaders. but these changes have shifted the usage of fieldcache from "top level" indexreaders (frequently a multireader or directoryreader) down to the leaf level segmentreaders. as a result, existing applications that directly access the fieldcache may find ram usage increase significantly when upgrading to 2.9 or later. this class provides an api for these applications (or their unit tests) to check at run time if the fieldcache contains "insane" usages of the fieldcache.  @lucene.experimental @see fieldcache @see fieldcachesanitychecker.insanity @see fieldcachesanitychecker.insanitytype noop if set, estimate size for all cacheentry objects will be calculateed. quick and dirty convenience method @see #check quick and dirty convenience method that instantiates an instance with "good defaults" and uses it to test the cacheentrys @see #check tests a cacheentry[] for indication of "insane" cache usage.  note:fieldcache creationplaceholder objects are ignored. (:todo: is this a bad idea? are we masking a real problem?)  internal helper method used by check that iterates over valmismatchkeys and generates a collection of insanity instances accordingly. the mapofsets are used to populate the insanity objects. @see insanitytype#valuemismatch internal helper method used by check that iterates over the keys of readerfieldtovalids and generates a collection of insanity instances whenever two (or more) readerfield instances are found that have an ancestry relationships. @see insanitytype#subreader checks if the seed is an indexreader, and if so will walk the hierarchy of subreaders building up a list of the objects returned by {@code seed.getcorecachekey()} simple pair object for using "readerkey + fieldname" a map key simple container for a collection of related cacheentry objects that in conjunction with each other represent some "insane" usage of the fieldcache. type of insane behavior this object represents description of hte insane behavior cacheentry objects which suggest a problem multi-line representation of this insanity object, starting with the type and msg, followed by each cacheentry.tostring() on it's own line prefaced by a tab character an enumeration of the different types of "insane" behavior that may be detected in a fieldcache. @see insanitytype#subreader @see insanitytype#valuemismatch @see insanitytype#expected indicates an overlap in cache usage on a given field in sub/super readers.  indicates entries have the same reader+fieldname but different cached values. this can happen if different datatypes, or parsers are used -- and while it's not necessarily a bug it's typically an indication of a possible problem.   note: only the reader, fieldname, and cached value are actually tested -- if two cache entries have different parsers or datatypes but the cached values are the same object (== not just equal()) this method does not consider that a red flag. this allows for subtle variations in the way a parser is specified (null vs default_long_parser, etc...)  indicates an expected bit of "insanity". this may be useful for clients that wish to preserve/log information about insane usage but indicate that it was expected. the indirect mapping lets mapofset dedup identical valids for us  maps the (valid) identityhashcode of cache values to sets of cacheentry instances maps readerfield keys to sets of valueids  any keys that we know result in more then one valid iterate over all the cacheentries to get the mappings we'll need it's ok to have dup entries, where one is eg float[] and the other is the bits (from getdocwithfield()) indirect mapping, so the mapofset will dedup identical valids for us we have multiple values for some readerfields wrapper we've already process this kid as rf and found other problems track those problems as our own we have cache entries for the kid every mapping in badkids represents an insanity put parent entr(ies) in first now the entries for the descendants will grow as we iter todo: we don't check closed readers here (as gettopreadercontext throws alreadyclosedexception), what should we do? reflection? it is composite reader ignore this reader need to skip the first, because it was the seed"
org.apache.lucene.util.Constants "some useful constants. jvm vendor info. the value of system.getproperty("java.version"). the value of system.getproperty("os.name"). true iff running on linux. true iff running on windows. true iff running on sunos. true iff running on mac os x @deprecated with lucene 4.0, we are always on java 6 true iff running on a 64bit jvm this is the internal lucene version, recorded into each segment. this is the lucene version for display purposes. can't construct prevent inlining in foreign class files system.out.println("address size: " + addresssize); this method only exists in java 7: this method only exists in java 8: this method prevents inlining the final version constant in compiled classes, see: http://www.javaworld.com/community/node/3400 note: we track per-segment version as a string with the "x.y" format, e.g. "4.0", "3.1", "3.0". therefore when we change this constant, we should keep the format. alpha/beta"
org.apache.lucene.util.DoubleBarrelLRUCache "simple concurrent lru cache, using a "double barrel" approach where two concurrenthashmaps record entries. at any given time, one hash is primary and the other is secondary. {@link #get} first checks primary, and if that's a miss, checks secondary. if secondary has the entry, it's promoted to primary (note: the key is cloned at this point). once primary is full, the secondary is cleared and the two are swapped. this is not as space efficient as other possible concurrent approaches (see lucene-2075): to achieve perfect lru(n) it requires 2n storage. but, this approach is relatively simple and seems in practice to not grow unbounded in size when under hideously high load. @lucene.internal object providing clone(); the key class must subclass this. try primary first not found -- try secondary promote to primary time to swap note: there is saturation risk here, that the thread that's doing the clear() takes too long to do so, while other threads continue to add to primary, but in practice this seems not to be an issue (see lucene-2075 for benchmark & details) first, clear secondary second, swap third, reset countdown"
org.apache.lucene.util.OpenBitSet "an "open" bitset implementation that allows direct access to the array of words storing the bits.  unlike java.util.bitset, the fact that bits are packed into an array of longs is part of the interface. this allows efficient implementation of other algorithms by someone other than the constructs an openbitset large enough to hold numbits. constructs an openbitset from an existing long[].  the first 64 bits are in long[0], with bit index 0 at the least significant bit, and bit index 63 at the most significant. given a bit index, the word containing it is long[index/64], and it is at bit number index%64 within that word.  numwords are the number of elements in the array that contain set bits (non-zero longs). numwords should be &lt= bits.length, and any existing words in the array at position &gt= numwords should be zero. this docidset implementation is cacheable. returns the current capacity in bits (1 greater than the index of the last bit) returns the current capacity of this set. included for compatibility. this is not equal to {@link #cardinality} returns true if there are no set bits expert: returns the long[] storing the bits expert: sets a new long[] to use as the bit storage expert: gets the number of longs in the array that are in use expert: sets the number of longs in the array that are in use returns true or false for the specified bit index. returns true or false for the specified bit index. the index should be less than the openbitset size returns true or false for the specified bit index returns true or false for the specified bit index. the index should be less than the openbitset size. // alternate implementation of get() public boolean get1(int index) { int i = index >> 6; // div 64 int bit = index & 0x3f; // mod 64 return ((bits[i]>>>bit) & 0x01) != 0; // this does a long shift and a bittest (on x86) vs // a long shift, and a long and, (the test for zero is prob a no-op) // testing on a p4 indicates this is slower than (bits[i] & bitmask) != 0; } returns 1 if the bit is set, 0 if not. the index should be less than the openbitset size public boolean get2(int index) { int word = index >> 6; // div 64 int bit = index & 0x0000003f; // mod 64 return (bits[word] >>64 == -1 for that reason, make sure not to use endmask if the bits to flip will be zero in the last word (redefine endword to be the last changed...) long startmask = -1l >> (64-(endindex & 0x3f)); // example: 00111...111111 public static int pop(long v0, long v1, long v2, long v3) { // derived from pop_array by setting last four elems to 0. // exchanges one pop() call for 10 elementary operations // saving about 7 instructions... is there a better way? long twosa=v0 & v1; long ones=v0^v1; long u2=ones^v2; long twosb =(ones&v2)|(u2&v3); ones=u2^v3; long fours=(twosa&twosb); long twos=twosa^twosb; return (pop(fours)<<2) + (pop(twos)<<1) + pop(ones); } number of set bits returns the popcount or cardinality of the intersection of the two sets. neither set is modified. returns the popcount or cardinality of the union of the two sets. neither set is modified. returns the popcount or cardinality of "a and not b" or "intersection(a, not(b))". neither set is modified. returns the popcount or cardinality of the exclusive-or of the two sets. neither set is modified. returns the index of the first set bit starting at the index specified. -1 is returned if there are no more set bits. returns the index of the first set bit starting at the index specified. -1 is returned if there are no more set bits. returns the index of the first set bit starting downwards at the index specified. -1 is returned if there are no more set bits. returns the index of the first set bit starting downwards at the index specified. -1 is returned if there are no more set bits. this = this and other this = this or other remove all elements set in other. this = this and_not other this = this xor other see {@link intersect} see {@link union} see {@link andnot} returns true if the sets have any elements in common expand the long[] with the size given as a number of words (64 bit longs). getnumwords() is unchanged by this call. ensure that the long[] is big enough to hold numbits, expanding it if necessary. getnumwords() is unchanged by this call. lowers numwords, the number of words in use, by checking for trailing zero words. returns the number of 64 bit words it would take to hold numbits returns true if both sets have the same bits set number of words (elements) used in the array used only for assert: div 64 signed shift will keep a negative index and force an array-index-out-of-bounds-exception, removing the need for an explicit check. mod 64 div 64 signed shift will keep a negative index and force an array-index-out-of-bounds-exception, removing the need for an explicit check. mod 64 div 64 mod 64 div 64 mod 64 alternate implementation of get() div 64 mod 64 this does a long shift and a bittest (on x86) vs a long shift, and a long and, (the test for zero is prob a no-op) testing on a p4 indicates this is slower than (bits[i] & bitmask) != 0; div 64 mod 64 div 64 mod 64 hmmm, this would work if bit order were reversed we could right shift and check for parity bit, if it was available to us. div 64 mod 64 since endindex is one past the end, this is index of the last word to be changed. 64-(endindex&0x3f) is the same as -endindex due to wrap hmmm, it takes one more instruction to clear than it does to set... any way to work around this? if there were only 63 bits per word, we could use a right shift of 10111111...111 in binary to position the 0 in the correct place (using sign extension). could also use long.rotateright() or rotateleft() if they were converted by the jvm into a native instruction. bits[word] &= long.rotateleft(0xfffffffe,bit); div 64 mod 64 div 64 mod 64 since endindex is one past the end, this is index of the last word to be changed. 64-(endindex&0x3f) is the same as -endindex due to wrap invert masks since we are clearing since endindex is one past the end, this is index of the last word to be changed. 64-(endindex&0x3f) is the same as -endindex due to wrap invert masks since we are clearing div 64 mod 64 div 64 mod 64 div 64 mod 64 div 64 mod 64 mod 64 div 64 mod 64 div 64 mod 64 since endindex is one past the end, this is index of the last word to be changed. example: 11111...111000 example: 00111...111111 64-(endindex&0x3f) is the same as -endindex due to wrap derived from pop_array by setting last four elems to 0. exchanges one pop() call for 10 elementary operations saving about 7 instructions... is there a better way? index within the word skip all the bits to the right of index index within the word skip all the bits to the right of index last possible bit index within the word skip all the bits to the left of index see lucene-3197 last possible bit index within the word skip all the bits to the left of index see lucene-3197 hopefully an array clone is as fast(er) than arraycopy testing against zero can be more efficient fill zeros from the new shorter length to the old length some bitset compatability methods see {@link intersect} / see {@link union} / see {@link andnot} / make a the larger set. check for any set bits out of the range of b start with a zero hash and use a mix that results in zero if the input is zero. this effectively truncates trailing zeros without an explicit check. rotate left fold leftmost bits into right and add a constant to prevent empty sets from returning 0, which is too common."
org.apache.lucene.util.RecyclingByteBlockAllocator "a {@link byteblockpool.allocator} implementation that recycles unused byte blocks in a buffer and reuses them in subsequent calls to {@link #getbyteblock()}.  note: this class is not thread-safe  @lucene.internal creates a new {@link recyclingbyteblockallocator} the block size in bytes maximum number of buffered byte block {@link counter} reference counting internally allocated bytes creates a new {@link recyclingbyteblockallocator}. the block size in bytes maximum number of buffered byte block creates a new {@link recyclingbyteblockallocator} with a block size of {@link byteblockpool#byte_block_size}, upper buffered docs limit of {@link #default_buffered_blocks} ({@value #default_buffered_blocks}). number of currently buffered blocks number of bytes currently allocated by this {@link allocator} maximum number of buffered byte blocks removes the given number of byte blocks from the buffer if possible. the number of byte blocks to remove number of actually removed buffers"
org.apache.lucene.util.IntBlockPool "a pool for int blocks similar to {@link byteblockpool} @lucene.internal abstract class for allocating and freeing int blocks. a simple {@link allocator} that never recycles. creates a new {@link directallocator} with a default block size array of buffers currently used in the pool. buffers are allocated if needed don't modify this outside of this class index into the buffers array pointing to the current buffer used as the head pointer to the current position in head buffer current head buffer current head offset creates a new {@link intblockpool} with a default {@link allocator}. @see intblockpool#nextbuffer() creates a new {@link intblockpool} with the given {@link allocator}. @see intblockpool#nextbuffer() resets the pool to its initial state reusing the first buffer. calling {@link intblockpool#nextbuffer()} is not needed after reset. expert: resets the pool to its initial state reusing the first buffer. if true the buffers are filled with 0. this should be set to true if this pool is used with {@link slicewriter}. if true the first buffer will be reused and calling {@link intblockpool#nextbuffer()} is not needed after reset iff the block pool was used before ie. {@link intblockpool#nextbuffer()} was called before. advances the pool to its next buffer. this method should be called once after the constructor to initialize the pool. in contrast to the constructor a {@link intblockpool#reset()} call will advance the pool to its first buffer immediately. creates a new int slice with the given starting size and returns the slices offset in the pool. @see slicereader an array holding the offset into the {@link intblockpool#level_size_array} to quickly navigate to the next slice level. an array holding the level sizes for int slices. the first level size for new slices allocates a new slice from the given offset a {@link slicewriter} that allows to write multiple integer slices into a given {@link intblockpool}. @see slicereader @lucene.internal  writes the given value into the slice and resizes the slice if needed starts a new slice and returns the start offset. the returned value should be used as the start offset to initialize a {@link slicereader}. returns the offset of the currently written slice. the returned value should be used as the end offset to initialize a {@link slicereader} once this slice is fully written or to reset the this writer if another slice needs to be written. a {@link slicereader} that can read int slices written by a {@link slicewriter} @lucene.internal creates a new {@link slicereader} on the given pool resets the reader to a slice give the slices absolute start and end offset in the pool returns true iff the current slice is fully read. if this method returns true {@link slicereader#readint()} should not be called again on this slice. reads the next int from the current slice and returns it. @see slicereader#endofslice() we allocated at least one buffer fully zero fill buffers that we fully used partial zero fill the final buffer recycle all but the first buffer re-use the first buffer for slices the buffer must only have 0 values no need to make this public unless we support different sizes todo make the levels and the sizes configurable maybe allocate another block write forwarding address at end of last slice: write new level: end of slice; allocate a new one there is only this one slice to read skip to our next slice we are advancing to the final slice this is not the final slice (subtract 4 for the forwarding address at the end of this new slice)"
org.apache.lucene.util.NamedThreadFactory "a default {@link threadfactory} implementation that accepts the name prefix of the creates a new {@link namedthreadfactory} instance the name prefix assigned to each thread creates a new {@link thread} @see java.util.concurrent.threadfactory#newthread(java.lang.runnable)"
org.apache.lucene.util.OpenBitSetIterator "an iterator to iterate over set bits in an openbitset. this is faster than nextsetbit() for iterating over the complete set of bits, especially when the density of the bits set is high. the python code that generated bitlist def bits2int(val): arr=0 for shift in range(8,0,-1): if val & 0x80: arr = (arr >>32); } if ((y & 0x0000ffff) == 0) { wordshift +=16; y>>>=16; } if ((y & 0x000000ff) == 0) { wordshift +=8; y>>>=8; } indexarray = bitlist[y & 0xff]; word >>>= (wordshift +1); } private void shift3() { int lower = (int)word; int lowbyte = lower & 0xff; if (lowbyte != 0) { indexarray=bitlist[lowbyte]; return; } shift(); } the general idea: instead of having an array per byte that has the offsets of the next set bit, that array could be packed inside a 32 bit integer (8 4 bit numbers). that should be faster than accessing an array for each index, and the total array size is kept smaller (256sizeof(int))=1k hmmm, what about an iterator that finds zeros though, or a reverse iterator... should they be separate classes for efficiency, or have a common root interface? (or maybe both? could ask for a setbitsiterator, etc... 64 bit shifts 32 bit shifts, but a long shift needed at the end loop invariant code motion should move this after the first time, should i go with a linear search, or stick with the binary search in shift? should i<<6 be cached as a separate variable? it would only save one cycle in the best circumstances. setup so next() will also return -1 compensate for 1 based arrindex should i<<6 be cached as a separate variable? it would only save one cycle in the best circumstances."
org.apache.lucene.util.IntsRef "represents int[], as a slice (offset + length) into an existing int[]. the {@link #ints} member should never be null; use {@link #empty_ints} if necessary. @lucene.internal an empty integer array for convenience the contents of the intsref. should never be {@code null}. offset of first valid integer. length of used ints. create a intsref with {@link #empty_ints} create a intsref pointing to a new array of size capacity. offset and length will both be zero. this instance will directly reference ints w/o making a copy. ints should not be null. signed int order comparison used to grow the reference array. in general this should not be used as it does not take the offset into account. @lucene.internal creates a new intsref that points to a copy of the ints from other  the returned intsref will have a length of other.length and an offset of zero. one is a prefix of the other, or, they are equal:"
org.apache.lucene.util.ThreadInterruptedException "thrown by lucene on detecting that thread.interrupt() had been called. unlike java's interruptedexception, this exception is not checked.."
org.apache.lucene.util.fst.CharSequenceOutputs "an fst {@link outputs} implementation where each output is a sequence of characters. @lucene.experimental no common prefix output1 is a prefix of output2 output2 is a prefix of output1 no prefix removed entire output removed todo: maybe utf8?"
org.apache.lucene.util.fst.BytesRefFSTEnum "enumerates all input (bytesref) + output pairs in an fst. @lucene.experimental holds a single input (bytesref) + output pair. dofloor controls the behavior of advance: if it's true dofloor is true, advance positions to the biggest term before target. seeks to smallest term that's >= target. seeks to biggest term that's <= target. seeks to exactly this term, returning null if the term doesn't exist. this is faster than using {@link #seekfloor} or {@link #seekceil} because it short-circuits as soon the match is not found. system.out.println(" enum.next"); current.offset fixed at 1"
org.apache.lucene.util.fst.Outputs "represents the outputs for an fst, providing the basic algebra required for building and traversing the fst. note that any operation that returns no_output must return the same singleton object from {@link #getnooutput}. @lucene.experimental eg common("foo", "foobar") -> "foo" eg subtract("foobar", "foo") -> "bar" eg add("foo", "bar") -> "foobar" encode an output value into a {@link dataoutput}. encode an final node output value into a {@link dataoutput}. by default this just calls {@link #write(object, dataoutput)}. decode an output value previously written with {@link #write(object, dataoutput)}. decode an output value previously written with {@link #writefinaloutput(object, dataoutput)}. by default this just calls {@link #read(datainput)}. note: this output is compared with == so you must ensure that all methods return the single object if it's really no output todo: maybe change this api to allow for re-use of the output instances -- this is an insane amount of garbage (new object per byte/char/int) if eg used during analysis todo: maybe make valid(t output) public...? for asserts"
org.apache.lucene.util.fst.PairOutputs "an fst {@link outputs} implementation, holding two other outputs. @lucene.experimental holds a single pair of two outputs. create a new pair use newpair for assert"
org.apache.lucene.util.fst.ByteSequenceOutputs "an fst {@link outputs} implementation where each output is a sequence of bytes. @lucene.experimental no common prefix output1 is a prefix of output2 output2 is a prefix of output1 no prefix removed entire output removed"
org.apache.lucene.util.fst.IntSequenceOutputs "an fst {@link outputs} implementation where each output is a sequence of ints. @lucene.experimental no common prefix output1 is a prefix of output2 output2 is a prefix of output1 no prefix removed entire output removed"
org.apache.lucene.util.fst.FST "represents an finite state machine (fst), using a compact byte[] format.  the format is similar to what's used by morfologik (http://sourceforge.net/projects/morfologik).  see the {@link org.apache.lucene.util.fst package documentation} for some simple examples. note: the fst cannot be larger than ~2.1 gb because it uses int to address the byte[]. @lucene.experimental specifies allowed range of each int input label for this fst. @see #shouldexpand(uncompilednode) @see #shouldexpand(uncompilednode) @see #shouldexpand(uncompilednode) changed numbytesperarc for array'd case from byte to int. write byte2 labels as 2-byte short, not vint. added optional packed format. if arc has this label then that arc is final/accepted represents a single arc. to node (ord or address) returns this load a previously saved fst. returns bytes used to represent the fst writes an automaton to a file. reads an automaton from a file. returns true if the node at this address has any outgoing arcs fills virtual 'start' arc, ie, an empty incoming arc to the fst's start node follows the follow arc and reads the last arc of its target; this changes the provided arc (2nd arg) in-place and returns it. the second argument (arc). follow the follow arc and read the first arc of its target; this changes the provided arc (2nd arg) in-place and returns it. the second argument (arc). checks if arc's target state is in expanded (or vector) format. true if arc points to a state in an expanded array format. in-place read; returns the arc. peeks at next arc's label; does not alter arc. do not call this if arc.islast()! never returns null, but you should never call this if arc.islast() is true. finds an arc leaving the incoming arc, replacing the arc in place. this returns null if the arc was not found, else the incoming arc. nodes will be expanded if their depth (distance from the root node) is &lt;= this value and their number of arcs is &gt;= {@link #fixed_array_num_arcs_shallow}.  fixed array consumes more ram but enables binary search on the arcs (instead of a linear scan) on lookup by arc label. true if node should be stored in an expanded (array) form. @see #fixed_array_num_arcs_deep @see builder.uncompilednode#depth returns a {@link bytesreader} for this fst, positioned at position 0. returns a {@link bytesreader} for this fst, positioned at the provided position. reads the bytes from this fst. use {@link #getbytesreader(int)} to obtain an instance for this fst; re-use across calls (but only within a single thread) for better performance. public void countsinglechains() throws ioexception { // todo: must assert this fst was built with // "willrewrite" final list> queue = new arraylist>(); // todo: use bitset to not revisit nodes already // visited fixedbitset seen = new fixedbitset(1+nodecount); int saved = 0; queue.add(new arcandstate(getfirstarc(new arc()), new intsref())); arc scratcharc = new arc(); while(queue.size() > 0) { //system.out.println("cycle size=" + queue.size()); //for(arcandstate ent : queue) { // system.out.println(" " + util.tobytesref(ent.chain, new bytesref())); // } final arcandstate arcandstate = queue.get(queue.size()-1); seen.set(arcandstate.arc.node); final bytesref br = util.tobytesref(arcandstate.chain, new bytesref()); if (br.length > 0 && br.bytes[br.length-1] == -1) { br.length--; } //system.out.println(" top node=" + arcandstate.arc.target + " chain=" + br.utf8tostring()); if (targethasarcs(arcandstate.arc) && !seen.get(arcandstate.arc.target)) { // push readfirsttargetarc(arcandstate.arc, scratcharc); //system.out.println(" push label=" + (char) scratcharc.label); //system.out.println(" tonode=" + scratcharc.target + " last?=" + scratcharc.islast()); final intsref chain = intsref.deepcopyof(arcandstate.chain); chain.grow(1+chain.length); // todo //assert scratcharc.label != end_label; chain.ints[chain.length] = scratcharc.label; chain.length++; if (scratcharc.islast()) { if (scratcharc.target != -1 && incounts[scratcharc.target] == 1) { //system.out.println(" append"); } else { if (arcandstate.chain.length > 1) { saved += chain.length-2; try { system.out.println("chain: " + util.tobytesref(chain, new bytesref()).utf8tostring()); } catch (assertionerror ae) { system.out.println("chain: " + util.tobytesref(chain, new bytesref())); } } chain.length = 0; } } else { //system.out.println(" reset"); if (arcandstate.chain.length > 1) { saved += arcandstate.chain.length-2; try { system.out.println("chain: " + util.tobytesref(arcandstate.chain, new bytesref()).utf8tostring()); } catch (assertionerror ae) { system.out.println("chain: " + util.tobytesref(arcandstate.chain, new bytesref())); } } if (scratcharc.target != -1 && incounts[scratcharc.target] != 1) { chain.length = 0; } else { chain.ints[0] = scratcharc.label; chain.length = 1; } } // todo: instead of new arc() we can re-use from // a by-depth array queue.add(new arcandstate(new arc().copyfrom(scratcharc), chain)); } else if (!arcandstate.arc.islast()) { // next readnextarc(arcandstate.arc); //system.out.println(" next label=" + (char) arcandstate.arc.label + " len=" + arcandstate.chain.length); if (arcandstate.chain.length != 0) { arcandstate.chain.ints[arcandstate.chain.length-1] = arcandstate.arc.label; } } else { if (arcandstate.chain.length > 1) { saved += arcandstate.chain.length-2; system.out.println("chain: " + util.tobytesref(arcandstate.chain, new bytesref()).utf8tostring()); } // pop //system.out.println(" pop"); queue.remove(queue.size()-1); while(queue.size() > 0 && queue.get(queue.size()-1).arc.islast()) { queue.remove(queue.size()-1); } if (queue.size() > 0) { final arcandstate arcandstate2 = queue.get(queue.size()-1); readnextarc(arcandstate2.arc); //system.out.println(" read next=" + (char) arcandstate2.arc.label + " queue=" + queue.size()); assert arcandstate2.arc.label != end_label; if (arcandstate2.chain.length != 0) { arcandstate2.chain.ints[arcandstate2.chain.length-1] = arcandstate2.arc.label; } } } } system.out.println("tot saved " + saved); } expert: creates an fst by packing this one. this process requires substantial additional ram (currently up to ~8 bytes per node depending on acceptableoverheadratio), but then should produce a smaller fst. the implementation of this method uses ideas from smaller representation of finite state automata, which describes techniques to reduce the size of a fst. however, this is not a strict implementation of the algorithms described in this paper. if (!usearcarray && nodeupto  num_arcs_array: if set, the target node is delta coded vs current position: 0 => only root node. increment version to change it never serialized; just used to represent the virtual final node w/ no arcs: never serialized; just used to represent the virtual non-final node w/ no arcs: if non-null, this fst accepts the empty string and produces this output not private to avoid synthetic access$nnn methods: used for the bit_target_next optimization (whereby instead of storing the address of the target node for a given arc, we mark a single bit noting that the next node in the byte[] is the target node): from node (ord or address); currently only used when building an fst w/ willpackfst=true: address (into the byte[]), or ord/address if label == end_label this is non-zero if current arcs are fixed array: todo: we could be smarter here, and prune periodically as we go; high in-count nodes will "usually" become clear early on: make a new empty fst, for building; builder invokes this ctor note: only reads most recent format; we don't have back-compat promise for fsts (they are experimental): accepts empty string de-serialize empty-string output: note: bogus because this is only used during building; we need to break out mutable fst from immutable deref straight caches first 128 labels todo: really we should encode this as an arc, arriving to the root node, instead of special casing here: accepts empty string serialize empty-string output: reverse writeint(v); unsigned byte: unsigned short: serializes new node by appending its bytes to the end of the current byte[] system.out.println("fst.addnode pos=" + writer.poswrite + " numarcs=" + nodein.numarcs); system.out.println(" startaddr=" + startaddress); write a "false" first arc: placeholder -- we'll come back and write the number of bytes per arc (int) here: todo: we could make this a vint instead system.out.println(" do fixed arcs array arcsstart=" + fixedarraystart); todo: for better perf (but more ram used) we could avoid this except when arc is "near" the last arc: system.out.println(" write arc: label=" + (char) arc.label + " flags=" + flags + " target=" + target.node + " pos=" + writer.poswrite + " output=" + outputs.outputtostring(arc.output)); system.out.println(" write output"); system.out.println(" write final output"); system.out.println(" write target"); just write the arcs "like normal" on first pass, but record how many bytes each one took, and max byte size: system.out.println(" bytes=" + bytesperarc[arcidx]); todo: if arc'd arrays will be "too wasteful" by some measure, eg if arcs have vastly different sized outputs, then we should selectively disable array for such cases system.out.println(" dofixedarray"); 2nd pass just "expands" all arcs to take up a fixed byte size todo: we could make this a vint instead expand the arcs in place, backwards system.out.println(" repack arcidx=" + arcidx + " srcpos=" + srcpos + " destpos=" + destpos); reverse bytes in-place; we do this so that the "bit_target_next" opto can work, ie, it reads the node just before the current one system.out.println(" endaddress=" + endaddress); nodes are addressed by 1+ord: system.out.println(" write nodeaddress[" + nodecount + "] = " + endaddress); if there are no nodes, ie, the fst only accepts the empty string, then startnode is 0 system.out.println("readlast"); system.out.println(" end node"); array: jump straight to end system.out.println(" array numarcs=" + arc.numarcs + " bpa=" + arc.bytesperarc); non-array: linear scan system.out.println(" scan"); skip this arc: undo the byte flags we read: int pos = address; system.out.println(" readfirsttarget follow.target=" + follow.target + " isfinal=" + follow.isfinal()); insert "fake" final first arc: note: nextarc is a node (not an address!) in this case: system.out.println(" insert isfinal; nextarc=" + follow.target + " islast=" + arc.islast() + " output=" + outputs.outputtostring(arc.output)); system.out.println(" readfirstrealtargtarc address=" + address); system.out.println(" flags=" + arc.flags); system.out.println(" fixedarray"); this is first arc in a fixed-array system.out.println(" bytesper=" + arc.bytesperarc + " numarcs=" + arc.numarcs + " arcsstart=" + pos); arc.flags = b; this was a fake inserted "final" arc system.out.println(" nextarc fake " + arc.nextarc); system.out.println(" nextarc fake array"); system.out.println(" nextarc real array"); arcs are at fixed entries arcs are packed system.out.println(" nextarc real packed"); skip flags todo: can't assert this because we call from readfirstarc assert !flag(arc.flags, bit_last_arc); this is a continuing arc in a fixed array arcs are at fixed entries arcs are packed todo: would be nice to make this lazy -- maybe caller doesn't need the target and is scanning arcs... must scan address is delta-coded from current address: system.out.println(" delta pos=" + pos + " delta=" + code + " target=" + arc.target); deref system.out.println(" deref code=" + code + " target=" + arc.target); absolute system.out.println(" abs code=" + code + " dereflen=" + nodereftoaddress.length); note: nextarc is a node (not an address!) in this case: short-circuit if this arc is in the root arc cache: system.out.println("fta label=" + (char) labeltomatch); arcs are full array; do binary search: system.out.println(" cycle"); system.out.println(" found!"); linear scan system.out.println(" non-bs cycle"); todo: we should fix this code to not have to create object for the output of every arc we scan... only for the matching arc, if found system.out.println(" found!"); 1+ in order to count the -1 implicit final node non-static: writes to fst's byte[] pad: ensure no node gets address 0 which is reserved to mean the stop state w/ no arcs todo: maybe re-use via threadlocal? todo: can we use just bytearraydatainput...? need to add a .skipbytes to datainput.. hmm and .setposition todo: must assert this fst was built with "willrewrite" todo: use bitset to not revisit nodes already visited system.out.println("cycle size=" + queue.size()); for(arcandstate ent : queue) { system.out.println(" " + util.tobytesref(ent.chain, new bytesref())); } system.out.println(" top node=" + arcandstate.arc.target + " chain=" + br.utf8tostring()); push system.out.println(" push label=" + (char) scratcharc.label); system.out.println(" tonode=" + scratcharc.target + " last?=" + scratcharc.islast()); todo assert scratcharc.label != end_label; system.out.println(" append"); system.out.println(" reset"); todo: instead of new arc() we can re-use from a by-depth array next system.out.println(" next label=" + (char) arcandstate.arc.label + " len=" + arcandstate.chain.length); pop system.out.println(" pop"); system.out.println(" read next=" + (char) arcandstate2.arc.label + " queue=" + queue.size()); creates a packed fst note: bogus because this is only used during building; we need to break out mutable fst from immutable todo: other things to try - renumber the nodes to get more next / better locality? - allow multiple input labels on an arc, so singular chain of inputs can take one arc (on wikipedia terms this could save another ~6%) - in the ord case, the output '1' is presumably very common (after no_output)... maybe use a bit for it..? - use spare bits in flags.... for top few labels / outputs / targets find top nodes with highest number of incoming arcs: todo: we could use more ram efficient selection algo here... free up ram: system.out.println("map node=" + n.node + " incount=" + n.count + " to newid=" + downto); +1 because node ords start at 1 (0 is reserved as stop node): fill initial coarse guess: iterate until we converge: system.out.println("\niter"); for assert: skip 0 byte since 0 is reserved target: int totwasted = 0; since we re-reverse the bytes, we now write the nodes backwards, so that bit_target_next is unchanged: system.out.println(" node: " + node + " address=" + address); system.out.println(" change: " + (address - newnodeaddress[node])); for assert: retry loop: possibly iterate more than once, if this is an array'd node and bytesperarc changes: retry writing this node write false first arc: system.out.println("node " + node + ": " + arc.numarcs + " arcs"); int wasted = 0; iterate over all arcs for this node system.out.println(" arc label=" + arc.label + " target=" + arc.target + " pos=" + writer.poswrite); system.out.println("neg: " + delta); system.out.println("neg: " + delta); system.out.println(" delta"); system.out.println(" " + arcbytes + " bytes"); note: this may in fact go "backwards", if somehow (rarely, possibly never) we use more bytesperarc in this rewrite than the incoming fst did... but in this case we will retry (below) so it's ok to ovewrite bytes: wasted += bytesperarc - arcbytes; converged system.out.println(" bba=" + bytesperarc + " wasted=" + wasted); totwasted += wasted; system.out.println(" retry this node maxbytesperarc=" + maxbytesperarc + " vs " + bytesperarc); retry: we don't renumber the nodes (just reverse their order) so nodes should only point forward to other nodes because we only produce acyclic fsts w/ nodes only pointing "forwards": system.out.println("tot wasted=" + totwasted); converged! system.out.println(" " + changedcount + " of " + fst.nodecount + " changed; retry"); system.out.println("new startnode=" + fst.startnode + " old startnode=" + startnode); system.out.println("resize " + fst.bytes.length + " down to " + writer.poswrite); final int size = fst.sizeinbytes(); system.out.println("nextcount=" + nextcount + " topcount=" + topcount + " deltacount=" + deltacount + " abscount=" + abscount); tie-break: smaller node compares as greater than"
org.apache.lucene.util.fst.Builder "builds a minimal fst (maps an intsref term to an arbitrary output) from pre-sorted terms with outputs. the fst becomes an fsa if you use nooutputs. the fst is written on-the-fly into a compact serialized format byte array, which can be saved to / loaded from a directory or used directly for traversal. the fst is always finite (no cycles). note: the algorithm is described at http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 the parameterized type t is the output type. see the subclasses of {@link outputs}. @lucene.experimental expert: this is invoked by builder whenever a suffix is serialized. instantiates an fst/fsa builder without any pruning. a shortcut to {@link #builder(fst.input_type, int, int, boolean, boolean, int, outputs, freezetail, boolean, boolean)} with pruning options turned off. instantiates an fst/fsa builder with {@link packedints#default} acceptableoverheadratio. instantiates an fst/fsa builder with all the possible tuning and construction tweaks. read parameter documentation carefully. the input type (transition labels). can be anything from {@link input_type} enumeration. shorter types will consume less memory. strings (character sequences) are represented as {@link input_type#byte4} (full unicode codepoints). 1 if pruning the input graph during construction, this threshold is used for telling if a node is kept or pruned. if transition_count(node) &gt;= minsuffixcount1, the node is kept. 2 (note: only mike mccandless knows what this one is really doing...) if true, the shared suffixes will be compacted into unique paths. this requires an additional hash map for lookups in memory. setting this parameter to false creates a single path for all input sequences. this will result in a larger graph, but may require less memory and will speed up construction. only used if dosharesuffix is true. set this to true to ensure fst is fully minimal, at cost of more cpu and more ram during building. only used if dosharesuffix is true. set this to integer.max_value to ensure fst is fully minimal, at cost of more cpu and more ram during building. the output type for each input sequence. applies only if building an fst. for fsa, use {@link nooutputs#getsingleton()} and {@link nooutputs#getnooutput()} as the singleton output object. pass true to create a packed fst. how to trade speed for space when building the fst. this option is only relevant when dopackfst is true. @see packedints#getmutable(int, int, float) pass false to disable the array arc optimization while building the fst; this will make the resulting fst smaller but slower to traverse. private string tostring(bytesref b) { try { return b.utf8tostring() + " " + b; } catch (throwable t) { return b.tostring(); } } it's ok to add the same input twice in a row with different outputs, as long as outputs impls the merge method. note that input is fully consumed after this method is returned (so caller is free to reuse), but output is not. so if your outputs are changeable (eg {@link bytesequenceoutputs} or {@link intsequenceoutputs}) then you cannot reuse across calls. if (debug) { bytesref b = new bytesref(input.length); for(int x=0;x<input.length;x++) { b.bytes[x] = (byte) input.ints[x]; } b.length = input.length; if (output == no_output) { system.out.println("\nfst add: input=" + tostring(b) + " " + b); } else { system.out.println("\nfst add: input=" + tostring(b) + " " + b + " output=" + fst.outputs.outputtostring(output)); } } returns final fst. note: this will return null if nothing is accepted by the fst. expert: holds a pending (seen but not yet serialized) arc. expert: holds a pending (seen but not yet serialized) node. this node's depth, starting from the automaton root. the node's depth starting from the automaton root. needed for lucene-2934 (node expansion based on conditions other than the fanout size). javadoc private static final boolean debug = true; simplistic pruning: we prune node (and all following nodes) if less than this number of terms go through it: better pruning: we prune node (and all following nodes) if the prior node has less than this number of terms go through it: for packing note: cutting this over to arraylist instead loses ~6% in build performance on 9.8m wikipedia terms; so we left this as an array: current "frontier" custom plugin: system.out.println(" compiletail " + prefixlenplus1); prune if parent's inputcount is less than suffixmincount2 my parent, about to be compiled, doesn't make the cut, so i'm definitely pruned if minsuffixcount2 is 1, we keep only up until the 'distinguished edge', ie we keep only the 'divergent' part of the fst. if my parent, about to be compiled, has inputcount 1 then we are already past the distinguished edge. note: this only works if the fst outputs are not "compressible" (simple ords are compressible). my parent, about to be compiled, does make the cut, so i'm definitely not pruned if pruning is disabled (count is 0) we can always compile current node system.out.println(" label=" + ((char) lastinput.ints[lastinput.offset+idx-1]) + " idx=" + idx + " inputcount=" + frontier[idx].inputcount + " docompile=" + docompile + " doprune=" + doprune); drop all arcs this node doesn't make it -- deref it we "fake" the node as being final if it has no outgoing arcs; in theory we could leave it as non-final (the fst can represent this), but fstenum, util, etc., have trouble w/ non-final dead-end states: this node makes it and we now compile it. first, compile any targets that were previously undecided: replacelast just to install nextfinaloutput/isfinal onto the arc this node will stay in play for now, since we are undecided on whether to prune it. later, it will be either compiled or pruned, so we must allocate a new node: for debugging de-dup no_output since it must be a singleton: system.out.println("\nadd: " + input); empty input: only allowed as first input. we have to special case this because the packed fst format cannot represent the empty input since 'finalness' is stored on the incoming arc, not on the node compare shared prefix length system.out.println(" incr " + pos1 + " ct=" + frontier[pos1].inputcount + " n=" + frontier[pos1]); minimize/compile states from previous input's orphan'd suffix init tail states for current input push conflicting outputs forward, only as far as needed same input more than 1 time in a row, mapping to multiple outputs this new arc is private to this new input; set its arc output to the leftover output: save last input system.out.println(" count[0]=" + frontier[0].inputcount); minimize nodes in the last word's suffix empty string got pruned if (debug) system.out.println(" builder.finish root.isfinal=" + root.isfinal + " root.output=" + root.output); not yet compiled system.out.println("seg=" + segment + " force final arc=" + (char) arc.label); really an "unsigned" byte note: not many instances of node or compilednode are in memory while the fst is being built; it's only the current "frontier": todo: instead of recording isfinal/output on the node, maybe we should use -1 arc to mean "end" (like we do when reading the fst). would simplify much code here... we don't clear the depth here because it never changes for nodes on the frontier (even when reused). assert target.node != -2; pushes an output prefix forward onto all arcs"
org.apache.lucene.util.fst.NoOutputs "a null fst {@link outputs} implementation; use this if you just want to build an fsa. @lucene.experimental nodehash calls hashcode for this output; we fix this so we get deterministic hashing. assert false; assert false; return null;"
org.apache.lucene.util.fst.NodeHash "used to dedup states (lookup already-frozen states) hash code for an unfrozen node. this must be identical to the un-frozen case (below)!! system.out.println("hash unfrozen"); todo: maybe if number of arcs is high we can safely subsample? system.out.println(" label=" + arc.label + " target=" + ((builder.compilednode) arc.target).node + " h=" + h + " output=" + fst.outputs.outputtostring(arc.output) + " isfinal?=" + arc.isfinal); system.out.println(" ret " + (h&integer.max_value)); hash code for a frozen node system.out.println("hash frozen node=" + node); system.out.println(" label=" + scratcharc.label + " target=" + scratcharc.target + " h=" + h + " output=" + fst.outputs.outputtostring(scratcharc.output) + " next?=" + scratcharc.flag(4) + " final?=" + scratcharc.isfinal()); system.out.println(" ret " + (h&integer.max_value)); system.out.println("hash: add count=" + count + " vs " + table.length); freeze & add system.out.println(" now freeze node=" + node); same node is already here quadratic probe called only by rehash quadratic probe"
org.apache.lucene.util.fst.PositiveIntOutputs "an fst {@link outputs} implementation where each output is a non-negative long value. @lucene.experimental returns the instance of positiveintoutputs. expert: pass doshare=false to disable output sharing. in some cases this may result in a smaller fst, however it will also break methods like {@link util#getbyoutput} and {@link util#shortestpaths}."
org.apache.lucene.util.fst.IntsRefFSTEnum "enumerates all input (intsref) + output pairs in an fst. @lucene.experimental holds a single input (intsref) + output pair. dofloor controls the behavior of advance: if it's true dofloor is true, advance positions to the biggest term before target. seeks to smallest term that's >= target. seeks to biggest term that's <= target. seeks to exactly this term, returning null if the term doesn't exist. this is faster than using {@link #seekfloor} or {@link #seekceil} because it short-circuits as soon the match is not found. system.out.println(" enum.next"); current.offset fixed at 1"
org.apache.lucene.util.fst.FSTEnum "can next() and advance() through the terms in an fst @lucene.experimental dofloor controls the behavior of advance: if it's true dofloor is true, advance positions to the biggest term before target. rewinds enum state to match the shared prefix between current term and target term seeks to smallest term that's >= target. seeks to largest term that's <= target. seeks to exactly target term. outputs are cumulative system.out.println(" init"); system.out.println(" rewind upto=" + upto + " vs targetlength=" + targetlength); seek forward system.out.println(" seek fwd"); seek backwards -- reset this arc to the first arc system.out.println(" seek first arc"); system.out.println(" fall through upto=" + upto); system.out.println("fe: next upto=" + upto); system.out.println(" init"); pop system.out.println(" check pop curarc target=" + arcs[upto].target + " label=" + arcs[upto].label + " islast?=" + arcs[upto].islast()); system.out.println(" eof"); todo: should we return a status here (seek_found / seek_not_found / seek_end)? saves the eq check above? system.out.println(" advance len=" + target.length + " curlen=" + current.length); todo: possibly caller could/should provide common prefix length? ie this work may be redundant if caller is in fact intersecting against its own automaton system.out.println("fe.seekceil upto=" + upto); save time by starting at the end of the shared prefix b/w our current term & the target: system.out.println(" after rewind upto=" + upto); system.out.println(" init targetlabel=" + targetlabel); now scan forward, matching the new suffix of the target system.out.println(" cycle upto=" + upto + " arc.label=" + arc.label + " (" + (char) arc.label + ") vs targetlabel=" + targetlabel); arcs are fixed array -- use binary search to find the target. system.out.println("do arc array low=" + low + " high=" + high + " targetlabel=" + targetlabel); system.out.println(" cycle low=" + low + " high=" + high + " mid=" + mid + " midlabel=" + midlabel + " cmp=" + cmp); note: this code is dup'd w/ the code below (in the outer else clause): match dead end dead end (target is after the last arc); rollback to last fork then push system.out.println(" rollback upto=" + upto + " arc.label=" + prevarc.label + " islast?=" + prevarc.islast()); arcs are not array'd -- must do linear scan: recurse dead end (target is after the last arc); rollback to last fork then push system.out.println(" rollback upto=" + upto + " arc.label=" + prevarc.label + " islast?=" + prevarc.islast()); keep scanning system.out.println(" next scan"); todo: should we return a status here (seek_found / seek_not_found / seek_end)? saves the eq check above? todo: possibly caller could/should provide common prefix length? ie this work may be redundant if caller is in fact intersecting against its own automaton system.out.println("fe: seek floor upto=" + upto); save cpu by starting at the end of the shared prefix b/w our current term & the target: system.out.println("fe: after rewind upto=" + upto); system.out.println("fe: init targetlabel=" + targetlabel); now scan forward, matching the new suffix of the target system.out.println(" cycle upto=" + upto + " arc.label=" + arc.label + " (" + (char) arc.label + ") targetlabel=" + targetlabel + " islast?=" + arc.islast() + " bba=" + arc.bytesperarc); arcs are fixed array -- use binary search to find the target. system.out.println("do arc array low=" + low + " high=" + high + " targetlabel=" + targetlabel); system.out.println(" cycle low=" + low + " high=" + high + " mid=" + mid + " midlabel=" + midlabel + " cmp=" + cmp); note: this code is dup'd w/ the code below (in the outer else clause): match -- recurse system.out.println(" match! arcidx=" + mid); system.out.println(" before first"); very first arc is after our target todo: if each arc could somehow read the arc just before, we can save this re-scan. the ceil case doesn't need this because it reads the next arc instead: first, walk backwards until we find a first arc that's before our target label: then, scan forwards to the arc just before the targetlabel: there is a floor arc: system.out.println(" hasfloor arcidx=" + (arc.arcidx+1)); match -- recurse todo: if each arc could somehow read the arc just before, we can save this re-scan. the ceil case doesn't need this because it reads the next arc instead: first, walk backwards until we find a first arc that's before our target label: then, scan forwards to the arc just before the targetlabel: system.out.println(" check next label=" + fst.readnextarclabel(arc) + " (" + (char) fst.readnextarclabel(arc) + ")"); keep scanning todo: possibly caller could/should provide common prefix length? ie this work may be redundant if caller is in fact intersecting against its own automaton system.out.println("fe: seek exact upto=" + upto); save time by starting at the end of the shared prefix b/w our current term & the target: system.out.println("fe: after rewind upto=" + upto); system.out.println(" cycle target=" + (targetlabel == -1 ? "-1" : (char) targetlabel)); short circuit upto--; upto = 0; system.out.println(" no match upto=" + upto); match -- recurse: system.out.println(" return found; upto=" + upto + " output=" + output[upto] + " nextarc=" + nextarc.islast()); appends current arc, and then recurses from its target, appending first arc all the way to the final node final node system.out.println(" pushfirst label=" + (char) arc.label + " upto=" + upto + " output=" + fst.outputs.outputtostring(output[upto])); recurses from current arc, appending last arc all the way to the first final node final node"
org.apache.lucene.util.fst.Util "static helper methods. @lucene.experimental looks up the output for this input, or null if the input is not accepted. looks up the output for this input, or null if the input is not accepted reverse lookup (lookup by output instead of by input), in the special case when your fsts outputs are strictly ascending. this locates the input/output pair where the output is equal to the target, and will return null if that output does not exist. note: this only works with {@code fst}, only works when the outputs are ascending in order with the inputs and only works when you shared the outputs (pass doshare=true to {@link positiveintoutputs#getsingleton}). for example, simple ordinals (0, 1, 2, ...), or file offets (when appending to a file) fit this. compares first by the provided comparator, and then tie breaks by path.input. utility class to find top n shortest paths from start point(s). adds all leaving arcs, including 'finished' arc, if the node is final, from this node into the queue. holds a single input (intsref) + output, returned by {@link #shortestpaths shortestpaths()}. starting from node, find the top n min cost completions to a final node. note: you must share the outputs when you build the fst (pass doshare=true to {@link positiveintoutputs#getsingleton}). dumps an {@link fst} to a graphviz's dot language description for visualization. example of use:  printwriter pw = new printwriter(&quot;out.dot&quot;); util.todot(fst, pw, true, true); pw.close();  and then, from command line:  dot -tpng -o out.png out.dot   note: larger fsts (a few thousand nodes) won't even render, don't bother. if true, the resulting dot file will try to order states in layers of breadth-first traversal. this may mess up arcs, but makes the output fst's structure a bit clearer. if true states will have labels equal to their offsets in their binary format. expands the graph considerably. @see "http://www.graphviz.org/" boolean isfinal = false; t finaloutput = null; fst.readfirsttargetarc(arc, scratcharc); if (scratcharc.isfinal() && fst.targethasarcs(scratcharc)) { // target is final isfinal = true; finaloutput = scratcharc.output == no_output ? null : scratcharc.output; system.out.println("dot hit final label=" + (char) scratcharc.label); } emit a single state in the dot language. ensures an arc's label is indeed printable (dot uses us-ascii). just maps each utf16 unit (char) to the ints in an intsref. decodes the unicode codepoints from the provided charsequence and places them in the provided scratch intsref, which must not be null, returning it. decodes the unicode codepoints from the provided char[] and places them in the provided scratch intsref, which must not be null, returning it. just takes unsigned byte values from the bytesref and converts into an intsref. just converts intsref to bytesref; you must ensure the int values fit into a byte. public static  void dottofile(fst fst, string filepath) throws ioexception { writer w = new outputstreamwriter(new fileoutputstream(filepath)); todot(fst, w, true, true); w.close(); } reads the first arc greater or equal that the given label into the provided arc in place and returns it iff found, otherwise return null. the label to ceil on the fst to operate on the arc to follow reading the label from the arc to read into in place the fst's {@link bytesreader} todo: would be nice not to alloc this on every lookup accumulate output as we go todo: maybe a charsref version for byte2 todo: would be nice not to alloc this on every lookup accumulate output as we go todo: would be nice not to alloc this on every lookup system.out.println("reverselookup output=" + targetoutput); system.out.println("loop: output=" + output + " upto=" + upto + " arc=" + arc); system.out.println(" isfinal finaloutput=" + finaloutput); system.out.println(" found!"); system.out.println(" not found!"); system.out.println(" targethasarcs"); system.out.println("bsearch: numarcs=" + arc.numarcs + " target=" + targetoutput + " output=" + output); system.out.println(" cycle mid=" + mid + " label=" + (char) label + " output=" + minarcoutput); system.out.println(" cycle label=" + arc.label + " output=" + arc.output); this is the min output we'd hit if we follow this arc: recurse on this arc: system.out.println(" match! break"); output doesn't exist recurse on previous arc: system.out.println(" recurse prev label=" + (char) arc.label + " output=" + output); recurse on this arc: system.out.println(" recurse last label=" + (char) arc.label + " output=" + output); read next arc in this node: system.out.println(" after copy label=" + (char) prevarc.label + " vs " + (char) arc.label); system.out.println(" no target arcs; not found!"); if back plus this arc is competitive then add to queue: system.out.println(" addifcompetitive queue.size()=" + queue.size() + " path=" + path + " + label=" + path.arc.label); doesn't compete tie break by alpha sort on the input: we should never see dups: doesn't compete competes queue isn't full yet, so any path we hit competes: copy over the current input to the new input and add the arc.label to the end de-dup no_output since it must be a singleton: system.out.println("add start paths"); bootstrap: find the min starting arc system.out.println("search topn=" + topn); todo: we could enable fst to sorting arcs by weight as it freezes... can easily do this on first pass (w/o requiring rewrite) todo: maybe we should make an fst.input_type.byte0.5!? (nibbles) for each top n path: system.out.println("\nfind next path: queue.size=" + queue.size()); ran out of paths remove top path since we are now going to pursue it: there were less than topn paths available: system.out.println(" empty string! cost=" + path.cost); empty string! last path -- don't bother w/ queue anymore: system.out.println(" path: " + path); we take path and find its "0 output completion", ie, just keep traversing the first arc with no_output that we can find, since this must lead to the minimum path that completes from path.arc. for each input letter: system.out.println("\n cycle path: " + path); for each arc leaving this node: system.out.println(" arc=" + (char) path.arc.label + " cost=" + path.arc.output); tricky: instead of comparing output == 0, we must express it via the comparator compare(output, 0) == 0 todo: maybe we can save this copyfrom if we are more clever above... eg on finding the first no_output arc we'd switch to using scratcharc add final output: system.out.println(" done!: " + path); all paths are kept, so we can pass topn for maxqueuedepth and the pruning is admissible: since this search is initialized with a single start node it is okay to start with an empty input path here this is the start arc in the automaton (from the epsilon state to the first state with outgoing transitions. a queue of transitions to consider for the next level. a queue of transitions to consider when processing the next level. system.out.println("todot: startarc: " + startarc); a list of states on the same level (for ranking). a bitset of already seen states (target offset). shape for states. emit dot prologue. final fst.arc scratcharc = new fst.arc(); we could double buffer here, but it doesn't matter probably. system.out.println("next level=" + level); transitions and states at level: " + level + "\n"); system.out.println(" pop: " + arc); scan all target arcs system.out.println(" readfirsttarget..."); system.out.println(" firsttarget: " + arc); system.out.println(" cycle arc=" + arc); emit the unseen state and add it to the queue for the next level. target is final to see the node address, use this instead: emitdotstate(out, integer.tostring(arc.target), stateshape, statecolor, string.valueof(arc.target)); tricky special case: sometimes, due to pruning, the builder can [sillily] produce an fst with an arc into the final end state (-1) but also with a next final output; in this case we pull that output up onto this arc break the loop if we're on the last arc of this state. system.out.println(" break"); emit state ranking information. emit terminating state (always there anyway). note: we allow -128 to 255 uncomment for debugging: todo maybe this is a useful in the fst class - we could simplify some other code like fstenum? note: nextarc is a node (not an address!) in this case: arcs are fixed array -- use binary search to find the target. system.out.println("do arc array low=" + low + " high=" + high + " targetlabel=" + targetlabel); system.out.println(" cycle low=" + low + " high=" + high + " mid=" + mid + " midlabel=" + midlabel + " cmp=" + cmp); dead end! linear scan system.out.println(" non-bs cycle"); todo: we should fix this code to not have to create object for the output of every arc we scan... only for the matching arc, if found system.out.println(" found!");"
org.apache.lucene.util.Counter "simple counter class @lucene.internal @lucene.experimental adds the given delta to the counters current value the delta to add counters updated value returns the counters current value counters current value returns a new counter. the returned counter is not thread-safe. returns a new counter. true if the returned counter can be used by multiple threads concurrently. new counter."
org.apache.lucene.util.WeakIdentityMap "implements a combination of {@link java.util.weakhashmap} and {@link java.util.identityhashmap}. useful for caches that need to key off of a {@code ==} comparison instead of a {@code .equals}. this class is not a general-purpose {@link java.util.map} implementation! it intentionally violates map's general contract, which mandates the use of the equals method when comparing objects. this class is designed for use only in the rare cases wherein reference-equality semantics are required. this implementation was forked from apache cxf but modified to not implement the {@link java.util.map} interface and without any set views on it, as those are error-prone and inefficient, if not implemented carefully. the map only contains {@link iterator} implementations on the values and not-gced keys. lucene's implementation also supports {@code null} keys, but those are never weak! @lucene.internal creates a new {@code weakidentitymap} based on a non-synchronized {@link hashmap}. creates a new {@code weakidentitymap} based on a {@link concurrenthashmap}. removes all of the mappings from this map. returns {@code true} if this map contains a mapping for the specified key. returns the value to which the specified key is mapped. associates the specified value with the specified key in this map. if the map previously contained a mapping for this key, the old value is replaced. returns {@code true} if this map contains no key-value mappings. removes the mapping for a key from this weak hash map if it is present. returns the value to which this map previously associated the key, or {@code null} if the map contained no mapping for the key. a return value of {@code null} does not necessarily indicate that the map contained. returns the number of key-value mappings in this map. this result is a snapshot, and may not reflect unprocessed entries that will be removed before next attempted access because they are no longer referenced. returns an iterator over all weak keys of this map. keys already garbage collected will not be returned. this iterator does not support removals. returns an iterator over all values of this map. this iterator may return values whose key is already garbage collected while iterator is consumed. important: don't use oal.util.filteriterator here: we need strong reference to current key after setnext()!!! holds strong reference to next element in backing iterator: the backing iterator was already consumed: release strong reference and invalidate current value: the key was already gced, we can remove it from backing map: unfold "null" special value: we keep a hard reference to our null key, so map supports null keys that never get gced:"
org.apache.lucene.util.SmallFloat "floating point numbers smaller than 32 bits. @lucene.internal no instance converts a 32 bit float to an 8 bit float. values less than zero are all mapped to zero. values are truncated (rounded down) to the nearest 8 bit value. values between zero and the smallest representable value are rounded up. the 32 bit float to be converted to an 8 bit float (byte) the number of mantissa bits to use in the byte, with the remainder to be used in the exponent the zero-point in the range of exponent values 8 bit float representation converts an 8 bit float to a 32 bit float. floattobyte(b, mantissabits=3, zeroexponent=15) smallest non-zero value = 5.820766e-10 largest value = 7.5161928e9 epsilon = 0.125 bytetofloat(b, mantissabits=3, zeroexponent=15) floattobyte(b, mantissabits=5, zeroexponent=2) smallest nonzero value = 0.033203125 largest value = 1984.0 epsilon = 0.03125 bytetofloat(b, mantissabits=5, zeroexponent=2) adjustment from a float zero exponent to our zero exponent, shifted over to our exponent position. negative numbers and zero both map to 0 byte underflow is mapped to smallest non-zero number. overflow maps to largest number on java1.5 & 1.6 jvms, prebuilding a decoding array and doing a lookup is only a little bit faster (anywhere from 0% to 7%)  some specializations of the generic functions follow. the generic functions are just as fast with current (1.5) -server jvms, but still slower with client jvms.  on java1.5 & 1.6 jvms, prebuilding a decoding array and doing a lookup is only a little bit faster (anywhere from 0% to 7%) on java1.5 & 1.6 jvms, prebuilding a decoding array and doing a lookup is only a little bit faster (anywhere from 0% to 7%)"
org.apache.lucene.util.MapOfSets "helper class for keeping lists of objects associated with keys. warning: this class is not thread safe @lucene.internal the backing store for this object access to the map backing this object. adds val to the set associated with key in the map. if key is not already in the map, a new set will first be adds multiple vals to the set associated with key in the map. if key is not already in the map, a new set will first be"
org.apache.lucene.util.RecyclingIntBlockAllocator "a {@link allocator} implementation that recycles unused int blocks in a buffer and reuses them in subsequent calls to {@link #getintblock()}.  note: this class is not thread-safe  @lucene.internal creates a new {@link recyclingintblockallocator} the block size in bytes maximum number of buffered int block {@link counter} reference counting internally allocated bytes creates a new {@link recyclingintblockallocator}. the size of each block returned by this allocator maximum number of buffered int blocks creates a new {@link recyclingintblockallocator} with a block size of {@link intblockpool#int_block_size}, upper buffered docs limit of {@link #default_buffered_blocks} ({@value #default_buffered_blocks}). number of currently buffered blocks number of bytes currently allocated by this {@link allocator} maximum number of buffered byte blocks removes the given number of int blocks from the buffer if possible. the number of int blocks to remove number of actually removed buffers"
org.apache.lucene.util.CollectionUtil "methods for manipulating (sorting) collections. sort methods work directly on the supplied lists and don't copy to/from arrays before/after. for medium size collections as used in the lucene indexer that is much more efficient. @lucene.internal sortertemplate with custom {@link comparator} natural sortertemplate sorts the given random access {@link list} using the {@link comparator}. the list must implement {@link randomaccess}. this method uses the quick sort algorithm, but falls back to insertion sort for small lists. @throws illegalargumentexception if list is e.g. a linked list without random access. sorts the given random access {@link list} in natural order. the list must implement {@link randomaccess}. this method uses the quick sort algorithm, but falls back to insertion sort for small lists. @throws illegalargumentexception if list is e.g. a linked list without random access. sorts the given random access {@link list} using the {@link comparator}. the list must implement {@link randomaccess}. this method uses the merge sort algorithm, but falls back to insertion sort for small lists. @throws illegalargumentexception if list is e.g. a linked list without random access. sorts the given random access {@link list} in natural order. the list must implement {@link randomaccess}. this method uses the merge sort algorithm, but falls back to insertion sort for small lists. @throws illegalargumentexception if list is e.g. a linked list without random access. sorts the given random access {@link list} using the {@link comparator}. the list must implement {@link randomaccess}. this method uses the insertion sort algorithm. it is only recommended to use this algorithm for partially sorted small lists! @throws illegalargumentexception if list is e.g. a linked list without random access. sorts the given random access {@link list} in natural order. the list must implement {@link randomaccess}. this method uses the insertion sort algorithm. it is only recommended to use this algorithm for partially sorted small lists! @throws illegalargumentexception if list is e.g. a linked list without random access. no instance mergesorts: insertionsorts:"
org.apache.lucene.util.InfoStream "debugging api for lucene classes such as {@link indexwriter} and {@link segmentinfos}.  note: enabling infostreams may cause performance degradation in some components. @lucene.internal instance of infostream that does no logging at all. prints a message returns true if messages are enabled and should be posted to {@link #message}. the default {@code infostream} used by a newly instantiated classes. @see #setdefault sets the default {@code infostream} used by a newly instantiated classes. it cannot be {@code null}, to disable logging use {@link #no_output}. @see #getdefault javadocs javadocs"
org.apache.lucene.util.Bits "interface for bitset-like structures. @lucene.experimental returns the value of the bit with the specified index. index, should be non-negative and &lt; {@link #length()}. the result of passing negative or out of bounds values is undefined by this interface, just don't do it! true if the bit is set, false otherwise. returns the number of bits in this set bits impl of the specified length with all bits set. bits impl of the specified length with no bits set."
org.apache.lucene.util.FilterIterator "an {@link iterator} implementation that filters elements with a boolean predicate. @see #predicatefunction returns true, if this element should be returned by {@link #next()}."
org.apache.lucene.util.IndexableBinaryStringTools "provides support for converting byte sequences to strings and back again. the resulting strings preserve the original byte sequences' sort order.  the strings are constructed using a base 8000h encoding of the original binary data - each char of an encoded string represents a 15-bit chunk from the byte sequence. base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [u+d8000-u+dfff] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit.  although unset bits are used as padding in the final char, the original byte sequence could contain trailing bytes with no set bits (null bytes): padding is indistinguishable from valid information. to overcome this problem, a char is appended, indicating the number of encoded bytes in the final content char.  @lucene.experimental @deprecated implement {@link termtobytesrefattribute} and store bytes directly instead. this class will be removed in lucene 5.0 returns the number of chars required to encode the given bytes. byte sequence to be encoded initial offset into inputarray number of bytes in inputarray number of chars required to encode the number of bytes. returns the number of bytes required to decode the given char sequence. char sequence to be decoded initial offset number of characters number of bytes required to decode the given char sequence encodes the input byte sequence into the output char sequence. before calling this method, ensure that the output array has sufficient capacity by calling {@link #getencodedlength(byte[], int, int)}. byte sequence to be encoded initial offset into inputarray number of bytes in inputarray char sequence to store encoded result initial offset into outputarray length of output, must be getencodedlength decodes the input char sequence into the output byte sequence. before calling this method, ensure that the output array has sufficient capacity by calling {@link #getdecodedlength(char[], int, int)}. char sequence to be decoded initial offset into inputarray number of chars in inputarray byte sequence to store encoded result initial offset into outputarray length of output, must be getdecodedlength(inputarray, inputoffset, inputlength) javadoc codingcase(int initialshift, int finalshift) codingcase(int initialshift, int middleshift, int finalshift) export only static methods use long for intermediaries to protect against overflow use long for intermediaries to protect against overflow numbytes is 3 produce final char (if any) and trailing count chars. codingcase.numbytes must be 3 add trailing char containing the number of full bytes in final char add trailing char containing the number of full bytes in final char no left over bits - last char is completely filled. add trailing char containing the number of full bytes in final char numbytes is 3 handle final char numbytes is 3"
org.apache.lucene.util.SPIClassIterator "helper class for loading spi classes from classpath (meta-inf files). this is a light impl of {@link java.util.serviceloader} but is guaranteed to be bug-free regarding classpath order and does not instantiate or initialize the classes found. @lucene.internal hasnext() implicitely loads the next profile, so it is essential to call this here! don't initialize the class (pass false as 2nd parameter):"
org.apache.lucene.util.RamUsageEstimator "estimates the size (memory representation) of java objects. @see #sizeof(object) @see #shallowsizeof(object) @see #shallowsizeofinstance(class) @lucene.internal jvm diagnostic features. jvm info string for debugging and reports. one kilobyte bytes. one megabyte bytes. one gigabyte bytes. no instantiation. number of bytes this jvm uses to represent an object reference. number of bytes to represent an object header (no fields, no alignments). number of bytes to represent an array header (no content, but with alignments). a constant specifying the object alignment boundary inside the jvm. objects will always take a full multiple of this constant, possibly wasting some space. sizes of primitive classes. a handle to sun.misc.unsafe. a handle to sun.misc.unsafe#fieldoffset(field). all the supported "internal" jvm features detected at clinit. initialize constants and try to collect information about the jvm internals. cached information about a given class. returns true, if the current jvm is fully supported by {@code ramusageestimator}. if this method returns {@code false} you are maybe using a 3rd party java vm that is not supporting oracle/sun private apis. the memory estimates can be imprecise then (no way of detecting compressed references, alignments, etc.). lucene still tries to use sensible defaults. aligns an object size to be the next multiple of {@link #num_bytes_object_alignment}. returns the size in bytes of the byte[] object. returns the size in bytes of the boolean[] object. returns the size in bytes of the char[] object. returns the size in bytes of the short[] object. returns the size in bytes of the int[] object. returns the size in bytes of the float[] object. returns the size in bytes of the long[] object. returns the size in bytes of the double[] object. estimates the ram usage by the given object. it will walk the object tree and sum up all referenced objects. resource usage: this method internally uses a set of every object seen during traversals so it does allocate memory (it isn't side-effect free). after the method exits, this memory should be gced. estimates a "shallow" memory usage of the given object. for arrays, this will be the memory taken by array storage (no subreferences will be followed). for objects, this will be the memory taken by the fields. jvm object alignments are also applied. returns the shallow instance size in bytes an instance of the given class would occupy. this works with all conventional classes and primitive types, but not with arrays (the size then depends on the number of elements and varies from object to object). @see #shallowsizeof(object) @throws illegalargumentexception if {@code clazz} is an array class. return shallow size of any array. non-recursive version of object descend. this consumes more memory than recursive in-depth traversal but prevents stack overflows on long chains of objects or complex graphs (a max. recursion depth on my machine was ~5000 objects linked in a chain so not too much). consider an array, possibly of primitive types. push any of its references to the processing stack and accumulate this array's shallow size. consider an object. push any references it has to the processing stack and accumulate this object's shallow size. create a cached information about shallow size and reference fields for a given class. this method returns the maximum representation size of an object. sizesofar is the object's size measured so far. f is the field being probed. the returned offset will be the maximum of whatever was measured so far and f field's offset and representation size (unaligned). return the set of unsupported jvm features that improve the estimation. return the set of supported jvm features that improve the estimation. returns size in human-readable units (gb, mb, kb or bytes). returns size in human-readable units (gb, mb, kb or bytes). return a human-readable size of a given object. @see #sizeof(object) @see #humanreadableunits(long) an identity hash set implemented using open addressing. no null keys are allowed. todo: if this is useful outside this class, make it public - needs some work default load factor. minimum capacity for the set. all of set entries. always of power of two length. cached number of assigned slots. the load factor for this set (fraction of allocated or deleted slots before the buffers must be rehashed or reallocated). cached capacity threshold at which we must resize the buffers. creates a hash set with the default capacity of 16. load factor of {@value #default_load_factor}. ` creates a hash set with the given capacity, load factor of {@value #default_load_factor}. creates a hash set with the given capacity and load factor. adds a reference to the set. null keys are not allowed. checks if the set contains a given ref. rehash via murmurhash. the implementation is based on the finalization step from austin appleby's murmurhash3. @see "http://sites.google.com/site/murmurhash/" expand the internal storage buffers (capacity) or rehash current keys and values if there are a lot of deleted slots. rehash all assigned slots from the old hash table. allocate internal buffers for a given capacity. new capacity (must be a power of two). return the next possible capacity, counting from the current buffers' size. round the capacity to the next allowed value. initialize empirically measured defaults. we'll modify them to the current jvm settings later on if possible. the following is objectheader + num_bytes_int, but aligned (object alignment) so on 64 bit jvms it'll be align(16 + 4, @8) = 24. ignore. get object reference size by getting scale factor of object[] arrays: ignore. "best guess" based on reference size. we will attempt to modify these to exact values if there is supported infrastructure. get the object header size: - first try out if the field offsets are not scaled (see warning in unsafe docs) - get the object header size by getting the field offset of the first field of a dummy object if the scaling is byte-wise and unsafe is available, enable dynamic size measurement for estimateramusage(). ignore. get the array header size by retrieving the array base offset (offset of the first element of an array). we calculate that only for byte[] arrays, it's actually the same for all types: ignore. try to get the object alignment (the default seems to be 8 on hotspot, regardless of the architecture). ignore. object with just one field to determine the object header size by getting the offset of the dummy field: another test object for checking, if the difference in offsets of dummy1 and dummy2 is 8 bytes. only then we can be sure that those are real, unscaled offsets: walk type hierarchy objects seen so far. class cache with reference field and precalculated shallow size. stack of objects pending traversal. recursion caused stack overflows. push refs for traversal later. fast path to eliminate redundancies. this should never happen as we enabled setaccessible(). help the gc (?). this should never happen (unsafe does not declare checked exceptions for this method), but who knows? todo: no alignments based on field type/ subclass fields alignments? already found. maximum positive integer that is a power of two."
org.apache.lucene.util.IOUtils "this class emulates the new java 7 "try-with-resources" statement. remove once lucene is on java 7. @lucene.internal utf-8 charset string @see charset#forname(string) utf-8 {@link charset} instance to prevent repeated {@link charset#forname(string)} lookups closes all given closeables, suppressing all thrown exceptions. some of the closeables may be null, they are ignored. after everything is closed, method either throws priorexception, if one is supplied, or the first of suppressed exceptions, or completes normally. sample usage:  closeable resource1 = null, resource2 = null, resource3 = null; expectedexception priore = null; try { resource1 = ...; resource2 = ...; resource3 = ...; // acquisition may throw expectedexception ..do..stuff.. // may throw expectedexception } catch (expectedexception e) { priore = e; } finally { closewhilehandlingexception(priore, resource1, resource2, resource3); }   null or an exception that will be rethrown after method completion objects to call close() on closes all given closeables, suppressing all thrown exceptions. @see #closewhilehandlingexception(exception, closeable...) closes all given closeables. some of the closeables may be null; they are ignored. after everything is closed, the method either throws the first exception it hit while closing, or completes normally if there were no exceptions. objects to call close() on closes all given closeables. @see #close(closeable...) closes all given closeables, suppressing all thrown exceptions. some of the closeables may be null, they are ignored. objects to call close() on closes all given closeables, suppressing all thrown exceptions. @see #closewhilehandlingexception(closeable...) this reflected {@link method} is {@code null} before java 7 adds a throwable to the list of suppressed exceptions of the first throwable (if java 7 is detected) this exception should get the suppressed one added the suppressed exception wrapping the given {@link inputstream} in a reader using a {@link charsetdecoder}. unlike java's defaults this reader will throw an exception if your it detects the read charset doesn't match the expected {@link charset}.  decoding readers are useful to load configuration files, stopword lists or synonym files to detect character set problems. however, its not recommended to use as a common purpose reader. the stream to wrap in a reader the expected charset wrapping reader opens a reader for the given {@link file} using a {@link charsetdecoder}. unlike java's defaults this reader will throw an exception if your it detects the read charset doesn't match the expected {@link charset}.  decoding readers are useful to load configuration files, stopword lists or synonym files to detect character set problems. however, its not recommended to use as a common purpose reader. the file to open a reader on the expected charset reader to read the given file opens a reader for the given resource using a {@link charsetdecoder}. unlike java's defaults this reader will throw an exception if your it detects the read charset doesn't match the expected {@link charset}.  decoding readers are useful to load configuration files, stopword lists or synonym files to detect character set problems. however, its not recommended to use as a common purpose reader. the class used to locate the resource the resource name to load the expected charset reader to read the given file deletes all given files, suppressing all thrown ioexceptions.  note that the files should not be null. copy one file's contents to another file. the target will be overwritten if it exists. the source must exist. no instance acquisition may throw expectedexception may throw expectedexception ignore any exceptions caused by invoking (e.g. security constraints) ignore"
org.apache.lucene.util.DocIdBitSet "simple docidset and docidsetiterator backed by a bitset this docidset implementation is cacheable. returns the underlying bitset. the size may not be correct... (docid + 1) on next line requires -1 initial value for docnr: -1 returned by bitset.nextsetbit() when exhausted -1 returned by bitset.nextsetbit() when exhausted"
org.apache.lucene.util.CharsRef "represents char[], as a slice (offset + length) into an existing char[]. the {@link #chars} member should never be null; use {@link #empty_chars} if necessary. @lucene.internal an empty character array for convenience the contents of the charsref. should never be {@code null}. offset of first valid character. length of used characters. creates a new {@link charsref} initialized an empty array zero-length creates a new {@link charsref} initialized with an array of the given capacity creates a new {@link charsref} initialized with the given array, offset and length creates a new {@link charsref} initialized with the given strings character array signed int order comparison copies the given {@link charsref} referenced content into this instance. the {@link charsref} to copy used to grow the reference array. in general this should not be used as it does not take the offset into account. @lucene.internal copies the given array into this charsref. appends the given array to this charsref @deprecated this comparator is only a transition mechanism @deprecated this comparator is only a transition mechanism @deprecated this comparator is only a transition mechanism achar != bchar, fix up each one if they're both in or above the surrogate range, then compare them now achar and bchar are in code point order int must be 32 bits wide creates a new charsref that points to a copy of the chars from other  the returned charsref will have a length of other.length and an offset of zero. one is a prefix of the other, or, they are equal: note: must do a real check here to meet the specs of charsequence note: must do a real check here to meet the specs of charsequence only singleton http://icu-project.org/docs/papers/utf16_code_point_order.html one is a prefix of the other, or, they are equal:"
org.apache.lucene.util.StringHelper "methods for manipulating strings. @lucene.internal compares two {@link bytesref}, element by element, and returns the number of elements common to both arrays. the first {@link bytesref} to compare the second {@link bytesref} to compare number of common elements. comparator over versioned strings such as x.yy.z @lucene.internal returns true iff the ref starts with the given prefix. otherwise false. the {@link bytesref} to test the expected prefix true iff the ref starts with the given prefix. otherwise false. returns true iff the ref ends with the given suffix. otherwise false. the {@link bytesref} to test the expected suffix true iff the ref ends with the given suffix. otherwise false. a has some extra trailing tokens. if these are all zeroes, thats ok. b has some extra trailing tokens. if these are all zeroes, thats ok."
org.apache.lucene.util.MathUtil "math static utility methods. returns {@code x  1} no instance:"
org.apache.lucene.util.Version "use by certain classes to match version compatibility across releases of lucene. warning: when changing the version parameter that you supply to components in lucene, do not simply change the version at search-time, but instead also adjust your indexing code to match, and re-index. match settings and bugs in lucene's 3.0 release. @deprecated (4.0) use latest match settings and bugs in lucene's 3.1 release. @deprecated (4.0) use latest match settings and bugs in lucene's 3.2 release. @deprecated (4.0) use latest match settings and bugs in lucene's 3.3 release. @deprecated (4.0) use latest match settings and bugs in lucene's 3.4 release. @deprecated (4.0) use latest match settings and bugs in lucene's 3.5 release. @deprecated (4.0) use latest match settings and bugs in lucene's 3.6 release. @deprecated (4.0) use latest match settings and bugs in lucene's 3.6 release. @deprecated (4.1) use latest match settings and bugs in lucene's 4.1 release.  use this to get the latest &amp; greatest settings, bug fixes, etc, for lucene. add new constants for later versions here to respect order! warning: if you use this setting, and then upgrade to a newer release of lucene, sizable changes may happen. if backwards compatibility is important then you should instead explicitly specify an actual version.  if you use this constant then you may need to re-index all of your documents when upgrading lucene, as the way text is indexed may have changed. additionally, you may need to re-test your entire application to ensure it behaves as expected, as some defaults may have changed and may break functionality in your application. @deprecated use an actual version instead. remove me when java 5 is no longer supported this is a workaround for a jdk bug that wrongly emits a warning."
org.apache.lucene.util.AttributeSource "an attributesource contains a list of different {@link attributeimpl}s, and methods to add and get them. there can only be a single instance of an attribute in the same attributesource instance. this is ensured by passing in the actual type of the attribute (class&lt;attribute&gt;) to the {@link #addattribute(class)}, which then checks if an instance of that type is already present. if yes, it returns the instance, otherwise it creates a new instance and returns it. an attributefactory creates instances of {@link attributeimpl}s. returns an {@link attributeimpl} for the supplied {@link attribute} interface class. this is the default factory that creates {@link attributeimpl}s using the class name of the supplied {@link attribute} interface class by appending impl to it. this class holds the state of an attributesource. @see #capturestate @see #restorestate an attributesource using the default attribute factory {@link attributesource.attributefactory#default_attribute_factory}. an attributesource that uses the same attributes as the supplied one. an attributesource using the supplied {@link attributefactory} for creating new {@link attribute} instances. returns the used attributefactory. returns a new iterator that iterates the attribute classes in the same order they were added in. returns a new iterator that iterates all unique attribute implementations. this iterator may contain less entries that {@link #getattributeclassesiterator}, if one instance implements more than one attribute interface. a cache that stores all interfaces for known implementation classes for performance (slow reflection) expert: adds a custom attributeimpl instance with one or more attribute interfaces. please note: it is not guaranteed, that att is added to the attributesource, because the provided attributes may already exist. you should always retrieve the wanted attributes using {@link #getattribute} after adding with this method and cast to your class. the recommended way to use custom implementations is using an {@link attributefactory}.  the caller must pass in a class&lt;? extends attribute&gt; value. this method first checks if an instance of that class is already in this attributesource and returns it. otherwise a new instance is returns true, iff this attributesource has any attributes the caller must pass in a class&lt;? extends attribute&gt; value. returns true, iff this attributesource contains the passed-in attribute. the caller must pass in a class&lt;? extends attribute&gt; value. returns the instance of the passed in attribute contained in this attributesource @throws illegalargumentexception if this attributesource does not contain the attribute. it is recommended to always use {@link #addattribute} even in consumers of tokenstreams, because you cannot know if a specific tokenstream really uses a specific attribute. {@link #addattribute} will automatically make the attribute available. if you want to only use the attribute, if it is available (to optimize consuming), use {@link #hasattribute}. resets all attributes in this attributesource by calling {@link attributeimpl#clear()} on each attribute implementation. captures the state of all attributes. the return value can be passed to {@link #restorestate} to restore the state of this or another attributesource. restores this state by copying the values of all attribute implementations that this state contains into the attributes implementations of the targetstream. the targetstream must contain a corresponding instance for each argument contained in this state (e.g. it is not possible to restore the state of an attributesource containing a termattribute into a attributesource using a token instance as implementation).  note that this method does not affect attributes of the targetstream that are not contained in this state. in other words, if for example the targetstream contains an offsetattribute, but this state doesn't, then the value of the offsetattribute remains unchanged. it might be desirable to reset its value to the default, in which case the caller should first call {@link tokenstream#clearattributes()} on the targetstream. this method returns the current attribute values as a string in the following format by calling the {@link #reflectwith(attributereflector)} method:  iff {@code prependattclass=true}: {@code "attributeclass#key=value,attributeclass#key=value"} iff {@code prependattclass=false}: {@code "key=value,key=value"}  @see #reflectwith(attributereflector) this method is for introspection of attributes, it should simply add the key/values this attributesource holds to the given {@link attributereflector}. this method iterates over all attribute implementations and calls the corresponding {@link attributeimpl#reflectwith} method. @see attributeimpl#reflectwith performs a clone of all {@link attributeimpl} instances returned in a new {@code attributesource} instance. this method can be used to e.g. create another tokenstream with exactly the same attributes (using {@link #attributesource(attributesource)}). you can also use it as a (non-performant) replacement for {@link #capturestate}, if you need to look into / modify the captured state. copies the contents of this {@code attributesource} to the given target {@code attributesource}. the given instance has to provide all {@link attribute}s this instance contains. the actual attribute implementations must be identical in both {@code attributesource} instances; ideally both attributesource instances should use the same {@link attributefactory}. you can use this method as a replacement for {@link #restorestate}, if you use {@link #cloneattributes} instead of {@link #capturestate}. for javadocs we have the slight chance that another thread may do the same, but who cares? these two maps must always be in sync!!! so they are private, final and read-only from the outside (read-only iterators) we have the slight chance that another thread may do the same, but who cares? find all interfaces that this attribute instance implements and that extend the attribute interface add all interfaces of this attributeimpl to the maps attribute is a superclass of this interface invalidate state to force recomputation in capturestate() it is only equal if all attribute impls are the same in the same order first clone the impls now the interfaces"
org.apache.lucene.util.CloseableThreadLocal "java's builtin threadlocal has a serious flaw: it can take an arbitrarily long amount of time to dereference the things you had stored in it, even once the threadlocal instance itself is no longer referenced. this is because there is single, master map stored for each thread, which all threadlocals share, and that master map only periodically purges "stale" entries. while not technically a memory leak, because eventually the memory will be reclaimed, it can take a long time and you can easily hit outofmemoryerror because from the gc's standpoint the stale entries are not reclaimable. this class works around that, by only enrolling weakreference values into the threadlocal, and separately holding a hard reference to each stored value. when you call {@link #close}, these hard references are cleared and then gc is freely able to reclaim space by objects stored in it. we can not rely on {@link threadlocal#remove()} as it only removes the value for the caller thread, whereas {@link #close} takes care of all threads. you should not call {@link #close} until all threads are done using the instance. @lucene.internal use a weakhashmap so that if a thread exits and is gc'able, its entry may be removed: increase this to decrease frequency of purging in get: on each get or set we decrement this; when it hits 0 we purge. after purge, we set this to purge_multiplier stillalivecount. this keeps amortized cost of purging linear. purge dead threads defensive: int overflow! clear the hard refs; then, the only remaining refs to all values we were storing are weak (unless somewhere else is still using them) and so gc may reclaim them: take care of the current thread right now; others will be taken care of via the weakreferences."
org.apache.lucene.util.NumericUtils "this is a helper class to generate prefix-encoded representations for numerical values and supplies converters to represent float/double values as sortable integers/longs. to quickly execute range queries in apache lucene, a range is divided recursively into multiple intervals for searching: the center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. this reduces the number of terms dramatically. this class generates terms to achieve this: first the numerical integer values need to be converted to bytes. for that integer values (32 bit or 64 bit) are made unsigned and the bits are converted to ascii chars with each 7 bit. the resulting byte[] is sortable like the original integer value (even using utf-8 sort order). each value is also prefixed (in the first char) by the shift value (number of bits removed) used during encoding. to also index floating point numbers, this class supplies two methods to convert them to integer values by changing their bit layout: {@link #doubletosortablelong}, {@link #floattosortableint}. you will have no precision loss by converting floating point numbers to integers and back (only that the integer form is not usable). other data types like dates can easily converted to longs or ints (e.g. date to long: {@link java.util.date#gettime}). for easy usage, the trie algorithm is implemented for indexing inside {@link numerictokenstream} that can index int, long, float, and double. for querying, {@link numericrangequery} and {@link numericrangefilter} implement the query part for the same data types. this class can also be used, to generate lexicographically sortable (according to {@link bytesref#getutf8sortedasutf16comparator()}) representations of numeric data types for other usages (e.g. sorting). @lucene.internal , api changed non backwards-compliant in 4.0 the default precision step used by {@link intfield}, {@link floatfield}, {@link longfield}, {@link doublefield}, {@link numerictokenstream}, {@link numericrangequery}, and {@link numericrangefilter}. longs are stored at lower precision by shifting off lower bits. the shift count is stored as shift_start_long+shift in the first byte the maximum term length (used for byte[] buffer size) for encoding long values. @see #longtoprefixcoded(long,int,bytesref) integers are stored at lower precision by shifting off lower bits. the shift count is stored as shift_start_int+shift in the first byte the maximum term length (used for byte[] buffer size) for encoding int values. @see #inttoprefixcoded(int,int,bytesref) returns prefix coded bits after reducing the precision by shift bits. this is method is used by {@link numerictokenstream}. after encoding, {@code bytes.offset} will always be 0. the numeric value how many bits to strip from the right will contain the encoded value hash code for indexing (termshash) returns prefix coded bits after reducing the precision by shift bits. this is method is used by {@link numerictokenstream}. after encoding, {@code bytes.offset} will always be 0. the numeric value how many bits to strip from the right will contain the encoded value hash code for indexing (termshash) returns the shift value from a prefix encoded {@code long}. @throws numberformatexception if the supplied {@link bytesref} is not correctly prefix encoded. returns the shift value from a prefix encoded {@code int}. @throws numberformatexception if the supplied {@link bytesref} is not correctly prefix encoded. returns a long from prefixcoded bytes. rightmost bits will be zero for lower precision codes. this method can be used to decode a term's value. @throws numberformatexception if the supplied {@link bytesref} is not correctly prefix encoded. @see #longtoprefixcoded(long,int,bytesref) returns an int from prefixcoded bytes. rightmost bits will be zero for lower precision codes. this method can be used to decode a term's value. @throws numberformatexception if the supplied {@link bytesref} is not correctly prefix encoded. @see #inttoprefixcoded(int,int,bytesref) converts a double value to a sortable signed long. the value is converted by getting their ieee 754 floating-point &quot;double format&quot; bit layout and then some bits are swapped, to be able to compare the result as long. by this the precision is not reduced, but the value can easily used as a long. the sort order (including {@link double#nan}) is defined by {@link double#compareto}; {@code nan} is greater than positive infinity. @see #sortablelongtodouble converts a sortable long back to a double. @see #doubletosortablelong converts a float value to a sortable signed int. the value is converted by getting their ieee 754 floating-point &quot;float format&quot; bit layout and then some bits are swapped, to be able to compare the result as int. by this the precision is not reduced, but the value can easily used as an int. the sort order (including {@link float#nan}) is defined by {@link float#compareto}; {@code nan} is greater than positive infinity. @see #sortableinttofloat converts a sortable int back to a float. @see #floattosortableint splits a long range recursively. you may implement a builder that adds clauses to a {@link org.apache.lucene.search.booleanquery} for each call to its {@link longrangebuilder#addrange(bytesref,bytesref)} method. this method is used by {@link numericrangequery}. splits an int range recursively. you may implement a builder that adds clauses to a {@link org.apache.lucene.search.booleanquery} for each call to its {@link intrangebuilder#addrange(bytesref,bytesref)} method. this method is used by {@link numericrangequery}. this helper does the splitting for both 32 and 64 bit. helper that delegates to correct range builder callback for {@link #splitlongrange}. you need to overwrite only one of the methods. @lucene.internal , api changed non backwards-compliant in 4.0 overwrite this method, if you like to receive the already prefix encoded range bounds. you can directly build classical (inclusive) range queries from them. overwrite this method, if you like to receive the raw long range bounds. you can use this for e.g. debugging purposes (print out range bounds). callback for {@link #splitintrange}. you need to overwrite only one of the methods. @lucene.internal , api changed non backwards-compliant in 4.0 overwrite this method, if you like to receive the already prefix encoded range bounds. you can directly build classical range (inclusive) queries from them. overwrite this method, if you like to receive the raw int range bounds. you can use this for e.g. debugging purposes (print out range bounds). javadocs javadocs javadocs javadocs for javadocs no instance! store 7 bits per byte for compatibility with utf-8 encoding of terms calculate hash store 7 bits per byte for compatibility with utf-8 encoding of terms calculate hash calculate new bounds for inner precision we are in the lowest precision or the next precision is not available. exit the split recursion loop recurse to next precision for the max bound set all lower bits (that were shifted away): this is important for testing or other usages of the splitted range (e.g. to reconstruct the full range). the prefixencoding will remove the bits anyway, so they do not hurt! delegate to correct range builder should not happen!"
org.apache.lucene.util.BytesRef "represents byte[], as a slice (offset + length) into an existing byte[]. the {@link #bytes} member should never be null; use {@link #empty_bytes} if necessary. important note: unless otherwise noted, lucene uses this class to represent terms that are encoded as utf8 bytes in the index. to convert them to a java {@link string} (which is utf16), use {@link #utf8tostring}. using code like {@code new string(bytes, offset, length)} to do this is wrong, as it does not respect the correct character set and may return wrong results (depending on the platform's defaults)! an empty byte array for convenience the contents of the bytesref. should never be {@code null}. offset of first valid byte. length of used bytes. create a bytesref with {@link #empty_bytes} this instance will directly reference bytes w/o making a copy. bytes should not be null. this instance will directly reference bytes w/o making a copy. bytes should not be null create a bytesref pointing to a new array of size capacity. offset and length will both be zero. initialize the byte[] from the utf8 bytes for the provided string. this must be well-formed unicode text, with no unpaired surrogates. copies the utf8 bytes for this string. must be well-formed unicode text, with no unpaired surrogates or invalid utf16 code units. expert: compares the bytes against another bytesref, returning true if the bytes are equal. another bytesref, should not be null. @lucene.internal calculates the hash code as required by termshash during indexing. it is defined as:  int hash = 0; for (int i = offset; i &lt; offset + length; i++) { hash = 31hash + bytes[i]; }  interprets stored bytes as utf8 bytes, returning the resulting string returns hex encoded bytes, eg [0x6c 0x75 0x63 0x65 0x6e 0x65] copies the bytes from the given {@link bytesref}  note: if this would exceed the array size, this method creates a new reference array. appends the bytes from the given {@link bytesref}  note: if this would exceed the array size, this method creates a new reference array. used to grow the reference array. in general this should not be used as it does not take the offset into account. @lucene.internal unsigned byte order comparison @deprecated this comparator is only a transition mechanism @deprecated this comparator is only a transition mechanism @deprecated this comparator is only a transition mechanism creates a new bytesref that points to a copy of the bytes from other  the returned bytesref will have a length of other.length and an offset of zero. todo broken if offset != 0 note: senseless if offset != 0 only singleton one is a prefix of the other, or, they are equal: only singleton see http://icu-project.org/docs/papers/utf16_code_point_order.html#utf-8-in-utf-16-order we know the terms are not equal, but, we may have to carefully fixup the bytes at the difference to match utf16's sort order: note: instead of moving supplementary code points (0xee and 0xef) to the unused 0xfe and 0xff, we move them to the unused 0xfc and 0xfd [reserved for future 6-byte character sequences] this reserves 0xff for preflex's term reordering (surrogate dance), and if unicode grows such that 6-byte sequences are needed we have much bigger problems anyway. one is a prefix of the other, or, they are equal:"
org.apache.lucene.util.LongsRef "represents long[], as a slice (offset + length) into an existing long[]. the {@link #longs} member should never be null; use {@link #empty_longs} if necessary. @lucene.internal an empty long array for convenience the contents of the longsref. should never be {@code null}. offset of first valid long. length of used longs. create a longsref with {@link #empty_longs} create a longsref pointing to a new array of size capacity. offset and length will both be zero. this instance will directly reference longs w/o making a copy. longs should not be null signed int order comparison used to grow the reference array. in general this should not be used as it does not take the offset into account. @lucene.internal creates a new intsref that points to a copy of the longs from other  the returned intsref will have a length of other.length and an offset of zero. one is a prefix of the other, or, they are equal:"
org.apache.lucene.util.UnicodeUtil "some of this code came from the excellent unicode conversion examples from: http://www.unicode.org/public/programs/cvtutf full copyright for that code follows: additional code came from the ibm icu library. http://www.icu-project.org full copyright for that code follows. class to encode java's utf16 char[] into utf8 byte[] without always allocating a new byte[] as string.getbytes("utf-8") does. @lucene.internal a binary term consisting of a number of 0xff bytes, likely to be bigger than other terms one would normally encounter, and definitely bigger than any utf-8 terms.  warning: this is not a valid utf8 term encode characters from a char[] source, starting at offset for length chars. returns a hash of the resulting bytes. after encoding, result.offset will always be 0. encode characters from a char[] source, starting at offset for length chars. after encoding, result.offset will always be 0. encode characters from this string, starting at offset for length characters. after encoding, result.offset will always be 0. private static boolean matches(char[] source, int offset, int length, byte[] result, int upto) { try { string s1 = new string(source, offset, length); string s2 = new string(result, 0, upto, "utf-8"); if (!s1.equals(s2)) { //system.out.println("diff: s1 len=" + s1.length()); //for(int i=0;i note: full characters are read, even if this reads past the length passed (and can result in an arrayoutofboundsexception if invalid utf-8 is passed). explicit checks for valid utf-8 are not performed. utility method for {@link #utf8toutf16(byte[], int, int, charsref)} @see #utf8toutf16(byte[], int, int, charsref) todo this is unrelated here find a better place for it no instance todo: broken if incoming result.offset != 0 pre-allocate for worst case 4-for-1 surrogate pair confirm valid high surrogate confirm valid low surrogate and write pair replace unpaired surrogate or out-of-order low surrogate with substitution character assert matches(source, offset, length, out, upto); todo: broken if incoming result.offset != 0 pre-allocate for worst case 4-for-1 surrogate pair confirm valid high surrogate confirm valid low surrogate and write pair replace unpaired surrogate or out-of-order low surrogate with substitution character assert matches(source, offset, length, out, upto); todo: broken if incoming result.offset != 0 pre-allocate for worst case 4-for-1 surrogate pair confirm valid high surrogate confirm valid low surrogate and write pair replace unpaired surrogate or out-of-order low surrogate with substitution character assert matches(s, offset, length, out, upto); only called from assert system.out.println("diff: s1 len=" + s1.length()); for(int i=0;i<s1.length();i++) system.out.println(" " + i + ": " + (int) s1.charat(i)); system.out.println("s2 len=" + s2.length()); for(int i=0;i<s2.length();i++) system.out.println(" " + i + ": " + (int) s2.charat(i)); if the input string was invalid, then the difference is ok only called from assert allow a difference if s1 is not valid utf-16 system.out.println("diff: s1 len=" + s1.length()); for(int i=0;i<s1.length();i++) system.out.println(" " + i + ": " + (int) s1.charat(i)); system.out.println(" s2 len=" + s2.length()); for(int i=0;i<s2.length();i++) system.out.println(" " + i + ": " + (int) s2.charat(i)); if the input string was invalid, then the difference is ok valid surrogate pair unmatched high surrogate unmatched high surrogate unmatched low surrogate valid surrogate pair unmatched low surrogate borrowed from python's 3.1.2 sources, objects/unicodeobject.c, and modified (see commented out section, and the -1s) to disallow the reserved for future (rfc 3629) 5/6 byte sequence characters, and invalid 0xfe and 0xff bytes. , 5, 5, 5, 5, 6, 6, 0, 0 todo: broken if incoming result.offset != 0 pre-alloc for worst case todo: ints cannot be null, should be an assert 5 useful bits 4 useful bits 3 useful bits for debugging todo: broken if chars.offset != 0"
org.apache.lucene.util.MutableBits "extension of bits for live documents. sets the bit specified by index to false. index, should be non-negative and &lt; {@link #length()}. the result of passing negative or out of bounds values is undefined by this interface, just don't do it!"
org.apache.lucene.util.packed.BulkOperationPacked10 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked13 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.Direct8 "direct wrapping of 8-bits values to a backing array. @lucene.internal because packed ints have not always been byte-aligned"
org.apache.lucene.util.packed.BulkOperationPacked7 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked20 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.Direct16 "direct wrapping of 16-bits values to a backing array. @lucene.internal because packed ints have not always been byte-aligned"
org.apache.lucene.util.packed.BulkOperationPacked5 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.Direct64 "direct wrapping of 64-bits values to a backing array. @lucene.internal"
org.apache.lucene.util.packed.BulkOperationPackedSingleBlock "non-specialized {@link bulkoperation} for {@link packedints.format#packed_single_block}."
org.apache.lucene.util.packed.BulkOperationPacked12 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BlockPackedWriter "a writer for large sequences of longs.  the sequence is divided into fixed-size blocks and for each block, the difference between each value and the minimum value of the block is encoded using as few bits as possible. memory usage of this class is proportional to the block size. each block has an overhead between 1 and 10 bytes to store the minimum value and the number of bits per value of the block. @see blockpackedreader @lucene.internal sole constructor. the number of values of a single block, must be a multiple of 64 append a new long. flush all buffered data to disk. this instance is not usable anymore after this method has been called. return the number of values which have been added. same as dataoutput.writevlong but accepts negative values no need to delta-encode make min as small as possible so that writevlong requires fewer bytes"
org.apache.lucene.util.packed.BulkOperationPacked2 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.Packed64SingleBlock "this class is similar to {@link packed64} except that it trades space for speed by ensuring that a single block needs to be read/written in order to read/write a value. go to the next block boundary bulk get stay at the block boundary no progress so far => already at a block boundary but no full block to get go to the next block boundary bulk set stay at the block boundary no progress so far => already at a block boundary but no full block to set there needs to be at least one full block to set for the block approach to be worth trying set values naively until the next block start bulk set of the inner blocks fill the gap"
org.apache.lucene.util.packed.BulkOperationPacked24 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked11 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.PackedDataInput "a {@link datainput} wrapper to read unaligned, variable-length packed integers. this api is much slower than the {@link packedints} fixed-length api but can be convenient to save space. @see packeddataoutput @lucene.internal create a new instance that wraps in. read the next long using exactly bitspervalue bits. if there are pending bits (at most 7), they will be ignored and the next value will be read starting at the next byte."
org.apache.lucene.util.packed.DirectPackedReader "reads directly from disk on each get masks[n-1] masks for bottom n bits special case: all bits are in the first byte take bits from the first byte add bits from inner bytes take bits from the last byte"
org.apache.lucene.util.packed.BulkOperationPacked23 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked19 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.PackedInts "simplistic compression for array of unsigned long values. each value is >= 0 and values values of size bitspervalue. computes how many long blocks are needed to store values values of size bitspervalue. tests whether the provided number of bits per value is supported by the format. returns the overhead per value, in bits. returns the overhead ratio (overhead per value / bits per value). simple class that holds a format and a number of bits per value. try to find the {@link format} and number of bits per value that would restore from disk the fastest reader whose overhead is less than acceptableoverheadratio.  the acceptableoverheadratio parameter makes sense for random-access {@link reader}s. in case you only plan to perform sequential access on this stream later on, you should probably use {@link packedints#compact}.  if you don't know how many values you are going to write, use valuecount = -1. a decoder for packed integers. the minimum number of long blocks to decode in a single call. the number of values that can be stored in blockcount() long blocks. read iterations blockcount() blocks from blocks, decode them and write iterations valuecount() values into values. the long blocks that hold packed integer values the offset where to start reading blocks the values buffer the offset where to start writing values controls how much data to decode read 8 iterations blockcount() blocks from blocks, decode them and write iterations valuecount() values into values. the long blocks that hold packed integer values the offset where to start reading blocks the values buffer the offset where to start writing values controls how much data to decode read iterations blockcount() blocks from blocks, decode them and write iterations valuecount() values into values. the long blocks that hold packed integer values the offset where to start reading blocks the values buffer the offset where to start writing values controls how much data to decode read 8 iterations blockcount() blocks from blocks, decode them and write iterations valuecount() values into values. the long blocks that hold packed integer values the offset where to start reading blocks the values buffer the offset where to start writing values controls how much data to decode an encoder for packed integers. the minimum number of long blocks to encode in a single call. the number of values that can be stored in blockcount() long blocks. read iterations valuecount() values from values, encode them and write iterations blockcount() blocks into blocks. the long blocks that hold packed integer values the offset where to start writing blocks the values buffer the offset where to start reading values controls how much data to encode read iterations valuecount() values from values, encode them and write 8 iterations blockcount() blocks into blocks. the long blocks that hold packed integer values the offset where to start writing blocks the values buffer the offset where to start reading values controls how much data to encode read iterations valuecount() values from values, encode them and write iterations blockcount() blocks into blocks. the long blocks that hold packed integer values the offset where to start writing blocks the values buffer the offset where to start reading values controls how much data to encode read iterations valuecount() values from values, encode them and write 8 iterations blockcount() blocks into blocks. the long blocks that hold packed integer values the offset where to start writing blocks the values buffer the offset where to start reading values controls how much data to encode a read-only random access array of positive integers. @lucene.internal the position of the wanted value. value at the stated index. bulk get: read at least one and at most len longs starting from index into arr[off:off+len] and return the actual number of values that have been read. number of bits used to store any given value. note: this does not imply that memory usage is {@code bitspervalue #values} as implementations are free to use non-space-optimal packing of bits. number of values. return the in-memory size in bytes. expert: if the bit-width of this reader matches one of java's native types, returns the underlying array (ie, byte[], short[], int[], long[]); else, returns null. note that when accessing the array you must upgrade the type (bitwise and with all ones), to interpret the full value as unsigned. ie, bytes[idx]&0xff, shorts[idx]&0xffff, etc. returns true if this implementation is backed by a native java array. @see #getarray run-once iterator interface, to decode previously saved packedints. returns next value returns at least 1 and at most count next values, the returned ref must not be modified returns number of bits per value returns number of values returns the current position a packed integer array that can be modified. @lucene.internal set the value at the given index in the array. where the value should be positioned. a value conforming to the constraints set by the array. bulk set: set at least one and at most len longs starting at off in arr into this mutable, starting at index. returns the actual number of values that have been set. fill the mutable from fromindex (inclusive) to toindex (exclusive) with val. sets all values to 0. save this mutable into out. instantiating a reader from the generated data will return a reader with the same number of bits per value. a simple base for readers that keeps track of valuecount and bitspervalue. @lucene.internal a write-once writer. @lucene.internal the format used to serialize values. add a value to the stream. the number of bits per value. perform end-of-stream operations. returns the current ord in the stream (number of values that have been written so far minus one). get a {@link decoder}. the format used to store packed ints the compatibility version the number of bits per value decoder get an {@link encoder}. the format used to store packed ints the compatibility version the number of bits per value encoder expert: restore a {@link reader} from a stream without reading metadata at the beginning of the stream. this method is useful to restore data from streams which have been restore a {@link reader} from a stream. the stream to read data from a reader @throws ioexception if there is a low-level i/o error @lucene.internal expert: restore a {@link readeriterator} from a stream without reading metadata at the beginning of the stream. this method is useful to restore data from streams which have been retrieve packedints as a {@link readeriterator} positioned at the beginning of a stored packed int structure. how much memory the iterator is allowed to use to read-ahead (likely to speed up iteration) iterator to access the values @throws ioexception if the structure could not be retrieved. @lucene.internal expert: construct a direct {@link reader} from a stream without reading metadata at the beginning of the stream. this method is useful to restore data from streams which have been construct a direct {@link reader} from an {@link indexinput}. this method is useful to restore data from streams which have been create a packed integer array with the given amount of values initialized to 0. the valuecount and the bitspervalue cannot be changed after creation. all mutables known by this factory are kept fully in ram.  positive values of acceptableoverheadratio will trade space for speed by selecting a faster but potentially less memory-efficient implementation. an acceptableoverheadratio of {@link packedints#compact} will make sure that the most memory-efficient implementation is selected whereas {@link packedints#fastest} will make sure that the fastest implementation is selected. the number of elements the number of bits available for any given value an acceptable overhead ratio per value mutable packed integer array @lucene.internal expert: create a packed integer array writer for the given output, format, value count, and number of bits per value.  the resulting stream will be long-aligned. this means that depending on the format which is used, up to 63 bits will be wasted. an easy way to make sure that no space is lost is to always use a valuecount that is a multiple of 64.  this method does not write any metadata to the stream, meaning that it is your responsibility to store it somewhere else in order to be able to recover data from the stream later on:  format (using {@link format#getid()}), valuecount, bitspervalue, {@link #version_current}.   it is possible to start writing values without knowing how many of them you are actually going to write. to do this, just pass -1 as valuecount. on the other hand, for any positive value of valuecount, the returned writer will make sure that you don't write more values than expected and pad the end of stream with zeros in case you have written less than valuecount when calling {@link writer#finish()}.  the mem parameter lets you control how much memory can be used to buffer changes in memory before flushing to disk. high values of mem are likely to improve throughput. on the other hand, if speed is not that important to you, a value of 0 will use as little memory as possible and should already offer reasonable throughput. the data output the format to use to serialize the values the number of values the number of bits per value how much memory (in bytes) can be used to speed up serialization a writer @see packedints#getreaderiteratornoheader(datainput, format, int, int, int, int) @see packedints#getreadernoheader(datainput, format, int, int, int) @lucene.internal create a packed integer array writer for the given output, format, value count, and number of bits per value.  the resulting stream will be long-aligned. this means that depending on the format which is used under the hoods, up to 63 bits will be wasted. an easy way to make sure that no space is lost is to always use a valuecount that is a multiple of 64.  this method writes metadata to the stream, so that the resulting stream is sufficient to restore a {@link reader} from it. you don't need to track valuecount or bitspervalue by yourself. in case this is a problem, you should probably look at {@link #getwriternoheader(dataoutput, format, int, int, int)}.  the acceptableoverheadratio parameter controls how readers that will be restored from this stream trade space for speed by selecting a faster but potentially less memory-efficient implementation. an acceptableoverheadratio of {@link packedints#compact} will make sure that the most memory-efficient implementation is selected whereas {@link packedints#fastest} will make sure that the fastest implementation is selected. in case you are only interested in reading this stream sequentially later on, you should probably use {@link packedints#compact}. the data output the number of values the number of bits per value an acceptable overhead ratio per value a writer @throws ioexception if there is a low-level i/o error @lucene.internal returns how many bits are required to hold values up to and including maxvalue the maximum value that should be representable. amount of bits needed to represent values from 0 to maxvalue. @lucene.internal calculates the maximum unsigned long that can be expressed with the given number of bits. the number of bits available for any given value. maximum value for the given bits. @lucene.internal copy src[srcpos:srcpos+len] into dest[destpos:destpos+len] using at most mem bytes. 1k packedints were long-aligned assume long-aligned in bits some consumers of direct readers assume that reading the last value will make the underlying indexinput go to the end of the packed stream, but this is not true because packed ints storage used to be long-aligned and is now byte-aligned, hence this additional condition when reading the last value use bulk operations"
org.apache.lucene.util.packed.Packed8ThreeBlocks "packs integers into 3 bytes (24 bits per value). @lucene.internal because packed ints have not always been byte-aligned"
org.apache.lucene.util.packed.BlockPackedReader "reader for sequences of longs written with {@link blockpackedwriter}. @see blockpackedwriter @lucene.internal sole constructor. the number of values of a block, must be equal to the block size of the {@link blockpackedwriter} which has been used to write the stream skip exactly count values. read the next value. read between 1 and count values. return the offset of the next value to read. same as datainput.readvlong but supports negative values 1. skip buffered values 2. skip as many blocks as necessary 3. skip last values"
org.apache.lucene.util.packed.BulkOperationPacked4 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.PackedDataOutput "a {@link dataoutput} wrapper to write unaligned, variable-length packed integers. @see packeddatainput @lucene.internal create a new instance that wraps out. write a value using exactly bitspervalue bits. flush pending bits to the underlying {@link dataoutput}."
org.apache.lucene.util.packed.BulkOperationPacked6 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked9 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked "non-specialized {@link bulkoperation} for {@link packedints.format#packed}. bitsleft < 0 bitsleft < 0 bitsleft < 0 bitsleft < 0"
org.apache.lucene.util.packed.BulkOperationPacked8 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.PackedReaderIterator ""
org.apache.lucene.util.packed.BulkOperationPacked21 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked14 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.Packed64 "space optimized random access capable array of values with a fixed number of bits/value. values are packed contiguously.  the implementation strives to perform af fast as possible under the constraint of contiguous bits, by avoiding expensive operations. this comes at the cost of code clarity.  technical details: this implementation is a refinement of a non-branching version. the non-branching get and set methods meant that 2 or 4 atomics in the underlying array were always accessed, even for the cases where only 1 or 2 were needed. even with caching, this had a detrimental effect on performance. related to this issue, the old implementation used lookup tables for shifts and masks, which also proved to be a bit slower than calculating the shifts and masks on the fly. see https://issues.apache.org/jira/browse/lucene-4062 for details. values are stores contiguously in the blocks array. a right-aligned mask of width bitspervalue used by {@link #get(int)}. optimization: saves one lookup in {@link #get(int)}. creates an array with the internal structures adjusted for the given limits and initialized to 0. the number of elements. the number of bits available for any given value. creates an array with content retrieved from the given datainput. a datainput, positioned at the start of packed64-content. the number of elements. the number of bits available for any given value. @throws java.io.ioexception if the values for the backing array could not be retrieved. the position of the value. value at the given index. 32 = int, 64 = long the #bits representing block_size x % block_size to know how much to read to size the array read as many longs as we can read the last bytes the abstract index in a bit stream the index in the backing long-array the number of value-bits in the second long single block two blocks go to the next block where the value does not span across two blocks bulk get stay at the block boundary no progress so far => already at a block boundary but no full block to get the abstract index in a contiguous bit stream the index in the backing long-array / block_size the number of value-bits in the second long single block two blocks go to the next block where the value does not span across two blocks bulk set stay at the block boundary no progress so far => already at a block boundary but no full block to get minimum number of values that use an exact number of full blocks there needs be at least 2 nalignedvalues aligned values for the block approach to be worth trying fill the first values naively until the next block start compute the long[] blocks for nalignedvalues consecutive values and use them to set as many values as possible without applying any mask or shift fill the gap"
org.apache.lucene.util.packed.BulkOperation "efficient sequential read/write of packed integers. for every number of bits per value, there is a minimum number of blocks (b) / values (v) you need to write in order to reach the next block boundary: - 16 bits per value -> b=1, v=4 - 24 bits per value -> b=3, v=8 - 50 bits per value -> b=25, v=32 - 63 bits per value -> b=63, v=64 - ...  a bulk read consists in copying iterationsv values that are contained in iterationsb blocks into a long[] (higher values of iterations are likely to yield a better throughput) => this requires n (b + v) longs in memory.  this method computes iterations as rambudget / (8 (b + v)) (since a long is 8 bytes).  the resulting number of iterations of this method is guaranteed not to overflow when multiplied by 8 {@link packedints.encoder#blockcount()} or 8 {@link packedints.decoder#blockcount()}. note: this is sparse (some entries are null): at least 1 don't allocate for more than the size of the reader"
org.apache.lucene.util.packed.BulkOperationPacked16 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked1 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.Direct32 "direct wrapping of 32-bits values to a backing array. @lucene.internal because packed ints have not always been byte-aligned"
org.apache.lucene.util.packed.DirectPacked64SingleBlockReader ""
org.apache.lucene.util.packed.BulkOperationPacked17 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.PackedWriter "packs high order byte first, to match indexoutput.writeint/long/short byte order"
org.apache.lucene.util.packed.BulkOperationPacked15 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked3 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.BulkOperationPacked22 "efficient sequential read/write of packed integers."
org.apache.lucene.util.packed.Packed16ThreeBlocks "packs integers into 3 shorts (48 bits per value). @lucene.internal because packed ints have not always been byte-aligned"
org.apache.lucene.util.packed.GrowableWriter "implements {@link packedints.mutable}, but grows the bit count of the underlying packed ints on-demand. @lucene.internal"
org.apache.lucene.util.packed.BulkOperationPacked18 "efficient sequential read/write of packed integers."
org.apache.lucene.util.PagedBytes "represents a logical byte[] as a series of pages. you can write-once into the logical byte[] (append only), using copy, and then retrieve slices (bytesref) into it using fill. @lucene.internal provides methods to read bytesrefs from a frozen pagedbytes. @see #freeze gets a slice out of {@link pagedbytes} starting at start with a given length. iff the slice spans across a block border this method will allocate sufficient resources and copy the paged data.  slices spanning more than one block are not supported.  @lucene.internal reads length as 1 or 2 byte vint prefix, starting at start.  note: this method does not support slices spanning across block borders.  given {@link bytesref} @lucene.internal reads length as 1 or 2 byte vint prefix, starting at start.  note: this method does not support slices spanning across block borders.  internal block number of the slice. @lucene.internal reads length as 1 or 2 byte vint prefix, starting at start and returns the start offset of the next part, suitable as start parameter on next call to sequentially read all {@link bytesref}.  note: this method does not support slices spanning across block borders.  start offset of the next part, suitable as start parameter on next call to sequentially read all {@link bytesref}. @lucene.internal gets a slice out of {@link pagedbytes} starting at start, the length is read as 1 or 2 byte vint prefix. iff the slice spans across a block border this method will allocate sufficient resources and copy the paged data.  slices spanning more than one block are not supported.  @lucene.internal @lucene.internal @lucene.internal 1&lt;&lt;blockbits must be bigger than biggest single bytesref slice that will be pulled read this many bytes from in copy bytesref in copy bytesref in, setting bytesref out to the result. do not use this if you will use freeze(true). this only supports bytes.length <= blocksize commits final byte[], trimming it if necessary and if trim=true copy bytes in, writing the length as a 1 or 2 byte vint prefix. returns the current byte position. seek to a position previously obtained from {@link #getposition}. return the current byte position. returns a datainput to read values from this pagedbytes instance. returns a dataoutput that you may use to write into this pagedbytes instance. if you do this, you should not call the other writing methods (eg, copy); results are undefined. within block split note: even though copyusinglengthprefix always allocs a new block if the byte[] to be added won't fit in current block, varderefbytesimpl.finishinternal does its own prefix + byte[] writing which can span two blocks, so we support that here on decode: within block split todo: we could also support variable block sizes last block last block"
org.apache.lucene.util.SentinelIntSet "a native int set where one value is reserved to mean "empty" @lucene.internal the minimum number of elements this set should be able to hold without re-hashing (i.e. the slots are guaranteed not to change) the integer value to use for empty returns the slot for this key returns the slot for this key, or -slot-1 if not found the count at which a rehash should be done should be able to hold "size" w/o rehashing"
org.apache.lucene.analysis.Token "a token is an occurrence of a term from the text of a field. it consists of a term's text, the start and end offset of the term in the text of the field, and a type string.  the start and end offsets permit applications to re-associate a token with its source text, e.g., to display highlighted query terms in a document browser, or to show matching text fragments in a kwic display, etc.  the type is a string, assigned by a lexical analyzer (a.k.a. tokenizer), naming the lexical or syntactic class that the token belongs to. for example an end of sentence marker token might be implemented with type "eos". the default token type is "word".  a token can optionally have metadata (a.k.a. payload) in the form of a variable length byte array. use {@link docsandpositionsenum#getpayload()} to retrieve the payloads from the index.  note: as of 2.9, token implements all {@link attribute} interfaces that are part of core lucene and can be found in the {@code tokenattributes} subpackage. even though it is not necessary to use token anymore, with the new tokenstream api it can be used as convenience class that implements all {@link attribute}s, which is especially useful to easily switch from the old to the new tokenstream api.  tokenizers and tokenfilters should try to re-use a token instance when possible for best performance, by implementing the {@link tokenstream#incrementtoken()} api. failing that, to create a new token you should first use one of the constructors that starts with null text. to load the token from a char[] use {@link #copybuffer(char[], int, int)}. to load from a string use {@link #setempty} followed by {@link #append(charsequence)} or {@link #append(charsequence, int, int)}. alternatively you can get the token's termbuffer by calling either {@link #buffer()}, if you know that your text is shorter than the capacity of the termbuffer or {@link #resizebuffer(int)}, if there is any possibility that you may need to grow the buffer. fill in the characters of your term into this buffer, with {@link string#getchars(int, int, char[], int)} if loading from a string, or with {@link system#arraycopy(object, int, object, int, int)}, and finally call {@link #setlength(int)} to set the length of the term text. see lucene-969 for details. typical token reuse patterns:   copying text from a string (type is reset to {@link #default_type} if not specified):  return reusabletoken.reinit(string, startoffset, endoffset[, type]);    copying some text from a string (type is reset to {@link #default_type} if not specified):  return reusabletoken.reinit(string, 0, string.length(), startoffset, endoffset[, type]);     copying text from char[] buffer (type is reset to {@link #default_type} if not specified):  return reusabletoken.reinit(buffer, 0, buffer.length, startoffset, endoffset[, type]);    copying some text from a char[] buffer (type is reset to {@link #default_type} if not specified):  return reusabletoken.reinit(buffer, start, end - start, startoffset, endoffset[, type]);    copying from one one token to another (type is reset to {@link #default_type} if not specified):  return reusabletoken.reinit(source.buffer(), 0, source.length(), source.startoffset(), source.endoffset()[, source.type()]);    a few things to note:  clear() initializes all of the fields to default values. this was changed in contrast to lucene 2.4, but should affect no one. because tokenstreams can be chained, one cannot assume that the token's current type is correct. the startoffset and endoffset represent the start and offset in the source text, so be careful in adjusting them. when caching a reusable token, clone it. when injecting a cached token into a stream that can be reset, clone it again.    please note: with lucene 3.1, the {@linkplain #tostring tostring()} method had to be changed to match the {@link charsequence} interface introduced by the interface {@link org.apache.lucene.analysis.tokenattributes.chartermattribute}. this method now only prints the term text, no additional information anymore.  constructs a token will null text. constructs a token with null text and start & end offsets. start offset in the source text end offset in the source text constructs a token with null text and start & end offsets plus the token type. start offset in the source text end offset in the source text the lexical type of this token constructs a token with null text and start & end offsets plus flags. note: flags is experimental. start offset in the source text end offset in the source text the bits to set for this token constructs a token with the given term text, and start & end offsets. the type defaults to "word." note: for better indexing speed you should instead use the char[] termbuffer methods to set the term text. term text start offset in the source text end offset in the source text constructs a token with the given text, start and end offsets, & type. note: for better indexing speed you should instead use the char[] termbuffer methods to set the term text. term text start offset in the source text end offset in the source text token type constructs a token with the given text, start and end offsets, & type. note: for better indexing speed you should instead use the char[] termbuffer methods to set the term text. term text start offset in the source text end offset in the source text token type bits constructs a token with the given term buffer (offset & length), start and end offsets buffer containing term text the index in the buffer of the first character number of valid characters in the buffer start offset in the source text end offset in the source text {@inheritdoc} @see positionincrementattribute {@inheritdoc} @see positionincrementattribute {@inheritdoc} @see positionlengthattribute {@inheritdoc} @see positionlengthattribute {@inheritdoc} @see offsetattribute {@inheritdoc} @see offsetattribute {@inheritdoc} @see offsetattribute {@inheritdoc} @see typeattribute {@inheritdoc} @see typeattribute {@inheritdoc} @see flagsattribute {@inheritdoc} @see flagsattribute {@inheritdoc} @see payloadattribute {@inheritdoc} @see payloadattribute resets the term text, payload, flags, and positionincrement, startoffset, endoffset and token type to default. makes a clone, but replaces the term buffer & start/end offset in the process. this is more efficient than doing a full clone (and then calling {@link #copybuffer}) because it saves a wasted copy of the old termbuffer. shorthand for calling {@link #clear}, {@link #copybuffer(char[], int, int)}, {@link #setoffset}, {@link #settype} token instance shorthand for calling {@link #clear}, {@link #copybuffer(char[], int, int)}, {@link #setoffset}, {@link #settype} on token.default_type token instance shorthand for calling {@link #clear}, {@link #append(charsequence)}, {@link #setoffset}, {@link #settype} token instance shorthand for calling {@link #clear}, {@link #append(charsequence, int, int)}, {@link #setoffset}, {@link #settype} token instance shorthand for calling {@link #clear}, {@link #append(charsequence)}, {@link #setoffset}, {@link #settype} on token.default_type token instance shorthand for calling {@link #clear}, {@link #append(charsequence, int, int)}, {@link #setoffset}, {@link #settype} on token.default_type token instance copy the prototype token's fields into this one. note: payloads are shared. source token to copy fields from copy the prototype token's fields into this one, with a different term. note: payloads are shared. existing token new term text copy the prototype token's fields into this one, with a different term. note: payloads are shared. existing token buffer containing new term text the index in the buffer of the first character number of valid characters in the buffer convenience factory that returns token as implementation for the basic attributes and return the default impl (with &quot;impl&quot; appended) for all other attributes. expert: creates a tokenattributefactory returning {@link token} as instance for the basic attributes and for all other attributes calls the given delegate factory. expert: creates an attributefactory returning {@link token} as instance for the basic attributes and for all other attributes calls the given delegate factory. for javadoc do a deep clone like clear() but doesn't clear termbuffer/text reinit shares the payload, so clone it:"
org.apache.lucene.analysis.CachingTokenFilter "this class can be used if the token attributes of a tokenstream are intended to be consumed more than once. it caches all token attribute states locally in a list. cachingtokenfilter implements the optional method {@link tokenstream#reset()}, which repositions the stream to the first token. create a new cachingtokenfilter around input, caching its token attributes, which can be replayed again after a call to {@link #reset()}. rewinds the iterator to the beginning of the cached list.  note that this does not call reset() on the wrapped tokenstream ever, even the first time. you should reset() the inner tokenstream before wrapping it with cachingtokenfilter. fill cache lazily the cache is exhausted, return false since the tokenfilter can be reset, the tokens need to be preserved as immutable. capture final state"
org.apache.lucene.analysis.tokenattributes.CharTermAttributeImpl "default implementation of {@link chartermattribute}. initialize this attribute with empty term text returns solely the term text as specified by the {@link charsequence} interface. this method changed the behavior with lucene 3.1, before it returned a string representation of the whole term with all attributes. this affects especially the {@link org.apache.lucene.analysis.token} subclass. not big enough; create a new array with slight over allocation and preserve content not big enough; create a new array with slight over allocation: termtobytesrefattribute interface not until java 6 @override not until java 6 @override charsequence interface appendable interface needed for appendable compliance needed for appendable compliance only use instanceof check series for longer csqs, else simply iterate no fall-through here, as termlength is updated! for performance some convenience methods in addition to csq's needed for appendable compliance needed for appendable compliance needed for appendable compliance attributeimpl do a deep clone"
org.apache.lucene.analysis.tokenattributes.KeywordAttribute "this attribute can be used to mark a token as a keyword. keyword aware {@link tokenstream}s can decide to modify a token based on the return value of {@link #iskeyword()} if the token is modified. stemming filters for instance can use this attribute to conditionally skip a term if {@link #iskeyword()} returns true. returns true if the current token is a keyword, otherwise false true if the current token is a keyword, otherwise false @see #setkeyword(boolean) marks the current token as keyword if set to true. true if the current token is a keyword, otherwise false. @see #iskeyword()"
org.apache.lucene.analysis.tokenattributes.KeywordAttributeImpl "default implementation of {@link keywordattribute}. initialize this attribute with the keyword value as false."
org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute "determines how many positions this token spans. very few analyzer components actually produce this attribute, and indexing ignores it, but it's useful to express the graph structure naturally produced by decompounding, word splitting/joining, synonym filtering, etc. note: this is optional, and most analyzers don't change the default value (1). set the position length of this token.  the default value is one. how many positions this token spans. @throws illegalargumentexception if positionlength is zero or negative. @see #getpositionlength() returns the position length of this token. @see #setpositionlength"
org.apache.lucene.analysis.tokenattributes.PositionLengthAttributeImpl "default implementation of {@link positionlengthattribute}. initializes this attribute with position length of 1."
org.apache.lucene.analysis.tokenattributes.OffsetAttributeImpl "default implementation of {@link offsetattribute}. initialize this attribute with startoffset and endoffset of 0. todo: we could assert that this is set-once, ie, current values are -1? very few token filters should change offsets once set by the tokenizer... and tokenizer should call clearatts before re-using offsetatt todo: we could use -1 as default here? then we can assert in setoffset..."
org.apache.lucene.analysis.tokenattributes.OffsetAttribute "the start and end character offset of a token. returns this token's starting offset, the position of the first character corresponding to this token in the source text.  note that the difference between {@link #endoffset()} and startoffset() may not be equal to termtext.length(), as the term text may have been altered by a stemmer or some other filter. @see #setoffset(int, int) set the starting and ending offset. @throws illegalargumentexception if startoffset or endoffset are negative, or if startoffset is greater than endoffset @see #startoffset() @see #endoffset() returns this token's ending offset, one greater than the position of the last character corresponding to this token in the source text. the length of the token in the source text is (endoffset() - {@link #startoffset()}). @see #setoffset(int, int)"
org.apache.lucene.analysis.tokenattributes.FlagsAttributeImpl "default implementation of {@link flagsattribute}. initialize this attribute with no bits set"
org.apache.lucene.analysis.tokenattributes.PositionIncrementAttributeImpl "default implementation of {@link positionincrementattribute}. initialize this attribute with position increment of 1"
org.apache.lucene.analysis.tokenattributes.PayloadAttribute "the payload of a token.  the payload is stored in the index at each position, and can be used to influence scoring when using payload-based queries in the {@link org.apache.lucene.search.payloads} and {@link org.apache.lucene.search.spans} packages.  note: because the payload will be stored at each position, its usually best to use the minimum number of bytes necessary. some codec implementations may optimize payload storage when all payloads have the same length. @see docsandpositionsenum returns this token's payload. @see #setpayload(bytesref) sets this token's payload. @see #getpayload() javadocs"
org.apache.lucene.analysis.tokenattributes.CharTermAttribute "the term text of a token. copies the contents of buffer, starting at offset for length characters, into the termbuffer array. the buffer to copy the index in the buffer of the first character to copy the number of characters to copy returns the internal termbuffer character array which you can then directly alter. if the array is too small for your token, use {@link #resizebuffer(int)} to increase it. after altering the buffer be sure to call {@link #setlength} to record the number of valid characters that were placed into the termbuffer.  note: the returned buffer may be larger than the valid {@link #length()}. grows the termbuffer to at least size newsize, preserving the existing content. minimum size of the new termbuffer set number of valid characters (length of the term) in the termbuffer array. use this to truncate the termbuffer or to synchronize with external manipulation of the termbuffer. note: to grow the size of the array, use {@link #resizebuffer(int)} first. the truncated length sets the length of the termbuffer to zero. use this method before appending contents using the {@link appendable} interface. appends the specified {@code string} to this character sequence. the characters of the {@code string} argument are appended, in order, increasing the length of this sequence by the length of the argument. if argument is {@code null}, then the four characters {@code "null"} are appended. appends the specified {@code stringbuilder} to this character sequence. the characters of the {@code stringbuilder} argument are appended, in order, increasing the length of this sequence by the length of the argument. if argument is {@code null}, then the four characters {@code "null"} are appended. appends the contents of the other {@code chartermattribute} to this character sequence. the characters of the {@code chartermattribute} argument are appended, in order, increasing the length of this sequence by the length of the argument. if argument is {@code null}, then the four characters {@code "null"} are appended. the following methods are redefined to get rid of ioexception declaration:"
org.apache.lucene.analysis.tokenattributes.PayloadAttributeImpl "default implementation of {@link payloadattribute}. initialize this attribute with no payload. initialize this attribute with the given payload."
org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute "determines the position of this token relative to the previous token in a tokenstream, used in phrase searching. the default value is one. some common uses for this are: set it to zero to put multiple terms in the same position. this is useful if, e.g., a word has multiple stems. searches for phrases including either stem will match. in this case, all but the first stem's increment should be set to zero: the increment of the first instance should be one. repeating a token with an increment of zero can also be used to boost the scores of matches on that token. set it to values greater than one to inhibit exact phrase matches. if, for example, one does not want phrases to match across removed stop words, then one could build a stop word filter that removes stop words and also sets the increment to the number of stop words removed before each non-stop word. then exact phrase queries will only match when the terms occur with no intervening stop words.  @see org.apache.lucene.index.docsandpositionsenum set the position increment. the default value is one. the distance from the prior term @throws illegalargumentexception if positionincrement is negative. @see #getpositionincrement() returns the position increment of this token. @see #setpositionincrement(int)"
org.apache.lucene.analysis.tokenattributes.TypeAttribute "a token's lexical type. the default value is "word". the default type returns this token's lexical type. defaults to "word". @see #settype(string) set the lexical type. @see #type()"
org.apache.lucene.analysis.tokenattributes.TypeAttributeImpl "default implementation of {@link typeattribute}. initialize this attribute with {@link typeattribute#default_type} initialize this attribute with type"
org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute "this attribute is requested by termshashperfield to index the contents. this attribute can be used to customize the final byte[] encoding of terms.  consumers of this attribute call {@link #getbytesref()} up-front, and then invoke {@link #fillbytesref()} for each term. example:  final termtobytesrefattribute termatt = tokenstream.getattribute(termtobytesrefattribute.class); final bytesref bytes = termatt.getbytesref(); while (termatt.incrementtoken() { // you must call termatt.fillbytesref() before doing something with the bytes. // this encodes the term value (internally it might be a char[], etc) into the bytes. int hashcode = termatt.fillbytesref(); if (isinteresting(bytes)) { // because the bytes are reused by the attribute (like chartermattribute's char[] buffer), // you should make a copy if you need persistent access to the bytes, otherwise they will // be rewritten across calls to incrementtoken() dosomethingwith(new bytesref(bytes)); } } ...  @lucene.experimental this is a very expert api, please use {@link chartermattributeimpl} and its implementation of this method for utf-8 terms. updates the bytes {@link #getbytesref()} to contain this term's final encoding, and returns its hashcode. hashcode as defined by {@link bytesref#hashcode}:  int hash = 0; for (int i = termbytes.offset; i &lt; termbytes.offset+termbytes.length; i++) { hash = 31hash + termbytes.bytes[i]; }  implement this for performance reasons, if your code can calculate the hash on-the-fly. if this is not the case, just return {@code termbytes.hashcode()}. retrieve this attribute's bytesref. the bytes are updated from the current term when the consumer calls {@link #fillbytesref()}. attributes internal bytesref. you must call termatt.fillbytesref() before doing something with the bytes. this encodes the term value (internally it might be a char[], etc) into the bytes. because the bytes are reused by the attribute (like chartermattribute's char[] buffer), you should make a copy if you need persistent access to the bytes, otherwise they will be rewritten across calls to incrementtoken()"
org.apache.lucene.analysis.tokenattributes.FlagsAttribute "this attribute can be used to pass different flags down the {@link tokenizer} chain, e.g. from one tokenfilter to another one.  this is completely distinct from {@link typeattribute}, although they do share similar purposes. the flags can be used to encode information about the token for use by other {@link org.apache.lucene.analysis.tokenfilter}s. @lucene.experimental while we think this is here to stay, we may want to change it to be a long. get the bitset for any bits that have been set. bits @see #getflags() set the flags to a new bitset. @see #getflags()"
org.apache.lucene.analysis.TokenStream "a tokenstream enumerates the sequence of tokens, either from {@link field}s of a {@link document} or from query text.  this is an abstract class; concrete subclasses are:  {@link tokenizer}, a tokenstream whose input is a reader; and {@link tokenfilter}, a tokenstream whose input is another tokenstream.  a new tokenstream api has been introduced with lucene 2.9. this api has moved from being {@link token}-based to {@link attribute}-based. while {@link token} still exists in 2.9 as a convenience class, the preferred way to store the information of a {@link token} is to use {@link attributeimpl}s.  tokenstream now extends {@link attributesource}, which provides access to all of the token {@link attribute}s for the tokenstream. note that only one instance per {@link attributeimpl} is a tokenstream using the default attribute factory. a tokenstream that uses the same attributes as the supplied one. a tokenstream using the supplied attributefactory for creating new {@link attribute} instances. consumers (i.e., {@link indexwriter}) use this method to advance the stream to the next token. implementing classes must implement this method and update the appropriate {@link attributeimpl}s with the attributes of the next token.  the producer must make no assumptions about the attributes after the method has been returned: the caller may arbitrarily change it. if the producer needs to preserve the state for subsequent calls, it can use {@link #capturestate} to create a copy of the current attribute state.  this method is called for every token of a document, so an efficient implementation is crucial for good performance. to avoid calls to {@link #addattribute(class)} and {@link #getattribute(class)}, references to all {@link attributeimpl}s that this stream uses should be retrieved during instantiation.  to ensure that filters and consumers know which attributes are available, the attributes must be added during instantiation. filters and consumers are not required to check for availability of attributes in {@link #incrementtoken()}. for end of stream; true otherwise this method is called by the consumer after the last token has been consumed, after {@link #incrementtoken()} returned false (using the new tokenstream api). streams implementing the old api should upgrade to use this feature.  this method can be used to perform any end-of-stream operations, such as setting the final offset of a stream. the final offset of a stream might differ from the offset of the last token eg in case one or more whitespaces followed after the last token, but a whitespacetokenizer was used. @throws ioexception if an i/o error occurs this method is called by a consumer before it begins consumption using {@link #incrementtoken()}.  resets this stream to a clean state. stateful implementations must implement this method so that they can be reused, just as if they had been releases resources associated with this stream. do nothing by default"
org.apache.lucene.analysis.AnalyzerWrapper "extension to {@link analyzer} suitable for analyzers which wrap other analyzers.  {@link #getwrappedanalyzer(string)} allows the analyzer to wrap multiple analyzers which are selected on a per field basis.  {@link #wrapcomponents(string, analyzer.tokenstreamcomponents)} allows the tokenstreamcomponents of the wrapped analyzer to then be wrapped (such as adding a new {@link tokenfilter} to form new tokenstreamcomponents. creates a new analyzerwrapper. since the {@link analyzer.reusestrategy} of the wrapped analyzers are unknown, {@link analyzer.perfieldreusestrategy} is assumed retrieves the wrapped analyzer appropriate for analyzing the field with the given name name of the field which is to be analyzed for the field with the given name. assumed to be non-null wraps / alters the given tokenstreamcomponents, taken from the wrapped analyzer, to form new components. it is through this method that new tokenfilters can be added by analyzerwrappers. name of the field which is to be analyzed tokenstreamcomponents taken from the wrapped analyzer / altered tokenstreamcomponents."
org.apache.lucene.analysis.TokenStreamToAutomaton "consumes a tokenstream and creates an {@link automaton} where the transition labels are utf8 bytes from the {@link termtobytesrefattribute}. between tokens we insert pos_sep and for holes we insert hole. sole constructor. subclass & implement this if you need to change the token (such as escaping certain bytes) before it's turned into a graph. we create transition between two adjacent tokens. we add this arc to represent a hole. pulls the graph (including {@link positionlengthattribute}) from the provided {@link tokenstream}, and creates the corresponding automaton where arcs are bytes from each term. private static void todot(automaton a) throws ioexception { final string s = a.todot(); writer w = new outputstreamwriter(new fileoutputstream("/tmp/out.dot")); w.write(s); w.close(); system.out.println("test: saved to /tmp/out.dot"); } todo: maybe also tofst? then we can translate atts into fst outputs/weights any tokens that ended at our position arrive to this state: any tokens that start at our position leave from this state: only temporarily holds states ahead of our current position: new node: no token ever arrived to this position ok: this is the first token this means there's a hole (eg, stopfilter does this): a token spanned over a hole; add holes "under" it: note: this isn't necessarily true. its just that we aren't surely det. we could optimize this further (e.g. buffer and sort synonyms at a position) but thats probably overkill. this is cheap and dirty todot(a); for debugging!"
org.apache.lucene.analysis.CharFilter "subclasses of charfilter can be chained to filter a reader they can be used as {@link java.io.reader} with additional offset correction. {@link tokenizer}s will automatically use {@link #correctoffset} if a charfilter subclass is used.  this class is abstract: at a minimum you must implement {@link #read(char[], int, int)}, transforming the input in some way from {@link #input}, and {@link #correct(int)} to adjust the offsets to match the originals.  you can optionally provide more efficient implementations of additional methods like {@link #read()}, {@link #read(char[])}, {@link #read(java.nio.charbuffer)}, but this is not required.  for examples and integration with {@link analyzer}, see the {@link org.apache.lucene.analysis analysis package documentation}. the underlying character-input stream. create a new charfilter wrapping the provided reader. a reader, can also be a charfilter for chaining. closes the underlying input stream.  note: the default implementation closes the input reader, so be sure to call super.close() when overriding this method. subclasses override to correct the current offset. current offset offset chains the corrected offset through the input charfilter(s). the way java.io.filterreader should work!"
org.apache.lucene.analysis.NumericTokenStream "expert: this class provides a {@link tokenstream} for indexing numeric values that can be used by {@link numericrangequery} or {@link numericrangefilter}. note that for simple usage, {@link intfield}, {@link longfield}, {@link floatfield} or {@link doublefield} is recommended. these fields disable norms and term freqs, as they are not usually needed during searching. if you need to change these settings, you should use this class. here's an example usage, for an int field:  fieldtype fieldtype = new fieldtype(textfield.type_not_stored); fieldtype.setomitnorms(true); fieldtype.setindexoptions(indexoptions.docs_only); field field = new field(name, new numerictokenstream(precisionstep).setintvalue(value), fieldtype); document.add(field);  for optimal performance, re-use the tokenstream and field instance for more than one document:  numerictokenstream stream = new numerictokenstream(precisionstep); fieldtype fieldtype = new fieldtype(textfield.type_not_stored); fieldtype.setomitnorms(true); fieldtype.setindexoptions(indexoptions.docs_only); field field = new field(name, stream, fieldtype); document document = new document(); document.add(field); for(all documents) { stream.setintvalue(value) writer.adddocument(document); }  this stream is not intended to be used in analyzers; it's more for iterating the different precisions during indexing a specific numeric value. note: as token streams are only consumed once the document is added to the index, if you index more than one numeric field, use a separate numerictokenstream instance for each. see {@link numericrangequery} for more details on the precisionstep parameter as well as how numeric fields work under the hood. the full precision token gets this token type assigned. the lower precision tokens gets this token type assigned. expert: use this attribute to get the details of the currently generated token. @lucene.experimental returns current shift value, undefined before first token returns current token's raw value as {@code long} with all {@link #getshift} applied, undefined before first token returns value size in bits (32 for {@code float}, {@code int}; 64 for {@code double}, {@code long}) don't call this method! @lucene.internal don't call this method! @lucene.internal don't call this method! @lucene.internal implementation of {@link numerictermattribute}. @lucene.internal creates, but does not yet initialize this attribute instance @see #init(long, int, int, int) creates a token stream for numeric values using the default precisionstep {@link numericutils#precision_step_default} (4). the stream is not yet initialized, before using set a value using the various set???value() methods. creates a token stream for numeric values with the specified precisionstep. the stream is not yet initialized, before using set a value using the various set???value() methods. expert: creates a token stream for numeric values with the specified precisionstep using the given {@link org.apache.lucene.util.attributesource.attributefactory}. the stream is not yet initialized, before using set a value using the various set???value() methods. initializes the token stream with the supplied long value. the value, for which this tokenstream should enumerate tokens. instance, because of this you can use it the following way: new field(name, new numerictokenstream(precisionstep).setlongvalue(value)) initializes the token stream with the supplied int value. the value, for which this tokenstream should enumerate tokens. instance, because of this you can use it the following way: new field(name, new numerictokenstream(precisionstep).setintvalue(value)) initializes the token stream with the supplied double value. the value, for which this tokenstream should enumerate tokens. instance, because of this you can use it the following way: new field(name, new numerictokenstream(precisionstep).setdoublevalue(value)) initializes the token stream with the supplied float value. the value, for which this tokenstream should enumerate tokens. instance, because of this you can use it the following way: new field(name, new numerictokenstream(precisionstep).setfloatvalue(value)) returns the precision step. for javadocs for javadocs for javadocs for javadocs for javadocs just a wrapper to prevent adding cta return empty token before first or after last this attribute has no contents to clear! we keep it untouched as it's fully controlled by outer class. this will only clear all other attributes in this tokenstream members valsize==0 means not initialized"
org.apache.lucene.analysis.Tokenizer "a tokenizer is a tokenstream whose input is a reader.  this is an abstract class; subclasses must override {@link #incrementtoken()}  note: subclasses overriding {@link #incrementtoken()} must call {@link attributesource#clearattributes()} before setting attributes. the text source for this tokenizer. construct a token stream processing the given input. construct a token stream processing the given input using the given attributefactory. construct a token stream processing the given input using the given attributesource. {@inheritdoc}  note: the default implementation closes the input reader, so be sure to call super.close() when overriding this method. return the corrected offset. if {@link #input} is a {@link charfilter} subclass this method calls {@link charfilter#correctoffset}, else returns currentoff. offset as seen in the output offset based on the input @see charfilter#correctoffset expert: set a new reader on the tokenizer. typically, an analyzer (in its tokenstream method) will use this to re-use a previously lucene-2387: don't hold onto reader after close, so gc can reclaim only used by assert, for testing"
org.apache.lucene.analysis.Analyzer "an analyzer builds tokenstreams, which analyze text. it thus represents a policy for extracting index terms from text.  in order to define what analysis is done, subclasses must define their {@link tokenstreamcomponents tokenstreamcomponents} in {@link #createcomponents(string, reader)}. the components are then reused in each call to {@link #tokenstream(string, reader)}.  simple example:  analyzer analyzer = new analyzer() { {@literal @override} protected tokenstreamcomponents createcomponents(string fieldname, reader reader) { tokenizer source = new footokenizer(reader); tokenstream filter = new foofilter(source); filter = new barfilter(filter); return new tokenstreamcomponents(source, filter); } };  for more examples, see the {@link org.apache.lucene.analysis analysis package documentation}.  for some concrete implementations bundled with lucene, look in the analysis modules:  common: analyzers for indexing content in different languages and domains. icu: exposes functionality from icu to apache lucene. kuromoji: morphological analyzer for japanese text. morfologik: dictionary-driven lemmatization for the polish language. phonetic: analysis for indexing phonetic signatures (for sounds-alike search). smart chinese: analyzer for simplified chinese, which indexes words. stempel: algorithmic stemmer for the polish language. uima: analysis integration with apache uima.  create a new analyzer, reusing the same set of components per-thread across calls to {@link #tokenstream(string, reader)}. expert: create a new analyzer with a custom {@link reusestrategy}.  note: if you just want to reuse on a per-field basis, its easier to use a subclass of {@link analyzerwrapper} such as  perfieldanalyerwrapper instead. creates a new {@link tokenstreamcomponents} instance for this analyzer. the name of the fields content passed to the {@link tokenstreamcomponents} sink as a reader the reader passed to the {@link tokenizer} constructor {@link tokenstreamcomponents} for this analyzer. returns a tokenstream suitable for fieldname, tokenizing the contents of reader.  this method uses {@link #createcomponents(string, reader)} to obtain an instance of {@link tokenstreamcomponents}. it returns the sink of the components and stores the components internally. subsequent calls to this method will reuse the previously stored components after resetting them through {@link tokenstreamcomponents#setreader(reader)}.  note: after calling this method, the consumer must follow the workflow described in {@link tokenstream} to properly consume its contents. see the {@link org.apache.lucene.analysis analysis package documentation} for some examples demonstrating this. the name of the field the override this if you want to add a charfilter chain.  the default implementation returns reader unchanged. indexablefield name being indexed original reader , optionally decorated with charfilter(s) invoked before indexing a indexablefield instance if terms have already been added to that field. this allows custom analyzers to place an automatic position increment gap between indexblefield instances using the same field name. the default value position increment gap is 0. with a 0 position increment gap and the typical default token position increment of 1, all terms in a field, including across indexablefield instances, are in successive positions, allowing exact phrasequery matches, for instance, across indexablefield instance boundaries. indexablefield name being indexed. increment gap, added to the next token emitted from {@link #tokenstream(string,reader)}. this value must be {@code >= 0}. just like {@link #getpositionincrementgap}, except for token offsets instead. by default this returns 1. this method is only called if the field produced at least one token for indexing. the field just indexed gap, added to the next token emitted from {@link #tokenstream(string,reader)}. this value must be {@code >= 0}. frees persistent resources used by this analyzer this class encapsulates the outer components of a token stream. it provides access to the source ({@link tokenizer}) and the outer end (sink), an instance of {@link tokenfilter} which also serves as the {@link tokenstream} returned by {@link analyzer#tokenstream(string, reader)}. original source of the tokens. sink tokenstream, such as the outer tokenfilter decorating the chain. this can be the source if there are no filters. creates a new {@link tokenstreamcomponents} instance. the analyzer's tokenizer the analyzer's resulting token stream creates a new {@link tokenstreamcomponents} instance. the analyzer's tokenizer resets the encapsulated components with the given reader. if the components cannot be reset, an exception should be thrown. a reader to reset the source component @throws ioexception if the component's reset method throws an {@link ioexception} returns the sink {@link tokenstream} sink {@link tokenstream} returns the component's {@link tokenizer} 's {@link tokenizer} strategy defining how tokenstreamcomponents are reused per call to {@link analyzer#tokenstream(string, java.io.reader)}. sole constructor. (for invocation by subclass constructors, typically implicit.) gets the reusable tokenstreamcomponents for the field with the given name name of the field whose reusable tokenstreamcomponents are to be retrieved tokenstreamcomponents for the field, or {@code null} if there was no previous components for the field stores the given tokenstreamcomponents as the reusable components for the field with the give name name of the field whose tokenstreamcomponents are being set tokenstreamcomponents which are to be reused for the field returns the currently stored value stored value or {@code null} if no value is stored @throws alreadyclosedexception if the reusestrategy is closed. sets the stored value value to store @throws alreadyclosedexception if the reusestrategy is closed. closes the reusestrategy, freeing any resources implementation of {@link reusestrategy} that reuses the same components for every field. creates a new instance, with empty per-thread values implementation of {@link reusestrategy} that reuses components per-field by maintaining a map of tokenstreamcomponent per field name. creates a new instance, with empty per-thread-per-field values"
org.apache.lucene.analysis.TokenFilter "a tokenfilter is a tokenstream whose input is another tokenstream.  this is an abstract class; subclasses must override {@link #incrementtoken()}. @see tokenstream the source of tokens for this filter. construct a token stream filtering the given input. {@inheritdoc}  note: the default implementation chains the call to the input tokenstream, so be sure to call super.end() first when overriding this method. {@inheritdoc}  note: the default implementation chains the call to the input tokenstream, so be sure to call super.close() when overriding this method. {@inheritdoc}  note: the default implementation chains the call to the input tokenstream, so be sure to call super.reset() when overriding this method."
