#CANONICAL_CLASS_NAME "AUTHOR"
org.apache.hadoop.conf.Configurable "something that may be configured with a {@link configuration}. set the configuration to be used by this object. return the configuration used by this object. www.apache.org/licenses/license-2.0"
org.apache.hadoop.conf.Configuration "provides access to configuration parameters. resources configurations are specified by resources. a resource contains a set of name/value pairs as xml data. each resource is named by either a string or by a {@link path}. if named by a string, then the classpath is examined for a file with that name. if named by a path, then the local filesystem is examined directly, without referring to the classpath. unless explicitly turned off, hadoop by default specifies two resources, loaded in-order from the classpath:  core-default.xml : read-only defaults for hadoop. core-site.xml: site-specific configuration for a given hadoop installation.  applications may add additional resources, which are loaded subsequent to these resources in the order they are added. final parameters configuration parameters may be declared final. once a resource declares a value final, no subsequently-loaded resource can alter that value. for example, one might define a final parameter with:  &lt;property&gt; &lt;name&gt;dfs.hosts.include&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/hosts.include&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; administrators typically define parameters as final in core-site.xml for values that list of configuration resources. the value reported as the setting resource when a key is set by code rather than a file resource by dumpconfiguration. list of configuration parameters marked final. configuration objects list of default resources. resources are loaded in the order of the list entries sentinel value to store negative cache results in {@link #cache_classes}. stores the mapping of key to the resource which modifies or loads the key most recently class to keep the information about the keys which replace the deprecated ones. this class stores the new keys which replace the deprecated keys and also gives a provision to have a custom message for each of the deprecated key that is being replaced. it also provides method to get the appropriate warning message which can be logged whenever the deprecated key is used. method to provide the warning message. it gives the custom message if non-null, and default message otherwise. the associated deprecated key. that is to be logged when a deprecated key is used. stores the deprecated keys, the new keys which replace the deprecated keys and custom message(if any provided). stores a mapping from superseding keys to the keys which they deprecate. adds the deprecated key to the deprecation map. it does not override any existing entries in the deprecation map. this is to be used only by the developers in order to add deprecation of keys, and attempts to call this method after loading resources once, would lead to unsupportedoperationexception if a key is deprecated in favor of multiple keys, they are all treated as aliases of each other, and setting any one of them resets all the others to the new value. @deprecated use {@link #adddeprecation(string key, string newkey, string custommessage)} instead adds the deprecated key to the deprecation map. it does not override any existing entries in the deprecation map. this is to be used only by the developers in order to add deprecation of keys, and attempts to call this method after loading resources once, would lead to unsupportedoperationexception adds the deprecated key to the deprecation map when no custom message is provided. it does not override any existing entries in the deprecation map. this is to be used only by the developers in order to add deprecation of keys, and attempts to call this method after loading resources once, would lead to unsupportedoperationexception if a key is deprecated in favor of multiple keys, they are all treated as aliases of each other, and setting any one of them resets all the others to the new value. key that is to be deprecated list of keys that take up the values of deprecated key @deprecated use {@link #adddeprecation(string key, string newkey)} instead adds the deprecated key to the deprecation map when no custom message is provided. it does not override any existing entries in the deprecation map. this is to be used only by the developers in order to add deprecation of keys, and attempts to call this method after loading resources once, would lead to unsupportedoperationexception key that is to be deprecated key that takes up the value of deprecated key checks whether the given key is deprecated. the parameter which is to be checked for deprecation true if the key is deprecated and false otherwise. returns the alternate name for a key if the property name is deprecated or if deprecates a property name. property name. name. checks for the presence of the property name in the deprecation map. returns the first of the list of new keys if present in the deprecation map or the name itself. if the property is not presently set but the property map contains an entry for the deprecated key, the value of the deprecated key is set as the value for the provided property name. the property name first property in the list of properties mapping the name or the name itself. a new configuration. a new configuration where the behavior of reading from the default resources can be turned off. if the parameter {@code loaddefaults} is false, the new instance will not load resources from the default files. specifies whether to load from the default files a new configuration with the same settings cloned from another. the configuration from which to clone settings. add a default resource. resources are loaded in the order of the resources added. file name. file should be present in the classpath. add a configuration resource. the properties of this resource will override properties of previously added resources, unless they were marked final. resource to be added, the classpath is examined for a file with that name. add a configuration resource. the properties of this resource will override properties of previously added resources, unless they were marked final. url of the resource to be added, the local filesystem is examined directly to find the resource, without referring to the classpath. add a configuration resource. the properties of this resource will override properties of previously added resources, unless they were marked final. file-path of resource to be added, the local filesystem is examined directly to find the resource, without referring to the classpath. add a configuration resource. the properties of this resource will override properties of previously added resources, unless they were marked final. warning: the contents of the inputstream will be cached, by this method. so use this sparingly because it does increase the memory consumption. inputstream to deserialize the object from. in will be read from when a get or set is called next. after it is read the stream will be closed. add a configuration resource. the properties of this resource will override properties of previously added resources, unless they were marked final. inputstream to deserialize the object from. the name of the resource because inputstream.tostring is not very descriptive some times. reload configuration from previously added resources. this method will clear all the configuration read from the added resources, and final parameters. this will make the resources to be read again before accessing the values. values that are added via set methods will overlay values read from the resources. get the value of the name property, null if no such property exists. if the key is deprecated, it returns the value of the first key which replaces the deprecated key and is not null values are processed for variable expansion before being returned. the property name. value of the name or its replacing property, or null if no such property exists. get the value of the name property as a trimmed string, null if no such property exists. if the key is deprecated, it returns the value of the first key which replaces the deprecated key and is not null values are processed for variable expansion before being returned. the property name. value of the name or its replacing property, or null if no such property exists. get the value of the name property, without doing variable expansion.if the key is deprecated, it returns the value of the first key which replaces the deprecated key and is not null. the property name. value of the name property or its replacing property and null if no such property exists. set the value of the name property. if name is deprecated or there is a deprecated name associated to it, it sets the value to both names. property name. property value. set the value of the name property. if name is deprecated or there is a deprecated name associated to it, it sets the value to both names. property name. property value. the place that this configuration value came from (for debugging). @throws illegalargumentexception when the value or name is null. unset a previously set property. sets a property if it is currently unset. the property name the new value get the value of the name. if the key is deprecated, it returns the value of the first key which replaces the deprecated key and is not null. if no such property exists, then defaultvalue is returned. property name. default value. value, or defaultvalue if the property doesn't exist. get the value of the name property as an int. if no such property exists, the provided default value is returned, or if the specified value is not a valid int, then an error is thrown. property name. default value. @throws numberformatexception when the value is invalid value as an int, or defaultvalue. get the value of the name property as a set of comma-delimited int values. if no such property exists, an empty array is returned. property name value interpreted as an array of comma-delimited int values set the value of the name property to an int. property name. int value of the property. get the value of the name property as a long. if no such property exists, the provided default value is returned, or if the specified value is not a valid long, then an error is thrown. property name. default value. @throws numberformatexception when the value is invalid value as a long, or defaultvalue. get the value of the name property as a long or human readable format. if no such property exists, the provided default value is returned, or if the specified value is not a valid long or human readable format, then an error is thrown. you can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) property name. default value. @throws numberformatexception when the value is invalid value as a long, or defaultvalue. set the value of the name property to a long. property name. long value of the property. get the value of the name property as a float. if no such property exists, the provided default value is returned, or if the specified value is not a valid float, then an error is thrown. property name. default value. @throws numberformatexception when the value is invalid value as a float, or defaultvalue. set the value of the name property to a float. property name. property value. get the value of the name property as a boolean. if no such property is specified, or if the specified value is not a valid boolean, then defaultvalue is returned. property name. default value. value as a boolean, or defaultvalue. set the value of the name property to a boolean. property name. boolean value of the property. set the given property, if it is currently unset. property name new value set the value of the name property to the given type. this is equivalent to set(&lt;name&gt;, value.tostring()). property name new value return value matching this enumerated type. property name value returned if no mapping exists @throws illegalargumentexception if mapping is illegal for the type provided get the value of the name property as a pattern. if no such property is specified, or if the specified value is not a valid pattern, then defaultvalue is returned. property name default value value as a compiled pattern, or defaultvalue set the given property to pattern. if the pattern is passed as null, sets the empty pattern which results in further calls to getpattern(...) returning the default value. property name new value gets information about why a property was set. typically this is the path to the resource objects (file, url, etc.) the property came from, but it can also indicate that it was set programatically, or because of the command line. - the property name to get the source of. - if the property or its source wasn't found. otherwise, returns a list of the sources of the resource. the older sources are the first ones in the list. so for example if a configuration is set from the command line, and then written out to a file that is read back in the first entry would indicate that it was set from the command line, while the second one would indicate the file that the new configuration was read in from. a class that represents a set of positive integer ranges. it parses strings of the form: "2-3,5,7-" where ranges are separated by comma and the lower/upper bounds are separated by dash. either the lower or upper bound may be omitted meaning all values up to or over. so the string above means 2, 3, 5, and 7, 8, 9, ... convert a string to an int treating empty strings as the default value. the string value the value for if the string is empty desired integer is the given value in the set of ranges the value to check the value in the ranges? if there are no values in this range, else false. parse the given attribute as a set of integer ranges the attribute name the default value if it is not set new set of ranges from the configured value get the comma delimited values of the name property as a collection of strings. if no such property is specified then empty collection is returned.  this is an optimized version of {@link #getstrings(string)} property name. value as a collection of strings. get the comma delimited values of the name property as an array of strings. if no such property is specified then null is returned. property name. value as an array of strings, or null. get the comma delimited values of the name property as an array of strings. if no such property is specified then default value is returned. property name. the default value value as an array of strings, or default value. get the comma delimited values of the name property as a collection of strings, trimmed of the leading and trailing whitespace. if no such property is specified then empty collection is returned. property name. value as a collection of strings, or empty collection get the comma delimited values of the name property as an array of strings, trimmed of the leading and trailing whitespace. if no such property is specified then an empty array is returned. property name. value as an array of trimmed strings, or empty array. get the comma delimited values of the name property as an array of strings, trimmed of the leading and trailing whitespace. if no such property is specified then default value is returned. property name. the default value value as an array of trimmed strings, or default value. set the array of string values for the name property as as comma delimited values. property name. the values get the socket address for name property as a inetsocketaddress. property name. the default value the default port set the socket address for the name property as a host:port. set the socket address a client can use to connect for the name property as a host:port. the wildcard address is replaced with the local host's address. property name. inetsocketaddress of a listener to store in the given property for clients to connect load a class by name. the class name. class object. @throws classnotfoundexception if the class is not found. load a class by name, returning null rather than throwing an exception if it couldn't be loaded. this is to avoid the overhead of creating an exception. the class name class object, or null if it could not be found. get the value of the name property as an array of class. the value of the property specifies a list of comma separated class names. if no such property is specified, then defaultvalue is returned. the property name. default value. value as a class[], or defaultvalue. get the value of the name property as a class. if no such property is specified, then defaultvalue is returned. the class name. default value. value as a class, or defaultvalue. get the value of the name property as a class implementing the interface specified by xface. if no such property is specified, then defaultvalue is returned. an exception is thrown if the returned class does not implement the named interface. the class name. default value. the interface implemented by the named class. value as a class, or defaultvalue. get the value of the name property as a list of objects implementing the interface specified by xface. an exception is thrown if any of the classes does not exist, or if it does not implement the named interface. the property name. the interface implemented by the classes named by name. list of objects implementing xface. set the value of the name property to the name of a theclass implementing the given interface xface. an exception is thrown if theclass does not implement the interface xface. property name. property value. the interface implemented by the named class. get a local file under a directory named by dirsprop with the given path. if dirsprop contains multiple directories, then one is chosen based on path's hash code. if the selected directory does not exist, an attempt is made to create it. directory in which to locate the file. file-path. file under the directory with the given path. get a local file name under a directory named in dirsprop with the given path. if dirsprop contains multiple directories, then one is chosen based on path's hash code. if the selected directory does not exist, an attempt is made to create it. directory in which to locate the file. file-path. file under the directory with the given path. get the {@link url} for the named resource. resource name. url for the named resource. get an input stream attached to the configuration resource with the given name. configuration resource name. input stream attached to the resource. get a {@link reader} attached to the configuration resource with the given name. configuration resource name. reader attached to the resource. return the number of keys in the configuration. of keys in the configuration. clears all keys from the configuration. get an {@link iterator} to go through the list of string key-value pairs in the configuration. iterator over the entries. write out the non-default properties in this configuration to the given {@link outputstream}. the output stream to write to. write out the non-default properties in this configuration to the given {@link writer}. the writer to write to. return the xml dom corresponding to this configuration. writes out all the parameters and their properties (final and resource) to the given {@link writer} the format of the output would be { "properties" : [ {key1,value1,key1.isfinal,key1.resource}, {key2,value2, key2.isfinal,key2.resource}... ] } it does not output the parameters of the configuration object which is loaded from an input stream. the writer to write to @throws ioexception get the {@link classloader} for this job. correct class loader. set the class loader that will be used to load the various objects. the new class loader. set the quietness-mode. in the quiet-mode, error and informational messages might not be logged. true to set quiet-mode on, false to turn it off. for debugging. list non-default properties to the terminal and exit. get keys matching the the regex  with matching keys a unique class which is used as a sentinel value in the caching for getclassbyname. {@see configuration#getclassbynameornull(string)} www.apache.org/licenses/license-2.0 to help look for other new configs for this deprecated config print deprecation warning if hadoop-site.xml is found in classpath add code for managing deprecated key mapping for example adddeprecation("oldkey1",new string[]{"newkey1","newkey2"}); adds deprecation for oldkey1 to two new keys(newkey1, newkey2). so get or set of oldkey1 will correctly populate/access values of newkey1 and newkey2 trigger reload clear site-limits add to resources remove ${ .. } return literal ${var}: var is unbound substitute if properties is null, it means a resource was newly added but the props were cleared so as to load it upon future requests. so lets force a load by asking a properties list. return a null right away if our properties still haven't loaded or the resource mapping isn't defined leave a marker that the class isn't found two putters can race here, but they'll put the same class not found cache hit try each local dir try each local dir get a copy of just the string to string pairs. after the old object methods that allow non-strings to be put into configurations are removed, we could replace properties with a map and get rid of this code. support the hadoop-site.xml as a deprecated case ignore all comments inside the xml file allow includes in the xml file an url resource a classpath resource a file resource can't use filesystem api or we get an infinite loop since filesystem uses configuration api. use java.io.file instead. ignore this parameter if it has already been marked as 'final' update new keys with deprecated key's value important to not hold configuration log while writing result, since 'out' may be an hdfs stream which needs to lock this configuration from another thread. ensure properties is set and deprecation is handled @override match load deprecated keys in common"
org.apache.hadoop.conf.Configured "base class for things that may be configured with a {@link configuration}. construct a configured. construct a configured. www.apache.org/licenses/license-2.0 inherit javadoc inherit javadoc"
org.apache.hadoop.conf.ConfServlet "a servlet to print out the running configuration data. return the configuration of the daemon hosting this servlet. this is populated when the httpserver starts. guts of the servlet - extracted for easy testing. www.apache.org/licenses/license-2.0"
org.apache.hadoop.conf.Reconfigurable "something whose {@link configuration} can be changed at run time. change a configuration property on this object to the value specified. change a configuration property on this object to the value specified and return the previous value that the configuration property was set to (or null if it was not previously set). if newval is null, set the property to its default value; if the property cannot be changed, throw a {@link reconfigurationexception}. return whether a given property is changeable at run time. if ispropertyreconfigurable returns true for a property, then changeconf should not throw an exception when changing this property. return all the properties that can be changed at run time. www.apache.org/licenses/license-2.0"
org.apache.hadoop.conf.ReconfigurableBase "utility base class for implementing the reconfigurable interface. subclasses should override reconfigurepropertyimpl to change individual properties and getreconfigurableproperties to get all properties that can be changed at run time. construct a reconfigurablebase. construct a reconfigurablebase with the {@link configuration} conf. {@inheritdoc} this method makes the change to this objects {@link configuration} and calls reconfigurepropertyimpl to update internal data structures. this method cannot be overridden, subclasses should instead override reconfigureproperty. {@inheritdoc} subclasses must override this. {@inheritdoc} subclasses may wish to override this with a more efficient implementation. change a configuration property. subclasses must override this. this method applies the change to all internal data structures derived from the configuration property that is being changed. if this object owns other reconfigurable objects reconfigureproperty should be called recursively to make sure that to make sure that the configuration of these objects is updated. www.apache.org/licenses/license-2.0"
org.apache.hadoop.conf.ReconfigurationException "exception indicating that configuration property cannot be changed at run time. construct the exception message. create a new instance of {@link reconfigurationexception}. create a new instance of {@link reconfigurationexception}. create a new instance of {@link reconfigurationexception}. get property that cannot be changed. get value to which property was supposed to be changed. get old value of property that cannot be changed. www.apache.org/licenses/license-2.0"
org.apache.hadoop.conf.ReconfigurationServlet "a servlet for changing a node's configuration. reloads the configuration file, verifies whether changes are possible and asks the admin to approve the change. print configuration options that can be changed. apply configuratio changes after admin has approved them. www.apache.org/licenses/license-2.0 the prefix used to fing the attribute holding the reconfigurable for a given request  we get the attribute prefix + servlet path change from default or value to different value parameter value != newconf value"
org.apache.hadoop.conf.ReconfigurationUtil "www.apache.org/licenses/license-2.0 iterate over old configuration now iterate over new configuration (to look for properties not present in old conf)"
org.apache.hadoop.fs.AbstractFileSystem "this class provides an interface for implementors of a hadoop file system (analogous to the vfs of unix). applications do not access this class; instead they access files across all file systems using {@link filecontext}. pathnames passed to abstractfilesystem can be fully qualified uri that matches the "this" file system (ie same scheme and evolving for a release,to be changed to stable recording statistics per a file system class. cache of constructors for each file system class. the statistics for this file system. prohibits names which contain a ".", "..", ":" or "/" create an object for the given class and initialize it from conf. class of which an object is create a file system instance for the specified uri using the conf. the conf is used to find the class name that implements the file system. the conf is also passed to the file system for its configuration. uri of the file system configuration for the file system the file system for the given uri @throws unsupportedfilesystemexception file system for uri is not found get the statistics for a particular file system. used as key to lookup statistics_table. only scheme and prints statistics for all file systems. the main factory method for creating a file system. get a file system for the uri's scheme and constructor to be called by subclasses. for this file system. the scheme supported by the implementor if true then theuri must have check that the uri's scheme matches get the uri for the file system based on the given uri. the path, query part of the given uri is stripped out and default file system port is used to form the uri. filesystem uri. if true the default port of this file system. port of this file system's uri scheme a uri with a port of -1 => default port; returns a uri whose scheme and check that a path belongs to this filesystem. if the path is fully qualified uri, then its scheme and get the path-part of a pathname. checks that uri matches this file system and that the path-part is a valid name. path -part of the path p make the path fully qualified to this file system qualified path some file systems like localfilesystem have an initial workingdir that is used as the starting workingdir. for other file systems like hdfs there is no built in notion of an initial workingdir. initial workingdir if the file system has such a notion otherwise return a null. return the current return a set of server default configuration values. default configuration values @throws ioexception an i/o error occurred return the fully-qualified path of path f resolving the path through any internal symlinks or mount point path to be resolved qualified path @throws filenotfoundexception, accesscontrolexception, ioexception unresolvedlinkexception if symbolic link on path cannot be resolved internally the specification of this method matches that of {@link filecontext#create(path, enumset, options.createopts...)} except that the path f must be fully qualified and the permission is absolute (i.e. umask has been applied). the specification of this method matches that of {@link #create(path, enumset, options.createopts...)} except that the opts have been declared explicitly. the specification of this method matches that of {@link filecontext#mkdir(path, fspermission, boolean)} except that the path f must be fully qualified and the permission is absolute (i.e. umask has been applied). the specification of this method matches that of {@link filecontext#delete(path, boolean)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#open(path)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#open(path, int)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#setreplication(path, short)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#rename(path, path, options.rename...)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#rename(path, path, options.rename...)} except that path f must be for this file system and no overwrite is performed. file systems that do not have a built in overwrite need implement only this method and can take advantage of the default impl of the other {@link #renameinternal(path, path, boolean)} the specification of this method matches that of {@link filecontext#rename(path, path, options.rename...)} except that path f must be for this file system. returns true if the file system supports symlinks, false otherwise. the specification of this method matches that of {@link filecontext#createsymlink(path, path, boolean)}; the specification of this method matches that of {@link filecontext#getlinktarget(path)}; we should never get here. any file system that threw an unresolvedlinkexception, causing this function to be called, needs to override this method. the specification of this method matches that of {@link filecontext#setpermission(path, fspermission)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#setowner(path, string, string)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#settimes(path, long, long)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#getfilechecksum(path)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#getfilestatus(path)} except that an unresolvedlinkexception may be thrown if a symlink is encountered in the path. the specification of this method matches that of {@link filecontext#getfilelinkstatus(path)} except that an unresolvedlinkexception may be thrown if a symlink is encountered in the path leading up to the final path component. if the file system does not support symlinks then the behavior is equivalent to {@link abstractfilesystem#getfilestatus(path)}. the specification of this method matches that of {@link filecontext#getfileblocklocations(path, long, long)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#getfsstatus(path)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#getfsstatus(path)}. the specification of this method matches that of {@link filecontext#liststatus(path)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext#listlocatedstatus(path)} except that path f must be for this file system. the specification of this method matches that of {@link filecontext.util#liststatus(path)} except that path f must be for this file system. iterator over the corrupt files under the given path (may contain duplicates if a file has more than one corrupt block) @throws ioexception the specification of this method matches that of {@link filecontext#setverifychecksum(boolean, path)} except that path f must be for this file system. get a canonical name for this file system. uri string that uniquely identifies this file system get one or more delegation tokens associated with the filesystem. normally a file system returns a single delegation token. a file system that manages multiple file systems underneath, could return set of delegation tokens for all the file systems it manages the account name that is allowed to renew the token. of delegation tokens. if delegation tokens not supported then return a list of size zero. @throws ioexception www.apache.org/licenses/license-2.0 check for ".." "." ":" "/" "; " a file system implementation that requires specify default port /");  no port supplied and default port is not specified " + uri.gethost() + ":" + port); schemes and hosts must match. allow for null ports must match, unless this fs instance is using the default port, in which case the port may be omitted from the given uri -1 => defaultport of uri scheme default impl is to return the path create a checksum option honoring if bytesperchecksum is specified, it will override the one set in checksumopt. any missing value will be filled in using the default. default implementation deals with overwrite in a non-atomic way it's ok to rename a file to a symlink and vice versa delete the destination that is a file or an empty directory default impl gets fsstatus of root object object"
org.apache.hadoop.fs.AvroFSInput "adapts an {@link fsdatainputstream} to avro's seekableinput interface. construct given an {@link fsdatainputstream} and its length. construct given a {@link filecontext} and a {@link path}. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.BlockLocation "represents the network location of a block, information about the hosts that contain block replicas, and other block metadata (e.g. the file offset associated with the block, length, whether it is corrupt, etc). default constructor constructor with host, name, offset and length constructor with host, name, offset, length and corrupt flag constructor with host, name, network topology, offset and length constructor with host, name, network topology, offset, length and corrupt flag get the list of hosts (hostname) hosting this block get the list of names (ip:xferport) hosting this block get the list of network topology paths for each of the hosts. the last component of the path is the "name" (ip:xferport). get the start offset of file associated with this block get the length of the block get the corrupt flag. set the start offset of file associated with this block set the length of block set the corrupt flag. set the hosts hosting this block set the names (host:port) hosting this block set the network topology paths of the hosts www.apache.org/licenses/license-2.0 datanode hostnames datanode ip:xferport for accessing the block full path name in network topology offset of the block in the file"
org.apache.hadoop.fs.BufferedFSInputStream "a class optimizes reading from fsinputstream by bufferring creates a bufferedfsinputstream with the specified buffer size, and saves its argument, the input stream in, for later use. an internal buffer array of length size is www.apache.org/licenses/license-2.0 optimize: check if the pos is in the buffer invalidate buffer"
org.apache.hadoop.fs.ByteBufferReadable "implementers of this interface provide a read api that writes to a bytebuffer, not a byte[]. reads up to buf.remaining() bytes into buf. callers should use buf.limit(..) to control the size of the desired read.  after a successful call, buf.position() and buf.limit() should be unchanged, and therefore any data can be immediately read from buf. buf.mark() may be cleared or updated.  in the case of an exception, the values of buf.position() and buf.limit() are undefined, and callers should be prepared to recover from this eventuality.  many implementations will throw {@link unsupportedoperationexception}, so callers that are not confident in support for this method from the underlying filesystem should be prepared to handle that exception.  implementations should treat 0-length requests as legitimate, and must not signal an error upon their receipt. the bytebuffer to receive the results of the read operation. up to buf.limit() - buf.position() bytes may be read. number of bytes available to read from buf @throws ioexception if there is some error performing the read www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.ChecksumException "thrown for checksum errors. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.ChecksumFileSystem "abstract checksumed filesystem. it provide a basice implementation of a checksumed filesystem, which creates a checksum file for each raw file. it generates & verifies checksums at the client side. set whether to verify checksum. get the raw file system return the name of the checksum file associated with a file. return true iff file is a checksum file name. return the length of the checksum file given the size of the actual file. return the bytes per checksum for open()'s fsinputstream it verifies that data matches checksums. return the file length skips over and discards n bytes of data from the input stream. the skip method skips over some smaller number of bytes when reaching end of file before n bytes have been skipped. the actual number of bytes skipped is returned. if n is negative, no bytes are skipped. n the number of bytes to be skipped. the actual number of bytes skipped. @exception ioexception if an i/o error occurs. checksumexception if the chunk to skip to is corrupted seek to the given position in the stream. the next read() will be from that position. this method does not allow seek past the end of the file. this produces ioexception. pos the postion to seek to. @exception ioexception if an i/o error occurs or seeks after eof checksumexception if the chunk to seek to is corrupted opens an fsdatainputstream at the indicated path. the file name to open the size of the buffer to be used. calculated the length of the checksum file in bytes. the length of the data file in bytes the number of bytes in a checksum block number of bytes in the checksum file this class provides an output stream for a checksummed file. it generates checksums for data. set replication for an existing file. implement the abstract setreplication of filesystem file name new replication @throws ioexception if successful; false if file does not exist or is a directory rename files/dirs implement the delete(path, boolean) in checksum file system. list the statuses of the files/directories in the given path if the path is a directory. given path statuses of the files/directories in the given patch @throws ioexception list the statuses of the files/directories in the given path if the path is a directory. given path statuses of the files/directories in the given patch @throws ioexception the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. if src and dst are directories, the copycrc parameter determines whether to copy crc files. report a checksum error to the file system. the file name containing the error the stream open on the file the position of the beginning of the bad data in the file the stream open on the checksum file the position of the beginning of the bad data in the checksum file retry is neccessary www.apache.org/licenses/license-2.0 quietly ignore loudly ignore parameter check we have a checksum buffer it is sane length we must read at least one chunk number of checksums based on len to read size of checksum buffer we're at the end of the file adjust amount of data to read based on how many checksum chunks we read the checksum length is equal to size passed divided by bytespersum + bytes written in the beginning of the checksum file. remove the checksum file since we aren't writing one try to rename checksum no src checksum, so remove dst checksum this works since the crcs are in the same directories and the files. so we just delete everything in the underlying filesystem source is a file remove old local checksum file copy checksum file"
org.apache.hadoop.fs.ChecksumFs "abstract checksumed fs. it provide a basic implementation of a checksumed fs, which creates a checksum file for each raw file. it generates & verifies checksums at the client side. evolving for a release,to be changed to stable set whether to verify checksum. get the raw file system. return the name of the checksum file associated with a file. return true iff file is a checksum file name. return the length of the checksum file given the size of the actual file. return the bytes per checksum. for open()'s fsinputstream it verifies that data matches checksums. return the file length skips over and discards n bytes of data from the input stream. the skip method skips over some smaller number of bytes when reaching end of file before n bytes have been skipped. the actual number of bytes skipped is returned. if n is negative, no bytes are skipped. n the number of bytes to be skipped. the actual number of bytes skipped. @exception ioexception if an i/o error occurs. checksumexception if the chunk to skip to is corrupted seek to the given position in the stream. the next read() will be from that position. this method does not allow seek past the end of the file. this produces ioexception. pos the postion to seek to. @exception ioexception if an i/o error occurs or seeks after eof checksumexception if the chunk to seek to is corrupted opens an fsdatainputstream at the indicated path. the file name to open the size of the buffer to be used. calculated the length of the checksum file in bytes. the length of the data file in bytes the number of bytes in a checksum block number of bytes in the checksum file this class provides an output stream for a checksummed file. it generates checksums for data. check if exists. source file true iff the named path is a directory. note: avoid using this method. instead reuse the filestatus returned by getfilestatus() or liststatus() methods. set replication for an existing file. implement the abstract setreplication of filesystem file name new replication @throws ioexception if successful; false if file does not exist or is a directory rename files/dirs. implement the delete(path, boolean) in checksum file system. report a checksum error to the file system. the file name containing the error the stream open on the file the position of the beginning of the bad data in the file the stream open on the checksum file the position of the beginning of the bad data in the checksum file retry is neccessary www.apache.org/licenses/license-2.0 quietly ignore loudly ignore parameter check we have a checksum buffer it is sane length we must read at least one chunk number of checksums based on len to read size of checksum buffer we're at the end of the file adjust amount of data to read based on how many checksum chunks we read the checksum length is equal to size passed divided by bytespersum + bytes written in the beginning of the checksum file. checksumopt is passed down to the raw fs. unless it implements checksum impelemts internally, checksumopt will be ignored. if the raw fs does checksum internally, we will end up with two layers of checksumming. i.e. checksumming checksum file. now create the chekcsumfile; adjust the buffsize f does not exist try to rename checksum this works since the crcs are in the same directories and the files. so we just delete everything in the underlying filesystem"
org.apache.hadoop.fs.CommonConfigurationKeys "this class contains constants for configuration keys used in the common code. it inherits all the publicly documented configuration keys and adds unsupported keys. default location for default value for fs_home_dir_key default umask for files default value for fs_permissions_umask_key how often does rpc client send pings to rpc server default value for ipc_ping_interval_key enables pings from rpc client to the server default value of ipc_client_ping_key responses larger than this will be logged default value for ipc_server_rpc_max_response_size_key number of threads in rpc server reading from the socket default value for ipc_server_rpc_read_threads_key how many calls per handler are allowed in the queue. default value for ipc_server_handler_queue_size_key internal buffer size for lzo compressor/decompressors default value for io_compression_codec_lzo_buffersize_key this is for specifying the implementation for the mappings from hostnames to the racks they belong to internal buffer size for snappy compressor/decompressors default value for io_compression_codec_snappy_buffersize_key internal buffer size for snappy compressor/decompressors default value for io_compression_codec_snappy_buffersize_key service ha health monitor and failover controller. how often to retry connecting to the service. how often to check the service. how long to sleep after an unexpected rpc error. timeout for the actual monitorhealth() calls. timeout that the fc waits for the new active to become active timeout that the fc waits for the old active to go to standby fc connection retries for graceful fencing timeout that the cli (manual) fc waits for monitorhealth, getservicestate static enable/disable aliases serving from jetty path to the kerberos ticket cache. setting this will force www.apache.org/licenses/license-2.0 1 min 4 hours"
org.apache.hadoop.fs.CommonConfigurationKeysPublic "this class contains constants for configuration keys used in the common code. it includes all publicly documented configuration keys. in general this class should not be used directly (use commonconfigurationkeys instead) see core-default.xml default value for io_native_lib_available_key see core-default.xml default value for net_topology_script_number_args_key see core-default.xml default value for fs_default_name_key see core-default.xml default value for fs_df_interval_key see core-default.xml see core-default.xml see core-default.xml see core-default.xml default value for fs_trash_checkpoint_interval_key not used anywhere, looks like default value for fs_local_block_size see core-default.xml default value for fs_automatic_close_key see core-default.xml see core-default.xml see core-default.xml see core-default.xml default value for fs_trash_interval_key see core-default.xml default value for io_mapfile_bloom_size_key see core-default.xml default value for io_mapfile_bloom_error_rate_key codec class that implements lzo compression algorithm see core-default.xml default value for io_map_index_interval_default see core-default.xml default value for io_map_index_skip_key see core-default.xml default value for io_seqfile_compress_blocksize_key see core-default.xml default value for io_file_buffer_size_key see core-default.xml default value for io_skip_checksum_errors_key @deprecated moved to mapreduce, see mapreduce.task.io.sort.mb in mapred-default.xml see https://issues.apache.org/jira/browse/hadoop-6801 default value for io_sort_mb_default @deprecated moved to mapreduce, see mapreduce.task.io.sort.factor in mapred-default.xml see https://issues.apache.org/jira/browse/hadoop-6801 default value for io_sort_factor_default see core-default.xml see core-default.xml default value for tfile_io_chunk_size_default see core-default.xml default value for tfile_fs_input_buffer_size_key see core-default.xml default value for tfile_fs_output_buffer_size_key see core-default.xml default value for ipc_client_connection_maxidletime_key see core-default.xml default value for ipc_client_connect_timeout_key see core-default.xml default value for ipc_client_connect_max_retries_key see core-default.xml default value for ipc_client_connect_max_retries_on_socket_timeouts_key see core-default.xml defalt value for ipc_client_tcpnodelay_key see core-default.xml default value for ipc_server_listen_queue_size_key see core-default.xml default value for ipc_client_kill_max_key see core-default.xml default value for ipc_client_idlethreshold_default see core-default.xml default value for ipc_server_tcpnodelay_key see core-default.xml see core-default.xml see core-default.xml default value for hadoop_util_hash_type_key see core-default.xml see core-default.xml see core-default.xml see core-default.xml see core-default.xml see core-default.xml see core-default.xml www.apache.org/licenses/license-2.0 the keys fs keys /"; defaults are not specified for following keys tbd: code is still using hardcoded values (e.g. "fs.automatic.close") instead of constant (e.g. fs_automatic_close_key)  issues.apache.org/jira/browse/hadoop-6801 issues.apache.org/jira/browse/hadoop-6801 10s 20s"
org.apache.hadoop.fs.ContentSummary "store the summary of a content (a directory or a file). constructor constructor constructor length directory count file count return the directory quota retuns (disk) space consumed returns (disk) space quota output format:    dir_count file_count content_size file_name output format:        quota remaining_quata space_quota space_quota_rem dir_count file_count content_size file_name the header string return the header of the output. if qoption is false, output directory count, file count, and content size; if qoption is true, output quota and remaining quota as well. a flag indicating if quota needs to be printed or not header of the output return the string representation of the object in the output format. if qoption is false, output directory count, file count, and content size; if qoption is true, output quota and remaining quota as well. a flag indicating if quota needs to be printed or not string representation of the object www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.CreateFlag "createflag specifies the file create semantic. create a file. see javadoc for more description already exists truncate/overwrite a file. same as posix o_trunc. see javadoc for description. append to a file. see javadoc for more description. force closed blocks to disk. similar to posix o_sync. see javadoc for description. validate the createflag and throw exception if it is invalid set of createflag @throws hadoopillegalargumentexception if the createflag is invalid validate the createflag for create operation object representing the path; usually string or {@link path} pass true if the path exists in the file system set of createflag @throws ioexception on error @throws hadoopillegalargumentexception if the createflag is invalid www.apache.org/licenses/license-2.0 both append and overwrite is an error"
org.apache.hadoop.fs.DelegateToFileSystem "implementation of abstractfilesystem based on the existing implementation of {@link filesystem}. we should never get here. any file system that threw an unresolvedlinkexception, causing this function to be called, should override getlinktarget. www.apache.org/licenses/license-2.0 call to primitivecreate default impl assumes that permissions do not matter calling the regular create is good enough. fss that implement permissions should override this. parent must exist. since this.create makes parent dirs automatically we must throw exception if parent does not exist. parent does exist - go ahead with create of file. call to primitivemkdir call to rename abstractfilesystem abstractfilesystem"
org.apache.hadoop.fs.DelegationTokenRenewer "a daemon thread that waits for the next file system to renew. the renewable interface used by the renewer. renew token. set delegation token. an action that will renew and replace the file system's delegation tokens automatically. when should the renew happen a weak reference to the file system so that it can be garbage collected get the delay until this event should happen. set a new time for the renewal. it can only be called when the action is not in the queue. the new time renew or replace the delegation token for this file system. @return @throws ioexception wait for 95% of a day between renewals queue to maintain the renewactions to be processed by the {@link #run()} for testing purposes create the singleton instance. however, the thread can be started lazily in {@link #addrenewaction(filesystem)} add a renew action to the queue. remove the associated renew action from the queue @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.DF "filesystem disk space usage statistics. uses the unix 'df' program to get mount points, and java.io.file for space utilization. tested on linux, freebsd, cygwin. default df refresh interval. canonical path to the volume we're checking. string indicating which filesystem volume we're checking. capacity of the measured filesystem in bytes. total used space on the filesystem in bytes. usable space remaining on the filesystem in bytes. amount of the volume full, as a percent. filesystem mount point for the indicated volume www.apache.org/licenses/license-2.0 / accessors ignoring the error since the exit code it enough skip headings for long filesystem name capacity available pct used capacity used available pct used"
org.apache.hadoop.fs.DU "filesystem disk space usage statistics. uses the unix 'du' program keeps track of disk usage. the path to check disk usage in refresh the disk usage at this interval @throws ioexception if we fail to refresh the disk usage keeps track of disk usage. the path to check disk usage in configuration object @throws ioexception if we fail to refresh the disk usage this thread refreshes the "used" variable. future improvements could be to not permanently run this thread, instead run when getused is called. decrease how much disk space we use. decrease by this value increase how much disk space we use. increase by this value space used @throws ioexception if the shell command fails path of which we're keeping track of disk usage start the disk usage checking thread. shut down the refreshing thread. www.apache.org/licenses/license-2.0 we set the shell interval to 0 so it will always run our command and use this one to set the thread sleep interval populate the used variable 10 minutes default refresh interval update the used variable save the latest exception so we can return it in getused() if the updating thread isn't started, update on demand if an exception was thrown in the last run, rethrow only start the thread if the interval is sane"
org.apache.hadoop.fs.FileAlreadyExistsException "used when target file already exists for any operation and is not configured to be overwritten. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.FileChecksum "an abstract class representing file checksums for files. the checksum algorithm name the length of the checksum in bytes the value of the checksum in bytes return true if both the algorithms and the values are the same. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.FileContext "the filecontext class provides an interface to the application writer for using the hadoop file system. it provides a set of methods for the usual operation: create, open, list, etc   path names   the hadoop file system supports a uri name space and uri names. it offers a forest of file systems that can be referenced using fully qualified uris. two common hadoop file systems implementations are   the local file system: file:///path  the hdfs file system hdfs://nnaddress:nnport/path  while uri names are very flexible, it requires knowing the name or address of the server. for convenience one often wants to access the default system in one's environment without knowing its name/address. this has an additional benefit that it allows one to change one's default fs (e.g. admin moves application from cluster1 to cluster2).  to facilitate this, hadoop supports a notion of a default file system. the evolving for a release,to be changed to stable default permission for directory and symlink in previous versions, this default permission was also used to create files, so files default permission for directory default permission for file priority of the filecontext shutdown hook. list of files that should be deleted on jvm shutdown. jvm shutdown hook thread. the filecontext is defined by. 1) defaultfs (slash) 2) wd 3) umask init the wd. workingdir is implemented at the filecontext layer not at the abstractfilesystem layer. if the defaultfs, such as localfilesystem has a notion of builtin wd, we use that as the initial wd. otherwise the wd is initialized to the home directory. remove relative part - return "absolute": if input is relative path ("foo/bar") add wd: ie "//foo/bar" a fully qualified uri ("hdfs://nn:p/foo/bar") or a slash-relative path ("/foo/bar") are returned unchanged. applications that use filecontext should use #makequalified() since they really want a fully qualified uri. hence this method is not called makeabsolute() and has been deliberately declared private. delete all the paths that were marked as delete-on-exit. pathnames with scheme and relative path are illegal. to be checked get the file system of supplied path. - absolute or fully qualified path file system of the path @throws unsupportedfilesystemexception if the file system for absorfqpath is not supported. @throws ioexcepton if the file system for absorfqpath could not be instantiated. protected static factory methods for getting a filecontexts that take a abstractfilesystem as input. to be used for testing. create a filecontext with specified fs as default using the specified config. filecontext with specifed fs as default. create a filecontext for specified file system using the default config. filecontext with the specified abstractfilesystem as the default fs. static factory methods for getting a filecontext. note new file contexts are create a filecontext using the default config read from the $hadoop_config/core.xml, unspecified key-values for config are defaulted from core-defaults.xml in the release jar. @throws unsupportedfilesystemexception if the file system from the default configuration is not supported filecontext for the local file system using the default config. @throws unsupportedfilesystemexception if the file system for {@link fsconstants#local_fs_uri} is not supported. create a filecontext for specified uri using the default config. filecontext with the specified uri as the default fs. @throws unsupportedfilesystemexception if the file system for defaultfsuri is not supported create a filecontext for specified default uri using the specified config. filecontext for specified uri @throws unsupportedfilesystemexception if the file system with specified is not supported @throws runtimeexception if the file system specified is supported but could not be instantiated, or if login fails. create a filecontext using the passed config. generally it is better to use {@link #getfilecontext(uri, configuration)} instead of this one. filecontext @throws unsupportedfilesystemexception if file system in the config is not supported - from which the filecontext is configured filecontext for the local file system using the specified config. @throws unsupportedfilesystemexception if default file system in the config is not supported this method is needed for tests. return type will change to afs once hadoop-6223 is completed set the working directory for wd-relative names (such a "foo/bar"). working directory feature is provided by simply prefixing relative names with the working dir. note this is different from unix where the wd is actually set to the inode. hence setworkingdir does not follow symlinks etc. this works better in a distributed environment that has multiple independent roots. {@link #getworkingdirectory()} should return what setworkingdir() set. new working directory @throws ioexception  newwdir can be one of:  relative path: "foo/bar"; absolute without scheme: "/foo/bar" fully qualified with scheme: "xx://auth/foo/bar"   illegal wds:  relative with scheme: "xx:foo/bar" non existent directory  wd is stored as a fully qualified path. we check if the given path is not relative first since resolve requires and returns an absolute path. gets the working directory for wd-relative names (such a "foo/bar"). gets the ugi in the file-context return the current umask of this filecontext set umask to the supplied parameter. the new umask resolve the path following any symlinks or mount points to be resolved qualified resolved path @throws filenotfoundexception if f does not exist @throws accesscontrolexception if access denied @throws ioexception if an io error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server runtimeexceptions: @throws invalidpathexception if path f is not valid make the path fully qualified if it is isn't. a fully-qualified path has scheme and create or overwrite file on indicated path and returns an output stream for writing into the file. the file name to open gives the semantics of create; see {@link createflag} file creation options; see {@link options.createopts}.  progress - to report progress on the operation - default null permission - umask is applied against permisssion: default is fspermissions:getdefault() createparent - create missing parent path; default is to not to create parents the defaults for the following are ss defaults of the file server implementing the target path. not all parameters make sense for all kinds of file system - eg. localfs ignores blocksize, replication, checksum  buffersize - buffersize used in fsdataoutputstream blocksize - block size for file blocks replicationfactor - replication for blocks checksumparam - checksum parameters. server default is used if not specified.   {@link fsdataoutputstream} for make(create) a directory and all the non-existent parents. - the dir to make - permissions is set permission&~umask - if true then missing parent dirs are delete a file. the path to delete. if path is a directory and set to true, the directory is deleted else throws an exception. in case of a file the recursive can be set to either true or false. @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server runtimeexceptions: @throws invalidpathexception if path f is invalid opens an fsdatainputstream at the indicated path using default buffersize. the file name to open @throws accesscontrolexception if access is denied @throws filenotfoundexception if file f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server opens an fsdatainputstream at the indicated path. the file name to open the size of the buffer to be used. @throws accesscontrolexception if access is denied @throws filenotfoundexception if file f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server set replication for an existing file. file name new replication if successful @throws accesscontrolexception if access is denied @throws filenotfoundexception if file f does not exist @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server renames path src to path dst  fails if src is a file and dst is a directory. fails if src is a directory and dst is a file. fails if the parent of dst does not exist or is a file.   if overwrite option is not passed as an argument, rename fails if the dst already exists.  if overwrite option is passed as an argument, rename overwrites the dst if it is a file or an empty directory. rename fails if dst is a non-empty directory.  note that atomicity of rename is dependent on the file system implementation. please refer to the file system documentation for details  path to be renamed new path after rename @throws accesscontrolexception if access is denied @throws filealreadyexistsexception if dst already exists and options has {@link options.rename#overwrite} option false. @throws filenotfoundexception if src does not exist @throws parentnotdirectoryexception if parent of dst is not a directory @throws unsupportedfilesystemexception if file system for src and dst is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server we do not know whether the source or the destination path was unresolved. resolve the source path up until the final path component, then fully resolve the destination. set permission of a path. - the new absolute permission (umask is not applied) @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server set owner of a path (i.e. a file or a directory). the parameters set access time of a file. the path set the modification time of this file. the number of milliseconds since epoch (jan 1, 1970). a value of -1 means that this call should not set modification time. set the access time of this file. the number of milliseconds since jan 1, 1970. a value of -1 means that this call should not set access time. @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server get the checksum of a file. file path file checksum. the default return value is null, which indicates that no checksum algorithm is implemented in the corresponding filesystem. @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server set the verify checksum flag for the file system denoted by the path. this is only applicable if the corresponding filesystem supports checksum. by default doesn't do anything. set the verifychecksum for the filesystem containing this path @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server return a file status object that represents the path. the path we want information from filestatus object @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server return a fully qualified version of the given symlink target if it has no scheme and return a file status object that represents the path. if the path refers to a symlink then the filestatus of the symlink is returned. the behavior is equivalent to #getfilestatus() if the underlying file system does not support symbolic links. f the path we want information from. filestatus object @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred returns the target of the given symbolic link as it was specified when the link was return blocklocation of the given file for the given offset and len. for a nonexistent file or regions, null will be returned. this call is most helpful with dfs, where it returns hostnames of machines that contain the given file. - get blocklocations of this file position (byte offset) (in bytes) locations for given file at specified offset of len @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server runtimeexceptions: @throws invalidpathexception if path f is invalid returns a status object describing the use and capacity of the file system denoted by the parh argument p. if the file system has multiple partitions, the use and capacity of the partition pointed to by the specified path is reflected. path for which status should be obtained. null means the root partition of the default file system. fsstatus object @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server creates a symbolic link to an existing file. an exception is thrown if the symlink exits, the list the statuses of the files/directories in the given path if the path is a directory. is the path iterator that traverses statuses of the files/directories in the given path @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server iterator over the corrupt files under the given path (may contain duplicates if a file has more than one corrupt block) @throws ioexception list the statuses of the files/directories in the given path if the path is a directory. return the file's status and block locations if the path is a file. if a returned status is a file, it contains the file's block locations. is the path iterator that traverses statuses of the files/directories in the given path if any io exception (for example the input directory gets deleted while listing is being executed), next() or hasnext() of the returned iterator may throw a runtimeexception with the io exception as the cause. @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server mark a path to be deleted on jvm shutdown. the existing path to delete. true if deleteonexit is successful, otherwise false. @throws accesscontrolexception if access is denied @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server utility/library methods built over the basic filecontext methods. since this are library functions, the oprtation are not atomic and some of them may partially complete if other threads are making changes to the same part of the name space. does the file exist? note: avoid using this method if you already have filestatus in hand. instead reuse the filestatus the file or dir to be checked @throws accesscontrolexception if access is denied @throws ioexception if an i/o error occurred @throws unsupportedfilesystemexception if file system for f is not supported exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server return a list of file status objects that corresponds to supplied paths excluding those non-existent paths. list of paths we want information from list of filestatus objects @throws accesscontrolexception if access is denied @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server return the {@link contentsummary} of path f. path {@link contentsummary} of path f. @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server see {@link #liststatus(path[], pathfilter)} filter files/directories in the given path using the filter files/directories in the given list of paths using filter files/directories in the given path using the list the statuses of the files/directories in the given path if the path is a directory. is the path array that contains statuses of the files/directories in the given path @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server list the statuses and block locations of the files in the given path. if the path is a directory, if recursive is false, returns files in the directory; if recursive is true, return files in the subtree rooted at the path. the subtree is traversed in the depth-first order. if the path is a file, return the file's status and block locations. files across symbolic links are also returned. is the path if the subdirectories need to be traversed recursively iterator that traverses statuses of the files if any io exception (for example a sub-directory gets deleted while listing is being executed), next() or hasnext() of the returned iterator may throw a runtimeexception with the io exception as the cause. @throws accesscontrolexception if access is denied @throws filenotfoundexception if f does not exist @throws unsupportedfilesystemexception if file system for f is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server returns true if the iterator has more files. true if the iterator has more files. @throws accesscontrolexception if not allowed to access next file's status or locations @throws filenotfoundexception if next file does not exist any more @throws unsupportedfilesystemexception if next file's fs is unsupported @throws ioexception for all other io errors for example, namenode is not avaialbe or namenode throws ioexception due to an error while getting the status or block locations process the input stat. if it is a file, return the file stat. if it is a directory, traverse the directory if recursive is true; ignore it if recursive is false. if it is a symlink, resolve the symlink first and then process it depending on if it is a file or directory. input status @throws accesscontrolexception if access is denied @throws filenotfoundexception if file is not found @throws unsupportedfilesystemexception if fs is not supported @throws ioexception for all other io errors returns the next file's status with its block locations @throws accesscontrolexception if not allowed to access next file's status or locations @throws filenotfoundexception if next file does not exist any more @throws unsupportedfilesystemexception if next file's fs is unsupported @throws ioexception for all other io errors for example, namenode is not avaialbe or namenode throws ioexception due to an error while getting the status or block locations return all the files that match filepattern and are not checksum files. results are sorted by their names.  a filename pattern is composed of regular characters and special pattern matching characters, which are:       ?   matches any single character.      matches zero or more characters.    [abc]   matches a single character from character set {a,b,c}.    [a-b]   matches a single character from the character range {a...b}. note: character a must be lexicographically less than or equal to character b.    [^a]   matches a single char that is not from character set or range {a}. note that the ^ character must occur immediately to the right of the opening bracket.    \c   removes (escapes) any special meaning of character c.    {ab,cd}   matches a string from the string set {ab, cd}     {ab,c{de,fh}}   matches a string from string set {ab, cde, cfh}    a regular expression specifying a pth pattern array of paths that match the path pattern @throws accesscontrolexception if access is denied @throws unsupportedfilesystemexception if file system for pathpattern is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server return an array of filestatus objects whose path names match pathpattern and is accepted by the for all the inpathpattern - without the scheme & for a path of n components, return a list of paths that match the components [level, n-1]. copy file from src to dest. see {@link #copy(path, path, boolean, boolean)} copy from src to dst, optionally deleting src and overwriting dst. - delete src if true overwrite dst if true; throw ioexception if dst exists and overwrite is false. if copy is successful @throws accesscontrolexception if access is denied @throws filealreadyexistsexception if dst already exists @throws filenotfoundexception if src does not exist @throws parentnotdirectoryexception if parent of dst is not a directory @throws unsupportedfilesystemexception if file system for src or dst is not supported @throws ioexception if an i/o error occurred exceptions applicable to file systems accessed over rpc: @throws rpcclientexception if an exception occurred in the rpc client @throws rpcserverexception if an exception occurred in the rpc server @throws unexpectedserverexception if server implementation throws undeclared exception to rpc server runtimeexceptions: @throws invalidpathexception if path dst is invalid check if copying srcname to dst would overwrite an existing file or directory. file or directory to be copied. destination to copy srcname to. whether it's ok to overwrite an existing file. @throws accesscontrolexception if access is denied. @throws ioexception if dst is an existing directory, or dst is an existing file and the overwrite option is not passed. are qualsrc and qualdst of the same file system? 1 - fully qualified path 2 - fully qualified path @return deletes all the paths in deleteonexit on jvm shutdown. resolves all symbolic links in the specified path. returns the new path object. resolves all symbolic links in the specified path leading up to, but not including the final path component. path to resolve new path object. returns the list of abstractfilesystems accessed in the path. the list may contain more than one abstractfilesystems objects in case of symlinks. path which needs to be resolved of abstractfilesystems accessed in the path @throws ioexception class used to perform an operation on and resolve symlinks in a path. the operation may potentially span multiple file systems. generic helper function overridden on instantiation to perform a specific operation on the given file system using the given path which may result in an unresolvedlinkexception. abstractfilesystem to perform the operation on. path given the file system. type determined by the specific implementation. @throws unresolvedlinkexception if symbolic link path could not be resolved @throws ioexception an i/o error occured performs the operation specified by the next function, calling it repeatedly until all symlinks in the given path are resolved. filecontext used to access file systems. the path to resolve symlinks in. type determined by the implementation of next. @throws ioexception get the statistics for a particular file system the uri to lookup the statistics. only scheme and clears all the statistics stored in abstractfilesystem, for all the file systems. prints the statistics to standard output. file system is identified by the scheme and of uri and statistics for each filesystem instantiated. the uri consists of scheme and get delegation tokens for the file systems accessed for a given path. path for which delegations tokens are requested. the account name that is allowed to renew the token. of delegation tokens. @throws ioexception www.apache.org/licenses/license-2.0 /path nnaddress:nnport/path  uses the default config which has your default fs configx is not changed, is passed down default fs for this filecontext. fully qualified for the inner class nn:p/foo/bar") or a slash-relative path is it the default fs for this filecontext? it is different filesystem auth/foo/bar" if one of the options is a permission, extract it & apply umask if not, add a default perms and apply umask; abstractfilesystem#create nb: makequalified uses the target's scheme and specified, and the scheme and host/a/b/link hostx/a/b/file resolved according to the target file system. /a/b/file resolved according to the target file sytem. eg resolving /a results in an exception because /a will not since hadoop's local file systems host/a and path host/b/file host/a/b and path host/file ignoring file symbolic link resolve symbolic link directory path has only zero component path has at least one component path is absolute, first component is "/" hence first component is the uri root glob the paths that match the parent path, ie. [0, components.length-1] fix the pathes to be abs now work on the last component of the path last component has a pattern list parent directories and then glob the results last component does not have a pattern get all the path names get all their statuses decide if the pathpattern contains a glob or not recurse to check if dst/srcname exists. dst does not exist - ok to copy.  if the destination is a subdirectory of the source, then generate exception  the maximum number of symbolic link components in a path nb: more than one abstractfilesystem can match a scheme, eg "file" resolves to localfs but could have come by rawlocalfs. loop until all symlinks are resolved or the limit is reached resolve the first unresolved path component"
org.apache.hadoop.fs.FileStatus "interface that represents the client side information for a file. constructor for file systems on which symbolic links are not supported get the length of this file, in bytes. length of this file, in bytes. is this a file? if this is a file is this a directory? if this is a directory old interface, instead use the explicit {@link filestatus#isfile()}, {@link filestatus#isdirectory()}, and {@link filestatus#issymlink()} if this is a directory. @deprecated use {@link filestatus#isfile()}, {@link filestatus#isdirectory()}, and {@link filestatus#issymlink()} instead. is this a symbolic link? if this is a symbolic link get the block size of the file. number of bytes get the replication factor of a file. replication factor of a file. get the modification time of the file. modification time of file in milliseconds since january 1, 1970 utc. get the access time of the file. access time of file in milliseconds since january 1, 1970 utc. get fspermission associated with the file. . if a filesystem does not have a notion of permissions or if permissions could not be determined, then default permissions equivalent of "rwxrwxrwx" is returned. get the owner of the file. of the file. the string could be empty if there is no notion of owner of a file in a filesystem or if it could not be determined (rare). get the group associated with the file. for the file. the string could be empty if there is no notion of group of a file in a filesystem or if it could not be determined (rare). these are provided so that these values could be loaded lazily by a filesystem (e.g. local file system). sets permission. if permission is null, default value is set sets owner. if it is null, default value is set sets group. if it is null, default value is set contents of the symbolic link. compare this object to another object o the object to be compared. a negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object. @throws classcastexception if the specified object's is not of type filestatus compare if this object is equal to another object o the object to be compared. true if two file status has the same path name; false if not. returns a hash code value for the object, which is defined as the hash code of the path name. a hash code value for the path name. www.apache.org/licenses/license-2.0 we should deprecate this soon? the variables isdir and symlink indicate the type: 1. isdir implies directory, in which case symlink must be null. 2. !isdir implies a file or symlink, symlink != null implies a symlink, otherwise it's a file. //////////////////////////////////////////////// writable ////////////////////////////////////////////////"
org.apache.hadoop.fs.FileSystem "an abstract base class for a fairly generic filesystem. it may be implemented as a distributed filesystem, or as a "local" one that reflects the locally-connected disk. the local version exists for small hadoop instances and for testing.  all priority of the filesystem shutdown hook. filesystem cache the key this instance is stored under in the cache. recording statistics per a filesystem class the statistics for this file system. a cache of files that should be deleted when filsystem is closed or the jvm is exited. this method adds a file system for testing so that we can find it later. it is only for testing. the uri to store it under the configuration to store it under the file system to store @throws ioexception get a filesystem instance based on the uri, the passed configuration and the returns the configured filesystem implementation. the configuration to use get the default filesystem uri from a configuration. the configuration to use uri of the default filesystem set the default filesystem uri in a configuration. the configuration to alter the new default filesystem uri set the default filesystem uri in a configuration. the configuration to alter the new default filesystem uri called after a new filesystem instance is constructed. a uri whose return the protocol scheme for the filesystem.  this implementation throws an unsupportedoperationexception. protocol scheme for the filesystem. returns a uri whose scheme and resolve the uri's hostname and add the default port if not in the uri @see netutils#getcanonicaluri(uri, int) get the default port for this file system. default port or 0 if there isn't one get a canonical service name for this file system. the token cache is the only @deprecated call #geturi() instead. @deprecated call #get(uri,configuration) instead. update old-format filesystem names, for back-compatibility. this should eventually be replaced with a checkname() method that throws an exception for old-format names. get the local file system. the configuration to configure the file system with localfilesystem returns the filesystem for this uri's scheme and returns the filesystem for this uri's scheme and returns the filesystem for this uri's scheme and returns a unique configured filesystem implementation. this always returns a new filesystem object. the configuration to use get a unique local file system object the configuration to configure the file system with localfilesystem this always returns a new filesystem object. close all cached filesystems. be sure those filesystems are not used anymore. @throws ioexception close all cached filesystems for a given ugi. be sure those filesystems are not used anymore. make sure that a path specifies a filesystem. to use get a new delegation token for this file system. this is an internal method that should have been declared protected but wasn't historically. callers should use {@link #adddelegationtokens(string, credentials)} the account name that is allowed to renew the token. new delegation token @throws ioexception obtain all delegation tokens used by this filesystem that are not already present in the given credentials. existing tokens will neither be verified as valid nor having the given renewer. missing tokens will be acquired and added to the given credentials. default impl: works for simple fs with its own token and also for an embedded fs whose tokens are those of its children file system (i.e. the embedded fs has not tokens of its own). the recursively obtain the tokens for this filesystem and all descended filesystems as determined by getchildfilesystems(). the get all the immediate child filesystems embedded in this filesystem. it does not recurse and get grand children. if a filesystem has multiple child filesystems, then it should return a unique list of those filesystems. default is to return null to signify no children. used by this filesystem create a file with the provided permission the permission of the file is set to be the provided permission as in setpermission, not permission&~umask it is implemented using two rpcs. it is understood that it is inefficient, but the implementation is thread-safe. the other option is to change the value of umask in configuration to be 0, but it is not thread-safe. file system handle the name of the file to be create a directory with the provided permission the permission of the directory is set to be the provided permission as in setpermission, not permission&~umask @see #create(filesystem, path, fspermission) file system handle the name of the directory to be check that a path belongs to this filesystem. to check return an array containing hostnames, offset and size of portions of the given file. for a nonexistent file or regions, null will be returned. this call is most helpful with dfs, where it returns hostnames of machines that contain the given file. the filesystem will simply return an elt containing 'localhost'. filesstatus to get data from offset into the given file length for which to get locations for return an array containing hostnames, offset and size of portions of the given file. for a nonexistent file or regions, null will be returned. this call is most helpful with dfs, where it returns hostnames of machines that contain the given file. the filesystem will simply return an elt containing 'localhost'. path is used to identify an fs since an fs could have another fs that it could be delegating the call to offset into the given file length for which to get locations for return a set of server default configuration values default configuration values @throws ioexception @deprecated use {@link #getserverdefaults(path)} instead return a set of server default configuration values path is used to identify an fs since an fs could have another fs that it could be delegating the call to default configuration values @throws ioexception return the fully-qualified path of path f resolving the path through any symlinks or mount point path to be resolved qualified path @throws filenotfoundexception opens an fsdatainputstream at the indicated path. the file name to open the size of the buffer to be used. opens an fsdatainputstream at the indicated path. the file to open create an fsdataoutputstream at the indicated path. files are overwritten by default. the file to create create an fsdataoutputstream at the indicated path. the file to create if a file with this name already exists, then if true, the file will be overwritten, and if false an exception will be thrown. create an fsdataoutputstream at the indicated path with write-progress reporting. files are overwritten by default. the file to create to report progress create an fsdataoutputstream at the indicated path. files are overwritten by default. the file to create the replication factor create an fsdataoutputstream at the indicated path with write-progress reporting. files are overwritten by default. the file to create the replication factor to report progress create an fsdataoutputstream at the indicated path. the file name to create if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown. the size of the buffer to be used. create an fsdataoutputstream at the indicated path with write-progress reporting. the path of the file to open if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown. the size of the buffer to be used. create an fsdataoutputstream at the indicated path. the file name to open if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown. the size of the buffer to be used. required block replication for the file. create an fsdataoutputstream at the indicated path with write-progress reporting. the file name to open if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown. the size of the buffer to be used. required block replication for the file. create an fsdataoutputstream at the indicated path with write-progress reporting. the file name to open if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown. the size of the buffer to be used. required block replication for the file. @throws ioexception @see #setpermission(path, fspermission) create an fsdataoutputstream at the indicated path with write-progress reporting. the file name to open {@link createflag}s to use for this stream. the size of the buffer to be used. required block replication for the file. @throws ioexception @see #setpermission(path, fspermission) create an fsdataoutputstream at the indicated path with a custom checksum option the file name to open {@link createflag}s to use for this stream. the size of the buffer to be used. required block replication for the file. checksum parameter. if null, the values found in conf will be used. @throws ioexception @see #setpermission(path, fspermission) . this create has been added to support the filecontext that processes the permission with umask before calling this method. this a temporary method added to support the transition from filesystem to filecontext for this version of the mkdirs method assumes that the permission is absolute. it has been added to support the filecontext that processes the permission with umask before calling this method. this a temporary method added to support the transition from filesystem to filecontext for this version of the mkdirs method assumes that the permission is absolute. it has been added to support the filecontext that processes the permission with umask before calling this method. this a temporary method added to support the transition from filesystem to filecontext for opens an fsdataoutputstream at the indicated path with write-progress reporting. same as create(), except fails if parent directory doesn't already exist. the file name to open if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown. the size of the buffer to be used. required block replication for the file. @throws ioexception @see #setpermission(path, fspermission) @deprecated api only for 0.20-append opens an fsdataoutputstream at the indicated path with write-progress reporting. same as create(), except fails if parent directory doesn't already exist. the file name to open if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown. the size of the buffer to be used. required block replication for the file. @throws ioexception @see #setpermission(path, fspermission) @deprecated api only for 0.20-append opens an fsdataoutputstream at the indicated path with write-progress reporting. same as create(), except fails if parent directory doesn't already exist. the file name to open {@link createflag}s to use for this stream. the size of the buffer to be used. required block replication for the file. @throws ioexception @see #setpermission(path, fspermission) @deprecated api only for 0.20-append creates the given path as a brand-new zero-length file. if create fails, or if it already existed, return false. path to use for create append to an existing file (optional operation). same as append(f, getconf().getint("io.file.buffer.size", 4096), null) the existing file to be appended. @throws ioexception append to an existing file (optional operation). same as append(f, buffersize, null). the existing file to be appended. the size of the buffer to be used. @throws ioexception append to an existing file (optional operation). the existing file to be appended. the size of the buffer to be used. for reporting progress if it is not null. @throws ioexception concat existing files together. the path to the target destination. the paths to the sources to use for the concatenation. @throws ioexception get replication. @deprecated use getfilestatus() instead file name replication @throws ioexception set replication for an existing file. file name new replication @throws ioexception if successful; false if file does not exist or is a directory renames path src to path dst. can take place on local fs or remote dfs. path to be renamed new path after rename @throws ioexception on failure if rename is successful renames path src to path dst  fails if src is a file and dst is a directory. fails if src is a directory and dst is a file. fails if the parent of dst does not exist or is a file.   if overwrite option is not passed as an argument, rename fails if the dst already exists.  if overwrite option is passed as an argument, rename overwrites the dst if it is a file or an empty directory. rename fails if dst is a non-empty directory.  note that atomicity of rename is dependent on the file system implementation. please refer to the file system documentation for details. this default implementation is non atomic.  this method is deprecated since it is a temporary method added to support the transition from filesystem to filecontext for delete a file @deprecated use {@link #delete(path, boolean)} instead. delete a file. the path to delete. if path is a directory and set to true, the directory is deleted else throws an exception. in case of a file the recursive can be set to either true or false. true if delete is successful else false. @throws ioexception mark a path to be deleted when filesystem is closed. when the jvm shuts down, all filesystem objects will be closed automatically. then, the marked path will be deleted as a result of closing the filesystem. the path has to exist in the file system. the path to delete. true if deleteonexit is successful, otherwise false. @throws ioexception cancel the deletion of the path when the filesystem is closed the path to cancel deletion delete all files that were marked as delete-on-exit. this recursively deletes all files in the specified paths. check if exists. source file true iff the named path is a directory. note: avoid using this method. instead reuse the filestatus returned by getfilestatus() or liststatus() methods. path to check true iff the named path is a regular file. note: avoid using this method. instead reuse the filestatus returned by getfilestatus() or liststatus() methods. path to check the number of bytes in a file. @deprecated use getfilestatus() instead return the {@link contentsummary} of a given {@link path}. path to use list the statuses of the files/directories in the given path if the path is a directory. given path statuses of the files/directories in the given patch @throws filenotfoundexception when the path does not exist; ioexception see specific implementation filter files/directories in the given path using the iterator over the corrupt files under the given path (may contain duplicates if a file has more than one corrupt block) @throws ioexception filter files/directories in the given path using the filter files/directories in the given list of paths using default path filter. a list of paths list of statuses for the files under the given paths after applying the filter default path filter @throws filenotfoundexception when the path does not exist; ioexception see specific implementation filter files/directories in the given list of paths using return all the files that match filepattern and are not checksum files. results are sorted by their names.  a filename pattern is composed of regular characters and special pattern matching characters, which are:       ?   matches any single character.      matches zero or more characters.    [abc]   matches a single character from character set {a,b,c}.    [a-b]   matches a single character from the character range {a...b}. note that character a must be lexicographically less than or equal to character b.    [^a]   matches a single character that is not from character set or range {a}. note that the ^ character must occur immediately to the right of the opening bracket.    \c   removes (escapes) any special meaning of character c.    {ab,cd}   matches a string from the string set {ab, cd}     {ab,c{de,fh}}   matches a string from the string set {ab, cde, cfh}    a regular expression specifying a pth pattern array of paths that match the path pattern @throws ioexception return an array of filestatus objects whose path names match pathpattern and is accepted by the the glob filter builds a regexp per path component. if the component does not contain a shell metachar, then it falls back to appending the raw string to the list of built up paths. this raw path needs to have the quoting removed. ie. convert all occurances of "\x" to "x" of the path component unquoted path component list the statuses of the files/directories in the given path if the path is a directory. return the file's status and block locations if the path is a file. if a returned status is a file, it contains the file's block locations. is the path iterator that traverses statuses of the files/directories in the given path @throws filenotfoundexception if f does not exist @throws ioexception if an i/o error occurred listing a directory the returned results include its block location if it is a file the results are filtered by the given path filter a path a path filter iterator that traverses statuses of the files/directories in the given path @throws filenotfoundexception if f does not exist @throws ioexception if any i/o error occurred list the statuses and block locations of the files in the given path. if the path is a directory, if recursive is false, returns files in the directory; if recursive is true, return files in the subtree rooted at the path. if the path is a file, return the file's status and block locations. is the path if the subdirectories need to be traversed recursively iterator that traverses statuses of the files @throws filenotfoundexception when the path does not exist; ioexception see specific implementation process the input stat. if it is a file, return the file stat. if it is a directory, traverse the directory if recursive is true; ignore it if recursive is false. input status @throws ioexception if any io error occurs return the current set the current working directory for the given file system. all relative paths will be resolved relative to it. _dir get the current working directory for the given file system directory pathname note: with the new filescontext class, getworkingdirectory() will be removed. the working directory is implemented in filescontext. some file systems like localfilesystem have an initial workingdir that we use as the starting workingdir. for other file systems like hdfs there is no built in notion of an inital workingdir. there is built in notion of workingdir then it is returned; else a null is returned. call {@link #mkdirs(path, fspermission)} with default permission. make the given file and all non-existent parents into directories. has the semantics of unix 'mkdir -p'. existence of the directory hierarchy is not an error. path to create to apply to f the src file is on the local disk. add it to fs at the given dst name and the source is kept intact afterwards path path the src files is on the local disk. add it to fs at the given dst name, removing the source afterwards. path path the src file is on the local disk. add it to fs at the given dst name, removing the source afterwards. path path the src file is on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed whether to delete the src path path the src files are on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed whether to delete the src whether to overwrite an existing file array of paths which are source path the src file is on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed whether to delete the src whether to overwrite an existing file path path the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. path path the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. remove the source afterwards path path the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. delsrc indicates if the src will be removed or not. whether to delete the src path path the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. delsrc indicates if the src will be removed or not. returns a local file that the called when we're all done writing to the target. a local fs will do nothing, because we've written to exactly the right place. a remote fs will copy the contents of tmplocalfile to the correct target at fsoutputfile. path of output file path to local tmp file no more filesystem operations are needed. will release any held locks. return the total size of all files in the filesystem. get the block size for a particular file. the filename number of bytes in a block @deprecated use getfilestatus() instead return the number of bytes that large input files should be optimally be split into to minimize i/o time. @deprecated use {@link #getdefaultblocksize(path)} instead return the number of bytes that large input files should be optimally be split into to minimize i/o time. the given path will be used to locate the actual filesystem. the full path does not have to exist. path of file default block size for the path's filesystem get the default replication. @deprecated use {@link #getdefaultreplication(path)} instead get the default replication for a path. the given path will be used to locate the actual filesystem. the full path does not have to exist. of the file replication for the path's filesystem return a file status object that represents the path. the path we want information from filestatus object @throws filenotfoundexception when the path does not exist; ioexception see specific implementation get the checksum of a file. the file path file checksum. the default return value is null, which indicates that no checksum algorithm is implemented in the corresponding filesystem. set the verify checksum flag. this is only applicable if the corresponding filesystem supports checksum. by default doesn't do anything. set the write checksum flag. this is only applicable if the corresponding filesystem supports checksum. by default doesn't do anything. returns a status object describing the use and capacity of the file system. if the file system has multiple partitions, the use and capacity of the root partition is reflected. fsstatus object @throws ioexception see specific implementation returns a status object describing the use and capacity of the file system. if the file system has multiple partitions, the use and capacity of the partition pointed to by the specified path is reflected. path for which status should be obtained. null means the default partition. fsstatus object @throws ioexception see specific implementation set permission of a path. set owner of a path (i.e. a file or a directory). the parameters set access time of a file the path set the modification time of this file. the number of milliseconds since jan 1, 1970. a value of -1 means that this call should not set modification time. set the access time of this file. the number of milliseconds since jan 1, 1970. a value of -1 means that this call should not set access time. caching filesystem objects a variable that makes all objects in the cache unique the objects inserted into the cache using this method are all unique close all filesystem instances in the cache. only close those that are marked for automatic closing filesystem.cache.key copy constructor. the input statistics object which is cloned. increment the bytes read in the statistics the additional bytes read increment the bytes written in the statistics the additional bytes written increment the number of read operations number of read operations increment the number of large read operations number of large read operations increment the number of write operations number of write operations get the total number of bytes read number of bytes get the total number of bytes written number of bytes get the number of file system read operations such as list files of read operations get the number of large file system read operations such as list files under a large directory of large read operations get the number of file system write operations such as create, append rename etc. of write operations reset the counts of bytes to 0. get the uri scheme associated with this statistics object. schema associated with this set of statistics get the map of statistics object indexed by uri scheme. map having a key as uri scheme and value as statistics object @deprecated use {@link #getallstatistics} instead return the filesystem classes that have statistics get the statistics for a particular file system the class to lookup statistics object reset all statistics for all file systems print all statistics for all file systems www.apache.org/licenses/license-2.0 convert old-format name to new-format name "local" is now "file:///". /\" instead."); /"; unqualified is "hdfs://" "+name+"/\" instead."); "+name; use default fs no if scheme matches default & default has return default no scheme: use default fs no if scheme matches default & default has return default collect token of the this filesystem and then of its embedded children fs has token, grab it now collect the tokens from the children create the file with default permission set its permission to the supplied one create the directory using the default permission set its permission to be the supplied one ///////////////////////////////////////////////////////////// filesystem ///////////////////////////////////////////////////////////// fs is relative  schemes match path's fs has an schemes match, so use this uri instead can't determine auth of the path canonicalize uri before comparing with this fs  crc32 is chosen as default as it is available in all releases that support checksum. the client trash configuration is ignored. checksum options are ignored by default. the file systems that implement checksum need to override this method. the full support is currently only available in dfs. default impl assumes that permissions do not matter and nor does the bytesperchecksum hence calling the regular create is good enough. fss that implement permissions should override this. default impl is to assume that permissions do not matter and hence calling the regular mkdirs is good enough. fss that implement permissions should override this. parent must exist. since the this.mkdirs makes parent dirs automatically we must throw exception if parent does not exist. parent does exist - go ahead with mkdir of leaf default impl is to assume that permissions do not matter and hence calling the regular mkdirs is good enough. fss that implement permissions should override this. default implementation delete the destination that is a file or an empty directory f does not exist f does not exist f is a file f is a directory no matches with multiple expansions is a non-matching glob sort gripes because filestatus comparable isn't parameterized... pathpattern has any globs determine starting point need to skip empty item at beginning of split list parse components and determine if it's a glob short through to filter check seed the parent directory path, return if it doesn't exist skip if there are no components other than the basedir iterate through each path component don't look for children in a file matched by a glob get all children matching the filter the component does not have a pattern don't care remove anything that didn't match the filter no final paths, if there were any globs return empty list file directory delete all files that were marked as delete-on-exit. default to 32mb: large enough to minimize the impact of seeks doesn't do anything doesn't do anything making it volatile to be able to do a double checked locking refetch the lock again a file system is close the new file system return the old file system now insert the new file system into the map make a copy of the keys in the map since we'll be modifying the map while iterating over it, which isn't safe. remove from cache make a pass over the list and collect the filesystems to close we cannot close inline since close() removes the entry from the map now make a pass over the target list and close each an artificial way to make a key unique " +"
org.apache.hadoop.fs.FileUtil "a collection of file-processing util methods convert an array of filestatus to an array of path an array of filestatus objects array of paths corresponding to the input convert an array of filestatus to an array of path. if stats if null, return path an array of filestatus objects default path to return in stats is null array of paths corresponding to the input delete a directory and all its contents. if we return false, the directory may be partially-deleted. (1) if dir is symlink to a file, the symlink is deleted. the file pointed to by the symlink is not deleted. (2) if dir is symlink to a directory, symlink is deleted. the directory pointed to by symlink is not deleted. (3) if dir is a normal file, it is deleted. (4) if dir is a normal directory, then dir and all its contents recursively are deleted. delete a directory and all its contents. if we return false, the directory may be partially-deleted. (1) if dir is symlink to a file, the symlink is deleted. the file pointed to by the symlink is not deleted. (2) if dir is symlink to a directory, symlink is deleted. the directory pointed to by symlink is not deleted. (3) if dir is a normal file, it is deleted. (4) if dir is a normal directory, then dir and all its contents recursively are deleted. the file or directory to be deleted true if permissions should be modified to delete a file. on success false on failure. pure-java implementation of "chmod +rwx f". delete the contents of a directory, not the directory itself. if we return false, the directory may be partially-deleted. if dir is a symlink to a directory, all the contents of the actual directory pointed to by dir will be deleted. delete the contents of a directory, not the directory itself. if we return false, the directory may be partially-deleted. if dir is a symlink to a directory, all the contents of the actual directory pointed to by dir will be deleted. if 'true', try grant +rwx permissions to this and all the underlying directories before trying to delete their contents. recursively delete a directory. {@link filesystem} on which the path is present directory to recursively delete @throws ioexception @deprecated use {@link filesystem#delete(path, boolean)} copy files between filesystems. copy files between filesystems. copy files between filesystems. copy all files in a directory to one output file (merge). copy local files to a filesystem. copy filesystem files to local files. copy filesystem files to local files. this class is only used on windows to invoke the cygpath command. convert a os-native filename to a path that works for the shell. the filename to convert unix pathname @throws ioexception on windows, there can be problems with the subprocess convert a os-native filename to a path that works for the shell. the filename to convert unix pathname @throws ioexception on windows, there can be problems with the subprocess convert a os-native filename to a path that works for the shell. the filename to convert whether to make canonical path for the file passed unix pathname @throws ioexception on windows, there can be problems with the subprocess takes an input dir and returns the du on that local directory. very basic implementation. the input dir to get the disk space of this local dir total disk space of the input local directory given a file input it will unzip the file in a the unzip directory passed as the second parameter the zip file as input the unzip directory where to unzip the zip file. @throws ioexception given a tar file as input it will untar the file in a the untar directory passed as the second parameter this utility will untar ".tar" files and ".tar.gz","tgz" files. the tar file as input. the untar directory where to untar the tar file. @throws ioexception class for creating hardlinks. supports unix, cygwin, windxp. @deprecated use {@link org.apache.hadoop.fs.hardlink} create a soft link between a src and destination only on a local disk. hdfs does not support this the target for symlink the symlink returned by the command change the permissions on a filename. the name of the file to change the permission string exit code from the command @throws ioexception @throws interruptedexception change the permissions on a file / directory, recursively, if needed. name of the file whose permissions are to change permission string true, if permissions should be changed recursively exit code from the command. @throws ioexception @throws interruptedexception create a tmp file for a base file. the base file of the tmp file name prefix of tmp if true, the tmp will be deleted when the vm exits newly move the src file to the name specified by target. the source file the target file @exception ioexception if this operation fails renameto() has two limitations on windows platform. src.renameto(target) fails if 1) if target already exists or 2) if target is already open for reading/writing. a wrapper for {@link file#listfiles()}. this java.io api returns null when a dir is not a directory or for any i/o error. instead of having null check everywhere file#listfiles() is used, we will add utility api to get around this problem. for the majority of cases where we prefer an ioexception to be thrown. directory for which listing should be performed of files or empty list @exception ioexception for invalid directory or for a bad disk. a wrapper for {@link file#list()}. this java.io api returns null when a dir is not a directory or for any i/o error. instead of having null check everywhere file#list() is used, we will add utility api to get around this problem. for the majority of cases where we prefer an ioexception to be thrown. directory for which listing should be performed of file names or empty string list @exception ioexception for invalid directory or for a bad disk. www.apache.org/licenses/license-2.0 try to chmod +rwx the parent folder of the 'dir': dir is (a) normal file, (b) symlink to a file, (c) empty directory or (d) symlink to a directory handle nonempty directory deletion to be able to list the dir and delete files from it we must grant the dir rwx permissions: normal file or symlink to another file continue deletion of other files/dirs under dir either directory or symlink to another directory. try deleting the directory as this might be a symlink this was indeed a symlink or an empty directory if not an empty directory or symlink let fullydelete handle it. continue deletion of other files/dirs under dir  if the destination is a subdirectory of the source, then generate exception  check if dest is directory this is a stub to assist with coordinated change between common and hdfs projects. it will be removed after the corresponding change is committed to hdfs. do nothing as of yet"
org.apache.hadoop.fs.FilterFileSystem "a filterfilesystem contains some other file system, which it uses as its basic file system, possibly transforming the data along the way or providing additional functionality. the class filterfilesystem itself simply overrides all methods of filesystem with versions that pass all requests to the contained file system. subclasses of filterfilesystem may further override some of these methods and may also provide additional methods and fields. so that extending classes can define it get the raw file system being filtered called after a new filesystem instance is constructed. a uri whose returns a uri whose scheme and returns a qualified uri whose scheme and make sure that a path specifies a filesystem. check that a path belongs to this filesystem. opens an fsdatainputstream at the indicated path. the file name to open the size of the buffer to be used. set replication for an existing file. file name new replication @throws ioexception if successful; false if file does not exist or is a directory renames path src to path dst. can take place on local fs or remote dfs. delete a file list files in a directory. list files and its block locations in a directory. set the current working directory for the given file system. all relative paths will be resolved relative to it. get the current working directory for the given file system directory pathname the src file is on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed the src files are on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed the src file is on the local disk. add it to fs at the given dst name. delsrc indicates if the source should be removed the src file is under fs, and the dst is on the local disk. copy it from fs control to the local dst name. delsrc indicates if the src will be removed or not. returns a local file that the called when we're all done writing to the target. a local fs will do nothing, because we've written to exactly the right place. a remote fs will copy the contents of tmplocalfile to the correct target at fsoutputfile. return the total size of all files in the filesystem. get file status. www.apache.org/licenses/license-2.0 this is less than ideal, but existing filesystems sometimes neglect to initialize the embedded filesystem swap in our scheme if the filtered fs is using a different scheme note: should deal with ///////////////////////////////////////////////////////////// filesystem ///////////////////////////////////////////////////////////// path variants delegate to underlying filesystem filesystem"
org.apache.hadoop.fs.FilterFs "a filterfs contains some other file system, which it uses as its basic file system, possibly transforming the data along the way or providing additional functionality. the class filterfs itself simply overrides all methods of abstractfilesystem with versions that pass all requests to the contained file system. subclasses of filterfs may further override some of these methods and may also provide additional methods and fields. evolving for a release,to be changed to stable www.apache.org/licenses/license-2.0 abstractfilesystem abstractfilesystem"
org.apache.hadoop.fs.FsConstants "filesystem related constants. viewfs: viewfs file system (ie the mount file system on client side) www.apache.org/licenses/license-2.0 uri for local filesystem /"); uri scheme for ftp /");"
org.apache.hadoop.fs.FSDataInputStream "utility that wraps a {@link fsinputstream} in a {@link datainputstream} and buffers input through a {@link bufferedinputstream}. seek to the given offset. offset to seek to get the current position in the input stream. position in the input stream read bytes from the given position in the stream to the given buffer. position in the input stream to seek buffer into which data is read offset into the buffer in which data is written maximum number of bytes to read number of bytes read into the buffer, or -1 if there is no more data because the end of the stream has been reached read bytes from the given position in the stream to the given buffer. continues to read until length bytes have been read. position in the input stream to seek buffer into which data is read offset into the buffer in which data is written the number of bytes to read @throws eofexception if the end of stream is reached while reading. if an exception is thrown an undetermined number of bytes in the buffer may have been written. see {@link #readfully(long, byte[], int, int)}. seek to the given position on an alternate copy of the data. targetpos position to seek to if a new source is found, false otherwise get a reference to the wrapped input stream. used by unit tests. underlying input stream www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.FSDataOutputStream "utility that wraps a {@link outputstream} in a {@link dataoutputstream}, buffers output through a {@link bufferedoutputstream} and creates a checksum file. get the current position in the output stream. current position in the output stream close the underlying output stream. get a reference to the wrapped output stream. used by unit tests. underlying output stream www.apache.org/licenses/license-2.0 update position return cached position this invokes positioncache.close() syncable syncable syncable"
org.apache.hadoop.fs.FSError "thrown for unexpected filesystem errors, presumed to reflect disk errors in the native filesystem. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.FSInputChecker "this is a generic input stream for verifying checksums for data before it is read by a the file name from which data is read from constructor the name of the file to be read number of read retries when checksumerror occurs constructor the name of the file to be read number of read retries when checksumerror occurs the type of checksum engine maximun chunk size the number byte of each checksum reads in checksum chunks into buf at offset and checksum into checksum. since checksums can be disabled, there are two cases implementors need to worry about: (a) needchecksum() will return false: - len can be any positive value - checksum will be null implementors should simply pass through to the underlying data stream. or (b) needchecksum() will return true: - len >= maxchunksize - checksum.length is a multiple of checksum_size implementors should read an integer number of data chunks into buf. the amount read should be bounded by len or by checksum.length / checksum_size maxchunksize. note that len may be a value that is not a multiple of maxchunksize, in which case the implementation may return less than len. the method is used for implementing read, therefore, it should be optimized for sequential reading. chunkpos desitination buffer offset in buf at which to store data maximum number of bytes to read the data buffer into which to write checksums of bytes read return position of beginning of chunk containing pos. a postion in the file starting position of the chunk which contains the byte return true if there is a need for checksum verification read one checksum-verified byte the next byte of data, or -1 if the end of the stream is reached. @exception ioexception if an i/o error occurs. read checksum verified bytes from this byte-input stream into the specified byte array, starting at the given offset.  this method implements the general contract of the corresponding {@link inputstream#read(byte[], int, int) read} method of the {@link inputstream} class. as an additional convenience, it attempts to read as many bytes as possible by repeatedly invoking the read method of the underlying stream. this iterated read continues until one of the following conditions becomes true:   the specified number of bytes have been read,  the read method of the underlying stream returns -1, indicating end-of-file.  if the first read on the underlying stream returns -1 to indicate end-of-file then this method returns -1. otherwise this method returns the number of bytes actually read. b destination buffer. off offset at which to start storing bytes. len maximum number of bytes to read. the number of bytes read, or -1 if the end of the stream has been reached. @exception ioexception if an i/o error occurs. checksumexception if any checksum error occurs fills the buffer with a chunk data. no mark is supported. this method assumes that all data in the buffer has already been read in, hence pos > count. read characters into a portion of an array, reading from the underlying stream at most once if necessary. read up one or more checksum chunk to array b at pos off it requires at least one checksum chunk boundary in between  and it stops reading at the last boundary or at the end of the stream; otherwise an illegalargumentexception is thrown. this makes sure that all data read are checksum verified. the buffer into which the data is read. the start offset in array b at which the data is written. the maximum number of bytes to read. the total number of bytes read into the buffer, or -1 if there is no more data because the end of the stream has been reached. @throws ioexception if an i/o error occurs. convert a checksum byte array to a long this is deprecated since 0.22 since it is no longer in use by this class. skips over and discards n bytes of data from the input stream. this method may skip more bytes than are remaining in the backing file. this produces no exception and the number of bytes skipped may include some number of bytes that were beyond the eof of the backing file. attempting to read from the stream after skipping past the end will result in -1 indicating the end of the file. if n is negative, no bytes are skipped. n the number of bytes to be skipped. the actual number of bytes skipped. @exception ioexception if an i/o error occurs. checksumexception if the chunk to skip to is corrupted seek to the given position in the stream. the next read() will be from that position. this method may seek past the end of the file. this produces no exception and an attempt to read from the stream will result in -1 indicating the end of the file. pos the postion to seek to. @exception ioexception if an i/o error occurs. checksumexception if the chunk to seek to is corrupted a utility function that tries to read up to len bytes from stm an input stream destiniation buffer offset at which to store data number of bytes to read number of bytes read @throws ioexception if there is any io error set the checksum related parameters whether to verify checksum which type of checksum to use maximun chunk size checksum size reset this fsinputchecker's state www.apache.org/licenses/license-2.0 data bytes for checksum (eg 512) buffer for non-chunk-aligned reading wrapper on checksum buffer the position of the reader inside buf the number of bytes currently in buf cached file position this should always be a multiple of maxchunksize number of checksum chunks that can be read at once into a buffer. chosen by benchmarks - higher values do not reduce cpu usage. the size of the data reads made to the underlying stream will be chunks_per_read maxchunksize. 32-bit checksum parameter check fill internal buffer read a chunk to read a chunk into the local buffer copy content of the local buffer to the invalidate buffer try a new replica since at least one of the sources is different, the read might succeed, so we'll retry. neither the data stream nor the checksum stream are being read from different sources, meaning we'll still get a checksum error if we try to do the read again. we throw an exception instead. optimize: check if the pos is in the buffer reset the current state seek to a checksum boundary scan to the desired position the code makes assumptions that checksums are always 32-bit. the size of the checksum array here determines how much we can read in a single call to readchunk invalidate buffer reset checksum"
org.apache.hadoop.fs.FSInputStream "fsinputstream is a generic old inputstream with a little bit of raf-style seek ability. seek to the given offset from the start of the file. the next read() will be from that location. can't seek past the end of the file. return the current offset from the start of the file seeks a different copy of the data. returns true if found a new source, false otherwise. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.FSOutputSummer "this is a generic output stream for generating checksums for data before it is written to the underlying stream write the data chunk in b staring at offset with a length of len, and its checksum write one byte writes len bytes from the specified byte array starting at offset off and generate a checksum for each data chunk.  this method stores bytes from the given array into this stream's buffer before it gets checksumed. the buffer gets checksumed and flushed to the underlying output stream when all data in a checksum chunk are in the buffer. if the buffer is empty and requested length is at least as large as the size of next checksum chunk size, this method will checksum and write the chunk directly to the underlying output stream. thus it avoids uneccessary data copy. b the data. off the start offset in the data. len the number of bytes to write. @exception ioexception if an i/o error occurs. write a portion of an array, flushing to the underlying stream at most once if necessary. forces any buffered output bytes to be checksumed and written out to the underlying output stream. forces any buffered output bytes to be checksumed and written out to the underlying output stream. if keep is true, then the state of this object remains intact. return the number of valid bytes currently in the buffer. generate checksum for the data chunk and output data chunk & checksum to the underlying output stream. if keep is true then keep the current checksum intact, do not reset it. converts a checksum integer value to a byte stream resets existing buffer with a new one of the specified size. www.apache.org/licenses/license-2.0 data checksum internal buffer for storing data before it is checksumed internal buffer for storing checksum the number of valid bytes in the buffer. local buffer is empty and checksum and output data copy local buffer is full"
org.apache.hadoop.fs.FsServerDefaults "provides server default configuration values to clients. www.apache.org/licenses/license-2.0 register a ctor ///////////////////////////////////////// writable /////////////////////////////////////////"
org.apache.hadoop.fs.FsShell "provide command line access to a filesystem. default ctor with no configuration. be sure to invoke {@link #setconf(configuration)} with a valid configuration prior to running commands. construct a fsshell with the given configuration. commands can be executed via {@link #run(string[])} the hadoop configuration returns the trash object associated with this shell. to the trash @throws ioexception upon error display help for commands with their short usage and long description displays short usage of commands sans the long description the following are helper methods for getinfo(). they are defined outside of the scope of the help/usage class because the run() method needs to invoke them too. run performs any necessary cleanup @throws ioexception upon error main() has some simple utility methods the command and its arguments @throws exception upon error the default ctor signals that the command being executed does not exist, while other ctor signals that a specific command does not exist. the latter is used by commands that process other commands, ex. -usage/-help www.apache.org/licenses/license-2.0 todo: dfsadmin subclasses fsshell so need to protect the command registration. this class should morph into a base class for commands, and then this method can be abstract note: usage/help are inner classes to allow access to outer methods that access commandfactory print all usages print one usage print all helps print one help display help or usage for one command display help or usage for all commands display list of short usages display long descriptions for each command todo: will eventually auto-wrap the text, but this matches the expected output for the hdfs tests... initialize fsshell instance.run catches ioe, so something is really wrong if here todo: this should be abstract in a base class"
org.apache.hadoop.fs.FsShellPermissions "this class is the home for file permissions related commands. moved to this separate class since fsshell is getting too large. register the permission related commands with the factory the command factory the pattern is almost as flexible as mode allowed by chmod shell command. the main restriction is that we recognize only rwxxt. to reduce errors we also enforce octal mode specifications of either 3 digits without a sticky bit setting or four digits with a sticky bit setting. used to change owner and/or group of files parse the first argument into an owner and group string describing new ownership used to change group of files www.apache.org/licenses/license-2.0 todo: remove "chmod : " so it's not doubled up in output, but it's here for backwards compatibility... used by chown/chgrp /allows only "allowedchars" above in names for owner and group should we do case insensitive match?"
org.apache.hadoop.fs.FsStatus "this class is used to represent the capacity, free and used space on a {@link filesystem}. construct a fsstatus object, using the specified statistics return the capacity in bytes of the file system return the number of bytes used on the file system return the number of remaining bytes on the file system www.apache.org/licenses/license-2.0 //////////////////////////////////////////////// writable ////////////////////////////////////////////////"
org.apache.hadoop.fs.FsUrlConnection "representation of a url connection to open inputstreams. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.FsUrlStreamHandler "urlstream handler relying on filesystem and on a given configuration to handle url protocols. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.FsUrlStreamHandlerFactory "factory for url stream handlers. there is only one handler whose job is to create urlconnections. a fsurlconnection relies on filesystem to choose the appropriate fs implementation. before returning our handler, we make sure that filesystem knows an implementation for the requested scheme/protocol. www.apache.org/licenses/license-2.0 the configuration holds supported fs implementation class names. this map stores whether a protocol is know or not by filesystem the url stream handler force init of filesystem code to avoid hadoop-9041 filesystem does not know the protocol, let the vm handle this"
org.apache.hadoop.fs.ftp.FtpConfigKeys "this class contains constants for configuration keys used in the ftp file system. note that the settings for unimplemented features are ignored. e.g. checksum related settings are just place holders. even when wrapped with {@link checksumfilesystem}, these settings are not used. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.ftp.FTPException "a class to wrap a {@link throwable} into a runtime exception. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.ftp.FTPFileSystem " a {@link filesystem} backed by an ftp client provided by apache commons net.  return the protocol scheme for the filesystem.  ftp connect to the ftp server using configuration parameters ftpclient instance @throws ioexception logout and disconnect the given ftpclient. @throws ioexception resolve against given working directory. @return a stream obtained via this call must be closed before using other apis of this class or else the invocation will block. this optional operation is not yet supported. convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of opening/closing a tcp connection. @deprecated use delete(path, boolean) instead convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of opening/closing a tcp connection. convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of opening/closing a tcp connection. convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of opening/closing a tcp connection. convert the file information in ftpfile to a {@link filestatus} object. convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of opening/closing a tcp connection. convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of opening/closing a tcp connection. assuming that parent of both source and destination is the same. is the assumption correct or it is suppose to work like 'move' ? convenience method, so that we don't open a new connection when using this method from within another method. otherwise every api invocation incurs the overhead of opening/closing a tcp connection. @return @throws ioexception www.apache.org/licenses/license-2.0 commons.apache.org/net/">apache commons net. get get host information from uri (overrides info in conf) get port information from uri, (overrides info in conf) get change to parent directory on the server. only then can we read the file on the server by opening up an inputstream. as a side effect the working directory on the server is changed to the parent directory of the file. the ftp client connection is closed when close() is called on the fsdatainputstream. the ftpclient is an inconsistent state. must close the stream which in turn will logout and disconnect from ftp server change to parent directory on the server. only then can we write to the file on the server by opening up an outputstream. as a side effect the working directory on the server is changed to the parent directory of the file. the ftp client connection is closed when close() is called on the fsdataoutputstream. the ftpclient is an inconsistent state. must close the stream which in turn will logout and disconnect from ftp server root dir length of root dir on server not known block size not known. modification time of root dir not known. file found in dir using default block size since there is no way in ftp client to know of block sizes on server. the assumption could be less than ideal. file does not exist return home directory always since we do not maintain state. we do not maintain the working directory state"
org.apache.hadoop.fs.ftp.FtpFs "the ftpfs implementation of abstractfilesystem. this impl delegates to the old filesystem evolving for a release,to be changed to stable this constructor has the signature needed by {@link abstractfilesystem#createfilesystem(uri, configuration)}. which must be that of localfs @throws ioexception @throws urisyntaxexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.ftp.FTPInputStream "www.apache.org/licenses/license-2.0 we don't support seek. not supported. do nothing"
org.apache.hadoop.fs.GlobExpander "expand globs in the given filepattern into a collection of file patterns so that in the expanded set no file pattern has a slash character ("/") in a curly bracket pair. file patterns @throws ioexception expand the leftmost outer curly bracket pair containing a slash character ("/") in filepattern. file patterns @throws ioexception finds the index of the leftmost opening curly bracket containing a slash character ("/") in filepattern. index of the leftmost opening curly bracket containing a slash character ("/"), or -1 if there is no such bracket @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.GlobFilter "a filter for posix glob pattern with brace expansions. creates a glob filter with the specified file pattern. the file pattern. @throws ioexception thrown if the file pattern is incorrect. creates a glob filter with the specified file pattern and an www.apache.org/licenses/license-2.0 existing code expects ioexception startwith("illegal file pattern")"
org.apache.hadoop.fs.GlobPattern "a class for posix glob pattern with brace expansions. construct the glob pattern object with a glob pattern string the glob pattern string compiled pattern compile glob pattern string the glob pattern pattern object match input against the compiled glob pattern input chars for successful matches set and compile a glob pattern the glob pattern string if this is a wildcard pattern (with special chars) www.apache.org/licenses/license-2.0 escape regex special chars that are not glob special chars start of a group non-capturing end of a group ^ inside [...] can be unescaped [! needs to be translated to [^ many set errors like [][] could not be easily detected here, as []], []-] and [-] are all valid posix glob and java regex. we'll just let the regex compiler do the real work."
org.apache.hadoop.fs.HardLink "class for creating hardlinks. supports unix/linux, winxp/2003/vista via cygwin, and mac os x. the hardlink class was formerly a static inner class of fsutil, and the methods provided were blatantly non-thread-safe. to enable volume-parallel update snapshots, we now provide static threadsafe methods that allocate new buffer string arrays upon each call. we also provide an api to hardlink all files in a directory with a single command, which is up to 128 times more efficient - and minimizes the impact of the extra buffer creations. this abstract class bridges the os-dependent implementations of the needed functionality for creating hardlinks and querying link counts. the particular implementation class is chosen during static initialization phase of the hardlink class. the "getter" methods construct shell command strings for various purposes. get the command string needed to hardlink a bunch of files from a single source directory into a target directory. the source directory is not specified here, but the command will be executed using the source directory as the "current working directory" of the shell invocation. - array of path-less file names, relative to the source directory - target directory where the hardlinks will be put - an array of strings suitable for use as a single shell command with {@link runtime.exec()} @throws ioexception - if any of the file or path names misbehave get the command string needed to hardlink a single file get the command string to query the hardlink count of a file calculate the total string length of the shell command resulting from execution of linkmult, plus the length of the source directory name (which will also be provided to the shell) - source directory, parent of filebasenames - array of path-less file names, relative to the source directory - target directory where the hardlinks will be put - total data length (must not exceed maxallowedcmdarglength) @throws ioexception get the maximum allowed string length of a shell command on this os, which is just the documented minimum guaranteed supported command length - aprx. 32kb for unix, and 8kb for windows. implementation of hardlinkcommandgetter class for unix @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#linkone(java.io.file, java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#linkmult(java.lang.string[], java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#linkcount(java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#getlinkmultarglength(java.io.file, java.lang.string[], java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#getmaxallowedcmdarglength() implementation of hardlinkcommandgetter class for windows note that the linkcount shell command for windows is actually a cygwin shell command, and depends on ${cygwin}/bin being in the windows path environment variable, so stat.exe can be found. @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#linkone(java.io.file, java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#linkmult(java.lang.string[], java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#linkcount(java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#getlinkmultarglength(java.io.file, java.lang.string[], java.io.file) @see org.apache.hadoop.fs.hardlink.hardlinkcommandgetter#getmaxallowedcmdarglength() calculate the nominal length of all contributors to the total commandstring length, including fixed overhead of the os-dependent command. it's protected rather than private, to assist unit testing, but real clients are not expected to need it -- see the way createhardlinkmult() uses it internally so the return this private value for use by unit tests. shell commands are not allowed to have a total string length exceeding this size. complexity is above. creates a hardlink - existing source file - desired target link file creates hardlinks from multiple existing files within one parent directory, into one target directory. - directory containing source files - list of path-less file names, as returned by parentdir.list() - where the hardlinks should be put. it must already exist. if the list of files is too long (overflows maxallowedcmdarglength), we will automatically split it into multiple invocations of the underlying method. implements {@link createhardlinkmult} with added variable "maxlength", to ease unit testing of the auto-splitting feature for long lists. likewise why it returns "callcount", the number of sub-arrays that the file list had to be split into. non-test clients are expected to call the public method instead. retrieves the number of links to the specified file. create an ioexception for failing to get link count. hardlink statistics counters and methods. not multi-thread safe, obviously. init is called during hardlink instantiation, above. these are intended for use by knowledgeable clients, not internally, because many of the internal methods are static and can't update these per-instance counters. www.apache.org/licenses/license-2.0 not static initialize the command "getters" statically, so can use their methods without instantiating the hardlink object windows unix override getlinkcountcommand for the particular unix variant linux is already set as the default - {"stat","-c%h", null} unix guarantees at least 32k bytes cmd length. subtract another 64b to allow for java 'exec' overhead may update this for specific unix variants, after static initialization phase unix wants argument order: "ln  " add 1 to account for terminal null or delimiter space add the fixed overhead of the hardlinkmult prefix and suffix length("ln") + 1 the windows command getter impl class and its member fields are package-private ("default") access instead of "private" to assist unit testing (sort of) on non-win servers windows guarantees only 8k - 1 bytes cmd length. subtract another 64b to allow for java 'exec' overhead windows wants argument order: "create  " the linkcount command is actually a cygwin shell command, not a windows shell command, so we should use "makeshellpath()" instead of "getcanonicalpath()". however, that causes another shell exec to "cygpath.exe", and "stat.exe" actually can handle dos-style paths (it just prints a couple hundred bytes of warning to stderr), so we use the more efficient "getcanonicalpath()". add 1 to account for terminal null or delimiter space add the fixed overhead of the hardlinkmult command (prefix, suffix, and dir suffix) construct and execute shell command this is the public method all non-test clients are expected to use. normal case - allow up to maxallowedcmdarglength characters in the cmd the os cmds can't handle empty list of filenames, but it's legal, so just return. if the list is too long, split into multiple invocations construct and execute shell command construct and execute shell command"
org.apache.hadoop.fs.HarFileSystem "this is an implementation of the hadoop archive filesystem. this archive filesystem has index files of the form _index and has contents of the form part-. the index files store the indexes of the real files. the index files are of the form _masterindex and _index. the master index is a level of indirection in to the index file to make the look ups faster. the index file is sorted with hash code of the paths that it contains and the master index contains pointers to the positions in index for ranges of hashcodes. public construction of harfilesystem return the protocol scheme for the filesystem.  har constructor to create a harfilesystem with an underlying filesystem. initialize a har filesystem per har archive. the archive home directory is the top level directory in the filesystem that contains the har archive. be careful with this method, you do not want to go on creating new filesystem instances per call to path.getfilesystem(). the uri of har is har://underlyingfsscheme-host:port/archivepath. or har:///archivepath. this assumes the underlying filesystem to be used in case not specified. find the parent path that is the archive path in the path. the last path segment that ends with .har is the path that will be returned. decode the raw uri to get the underlying uri raw har uri uri of the underlying filesystem return the top level archive. create a har specific auth har-underlyingfs:port the uri of underlying filesystem specific auth returns the uri of this filesystem. the uri is of the form har://underlyingfsschema-host:port/pathintheunderlyingfs this method returns the path inside the har filesystem. this is relative path inside the har filesystem. the fully qualified path in the har filesystem. path in the filesystem. this makes a path qualified in the har filesystem (non-javadoc) @see org.apache.hadoop.fs.filterfilesystem#makequalified( org.apache.hadoop.fs.path) fix offset and length of block locations. note that this method modifies the original array. block locations of har part file the start of the desired range in the contained file the length of the desired range the offset of the desired file in the har part file locations with fixed offset and length get block locations from the underlying fs and fix their offsets and lengths. the input filestatus to get block locations the start of the desired range in the contained file the length of the desired range locations for this segment of file @throws ioexception the hash of the path p inside iniside the filesystem the path in the harfilesystem hash code of the path. get filestatuses of all the children of a given directory. this just reads through index file and reads line by line to get all statuses for children of a directory. its a brute force way of getting all such filestatuses the parent path directory the list to add the children filestatuses to the string list of children for this parent the archive index filestatus combine the status stored in the index and the underlying status. status stored in the index caching the underlying file statuses combined file status @throws ioexception return the filestatus of files in har archive. the permission returned are that of the archive index files. the permissions are not persisted while creating a hadoop archive. the path in har filesystem . @throws ioexception since no checksum algorithm is implemented. returns a har input stream which fakes end of file. it reads the index files to get the part file name and the size and start of the file. not implemented. not implemented. liststatus returns the children of a directory after looking up the index files. return the top level archive path. not implemented. not implemented. copies the file in the har filesystem to a local file. not implemented. not implemented. not implemented. not implemented. hadoop archives input stream. this input stream fakes eof since archive files are part of bigger part files. create an input stream that fakes all the reads/positions/seeking. reset is not implemented  implementing position readable. position readable again. constructors for har input stream. the underlying filesystem the path in the underlying filesystem the start position in the part file the length of valid data in the part file the buffer size @throws ioexception constructor for har input stream. the underlying filesystem the path in the underlying file system the start position in the part file the length of valid data in the part file. @throws ioexception testing purposes only: www.apache.org/licenses/license-2.0 uri representation of this har filesystem the top level path of the archive in the underlying file system the har auth pointer into the static metadata cache underlyingfsscheme-host:port/archivepath. /archivepath. this assumes the underlying filesystem decode the name we got the right har path- now check if this is truly a har filesystem check for the underlying fs containing the index file the archive has been overwritten since we last read it remove the entry from the meta data cache get the version of the filesystem from the masterindex file the version is currently not useful since its the first version of archives we are using the default file system in the config so create a underlying uri and return it create a path -/."); -/."); query component not allowed do nothing should not happen underlyingfsschema-host:port/pathintheunderlyingfs the relative path of p. basically getting rid of /. parsing and doing string manipulation is not good - so just use the path api to do it. make sure that we just get the path component change this to har uri offset 1 past last byte of desired range offset of part block relative to beginning of desired file (may be negative if file starts in this part block) offset 1 past last byte of har block relative to beginning of desired file desired range starts after beginning of this har block fix offset to beginning of relevant range (relative to desired file) fix length to relevant portion of har block desired range includes beginning of this har block range ends before end of this har block fix length to remove irrelevant portion at the end get all part blocks that overlap with the desired file blocks a single line parser for hadoop archives status stored in a single line in the index files the format is of the form filename "dir"/"file" partfilename startindex length  this is equal to "none" if its a directory propsplits is used to retrieve the metainformation that har versions 1 & 2 missed (modification time, permission, owner group). these fields are stored in an encoded string placed in different locations depending on whether it's a file or directory entry. if it's a directory, the string will be placed at the partname location (directories have no partname because they don't have data to be stored). this is done because the number of fields in a directory entry is unbounded (all children are listed at the end) if it's a file, the string will be the last field. the fields below are stored in the file but are currently not used by harfilesystem permission = new fspermission(short.parseshort(propsplits[1])); owner = decodestring(propsplits[2]); group = decodestring(propsplits[3]); get the fs datainputstream for the underlying file look up the index. get the fs datainputstream for the underlying file we got it.. woo hooo!!! this might already be closed ignore need to see if the file is an index in file get the filestatus of the archive directory we will create fake filestatuses to return to the client does nothing. the underlying data input stream that the underlying filesystem will return. one byte buffer the start of this file in the part file the position pointer in the part file the end pointer in the part file not implemented do nothing end case do not need to implement this hdfs in itself does seektonewsource while reading. the masterindex of the archive the index file the first line contains the version of the index file make it always backwards-compatible each line contains a hashcode range and the index file name now start reading the real index file"
org.apache.hadoop.fs.HasFileDescriptor "having a filedescriptor filedescriptor @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.InvalidPathException "path string is invalid either because it has invalid characters or due to other file system specific reasons. constructs exception with the specified detail message. invalid path. constructs exception with the specified detail message. invalid path. reason path is invalid www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.kfs.IFSImpl "licensed under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. we need to provide the ability to the code in fs/kfs without really having a kfs deployment. in particular, the glue code that wraps around calls to kfsaccess object. this is accomplished by defining a filesystem implementation interface: -- for testing purposes, a dummy implementation of this interface will suffice; as long as the dummy implementation is close enough to doing what kfs does, we are good. -- for deployment purposes with kfs, this interface is implemented by the kfsimpl object. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.kfs.KFSConfigKeys "this class contains constants for configuration keys used in the kfs file system. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.kfs.KFSImpl "licensed under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. provide the implementation of kfs which turn into calls to kfsaccess. www.apache.org/licenses/license-2.0 when opening for append, # of replicas is ignored"
org.apache.hadoop.fs.kfs.KFSInputStream "licensed under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. implements the hadoop fsinputstream interfaces to allow applications to read files in kosmos file system (kfs). www.apache.org/licenses/license-2.0 use -1 to signify eof do nothing"
org.apache.hadoop.fs.kfs.KFSOutputStream "licensed under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. implements the hadoop fsoutputstream interfaces to allow applications to write to files in kosmos file system (kfs). www.apache.org/licenses/license-2.0 touch the progress before going into kfs since the call can block touch the progress before going into kfs since the call can block"
org.apache.hadoop.fs.kfs.KosmosFileSystem "licensed under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. implements the hadoop fs interfaces to allow applications to store files in kosmos file system (kfs). a filesystem backed by kfs. return the protocol scheme for the filesystem.  kfs return null if the file doesn't exist; otherwise, get the locations of the various chunks of the file file from kfs. www.apache.org/licenses/license-2.0 " + uri.get system.out.println("calling mkdirs on: " + srep); system.out.println("calling isdir on: " + srep); system.out.println("status of path: " + path + " is dir"); system.out.println("status of path: " + path + " is file"); system.out.println("calling rename on: " + sreps + " -> " + srepd); recursively delete the directory and its contents 64mb is the kfs block size"
org.apache.hadoop.fs.local.LocalConfigKeys "this class contains constants for configuration keys used in the local file system, raw local fs and checksum fs. note that the settings for unimplemented features are ignored. e.g. checksum related settings are just place holders. even when wrapped with {@link checksumfilesystem}, these settings are not used. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.local.LocalFs "the localfs implementation of checksumfs. evolving for a release,to be changed to stable this constructor has the signature needed by {@link abstractfilesystem#createfilesystem(uri, configuration)}. which must be that of localfs @throws ioexception @throws urisyntaxexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.local.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.local.RawLocalFs "the rawlocalfs implementation of abstractfilesystem. this impl delegates to the old filesystem evolving for a release,to be changed to stable this constructor has the signature needed by {@link abstractfilesystem#createfilesystem(uri, configuration)}. which must be that of localfs @throws ioexception @throws urisyntaxexception returns the target of the given symlink. returns the empty string if the given path does not refer to a symlink or there is an error acessing the symlink. nb: use readsymboliclink in java.nio.file.path once available. could use getcanonicalpath in file to get the target of the symlink but that does not indicate if the given path refers to a symlink. return a filestatus representing the given path. if the path refers to a symlink return a filestatus representing the link rather than the object the link refers to. the exists method in the file class returns false for dangling links so we can get a filenotfoundexception for links that exist. it's also possible that we raced with a delete of the link. use the readbasicfileattributes method in java.nio.file.attributes when available. we should never get here. valid local links are resolved transparently by the underlying local file system and accessing a dangling link will result in an ioexception, not an unresolvedlinkexception, so filecontext should never call this function. www.apache.org/licenses/license-2.0 no default port for file:/// nb: use createsymboliclink in java.nio.file.path once available if f refers to a regular file or directory otherwise f refers to a symlink f refers to a file or directory that does not exist"
org.apache.hadoop.fs.LocalDirAllocator "an implementation of a round-robin scheme for disk allocation for creating files. the way it works is that it is kept track what disk was last allocated for a file write. for the current request, the next disk from the set of disks would be allocated if the free space on the disk is sufficient enough to accommodate the file that is being considered for creation. if the space requirements cannot be met, the next disk in order would be tried and so on till a disk is found with sufficient capacity. once a disk with sufficient space is identified, a check is done to make sure that the disk is writable. also, there is an api provided that doesn't take the space requirements into consideration but just checks whether the disk under consideration is writable (this should be used for cases where the file size is not known apriori). an api is provided to read a path that was used when size of file to be allocated is unknown. create an allocator object this method must be used to obtain the dir allocation context for a particular value of the context name. the context name must be an item defined in the configuration object for which we want to control the dir allocations (e.g., mapred.local.dir). the method will create a context for that name if it doesn't already exist. get a path from the local fs. this method should be used if the size of the file is not known apriori. we go round-robin over the set of disks (via the configured dirs) and return the first complete path where we could create the parent directory of the passed path. the requested path (this will be get a path from the local fs. pass size as size_unknown if not known apriori. we round-robin over the set of disks (via the configured dirs) and return the first complete path which has enough space the requested path (this will be get a path from the local fs. pass size as size_unknown if not known apriori. we round-robin over the set of disks (via the configured dirs) and return the first complete path which has enough space the requested path (this will be get a path from the local fs for reading. we search through all the configured dirs for the file's existence and return the complete path to the file when we find one the requested file (this will be searched) the configuration object complete path to the file on a local disk @throws ioexception get all of the paths that currently exist in the working directories. the path underneath the roots the configuration to look up the roots in of the paths that exist under any of the roots @throws ioexception creates a temporary file in the local fs. pass size as -1 if not known apriori. we round-robin over the set of disks (via the configured dirs) and select the first complete path which has enough space. a file is method to check whether a context is valid /false removes the context from the context config items we search through all the configured dirs for the file's existence and return true when we find the requested file (this will be searched) the configuration object if files exist. false otherwise @throws ioexception get the current directory index for the given configuration item. current directory index for the given configuration item. this method gets called everytime before any read/write to make sure that any change to localdirs is reflected immediately. get the current directory index. current directory index. get a path from the local fs. if size is known, we go round-robin over the set of disks (via the configured dirs) and return the first complete path which has enough space. if size is not known, use roulette selection -- pick directories with probability proportional to their available space. creates a file on the local fs. pass size as {@link localdirallocator.size_unknown} if not known apriori. we round-robin over the set of disks (via the configured dirs) and return a file on the first path which has enough space. the file is guaranteed to go away when the jvm exits. get a path from the local fs for reading. we search through all the configured dirs for the file's existence and return the complete path to the file when we find one get all of the paths that currently exist in the working directories. the path underneath the roots the configuration to look up the roots in of the paths that exist under any of the roots @throws ioexception we search through all the configured dirs for the file's existence and return true when we find one www.apache.org/licenses/license-2.0 a map from the config item names like "mapred.local.dir" to the instance of the allocatorpercontext. this is a static object to make sure there exists exactly one instance per jvm filter problematic directories ignore randomize the first disk picked in the round-robin selection check whether we are able to create a directory here. if the disk happens to be rdonly we will fail remove the leading slash from the path (to make sure that the uri resolution results in a valid path on the dir being checked) do roulette selection: pick dir with probability proportional to available size build the "roulette wheel" keep rolling the wheel till we get a valid path skip this disk no path found find an appropriate directory create a temp file on this directory remove the leading slash from the path (to make sure that the uri resolution results in a valid path on the dir being checked) no path found remove the leading slash from the path (to make sure that the uri resolution results in a valid path on the dir being checked) ignore and try again"
org.apache.hadoop.fs.LocalFileSystem "implement the filesystem api for the checksumed local filesystem. return the protocol scheme for the filesystem.  file convert a path to a file. moves files to a bad file directory on the same device, so that their storage will not be reused. www.apache.org/licenses/license-2.0 /"); canonicalize f find highest writable parent dir of f on the same device move the file there close it first rename it move checksum file too"
org.apache.hadoop.fs.LocalFileSystemConfigKeys "this class contains constants for configuration keys used in the local file system, raw local fs and checksum fs. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.LocatedFileStatus "this class defines a filestatus that includes a file's block locations. constructor a file status a file's block locations constructor a file's length if the path is a directory _replication the file's replication factor a file's block size _time a file's modification time _time a file's access time a file's permission a file's owner a file's group symlink if the path is a symbolic link the path's qualified name a file's block locations get the file's block locations file's block locations compare this object to another object o the object to be compared. a negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object. @throws classcastexception if the specified object's is not of type filestatus compare if this object is equal to another object o the object to be compared. true if two file status has the same path name; false if not. returns a hash code value for the object, which is defined as the hash code of the path name. a hash code value for the path name. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum "for crc32 with the castagnoli polynomial same as this(0, 0, null) create a md5filechecksum www.apache.org/licenses/license-2.0 default to the one that is understood by all releases."
org.apache.hadoop.fs.MD5MD5CRC32FileChecksum "md5 of md5 of crc32. same as this(0, 0, null) create a md5filechecksum returns the crc type write that object to xml output. return the object represented in the attributes. www.apache.org/licenses/license-2.0 default to the one that is understood by all releases. old versions don't support crctype. we should never get here since finalcrctype will hold a valid type or we should have got an exception."
org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum "for crc32 with the gzip polynomial same as this(0, 0, null) create a md5filechecksum www.apache.org/licenses/license-2.0 default to the one that is understood by all releases."
org.apache.hadoop.fs.Options "this class contains options related to file system operations. class to support the varargs for create() options. this is not needed if checksumparam is specified. get an option of desired type is the desired class of the opt - not null - at least one opt must be passed opt from one of the opts of type theclass. returns null if there isn't any set an option the option to be set - the option is set into this array of opts createopts[] == opts + newvalue enum to support the varargs for rename() options this is used in filesystem and filecontext to specify checksum options. create a uninitialized one normal ctor checksum type bytes per checksum create a checksumopts that disables checksum a helper method for processing a helper method for processing www.apache.org/licenses/license-2.0 no newvalue in opt no options overwrite the rename destination the following is done to avoid unnecessary creation of new objects. tri-state variable: 0 default, 1 true default, false bytesperchecksum - order of preference   default.   default checksum type - order of preference  default. short out the common and easy cases take care of the rest of combinations"
org.apache.hadoop.fs.ParentNotDirectoryException "indicates that the parent of specified path is not a directory as expected. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.Path "names a file or directory in a {@link filesystem}. path strings use slash as the directory separator. a path string is absolute if it begins with a slash. the directory separator, a slash. resolve a child path against a parent path. resolve a child path against a parent path. resolve a child path against a parent path. resolve a child path against a parent path. construct a path from a string. path strings are uris, but with unescaped elements and some additional normalization. construct a path from a uri construct a path from components. convert this to a uri. return the filesystem that owns this path. is an absolute path (ie a slash relative path part) and a scheme is null and true if the path component (i.e. directory) of this uri is absolute. true if the path component of this uri is absolute. there is some ambiguity here. an absolute path is a slash relative name without a scheme or an if and only if this path represents the root of a file system returns the final component of this path. returns the parent of a path or null if at root. adds a suffix to the final name in the path. return the number of elements in this path. returns a qualified path object. deprecated - use {@link #makequalified(uri, path)} returns a qualified path object. www.apache.org/licenses/license-2.0 a hierarchical uri add a slash to parent's path so resolution is compatible with uri's disallow construction of a path from an empty string we can't use 'new uri(string)' directly, since it assumes things are escaped, which we don't require of paths. add a slash in front of paths with windows drive letters parse uri components parse uri scheme, if any has a scheme parse uri ", start) && has uri path is the rest of the string -- query & fragment not supported remove double slashes & backslashes ", "/"); trim trailing slash from non-root path (ignoring windows drive) empty path at root we can't use uri.tostring(), which escapes everything, because we want illegal characters unescaped in the string, for glob processing, etc. "); has windows drive but no scheme or remove slash before drive"
org.apache.hadoop.fs.PathAccessDeniedException "eacces for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathExistsException "exception corresponding to file exists - eexists for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathFilter "tests whether or not the specified abstract pathname should be included in a pathname list. path the abstract pathname to be tested true if and only if pathname should be included www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathIOException "exceptions based on standard posix/linux style exceptions for path related errors. returns an exception with the format "path: standard error string". this exception corresponds to error input/ouput(eio) constructor a generic i/o error exception for the exception appends the text of a throwable to the default error message for the exception a throwable to extract the error message avoid using this method. use a subclass of pathioexception if possible. for the exception custom string to use an the error text format: cmd: {operation} `path' {to `target'}: error string that generated the exception if the operation involved copying or moving, else null optional operation that will preface the path a string optional path if the exception involved two paths, ex. a copy operation the of the operation www.apache.org/licenses/license-2.0 note: this really should be a path, but a path is buggy and won't return the exact string used to construct the path, and it mangles uris with no"
org.apache.hadoop.fs.PathIsDirectoryException "eisdir for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathIsNotDirectoryException "enotdir for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathIsNotEmptyDirectoryException "generated by rm commands for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathNotFoundException "exception corresponding to permission denied - enoent for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathOperationException "enotsup for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.PathPermissionException "exception corresponding to operation not permitted - eperm for the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.permission.AccessControlException "an exception class for access control related issues. @deprecated use {@link org.apache.hadoop.security.accesscontrolexception} instead. default constructor is needed for unwrapping from {@link org.apache.hadoop.ipc.remoteexception}. constructs an {@link accesscontrolexception} with the specified detail message. the detail message. constructs a new exception with the specified cause and a detail message of (cause==null ? null : cause.tostring()) (which typically contains the class and detail message of cause). cause the cause (which is saved for later retrieval by the {@link #getcause()} method). (a null value is permitted, and indicates that the cause is nonexistent or unknown.) www.apache.org/licenses/license-2.0 required by {@link java.io.serializable}."
org.apache.hadoop.fs.permission.ChmodParser "parse a permission mode passed in from a chmod command and apply that mode against an existing file. apply permission against specified file and determine what the new mode would be file against which to apply mode 's new mode if applied. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.permission.FsAction "file system actions, e.g. read, write, etc. retain reference to value array. symbolic representation return true if this action implies that action. and operation. or operation. not operation. www.apache.org/licenses/license-2.0 posix style"
org.apache.hadoop.fs.permission.FsPermission "a class for file/directory permissions. create an immutable {@link fspermission} object. construct by the given {@link fsaction}. construct by the given mode. @see #toshort() copy constructor other permission construct by given mode, either in octal or symbolic format. mode as a string, either in octal or symbolic format @throws illegalargumentexception if mode is invalid return return group {@link fsaction}. return other {@link fsaction}. create and initialize a {@link fspermission} from {@link datainput}. encode the object to a short. apply a umask to this permission and return a new one. the umask is used by create, mkdir, and other hadoop filesystem operations. the mode argument for these operations is modified by removing the bits which are set in the umask. thus, the umask limits the permissions which newly umask property label deprecated key and code in getumask method to accommodate it may be removed in version .23 get the set the get the default permission for directory and symlink. in previous versions, this default permission was also used to create files, so files get the default permission for directory. get the default permission for file. create a fspermission from a unix symbolic permission string e.g. "-rw-rw-rw-" www.apache.org/licenses/license-2.0 register a ctor posix permission style to ensure backward compatibility first use the deprecated key. if the deprecated key is not present then check for the new key provide more explanation for if oldumask is not set, then throw the exception property was set with old key old and new umask values do not match - use old umask add sticky bit value if set"
org.apache.hadoop.fs.permission.PermissionParser "base class for parsing either chmod permissions or umask permissions. includes common code needed by either operation as implemented in umaskparser and chmodparser classes. begin parsing permission stored in modestr permission mode, either octal or symbolic use-case specific symbolic pattern to match against @throws illegalargumentexception if unable to parse modestr groups : 1 : [ugoa] 2 : [+-=] 3 : [rwxxt]+ 4 : [,\s] www.apache.org/licenses/license-2.0 are there multiple permissions stored in one chmod? same as specifying 'a' check if sticky bit is specified convert x to x; if x is specified add 'x' only if exeok or x was already set. remove x"
org.apache.hadoop.fs.permission.PermissionStatus "store permission related information. create an immutable {@link permissionstatus} object. constructor return return group name return permission apply umask. @see fspermission#applyumask(fspermission) create and initialize a {@link permissionstatus} from {@link datainput}. serialize a {@link permissionstatus} from its base components. www.apache.org/licenses/license-2.0 register a ctor"
org.apache.hadoop.fs.permission.UmaskParser "parse umask value provided as a string, either in octal or symbolic format and return it as a short value. umask values are slightly different from standard modes as they cannot specify sticky bit or x. not allow x or t to be used for file/directory creation only. symbolic umask is applied relative to file mode creation mask; the permission op characters '+' results in clearing the corresponding bit in the mask, '-' results in bits for indicated permission to be set in the mask. for octal umask, the specified bits are set in the file mode creation mask. www.apache.org/licenses/license-2.0 no leading 1 for sticky bit return the complement of octal equivalent of umask that was computed"
org.apache.hadoop.fs.PositionedReadable "stream that permits positional reading. read upto the specified number of bytes, from a given position within a file, and return the number of bytes read. this does not change the current offset of a file, and is thread-safe. read the specified number of bytes, from a given position within a file. this does not change the current offset of a file, and is thread-safe. read number of bytes equalt to the length of the buffer, from a given position within a file. this does not change the current offset of a file, and is thread-safe. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.RawLocalFileSystem "implement the filesystem api for the raw local filesystem. convert a path to a file. for open()'s fsinputstream. just forward to the fis for create()'s fsoutputstream. just forward to the fos delete the given path to a file or directory. the path to delete to delete sub-directories if the file or directory and all its contents were deleted @throws ioexception if p is non-empty and recursive is false creates the specified directory hierarchy. does not treat existence as an error. set the working directory to the given directory. we can add extra fields here. it breaks at least copyfiles.filepair(). we recognize if the information is already loaded by check if onwer.equals(""). use the command chown to set owner. use the command chmod to set permission. www.apache.org/licenses/license-2.0 /"); unexpected exception assume native fs error unexpected exception assume native fs error unexpected exception assume native fs error unexpected exception assume native fs error ignore the files not found since the dir list may have have changed since the names[] list was generated. file provides getusablespace() and getfreespace() file provides no api to obtain used space, assume used = total - free in the case of the local filesystem, we can just rename the file. we can write output directly to the final location it's in the right place - nothing to do. / loads permissions, owner, and group from `ls -ld` expected format -rw------- 1 files with acls might have a '+' owner[:[group]]"
org.apache.hadoop.fs.RemoteIterator "an iterator over a collection whose elements need to be fetched remotely returns true if the iteration has more elements. true if the iterator has more elements. @throws ioexception if any io error occurs returns the next element in the iteration. next element in the iteration. @throws nosuchelementexception iteration has no more elements. @throws ioexception if any io error occurs www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.Block "holds metadata about a block of data being stored in a {@link filesystemstore}. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.FileSystemStore "a facility for storing and retrieving {@link inode}s and {@link block}s. delete everything. used for testing. @throws ioexception diagnostic method to dump all inodes to the console. @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.INode "holds file metadata including type (regular file, or directory), and the list of blocks that are pointers to the data. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.Jets3tFileSystemStore "www.apache.org/licenses/license-2.0 close output stream to file then delete file to prevent a second close ignore"
org.apache.hadoop.fs.s3.MigrationTool " this class is a tool for migrating data from an older to a newer version of an s3 filesystem.   all files in the filesystem are migrated by re-writing the block metadata - no datafiles are touched.  www.apache.org/licenses/license-2.0 use version number to create store store oldstore = ... migrate(oldstore, newstore); should never happen since every implementation of the java platform is required to support utf-8. see http://java.sun.com/j2se/1.5.0/docs/api/java/nio/charset/charset.html should never happen since every implementation of the java platform is required to support utf-8. see http://java.sun.com/j2se/1.5.0/docs/api/java/nio/charset/charset.html"
org.apache.hadoop.fs.s3.S3Credentials " extracts aws credentials from the filesystem uri or configuration.  @throws illegalargumentexception if credentials for s3 cannot be determined. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.S3Exception "thrown if there is a problem communicating with amazon s3. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.S3FileSystem " a block-based {@link filesystem} backed by amazon s3.  @see natives3filesystem return the protocol scheme for the filesystem.  s3 currently ignored. this optional operation is not yet supported. currently ignored. filestatus for s3 file systems. www.apache.org/licenses/license-2.0 aws.amazon.com/s3">amazon s3. set store in initialize() " + uri.get src path doesn't exist dst path already exists - can't overwrite dst parent doesn't exist or is a file diagnostic methods"
org.apache.hadoop.fs.s3.S3FileSystemConfigKeys "this class contains constants for configuration keys used in the s3 file system. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.S3FileSystemException "thrown when there is a fatal exception while using {@link s3filesystem}. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3.S3InputStream "we don't support marks. www.apache.org/licenses/license-2.0  compute desired block  read block blocks[targetblock] from position offsetintoblock do nothing"
org.apache.hadoop.fs.s3.S3OutputStream "www.apache.org/licenses/license-2.0  to the local block backup, write just the bytes   track position   done with local copy   send it to s3  todo: use passed in progressable to report progress.  delete local backup, start new one"
org.apache.hadoop.fs.s3.VersionMismatchException "thrown when hadoop cannot read the version of the data stored in {@link s3filesystem}. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3native.FileMetadata " holds basic metadata for a file stored in a {@link nativefilesystemstore}.  www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore "www.apache.org/licenses/license-2.0 ignore following is brittle. is there a better way? never returned - keep compiler happy never returned - keep compiler happy never returned - keep compiler happy never returned - keep compiler happy"
org.apache.hadoop.fs.s3native.NativeFileSystemStore " an abstraction for a key-based {@link file} store.  delete all keys with the given prefix. used for testing. @throws ioexception diagnostic method to dump state to the console. @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3native.NativeS3FileSystem " a {@link filesystem} for reading and writing files stored on amazon s3. unlike {@link org.apache.hadoop.fs.s3.s3filesystem} this implementation stores files on s3 in their native form so they can be read by other s3 tools. a note about directories. s3 of course has no "native" support for them. the idiom we choose then is: for any directory return the protocol scheme for the filesystem.  s3n this optional operation is not yet supported.  if f is a file, this method will make a single call to s3. if f is a directory, this method will make a maximum of (n / 1000) + 2 calls to s3, where n is the total number of files and directories contained directly in f.  set the working directory to the given directory. www.apache.org/licenses/license-2.0 aws.amazon.com/s3">amazon s3. set store in initialize() " + uri.get allow uris without trailing slash after bucket to refer to root, like s3n://mybucket remove initial slash this is fine, we don't require a marker root always exists this is just the directory we have been asked to list will throw if the file doesn't exist rename() and delete() use this method to ensure that the parent directory of the source does not vanish. cannot rename root of file system figure out the final destination this is fine, we don't require a marker"
org.apache.hadoop.fs.s3native.PartialListing " holds information on a directory listing for a {@link nativefilesystemstore}. this includes the {@link filemetadata files} and directories (their names) contained in a directory.   this listing may be returned in chunks, so a priorlastkey is provided so that the next chunk may be requested.  @see nativefilesystemstore#list(string, int, string) www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.s3native.S3NativeFileSystemConfigKeys "this class contains constants for configuration keys used in the s3 file system. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.Seekable "stream that permits seeking. seek to the given offset from the start of the file. the next read() will be from that location. can't seek past the end of the file. return the current offset from the start of the file seeks a different copy of the data. returns true if found a new source, false otherwise. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.shell.Command "an abstract class for the execution of a file system command default name of the command the command's usage switches and arguments format the command's long description allows stdout to be captured if necessary allows stderr to be captured if necessary constructor constructor command's name excluding the leading character - execute the command on the input path the input path @throws ioexception if any error occurs for each source path, execute the command 0 if it runs successfully; -1 if it fails invokes the command handler. the default behavior is to process options, expand arguments, and then process each argument.  run |-> {@link #processoptions(linkedlist)} \-> {@link #processrawarguments(linkedlist)} |-> {@link #expandarguments(linkedlist)} | \-> {@link #expandargument(string)} \-> {@link #processarguments(linkedlist)} |-> {@link #processargument(pathdata)} | |-> {@link #processpathargument(pathdata)} | \-> {@link #processpaths(pathdata, pathdata...)} | \-> {@link #processpath(pathdata)} \-> {@link #processnonexistentpath(pathdata)}  most commands will chose to implement just {@link #processoptions(linkedlist)} and {@link #processpath(pathdata)} the list of command line arguments exit code for the command @throws illegalargumentexception if called with invalid arguments the exit code to be returned if any errors occur during execution. this method is needed to account for the inconsistency in the exit codes returned by various commands. non-zero exit code must be implemented by commands to process the command line flags and check the bounds of the remaining arguments. if an illegalargumentexception is thrown, the fsshell object will print the short usage of the command. the command line arguments @throws ioexception allows commands that don't use paths to handle the raw arguments. default behavior is to expand the arguments via {@link #expandarguments(linkedlist)} and pass the resulting list to {@link #processarguments(linkedlist)} the list of argument strings @throws ioexception expands a list of arguments into {@link pathdata} objects. the default behavior is to call {@link #expandargument(string)} on each element which by default globs the argument. the loop catches ioexceptions, increments the error count, and displays the exception. strings to expand into {@link pathdata} objects of all {@link pathdata} objects the arguments @throws ioexception if anything goes wrong... expand the given argument into a list of {@link pathdata} objects. the default behavior is to expand globs. commands may override to perform other expansions on an argument. string pattern to expand of {@link pathdata} objects @throws ioexception if anything goes wrong... processes the command's list of expanded arguments. {@link #processargument(pathdata)} will be invoked with each item in the list. the loop catches ioexceptions, increments the error count, and displays the exception. a list of {@link pathdata} to process @throws ioexception if anything goes wrong... processes a {@link pathdata} item, calling {@link #processpathargument(pathdata)} or {@link #processnonexistentpath(pathdata)} on each item. {@link pathdata} item to process @throws ioexception if anything goes wrong... this is the last chance to modify an argument before going into the (possibly) recursive {@link #processpaths(pathdata, pathdata...)} -> {@link #processpath(pathdata)} loop. ex. ls and du use this to expand out directories. a {@link pathdata} representing a path which exists @throws ioexception if anything goes wrong... provides a hook for handling paths that don't exist. by default it will throw an exception. primarily overriden by commands that create paths such as mkdir or touch. the {@link pathdata} that doesn't exist @throws filenotfoundexception if arg is a path and it doesn't exist @throws ioexception if anything else goes wrong... iterates over the given expanded paths and invokes {@link #processpath(pathdata)} on each element. if "recursive" is true, will do a post-visit dfs on directories. if called via a recurse, will be the parent dir, else null a list of {@link pathdata} objects to process @throws ioexception if anything goes wrong... hook for commands to implement an operation to be applied on each path for the command. note implementation of this method is optional if earlier methods in the chain handle the operation. a {@link pathdata} object @throws runtimeexception if invoked but not implemented @throws ioexception if anything else goes wrong in the hook for commands to implement an operation to be applied on each path for the command after being processed successfully a {@link pathdata} object @throws ioexception if anything goes wrong... gets the directory listing for a path and invokes {@link #processpaths(pathdata, pathdata...)} {@link pathdata} for directory to recurse into @throws ioexception if anything goes wrong... display an exception prefaced with the command name. also increments the error count for the command which will result in a non-zero exit code. exception to display display an error string prefaced with the command name. also increments the error count for the command which will result in a non-zero exit code. error message to display display an warning string prefaced with the command name. warning message to display the name of the command. will first try to use the assigned name else fallback to the command's preferred name of the command define the name of the command. as invoked the short usage suitable for the synopsis "name options" the long usage suitable for help output of the usage is the command deprecated? the replacement for a deprecated command if not deprecated, else alternative command get a public static class field the field to retrieve of the field www.apache.org/licenses/license-2.0 other exceptions are probably nasty it's a glob that failed to match null indicates that the call is not via recursion, ie. there is no parent directory that was expanded todo: this really should be iterative build up a list of exceptions that occurred this is an unexpected condition, so dump the whole exception since it's probably a nasty internal error where the backtrace would be useful"
org.apache.hadoop.fs.shell.CommandFactory "class to search for and register commands factory constructor for commands factory constructor for commands the hadoop configuration invokes "static void registercommands(commandfactory)" on the given class. this method abstracts the contract between the factory and the command class. do not assume that directly invoking registercommands on the given class will have the same effect. class to allow an opportunity to register register the given class as handling the given list of command names. the class implementing the command names one or more command names that will invoke this class register the given object as handling the given list of command names. avoid calling this method and use {@link #addclass(class, string...)} whenever possible to avoid startup overhead from excessive command object instantiations. this method is intended only for handling nested non-static classes that are re-usable. namely -help/-usage. the object implementing the command names one or more command names that will invoke this class returns an instance of the class implementing the given command. the class must have been registered via {@link #addclass(class, string...)} name of the command of the requested command get an instance of the requested command name of the command to lookup the hadoop configuration {@link command} or null if the command is unknown gets all of the registered commands sorted list of command names www.apache.org/licenses/license-2.0 just so it shows up in the list of commands"
org.apache.hadoop.fs.shell.CommandFormat "parse the args of a command and check the format of args. @deprecated use replacement since name is an unused parameter of command, but never used see replacement see replacement see replacement @see #commandformat(int, int, string...) simple parsing of command line arguments minimum arguments required maximum arguments permitted list of the allowed switches parse parameters starting from the given position consider using the variant that directly takes a list an array of input arguments the position at which starts to parse list of parameters parse parameters from the given list of args. the list is destructively modified to remove the options. as a list of input arguments return if the option is set or not string representation of an option is the option is set; false otherwise returns all the options that are set  of the enabled options used when the arguments exceed their bounds used when too many arguments are supplied to a command used when too few arguments are supplied to a command used when an unsupported option is supplied to a command www.apache.org/licenses/license-2.0 stop if not an opt, or the stdin arg "-" is found force end of option processing"
org.apache.hadoop.fs.shell.CommandUtils "www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.shell.CommandWithDestination "provides: argument processing to ensure the destination is valid for the number of source arguments. a processpaths that accepts both a source and resolved target. sources are resolved as children of a destination directory. this method is used to enable the force(-f) option while copying the files. true/false the last arg is expected to be a local path, if only one argument is given then the destination will be the current directory is the list of arguments the last arg is expected to be a remote path, if only one argument is given then the destination will be the remote called with a source and target destination pair for the operation for the operation @throws ioexception if anything goes wrong copies the source file to the target. item to copy where to copy the item @throws ioexception if copy fails copies the stream contents to a temporary file. if the copy is successful, the temporary file will be renamed to the real path, else the temporary file will be deleted. the input stream for the copy where to store the contents of the stream @throws ioexception if copy fails www.apache.org/licenses/license-2.0 if the path is a glob, then it must match one and only one path if more than one arg, the destination must be a directory if one arg, the dst must not exist or must be a directory todo: remove when filecontext is supported, this needs to either copy the symlink or deref the symlink modify dst as we descend to append the basename of the current directory being processed too bad we have no clue what failed need to update stat to know it exists now on the first loop, the dst may be directory or a file, so only create a child path if dst is a dir; after recursion, it's always a dir see if path looks like a dir last ditch effort to ensure temp file is removed helper filter filesystem that registers be deleted on exit unless successfully renamed just in case copybytes didn't tag might have been the rename method with an option to delete the target is deprecated too bad we don't know why it failed too bad we don't know why it failed cancel delete on exit if rename is successful purge any remaining temp files, but don't close underlying fs"
org.apache.hadoop.fs.shell.CopyCommands "various commands for copy files merge multiple files together copy local files to a remote filesystem copy local files to a remote filesystem www.apache.org/licenses/license-2.0 check for error collecting paths flag that a path is bad this command is handled a bit differently than others. the paths are batched up instead of actually being processed. this avoids unnecessarily streaming into the merge file and then encountering a path error that should abort the merge for directories, recurse one level to get its files, else skip it skip subdirs should have a -r option should have a -r option commands operating on local paths have no need for glob expansion note: this logic should be better, mimics previous implementation"
org.apache.hadoop.fs.shell.Count "count the number of directories, files, bytes, quota, and remaining quota. register the names for the count command the command factory that will instantiate this class constructor constructor @deprecated invoke via {@link fsshell} the count command the starting index of the arguments configuration www.apache.org/licenses/license-2.0 default path is the current working directory"
org.apache.hadoop.fs.shell.Delete "classes that delete paths remove non-directory paths remove any path remove only empty directories empty the trash www.apache.org/licenses/license-2.0 prevent -f on a non-existent glob from failing todo: if the problem (ie. creating the trash dir, moving the item to be deleted, etc), then the path will just be deleted because movetotrash returns false and it falls thru to fs.delete. this doesn't seem right todo: should probably allow path arguments for the filesystems"
org.apache.hadoop.fs.shell.Display "display contents of files displays file content to stdout same behavior as "-cat", but handles zip and textrecordinputstream and avro encodings. this class transforms a binary avro data file into an inputstream with data that is in a human readable json format. read a single byte from the stream. close the stream. www.apache.org/licenses/license-2.0 check type of stream first rfc 1952 must be gzip 's' 'e' might be a sequencefile check the type of compression instead, depending on codec class's own detection methods, based on the provided path. 'o' 'b' file is non-compressed, or not a file container we know. write a new line after the last avro record."
org.apache.hadoop.fs.shell.FsCommand "base class for all "hadoop fs" commands register the command classes used by the fs subcommand where to register the class @deprecated use {@link command#run(string...argv)} www.apache.org/licenses/license-2.0 this class may not look useful now, but it's a placeholder for future functionality to act as a registry for fs commands. currently it's being used to implement unnecessary abstract methods in the base class historical abstract method in command abstract method that normally is invoked by runall() which is overridden below"
org.apache.hadoop.fs.shell.FsUsage "base class for commands related to viewing filesystem usage, such as du and df show the size of a partition in the filesystem show disk usage show disk usage summary creates a table of aligned values based on the maximum width of each column as a string create a table w/o headers number of columns create a table with headers list of headers change the default left-align of columns of columns to right align add a row of objects to the table the values render the table to a stream printstream for output number of rows excluding header does table have any rows www.apache.org/licenses/license-2.0 go one level deep on dirs from cmdline unless in summary mode prevent trailing spaces if the final column is left-aligned"
org.apache.hadoop.fs.shell.Ls "get a listing of all files in that match the file patterns. compute column widths and rebuild the format string to find the max field width for each column get a recursive listing of all files in that match the file patterns. same as "-ls -r" www.apache.org/licenses/license-2.0 implicitly recurse once for cmdline directories permission string do not use '%-0s' as a formatting conversion, since it will throw a a missingformatwidthexception if it is used in string.format(). http://docs.oracle.com/javase/1.5.0/docs/api/java/util/formatter.html#intflags mod time & path"
org.apache.hadoop.fs.shell.Mkdir "create the given dir www.apache.org/licenses/license-2.0 check if parent exists. this is complicated because getparent(a/b/c/) returns a/b/c, but we want a/b"
org.apache.hadoop.fs.shell.MoveCommands "various commands for moving files move local files to a remote filesystem move remote files to a local filesystem move/rename paths on the same fileystem www.apache.org/licenses/license-2.0 unlike copy, don't merge existing dirs during move we have no way to know the actual error... we have no way to know the actual error..."
org.apache.hadoop.fs.shell.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.shell.PathData "encapsulates a path (path), its filestatus (stat), and its filesystem (fs). the stat field will be null if the path does not exist. creates an object to wrap the given parameters as fields. the string used to create the path will be recorded since the path object does not return exactly the same string used to initialize it a string for a path the configuration file @throws ioexception if anything goes wrong... creates an object to wrap the given parameters as fields. the string used to create the path will be recorded since the path object does not return exactly the same string used to initialize it a local file the configuration file @throws ioexception if anything goes wrong... looks up the file status for a path. if the path doesn't exist, then the status will be null the filesystem for the path a string for a path @throws ioexception if anything goes wrong creates an object to wrap the given parameters as fields. the string used to create the path will be recorded since the path object does not return exactly the same string used to initialize it. the filesystem a string of the path the filestatus (may be null if the path doesn't exist) get the filestatus info if true, stat will be null if the path doesn't exist for the given path @throws ioexception if anything goes wrong updates the paths's file status updated filestatus @throws ioexception if anything goes wrong... ensure that the file exists and if it is or is not a directory set it to the desired requirement. @throws pathioexception if file doesn't exist or the type does not match what was specified in typerequirement. returns a new pathdata with the given extension. for the suffix @throws ioexception shouldn't happen test if the parent directory exists indicating parent exists @throws ioexception upon unexpected error check if the path represents a directory as determined by the basename being "." or "..", or the path ending with a directory separator if this represents a directory returns a list of pathdata objects of the items contained in the given directory. of pathdata objects for its children @throws ioexception if anything else goes wrong... creates a new object for a child entry in this directory the basename will be appended to this object's path for the child @throws ioexception if this object does not exist or is not a directory given a child of this directory, use the directory's path and the child's basename to construct the string to the child. this preserves relative paths since path will fully qualify. a path contained within this directory of the path relative to this directory expand the given path as a glob pattern. non-existent paths do not throw an exception because creation commands like touch and mkdir need to create them. the "stat" field will be null if the path does not exist. the pattern to expand as a glob the hadoop configuration of {@link pathdata} objects. if the pattern is not a glob, and does not exist, the list will contain a single pathdata with a null stat @throws ioexception anything else goes wrong... returns the printable version of the path that is either the path as given on the commandline, or the full path of the path get the path to a local file representing the local path @throws illegalargumentexception if this.fs is not the localfilesystem construct a uri from a string with unescaped special characters that have non-standard sematics. e.g. /, ?, #. a custom parsing is needed to prevent misbihaviors. the input path in string form www.apache.org/licenses/license-2.0 need a static method for the ctor above todo: should consider wrapping other exceptions into pathexceptions always set the status. the caller must get the correct result if it catches the exception and later interrogates the status path will munch off the chars that indicate a dir, so there's no way to perform this test except by examining the raw basename we maintain preserve relative paths check getpath() so scheme slashes aren't considered part of the path remove any quoting in the glob pattern not a glob & file not found, so add the path with a null stat figure out what type of glob path was given, will convert globbed paths to match the type to preserve relativity convert stats to pathdata use as-is, but remove take just the uri's path make it relative to the current working dir find common ancestor take the remaining path fragment after the ancestor if cwd has a path fragment after the ancestor, convert them to ".." add the path separator to dirs to simplify finding the longest match find longest directory prefix no interpretation of symbols. just decode % escaped chars. we can't use 'new uri(string)' directly. since it doesn't do quoting internally, the internal parser may fail or break the string at wrong places. use of multi-argument ctors will quote those chars for us, but we need to do our own parsing and assembly. parse uri components parse uri scheme, if any has a non zero-length scheme parse uri ", start) && uri path is the rest of the string. ? or # are not interpreated, but any occurrence of them will be quoted by the uri ctor. construct the uri"
org.apache.hadoop.fs.shell.SetReplication "modifies the replication factor wait for all files in waitlist to have replication number equal to rep. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.shell.Stat "print statistics about path in specified format. format sequences: %b: size of file in blocks %g: group name of owner %n: filename %o: block size %r: replication %u: www.apache.org/licenses/license-2.0 default format string make sure there's still at least one arg this silently drops a trailing %? this leaves % alone, which causes the potential for future format options to break strings; should use %% to escape percents"
org.apache.hadoop.fs.shell.Tail "get a listing of all files in that match the file patterns. www.apache.org/licenses/license-2.0 milliseconds todo: hadoop-7234 will add glob support; for now, be backwards compat treat a negative offset as relative to end of the file, floor of 0 use conf so the system configured io block size is used"
org.apache.hadoop.fs.shell.Test "perform shell-like file tests www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.shell.Touchz "unix touch like commands (re)create zero-length file at the specified path. this will be replaced by a more unix-like touch when files may be modified. www.apache.org/licenses/license-2.0 todo: handle this"
org.apache.hadoop.fs.Syncable "this interface for flush/sync operation. @deprecated as of hadoop 0.21.0, replaced by hflush @see #hflush() flush out the data in client's similar to posix fsync, flush out the data in client's www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.Trash "provides a trash facility which supports pluggable trash policies. see the implementation of the configured trashpolicy for more details. construct a trash can accessor. a configuration construct a trash can accessor for the filesystem provided. the filesystem a configuration in case of the symlinks or mount points, one has to move the appropriate trashbin in the actual volume of the path p being deleted. hence we get the file system of the fully-qualified resolved-path and then move the path p to the trashbin in that volume, - the filesystem of path p - the path being deleted - to be moved to trasg - configuration if the item is already in the trash or trash is disabled @throws ioexception on error returns whether the trash is enabled for this filesystem move a file or directory to the current trash directory. if the item is already in the trash or trash is disabled create a trash checkpoint. delete old checkpoint(s). get the current working directory get the configured trash policy return a {@link runnable} that periodically empties the trash of all www.apache.org/licenses/license-2.0 configured trash policy instance if the trash interval is configured server side then clobber this configuration so that we always respect the server configuration. if we can not determine that trash is enabled server side then bail rather than potentially deleting a file when trash is enabled."
org.apache.hadoop.fs.TrashPolicy "this interface is used for implementing different trash policies. provides factory method to create instances of the configured trash policy. used to setup the trash policy. must be implemented by all trashpolicy implementations the configuration to be used the filesystem to be used the home directory returns whether the trash policy is enabled for this filesystem move a file or directory to the current trash directory. if the item is already in the trash or trash is disabled create a trash checkpoint. delete old trash checkpoint(s). get the current working directory of the trash policy return a {@link runnable} that periodically empties the trash of all get an instance of the configured trashpolicy based on the value of the configuration paramater fs.trash.classname. the configuration to be used the file system to be used the home directory instance of trashpolicy www.apache.org/licenses/license-2.0 the filesystem path to trash directory deletion interval for emptier initialize trashpolicy"
org.apache.hadoop.fs.TrashPolicyDefault "provides a trash feature. files are moved to a format of checkpoint directories used prior to hadoop 0.23. www.apache.org/licenses/license-2.0 make path absolute check that path exists already in trash try twice, in case checkpoint between the mkdirs() & rename() create current if the target path in trash already exists, then append with a current time in millisecs. move to current trash no trash, no checkpoint scan trash sub-directories skip current trash disabled sleep for interval exit on interrupt list all home dirs dump each trash check for old-style checkpoint directories left over after an upgrade from hadoop 1.x"
org.apache.hadoop.fs.UnresolvedLinkException "thrown when a symbolic link is encountered in a path. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.UnsupportedFileSystemException "file system for a given file system name/scheme is not supported constructs exception with the specified detail message. exception message. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.viewfs.ChRootedFileSystem "chrootedfilesystem is a file system with its root some path below the root of its base file system. example: for a base file system hdfs://nn1/ with chroot at /usr/foo, the members will be setup as shown below.  myfs is the base file system and points to hdfs at nn1 myuri is hdfs://nn1/ evolving for a release,to be changed to stable full path including the chroot constructor base file system configuration @throws ioexception called after a new filesystem instance is constructed. a uri whose strip out the root from the path. - fully qualified path p - the remaining path without the begining / @throws ioexception if the p is not prefixed with root 3 choices here: null or / or / www.apache.org/licenses/license-2.0 nn1/ with chroot at /usr/foo, the nn1/ the base uri + the chroot the root below the root of the base we don't use the wd of the myfs note fullpath will check that paths are relative to this filesystem. hence both are in same file system and a rename is valid"
org.apache.hadoop.fs.viewfs.ChRootedFs "chrootedfs is a file system with its root some path below the root of its base file system. example: for a base file system hdfs://nn1/ with chroot at /usr/foo, the members will be setup as shown below.  myfs is the base file system and points to hdfs at nn1 myuri is hdfs://nn1/ evolving for a release,to be changed to stable full path including the chroot we are making uri include the chrootedpath: e.g. file:///chrootedpath. this is questionable since path#makequalified(uri, path) ignores the pathpart of a uri. since this class is internal we can ignore this issue but if we were to make it external then this needs to be resolved. strip out the root from the path. - fully qualified path p - the remaining path without the begining / 3 choices here: return null or / or strip out the root out of myfs's inital wd. only reasonable choice for initialwd for chrooted fds is null we leave the link alone: if qualified or link relative then of course it is okay. if absolute (ie / relative) then the link has to be resolved relative to the changed root. www.apache.org/licenses/license-2.0 nn1/ with chroot at /usr/foo, the nn1/ the base file system whose root is changed the base uri + the chroot the root below the root of the base /chrootedpath. handle the two cases: scheme:/// and scheme:// note fullpath will check that paths are relative to this filesystem. hence both are in same file system and a rename is valid note fullpath will check that paths are relative to this filesystem. hence both are in same file system and a rename is valid"
org.apache.hadoop.fs.viewfs.ConfigUtil "utilities for config variables of the viewfs see {@link viewfs} get the config variable prefix for the specified mount table - the name of the mount table config variable prefix for the specified mount table get the config variable prefix for the default mount table config variable prefix for the default mount table add a link to the config for the specified mount table - add the link to this conf - the src path name - the target uri link add a link to the config for the default mount table - add the link to this conf - the src path name - the target uri link add config variable for homedir for default mount table - add to this conf - the home dir path starting with slash add config variable for homedir the specified mount table - add to this conf - the home dir path starting with slash get the value of the home dir conf value for default mount table - from this conf dir value, null if variable is not in conf get the value of the home dir conf value for specfied mount table - from this conf - the mount table dir value, null if variable is not in conf www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.viewfs.Constants "config variable prefixes for viewfs - see {@link org.apache.hadoop.fs.viewfs.viewfs} for examples. the mount table is specified in the config using these prefixes. see {@link org.apache.hadoop.fs.viewfs.configutil} for convenience lib. prefix for the config variable prefix for the viewfs mount-table prefix for the home dir for the mount table - if not specified then the hadoop default value (/ config variable name for the default mount table. config variable full prefix for the default mount table. config variable for specifying a simple link config variable for specifying a merge link config variable for specifying a merge of the root of the mount-table with the root of another file system. www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.viewfs.InodeTree "inodetree implements a mount-table as a tree of inodes. it is used to implement viewfs and viewfilesystem. in order to use it the caller must subclass it and implement the abstract methods {@link #gettargetfilesystem(inodedir)}, etc. the mountable is initialized from the config variables as specified in {@link viewfs}  is abstractfilesystem or filesystem the three main methods are {@link #inodetreel(configuration)} // constructor {@link #inodetree(configuration, string)} // constructor {@link #resolve(string, boolean)} breaks file path into component names. of names component names internal class for inode tree  internal class to represent an internal dir of the mount table  in internal class to represent a mount link a mount link can be single dir link or a merge dir link. a merge dir link is a merge (junction) of links to dirs: example : <merge of 2 dirs / construct a mergelink construct a simple link (i.e. not a mergelink) get the target of the link if a merge link then it returned as "," separated uri list. below the "public" methods of inodetree the create inode tree from the specified mount-table specified in config - the mount table keys are prefixed with fsconstants.config_viewfs_prefix - the name of the mount table - if null use defaultmt name @throws unsupportedfilesystemexception @throws urisyntaxexception @throws filealreadyexistsexception @throws ioexception resolve returns resolveresult. the caller can continue the resolution of the remainingpath in the targetfilesystem. if the input pathname leads to link to another file system then the targetfilesystem is the one denoted by the link (except it is file system chrooted to link target. if the input pathname leads to an internal mount-table entry then the target file system is one that represents the internal inode. resolve the pathname p relative to root inodedir - inout path which allows further resolution of the remaining path @throws filenotfoundexception dir value from mount table; null if no config value was found. www.apache.org/licenses/license-2.0 constructor constructor the root of the mount table the homedir config value for this mount table the full path to the root file system of this internal directory of mountt   true if mergelink file system object is merge link - use "," as separator between the merged uris string result = targetdirlinklist[0].tostring(); validate that src is valid absolute path ignore first initial slash, process all except last component error - expected a dir but got a link now process the last component add the link in 2 cases: does not exist or a link exists last component directory/link already exists target is list of uris a merge link ignore - we set home dir from config link or merge link /" : ("viewfs://" + vname + "/")); to resolve in the target filesystem isinternaldir of path resolution completed within the mount table to do: - more efficient to not split the path, but simply compare special case for when path is "/" ignore first slash we have resolved to an internal dir in mount table. note we have taken care of when path is "/" above for internal dirs rem-path does not start with / since the lookup that follows will do a children.get(remaningpath) and will have to strip-out the initial /"
org.apache.hadoop.fs.viewfs.NotInMountpointException "notinmountpointexception extends the unsupportedoperationexception. exception class used in cases where the given path is not mounted through viewfs. evolving for a release,to be changed to stable www.apache.org/licenses/license-2.0"
org.apache.hadoop.fs.viewfs.ViewFileSystem "viewfilesystem (extends the filesystem interface) implements a client-side mount table. its spec and implementation is identical to {@link viewfs}. evolving for a release,to be changed to stable prohibits names which contain a ".", "..", ":" or "/" make the path absolute and get the path-part of a pathname. checks that uri matches this file system and that the path-part is a valid name. path -part of the path p this is the constructor with the signature needed by {@link filesystem#createfilesystem(uri, configuration)} after this constructor is called initialize() is called. @throws ioexception return the protocol scheme for the filesystem.  viewfs called after a new filesystem instance is constructed. a uri whose convenience constructor for apps to call directly which must be that of viewfilesystem @throws ioexception convenience constructor for apps to call directly @throws ioexception // alternate 1: renames within same file system - valid but we disallow // alternate 2: (as described in next para - valid but we have disallowed it // // note we compare the uris. the uris include the link targets. // hence we allow renames across mount links as long as the mount links // point to the same target. if (!ressrc.targetfilesystem.geturi().equals( resdst.targetfilesystem.geturi())) { throw new ioexception("renames across mount points not supported"); } an instance of this class represents an internal dir of the viewfs that is internal dir of the mount table. it is a read only mount tables and create, mkdir or delete operations are not allowed. if called on create or mkdir then this target is the parent of the directory in which one is trying to create or mkdir; hence in this case the path name passed in is the last component. otherwise this target is the end point of the path and hence the path name passed in is null. www.apache.org/licenses/license-2.0 the src of the mount target of the mount; multiple targets imply mergemount of the the mount table the the fs state; ie the mount table check for ".." "." ":" "/" now build client side view (i.e. client side mount table) from config. return mergefs.createmergefs(mergefsurilist, config); this validates the path if internal dir or target is a mount link (ie remainingpath is slash) filestatus#getpath is a fully qualified path relative to the root of target file system. we need to change it to viewfs uri - relative to root of mount table. the implementors of rawlocalfilesystem were trying to be very smart. they implement filestatus#getowener lazily -- the object returned is really a rawlocalfilesystem that expect the filestatus#getpath to be unchanged so that it can get owner when needed. hence we need to interpose a new viewfilesystemfilestatus that works around. we need to change the name in the filestatus as described in {@link #getfilestatus } passing resolvelastcomponet as false to catch renaming a mount point to itself. we need to catch this as an internal operation and fail. alternate 1: renames within same file system - valid but we disallow alternate 2: (as described in next para - valid but we have disallowed it  note we compare the uris. the uris include the link targets. hence we allow renames across mount links as long as the mount links point to the same target.  alternate 3 : renames only within the the same mount links.  of the the mount table the note dir starts with / this is the stupid semantics of filesystem noop for viewfs"
org.apache.hadoop.fs.viewfs.ViewFs "viewfs (extends the abstractfilesystem interface) implements a client-side mount table. the viewfs file system is implemented completely in memory on the client side. the client-side mount table allows a client to provide a customized view of a file system namespace that is composed from one or more individual file systems (a localfs or hdfs, s3fs, etc). for example one could have a mount table that provides links such as   / evolving for a release,to be changed to stable this constructor has the signature needed by {@link abstractfilesystem#createfilesystem(uri, configuration)}. which must be that of viewfs @throws ioexception @throws urisyntaxexception // alternate 1: renames within same file system - valid but we disallow // alternate 2: (as described in next para - valid but we have disallowed it // // note we compare the uris. the uris include the link targets. // hence we allow renames across mount links as long as the mount links // point to the same target. if (!ressrc.targetfilesystem.geturi().equals( resdst.targetfilesystem.geturi())) { throw new ioexception("renames across mount points not supported"); } an instance of this class represents an internal dir of the viewfs ie internal dir of the mount table. it is a ready only mount tbale and create, mkdir or delete operations are not allowed. if called on create or mkdir then this target is the parent of the directory in which one is trying to create or mkdir; hence in this case the path name passed in is the last component. otherwise this target is the end point of the path and hence the path name passed in is null. www.apache.org/licenses/license-2.0 nncontaining nnproject1/projects/foo nnproject2/projects/bar nntmp/privatetmpfor / /) along with the nncontaining nnproject1/projects/foo nnproject2/projects/bar nntmp/privatetmpfor sanjaymountable/ nn nn nn99/ nn99/  of the the mount table the the fs state; ie the mount table the src of the mount target of the mount; multiple targets imply mergemount now build client side view (i.e. client side mount table) from config. return mergefs.createmergefs(mergefsurilist, config); if internal dir or target is a mount link (ie remainingpath is slash) filestatus#getpath is a fully qualified path relative to the root of target file system. we need to change it to viewfs uri - relative to root of mount table. the implementors of rawlocalfilesystem were trying to be very smart. they implement filestatus#getowener lazily -- the object returned is really a rawlocalfilesystem that expect the filestatus#getpath to be unchanged so that it can get owner when needed. hence we need to interpose a new viewfsfilestatus that works around. do not follow mount link init we need to change the name in the filestatus as described in {@link #getfilestatus } passing resolvelastcomponet as false to catch renaming a mount point itself we need to catch this as an internal operation and fail. alternate 1: renames within same file system - valid but we disallow alternate 2: (as described in next para - valid but we have disallowed it  note we compare the uris. the uris include the link targets. hence we allow renames across mount links as long as the mount links point to the same target.  alternate 3 : renames only within the the same mount links.  do not follow mount link this is a file system level operations, however viewfs points to many file systems. noop for viewfs. of the the mount table the the uri of the outer viewfs look up i internaldirs children - ignore first slash"
org.apache.hadoop.fs.viewfs.ViewFsFileStatus "this class is needed to address the problem described in {@link viewfilesystem#getfilestatus(org.apache.hadoop.fs.path)} and {@link viewfs#getfilestatus(org.apache.hadoop.fs.path)} www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.ActiveStandbyElector "this class implements a simple library to perform leader election on top of apache zookeeper. using zookeeper as a coordination service, leader election can be performed by atomically creating an ephemeral lock file (znode) on zookeeper. the service instance that successfully creates the znode becomes active and the rest become standbys.  this election mechanism is only efficient for small number of election candidates (order of 10's) because contention on single znode by a large number of candidates can result in zookeeper overload.  the elector does not guarantee fencing (protection of shared resources) among service instances. after it has notified an instance about becoming a leader, then that instance must ensure that it meets the service consistency requirements. if it cannot do so, then it is recommended to quit the election. the application implements the {@link activestandbyelectorcallback} to interact with the elector callback interface to interact with the activestandbyelector object.  the application will be notified with a callback only on state changes (i.e. there will never be successive calls to becomeactive without an intermediate call to enterneutralmode).  the callbacks will be running on zookeeper client library threads. the application should return from these callbacks quickly so as not to impede zookeeper client library performance and notifications. the app will typically remember the state change and return from the callback. it will then proceed with implementing actions around that state change. it is possible to be called back again while these actions are in flight and the app should handle this scenario. this method is called when the app becomes the active leader. if the service fails to become active, it should throw servicefailedexception. this will cause the elector to sleep for a short period, then re-join the election. callback implementations are expected to manage their own timeouts (e.g. when making an rpc to a remote node). this method is called when the app becomes a standby if the elector gets disconnected from zookeeper and does not know about the lock state, then it will notify the service via the enterneutralmode interface. the service may choose to ignore this or stop doing state changing operations. upon reconnection, the elector verifies the leader status and calls back on the becomeactive and becomestandby app interfaces.  zookeeper disconnects can happen due to network issues or loss of zookeeper quorum. thus enterneutralmode can be used to guard against split-brain issues. in such situations it might be prudent to call becomestandby too. however, such state change operations might be expensive and enterneutralmode can help guard against doing that for transient issues. if there is any fatal error (e.g. wrong acl's, unexpected zookeeper errors or zookeeper persistent unavailability) then notifyfatalerror is called to notify the app about it. if an old active has failed, rather than exited gracefully, then the new active may need to take some fencing actions against it before proceeding with failover. the application data provided by the prior active name of the lock znode used by the library. protected for access in test classes create a new activestandbyelector object  the elector is to participate in election, the app will call joinelection. the result will be notified by a callback on either the becomeactive or becomestandby app interfaces.  after this the elector will automatically monitor the leader status and perform re-election if necessary the app could potentially start off in standby mode and ignore the becomestandby call. to be set by the app. non-null data must be set. @throws hadoopillegalargumentexception if valid data is not supplied if the configured parent znode exists utility function to ensure that the configured base znode exists. this recursively creates the znode as well as all of its parents. clear all of the state held within the parent znode. this recursively deletes everything within the znode as well as the parent znode itself. it should only be used when it's certain that no electors are currently participating in the election. any service instance can drop out of the election by calling quitelection.  this will lose any leader status, if held, and stop monitoring of the lock node.  if the instance wants to participate in election again, then it needs to call joinelection().  this allows service instances to take themselves out of rotation for known impending unavailable states (e.g. long gc pause or software upgrade). true if the underlying daemon may need to be fenced if a failover occurs due to dropping out of the election. exception thrown when there is no active leader get data set by the active leader set by the active instance @throws activenotfoundexception when there is no active leader @throws keeperexception other zookeeper operation errors @throws interruptedexception @throws ioexception when zookeeper connection could not be established interface implementation of zookeeper callback for create interface implementation of zookeeper callback for monitor (exists) we failed to become active. re-join the election, but sleep for a few seconds after terminating our existing session, so that other nodes have a chance to become active. the failure to become active is already logged inside becomeactive(). interface implementation of zookeeper watch events (connection and node), proxied by {@link watcherwithclientref}. get a new zookeeper client instance. protected so that test class can inherit and pass in a mock object for zookeeper zookeeper client instance @throws ioexception @throws keeperexception zookeeper connectionloss exception sleep for the given number of milliseconds. this is non-static, and separated out, so that unit tests can override the behavior not to sleep. write the "activebreadcrumb" node, indicating that this node may need to be fenced on failover. try to delete the "activebreadcrumb" node when gracefully giving up active status. if this fails, it will simply warn, since the graceful release behavior is only an optimization. if there is a breadcrumb node indicating that another node may need fencing, try to fence that node. stat of the breadcrumb node that was read, or null if no breadcrumb node existed the callbacks and watchers pass a reference to the zk client which made the original call. we don't want to take action based on any callbacks from prior clients after we quit the election. the zk client passed into the watcher if it matches the current client watcher implementation which keeps a reference around to the original zk connection, and passes it back along with any events. latch fired whenever any event arrives. this is used in order to wait for the connected event when the client is first latch used to wait until the reference to zookeeper is set. waits for the next event from zookeeper to arrive. zookeeper connection timeout in milliseconds @throws keeperexception if the connection attempt times out. this will be a zookeeper connectionloss exception code. @throws ioexception if interrupted while connecting to zookeeper www.apache.org/licenses/license-2.0 createconnection for future api calls this is ok - just ensuring existence. if active is gracefully going back to standby mode, remove our permanent znode so no one fences us. handle the commonly expected cases that make sense for us we successfully znode exists and we did not retry the operation. so a different instance has if we had retried then the znode could have been attempt to the server (that we lost) and this node exists response is for the second attempt. verify this case via ephemeral node owner. this will happen on the callback for monitoring the lock. this isn't fatal - the client watcher will re-join the election the following owner check completes verification in case the lock znode creation was retried we own the lock znode. so we are the leader we dont own the lock znode. so we are a standby. the watch set by us will notify about changes the lock znode disappeared before we started monitoring it this isn't fatal - the client watcher will re-join the election the connection state has changed if the listener was asked to move to safe state then it needs to be undone ask the app to move to safe state because zookeeper connection is not active and we dont know our state the connection got terminated because of session timeout call listener to reconnect a watch on lock path in zookeeper has fired. so something has changed on the lock. ideally we should check that the path is the same as the lock path but trusting zookeeper for now some unexpected error has occurred unfortunately, the zookeeper constructor connects to zookeeper and may trigger the connected event immediately. so, if we register the watcher after constructing zookeeper, we may miss that event. instead, we construct the watcher first, and have it block any events it receives before we can set its zookeeper reference. wait for the asynchronous success/failure. this may throw an exception if we don't connect within the session timeout. some of the test cases rely on expiring the zk sessions and ensuring that the other node takes over. but, there's a race where the original lease holder could reconnect faster than the other thread manages to take the lock itself. this lock allows the tests to block the reconnection. it's a shame that this leaked into non-test code, but the lock is only acquired here so will never be contended. already active caller will handle quitting and rejoining the election. no previous active, just create the node there was a previous active, update the node sanity check the data. this shouldn't be strictly necessary, but better to play it safe. if we failed to read for any other reason, then likely we lost our session, or we don't have permissions, etc. in any case, we probably shouldn't become active, and failing the whole thing is the best bet."
org.apache.hadoop.ha.BadFencingConfigurationException "indicates that the operator has specified an invalid configuration for fencing methods. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.FailoverController "the failovercontroller is responsible for electing an active service on startup or when the current active is changing (eg due to failure), monitoring the health of a service, and performing a fail-over when a new active service is either manually selected by a need a copy of conf for graceful fence to set configurable retries for ipc client. refer hdfs-3561 perform pre-failover checks on the given service we plan to failover to, eg to prevent failing over to a service (eg due to it being inaccessible, already active, not healthy, etc). an option to ignore tosvc if it claims it is not ready to become active is provided in case performing a failover will allow it to become active, eg because it triggers a log roll so the standby can learn about new blocks and leave safemode. currently active service service to make active ignore tosvc if it reports that it is not ready @throws failoverfailedexception if we should avoid failover try to get the ha state of the node at the given address. this function is guaranteed to be "quick" -- ie it has a short timeout and no retries. its only purpose is to avoid fencing a node that has already restarted. failover from service 1 to service 2. if the failover fails then try to failback. currently active service service to make active to fence fromsvc even if not strictly necessary try to make tosvc active even if it is not ready @throws failoverfailedexception if the failover fails www.apache.org/licenses/license-2.0 configure less retries for graceful fence try to make fromsvc standby fence fromsvc if it's required or forced by the try to make tosvc active we failed to make tosvc active only try to failback if we didn't fence fromsvc unconditionally fence tosvc in case it is still trying to become active, eg we timed out waiting for its response. unconditionally force fromsvc to become active since it was previously active when we initiated failover."
org.apache.hadoop.ha.FailoverFailedException "exception thrown to indicate service failover has failed. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.FenceMethod "a fencing method is a method by which one node can forcibly prevent another node from making continued progress. this might be implemented by killing a process on the other node, by denying the other node's access to shared storage, or by accessing a pdu to cut the other node's power.  since these methods are often vendor- or device-specific, operators may implement this interface in order to achieve fencing.  fencing is configured by the operator as an ordered list of methods to attempt. each method will be tried in turn, and the next in the list will only be attempted if the previous one fails. see {@link nodefencer} for more information.  if an implementation also implements {@link configurable} then its setconf method will be called upon instantiation. verify that the given fencing method's arguments are valid. the arguments provided in the configuration. this may be null if the operator did not configure any arguments. @throws badfencingconfigurationexception if the arguments are invalid attempt to fence the target node. the address (host:ipcport) of the service to fence the configured arguments, which were checked at startup by {@link #checkargs(string)} if fencing was successful, false if unsuccessful or indeterminate @throws badfencingconfigurationexception if the configuration was determined to be invalid only at runtime www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.HAAdmin "a command-line tool for making calls in the haserviceprotocol. for example,. this can be used to force a service to standby or active mode, or to trigger a health-check. undocumented flag which allows an administrator to use manual failover state transitions even when auto-failover is enabled. this is an unsafe operation, which is why it is not documented in the usage below. output stream for errors, for use in tests ensure that we are allowed to manually manage the ha state of the target service. if automatic failover is configured, then the automatic failover controllers should be doing state management, and it is generally an error to use the haadmin command line to do so. the target to check if manual state management is allowed initiate a graceful failover by talking to the target node's zkfc. this sends an rpc to the zkfc, which coordinates the failover. the node to fail to code (0 for success) @throws ioexception if failover does not succeed return the serviceid as is, we are assuming it was given as a service address of form . add cli options which are specific to the failover command and no others. www.apache.org/licenses/license-2.0 check that auto-failover is consistently configured for both nodes. -forceactive doesn't make sense with auto-ha, since, if the node is not healthy, then its zkfc will immediately quit the election again the next time a health check runs.  -forcefence doesn't seem to have any real use cases with auto-ha so it isn't implemented. add command-specific options mutative commands take forcemanual option error already printed instruct the nns to honor this request even if they're configured for manual failover. we already checked command validity above, so getting here would be a coding error don't add forcemanual, since that's added separately for all commands that change state. strip off the first arg, since that's just the command name only -help"
org.apache.hadoop.ha.HAServiceProtocol "protocol interface that provides high availability related primitives to monitor and fail-over the service. this interface could be used by ha frameworks to manage the service. initial version of the protocol an ha service may be in active or standby state. during startup, it is in an unknown initializing state. information describing the source for a request to change state. this is used to differentiate requests from automatic vs cli failover controllers, and in the future may include epoch information. monitor the health of service. this periodically called by the ha frameworks to monitor the health of the service. service is expected to perform checks to ensure it is functional. if the service is not healthy due to failure or partial failure, it is expected to throw {@link healthcheckfailedexception}. the definition of service not healthy is left to the service. note that when health check of an active service fails, failover to standby may be done. @throws healthcheckfailedexception if the health check of a service fails. @throws accesscontrolexception if access is denied. @throws ioexception if other errors happen request service to transition to active state. no operation, if the service is already in active state. @throws servicefailedexception if transition from standby to active fails. @throws accesscontrolexception if access is denied. @throws ioexception if other errors happen request service to transition to standby state. no operation, if the service is already in standby state. @throws servicefailedexception if transition from active to standby fails. @throws accesscontrolexception if access is denied. @throws ioexception if other errors happen return the current status of the service. the status indicates the current state (e.g active/standby) as well as some additional information. {@see haservicestatus} @throws accesscontrolexception if access is denied. @throws ioexception if other errors happen www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.HAServiceProtocolHelper "helper for making {@link haserviceprotocol} rpc calls. this helper unwraps the {@link remoteexception} to specific exceptions. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.HAServiceStatus "www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.HAServiceTarget "represents a target of the client side ha administration commands. ipc address of the target node. ipc address of the zkfc on the target node fencer implementation configured for this target node @throws badfencingconfigurationexception if the fencing configuration appears to be invalid. this is divorced from the above {@link #getfencer()} method so that the configuration can be checked during the pre-flight phase of failover. proxy to connect to the target ha service. proxy to the zkfc which is associated with this ha service. hook to allow subclasses to add any parameters they would like to expose to fencing implementations/scripts. fencing methods are free to use this map as they see fit -- notably, the shell script implementation takes each entry, prepends 'target_', substitutes '_' for '.', and adds it to the environment of the script. subclass implementations should be sure to delegate to the superclass implementation as well as adding their own keys. map which can be mutated to pass parameters to the fencer if auto failover should be considered enabled www.apache.org/licenses/license-2.0 lower the timeout so we quickly fail to connect lower the timeout so we quickly fail to connect"
org.apache.hadoop.ha.HAZKUtil "utilities for working with zookeeper. parse acl permission string, partially borrowed from zookeepermain private method parse comma separated list of acl entries to secure generated nodes, e.g. sasl:hdfs/host1@my.domain:cdrwa,sasl:hdfs/host2@my.domain:cdrwa list @throws hadoopillegalargumentexception if an acl is invalid parse a comma-separated list of authentication mechanisms. each such mechanism should be of the form 'scheme:auth' -- the same syntax used for the 'addauth' command in the zk cli. the comma-separated auth mechanisms list of parsed authentications because zk acls and authentication information may be secret, allow the configuration values to be indirected through a file by specifying the configuration as "@/path/to/file". if this syntax is used, this function will return the contents of the file as a string. the value from the configuration the same value, or the contents of the referenced file if the configured value starts with "@" @throws ioexception if the file cannot be read an authentication token passed to zookeeper.addauthinfo www.apache.org/licenses/license-2.0 from zookeepermain private method"
org.apache.hadoop.ha.HealthCheckFailedException "exception thrown to indicate that health check of a service failed. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.HealthMonitor "this class is a daemon which runs in a loop, periodically heartbeating with an ha service. it is responsible for keeping track of that service's health and exposing callbacks to the failover controller when the health status changes. classes which need callbacks should implement the {@link callback} interface. the connected proxy the ha service to monitor listeners for state changes the health monitor is still starting up. the service is not responding to health check rpcs. the service is connected and healthy. the service is running but unhealthy. the health monitor itself failed unrecoverably and can no longer provide accurate information. current proxy object to the underlying service. note that this may return null in the case that the service is not responding. also note that, even if the last indicated state is healthy, the service may have gone down in the meantime. connect to the service to be monitored. stubbed out for easier testing. callback interface for state change events. this interface is called from a single thread which also performs the health monitoring. if the callback processing takes a long time, no further health checks will be made during this period, nor will other registered callbacks be called. if the callback itself throws an unchecked exception, no other callbacks following it will be called, and the health monitor will terminate, entering health_monitor_failed state. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.NodeFencer "this class parses the configured list of fencing methods, and is responsible for trying each one in turn while logging informative output. the fencing methods are configured as a carriage-return separated list. each line in the list is of the form: com.example.foo.mymethod(arg string) or com.example.foo.mymethod the class provided must implement the {@link fencemethod} interface. the fencing methods that ship with hadoop may also be referred to by shortened names:  shell(/path/to/some/script.sh args...) sshfence(...) (see {@link sshfencebytcpport})  standard fencing methods included with hadoop. www.apache.org/licenses/license-2.0 see if it's a short name for one of the built-in methods try to instantiate the check that it implements the right interface"
org.apache.hadoop.ha.proto.HAServiceProtocolProtos "generated by the protocol buffer compiler. do not edit! source: haserviceprotocol.proto @@protoc_insertion_point(enum_scope:hadoop.common.haservicestateproto) @@protoc_insertion_point(enum_scope:hadoop.common.harequestsource) required .hadoop.common.harequestsource reqsource = 1; use hastatechangerequestinfoproto.newbuilder() to construct. required .hadoop.common.harequestsource reqsource = 1; construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.hastatechangerequestinfoproto.newbuilder() required .hadoop.common.harequestsource reqsource = 1; @@protoc_insertion_point(builder_scope:hadoop.common.hastatechangerequestinfoproto) @@protoc_insertion_point(class_scope:hadoop.common.hastatechangerequestinfoproto) use monitorhealthrequestproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.monitorhealthrequestproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.monitorhealthrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.monitorhealthrequestproto) use monitorhealthresponseproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.monitorhealthresponseproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.monitorhealthresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.monitorhealthresponseproto) required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1; use transitiontoactiverequestproto.newbuilder() to construct. required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1; construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.transitiontoactiverequestproto.newbuilder() required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1; @@protoc_insertion_point(builder_scope:hadoop.common.transitiontoactiverequestproto) @@protoc_insertion_point(class_scope:hadoop.common.transitiontoactiverequestproto) use transitiontoactiveresponseproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.transitiontoactiveresponseproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.transitiontoactiveresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.transitiontoactiveresponseproto) required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1; use transitiontostandbyrequestproto.newbuilder() to construct. required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1; construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.transitiontostandbyrequestproto.newbuilder() required .hadoop.common.hastatechangerequestinfoproto reqinfo = 1; @@protoc_insertion_point(builder_scope:hadoop.common.transitiontostandbyrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.transitiontostandbyrequestproto) use transitiontostandbyresponseproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.transitiontostandbyresponseproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.transitiontostandbyresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.transitiontostandbyresponseproto) use getservicestatusrequestproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.getservicestatusrequestproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.getservicestatusrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.getservicestatusrequestproto) required .hadoop.common.haservicestateproto state = 1; optional bool readytobecomeactive = 2; optional string notreadyreason = 3; use getservicestatusresponseproto.newbuilder() to construct. required .hadoop.common.haservicestateproto state = 1; optional bool readytobecomeactive = 2; optional string notreadyreason = 3; construct using org.apache.hadoop.ha.proto.haserviceprotocolprotos.getservicestatusresponseproto.newbuilder() required .hadoop.common.haservicestateproto state = 1; optional bool readytobecomeactive = 2; optional string notreadyreason = 3; @@protoc_insertion_point(builder_scope:hadoop.common.getservicestatusresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.getservicestatusresponseproto) @@protoc_insertion_point(outer_class_scope)"
org.apache.hadoop.ha.proto.ZKFCProtocolProtos "generated by the protocol buffer compiler. do not edit! source: zkfcprotocol.proto required uint32 millistocede = 1; use cedeactiverequestproto.newbuilder() to construct. required uint32 millistocede = 1; construct using org.apache.hadoop.ha.proto.zkfcprotocolprotos.cedeactiverequestproto.newbuilder() required uint32 millistocede = 1; @@protoc_insertion_point(builder_scope:hadoop.common.cedeactiverequestproto) @@protoc_insertion_point(class_scope:hadoop.common.cedeactiverequestproto) use cedeactiveresponseproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.zkfcprotocolprotos.cedeactiveresponseproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.cedeactiveresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.cedeactiveresponseproto) use gracefulfailoverrequestproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.zkfcprotocolprotos.gracefulfailoverrequestproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.gracefulfailoverrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.gracefulfailoverrequestproto) use gracefulfailoverresponseproto.newbuilder() to construct. construct using org.apache.hadoop.ha.proto.zkfcprotocolprotos.gracefulfailoverresponseproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.gracefulfailoverresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.gracefulfailoverresponseproto) @@protoc_insertion_point(outer_class_scope)"
org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB "this class is the client side translator to translate the requests made on {@link haserviceprotocol} interfaces to the rpc server implementing {@link haserviceprotocolpb}. rpccontroller is not used and hence is set to null www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB "if any methods need annotation, it can be added here www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB "this class is used on the server side. calls come across the wire for the for protocol {@link haserviceprotocolpb}. this class translates the pb data types to the native data types used inside the nn as specified in the generic clientprotocol. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB "www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.protocolPB.ZKFCProtocolPB "if any methods need annotation, it can be added here www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB "www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.ServiceFailedException "exception thrown to indicate that an operation performed to modify the state of a service or application failed. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.ShellCommandFencer "fencing method that runs a shell command. it should be specified in the fencing configuration like:  shell(/path/to/my/script.sh arg1 arg2 ...)  the string between '(' and ')' is passed directly to a bash shell and may not include any closing parentheses. the shell command will be run with an environment set up to contain all of the current hadoop configuration variables, with the '_' character replacing any '.' characters in the configuration keys. if the shell command returns an exit code of 0, the fencing is determined to be successful. if it returns any other exit code, the fencing was not successful and the next fencing method in the list will be attempted. note: this fencing method does not implement any timeout. if timeouts are necessary, they should be implemented in the shell script itself (eg by forking a subshell to kill its parent in some number of seconds). length at which to abbreviate command in long messages prefix for target parameters added to the environment abbreviate a string by putting '...' in the middle of it, in an attempt to keep logs from getting too messy. the string to abbreviate maximum length to abbreviate to string attempt to use evil reflection tricks to determine the pid of a launched process. this is helpful to ops if debugging a fencing process that might have gone wrong. if running on a system or jvm where this doesn't work, it will simply return null. set the environment of the subprocess to be the configuration, with '.'s replaced by '_'s. add information about the target to the the environment of the subprocess. www.apache.org/licenses/license-2.0 nothing else we can really check without actually running the command pump logs to stderr"
org.apache.hadoop.ha.SshFenceByTcpPort "this fencing implementation sshes to the target node and uses f verify that the argument, if given, in the conf is parseable. execute a command through the ssh session, pumping its stderr and stdout to our own logs. container for the parsed arg line for this fencing method. adapter from jsch's logger interface to our log4j www.apache.org/licenses/license-2.0 exit code 0 indicates the process was successfully killed. exit code 1 indicates either that the process was not running or that f (eg running as a different the service is still listening - we are unable to fence other pump stdout of the command to our warn logs pump stderr of the command to our warn logs parse optional"
org.apache.hadoop.ha.StreamPumper "class responsible for pumping the streams of the subprocess out to log4j. stderr is pumped to warn level and stdout is pumped to info level www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.ZKFailoverController "all of the conf keys used by the zkfc. this is used in order to allow them to be overridden on a per-nameservice or per-namenode basis. unable to format the parent znode in zk the parent znode doesn't exist in zk fencing is not properly configured automatic failover is not enabled cannot connect to zookeeper set if a fatal error occurs a future nanotime before which the zkfc will not join the election. this is used during graceful failover. executor on which {@link #schedulerecheck(long)} schedules events return the name of a znode inside the configured parent znode in which the zkfc will do all of its work. this is so that multiple federated nameservices can run on the same zk quorum without having to manually configure them to separate subdirectories. todo: we need to make sure that if we get fenced and then quickly restarted, none of these calls will retry across the restart boundary perhaps the solution is that, whenever the nn starts, it gets a unique id, and when we start becoming active, we record it, and then any future calls use the same id store the results of the last attempt to become active. this is used so that, during manually initiated failover, we can report back the results of the attempt to become active to the initiator of the failover. wait until one of the following events:  another thread publishes the results of an attempt to become active using {@link #recordactiveattempt(activeattemptrecord)} the node enters bad health status the specified timeout elapses  number of millis to wait published record, or null if the timeout elapses or the service becomes unhealthy @throws interruptedexception if the thread is interrupted. request from graceful failover to cede active role. causes this zkfc to transition its local node to standby, then quit the election for the specified period of time, after which it will rejoin iff it is healthy. coordinate a graceful failover to this node. @throws servicefailedexception if the node fails to become active @throws ioexception some other error occurs coordinate a graceful failover. this proceeds in several phases: 1) pre-flight checks: ensure that the local node is healthy, and thus a candidate for failover. 2) determine the current active node. if it is the local node, no need to failover - return success. 3) ask that node to yield from the election for a number of seconds. 4) allow the normal election path to run in other threads. wait until we either become unhealthy or we see an election attempt recorded by the normal code path. 5) allow the old active to rejoin the election, so a future failback is possible. ensure that the local node is in a healthy state, and thus eligible for graceful failover. @throws servicefailedexception if the node is unhealthy {@link haservicetarget} for the current active node in the cluster, or null if no node is active. @throws ioexception if a zk-related issue occurs @throws interruptedexception if thread is interrupted check the current state of the service, and join the election if it should be in the election. schedule a call to {@link #recheckelectability()} in the future. last health state passed to the fc by the healthmonitor. callbacks from elector callbacks from healthmonitor www.apache.org/licenses/license-2.0 ------------------------------------------ begin actual guts of failover controller ------------------------------------------ parse acls from configuration. parse authentication from configuration. sanity check configuration. only get here on fatal periodically check health state, because entering an unhealthy state could prevent us from ever attempting to become active. we can detect this and respond to the immediately. early out if service became unhealthy only wait 1sec so that we periodically recheck the health state above. timeout elapsed. todo handle this. it's a likely case since we probably got fenced at the same time. it's possible that it's in standby but just about to go into active, no? is there some race here? lock elector to maintain lock ordering of elector -> zkfc phase 1: pre-flight checks phase 2: determine old/current active node. check that we're not ourselves active, etc. no node is currently active. so, if we aren't already active ourselves by means of a normal election, then there's probably something preventing us from becoming active. phase 3: ask the old active to yield from the election. phase 4: wait for the normal election to make the local node active. we didn't even make an attempt to become active. phase 5. at this point, we made some attempt to become active. so we can tell the old active to rejoin if it wants. this allows a quick fail-back if we immediately crash. propagate failure check health maintain lock ordering of elector -> zkfc"
org.apache.hadoop.ha.ZKFCProtocol "protocol exposed by the zkfailovercontroller, allowing for graceful failover. initial version of the protocol request that this service yield from the active node election for the specified time period. if the node is not currently active, it simply prevents any attempts to become active for the specified time period. otherwise, it first tries to transition the local service to standby state, and then quits the election. if the attempt to transition to standby succeeds, then the zkfc receiving this rpc will delete its own breadcrumb node in zookeeper. thus, the next node to become active will not run any fencing process. otherwise, the breadcrumb will be left, such that the next active will fence this node. after the specified time period elapses, the node will attempt to re-join the election, provided that its service is healthy. if the node has previously been instructed to cede active, and is still within the specified time period, the later command's time period will take precedence, resetting the timer. a call to cedeactive which specifies a 0 or negative time period will allow the target node to immediately rejoin the election, so long as it is healthy. period for which the node should not attempt to become active @throws ioexception if the operation fails @throws accesscontrolexception if the operation is disallowed request that this node try to become active through a graceful failover. if the node is already active, this is a no-op and simply returns success without taking any further action. if the node is not healthy, it will throw an exception indicating that it is not able to become active. if the node is healthy and not active, it will try to initiate a graceful failover to become active, returning only when it has successfully become active. see {@link zkfailovercontroller#gracefulfailovertoyou()} for the implementation details. if the node fails to successfully coordinate the failover, throws an exception indicating the reason for failure. @throws ioexception if graceful failover fails @throws accesscontrolexception if the operation is disallowed www.apache.org/licenses/license-2.0"
org.apache.hadoop.ha.ZKFCRpcServer "www.apache.org/licenses/license-2.0 set service-level"
org.apache.hadoop.HadoopIllegalArgumentException "indicates that a method has been passed illegal or invalid argument. this exception is thrown instead of illegalargumentexception to differentiate the exception thrown in hadoop implementation from the one thrown in jdk. constructs exception with the specified detail message. detailed message. www.apache.org/licenses/license-2.0"
org.apache.hadoop.HadoopVersionAnnotation "a package attribute that captures the version of hadoop that was compiled. get the hadoop version version string "0.6.3-dev" get the get the date when hadoop was compiled. date in unix 'date' format get the url for the subversion repository. get the subversion revision. revision number as a string (eg. "451451") get the branch from which this was compiled. branch name, e.g. "trunk" or "branches/branch-0.20" get a checksum of the source files from which hadoop was compiled. string that uniquely identifies the source www.apache.org/licenses/license-2.0"
org.apache.hadoop.http.AdminAuthorizedServlet "general servlet which is admin- www.apache.org/licenses/license-2.0 do the"
org.apache.hadoop.http.FilterContainer "a container class for javax.servlet.filter. add a filter to the container. filter name filter class name a map from parameter names to initial values add a global filter to the container. filter name filter class name a map from parameter names to initial values www.apache.org/licenses/license-2.0"
org.apache.hadoop.http.FilterInitializer "initialize a javax.servlet.filter. initialize a filter to a filtercontainer. the filter container configuration for run-time parameters www.apache.org/licenses/license-2.0"
org.apache.hadoop.http.HtmlQuoting "this class is responsible for quoting html characters. does the given string need to be quoted? the string to check the starting position the number of bytes to check the string contain any of the active html characters? does the given string need to be quoted? the string to check the string contain any of the active html characters? quote all of the active html characters in the given string as they are added to the buffer. the stream to write the output to the byte array to take the characters from the index of the first byte to quote the number of bytes to quote quote the given item to make it html-safe. the string to quote quoted string return an output stream that quotes all of the output. the stream to write the quoted output to new stream that the application show write to @throws ioexception if the underlying output fails remove html quoting from a string. the string to unquote unquoted string www.apache.org/licenses/license-2.0 won't happen, since it is a bytearrayoutputstream nothing was quoted"
org.apache.hadoop.http.HttpConfig "singleton to get access to http related configuration. www.apache.org/licenses/license-2.0 " : "http://";"
org.apache.hadoop.http.HttpServer "create a jetty embedded server to answer http requests. the primary goal is to serve up status information for the server. there are three contexts: "/logs/" -> points to the log directory "/static/" -> points to common static files (src/webapps/static) "/" -> the jsp server code from (src/webapps/) same as this(name, bindaddress, port, findport, null); create a status server on the given port. allows you to specify the path specifications that this server will be serving so that they will be added to the filters properly. the name of the server the address for this server the port to use on the server whether the server should start at the given port and increment by 1 until it finds a free port. configuration path specifications that this httpserver will be serving. these will be added to any filters. create a status server on the given port. the jsp scripts are taken from src/webapps/. the name of the server the port to use on the server whether the server should start at the given port and increment by 1 until it finds a free port. configuration create a status server on the given port. the jsp scripts are taken from src/webapps/. the name of the server the address for this server the port to use on the server whether the server should start at the given port and increment by 1 until it finds a free port. configuration {@link accesscontrollist} of the admins create a status server on the given port. the jsp scripts are taken from src/webapps/. the name of the server the address for this server the port to use on the server whether the server should start at the given port and increment by 1 until it finds a free port. configuration {@link accesscontrollist} of the admins a jetty connection listener path specifications that this httpserver will be serving. these will be added to any filters. "}); } / create a required listener for the jetty instance listening on the port provided. this wrapper and all subclasses must create at least one listener. get an array of filterconfiguration specified in the conf add default apps. the application directory @throws ioexception "); if (conf.getboolean( commonconfigurationkeys.hadoop_jetty_logs_serve_aliases, commonconfigurationkeys.default_hadoop_jetty_logs_serve_aliases)) { logcontext.getinitparams().put( "org.mortbay.jetty.servlet.default.aliases", "true"); } logcontext.setdisplayname("logs"); setcontextattributes(logcontext, conf); addnocachefilter(webappcontext); defaultcontexts.put(logcontext, true); } // set up the context for "/static/" context staticcontext = new context(parent, "/static"); staticcontext.setresourcebase(appdir + "/static"); staticcontext.addservlet(defaultservlet.class, "/"); staticcontext.setdisplayname("static"); setcontextattributes(staticcontext, conf); defaultcontexts.put(staticcontext, true); } private void setcontextattributes(context context, configuration conf) { context.getservletcontext().setattribute(conf_context_attribute, conf); context.getservletcontext().setattribute(admins_acl, adminsacl); } / add default servlets. add a context the path spec for the context the directory containing the context if true, the servlet is added to the filter path mapping @throws ioexception set a value in the webapp context. these values are available to the jsp pages as "application.getattribute(name)". the name of the attribute the value of the attribute add a jersey resource package. the java package name containing the jersey resource. the path spec for the servlet add a servlet in the server. the name of the servlet (can be passed as null) the path spec for the servlet the servlet class add an internal servlet in the server. note: this method is to be used for adding servlets that facilitate internal communication and not for add an internal servlet in the server, specifying whether or not to protect with kerberos authentication. note: this method is to be used for adding servlets that facilitate internal communication and not for " }; for (map.entry e : defaultcontexts.entryset()) { if (e.getvalue()) { context ctx = e.getkey(); definefilter(ctx, name, classname, parameters, all_urls); log.info("added filter " + name + " (class=" + classname + ") to context " + ctx.getdisplayname()); } } filternames.add(name); } @override public void addglobalfilter(string name, string classname, map parameters) { final string[] all_urls = { "/" }; definefilter(webappcontext, name, classname, parameters, all_urls); for (context ctx : defaultcontexts.keyset()) { definefilter(ctx, name, classname, parameters, all_urls); } log.info("added global filter '" + name + "' (class=" + classname + ")"); } / define a filter for a context and set up default url mappings. add the path spec to the filter path mapping. the path spec the webapplicationcontext to add to get the value in the webapp context. the name of the attribute value of the attribute get the pathname to the webapps files. eg "secondary" or "datanode" pathname as a url @throws filenotfoundexception if 'webapps' directory cannot be found on classpath. get the port that the server is on port set the min, max number of worker threads (simultaneous connections). configure an ssl listener on the server. address to listen on location of the keystore password for the keystore password for the key @deprecated use {@link #addssllistener(inetsocketaddress, configuration, boolean)} configure an ssl listener on the server. address to listen on conf to retrieve ssl options whether x509 certificate authentication is required start the server. does not wait for the server to start. open the main listener for the server @throws exception return the bind address of the listener. of the listener stop the server test for the availability of the web server if the web server is started, false otherwise return the host and port of the httpserver, if live classname and any http url checks the does the get the admin acls from the given servletcontext and check if the given a very simple servlet to serve up a text representation of the current stack traces. it both returns the stacks to the caller and logs them. currently the stack traces are done sequentially rather than exactly the same data. a servlet input filter that quotes all html active characters in the parameter names and values. the goal is to quote the characters to make all of the servlets resistant to cross-site scripting attacks. return the set of parameter names, quoting each name. unquote the name and quote the value. quote the url so that quote the server name so that infer the mime type for the response based on the extension of the request uri. returns null if unknown. www.apache.org/licenses/license-2.0 the servletcontext attribute where the daemon configuration gets stored. if http_max_threads is not configured, queuethreadpool() will use the default value (currently 250). set up the context for "/logs/" if "hadoop.log.dir" property is defined. set up the context for "/static/" set up default servlets setting up ssl truststore for authenticating clients make sure there is no handler failures. make sure there are no errors initializing the context. have to stop the webserver, or else its non-daemon threads will hang forever. it's already bound expect that listener was started securely jetty has a bug where you can't reopen a listener that previously failed to open w/o issuing a close first, even if the port is changed try the next port number not bound, return requested port clear & stop webappcontext attributes to avoid memory leaks. " + listener.gethost() + ":" + listener.getlocalport() + "/" if there is no html with unspecified encoding, we want to force html with utf-8 encoding this is to avoid the following security issue: http://openmya.hacker.jp/hasegawa/security/utf7cs.html"
org.apache.hadoop.http.lib.StaticUserWebFilter "provides a servlet filter that pretends to authenticate a fake retrieve the static www.apache.org/licenses/license-2.0 nothing if the we can't use the normal configuration deprecation mechanism here since we need to split out the"
org.apache.hadoop.http.NoCacheFilter "www.apache.org/licenses/license-2.0"
org.apache.hadoop.http.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.AbstractMapWritable "abstract base class for mapwritable and sortedmapwritable unlike org.apache.nutch.crawl.mapwritable, this class allows creation of mapwritable&lt;writable, mapwritable&gt; so the class_to_id and id_to_class maps travel with the class instead of being static. class ids range from 1 to 127 so there can be at most 127 distinct classes in any specific map instance. class to id mappings id to class mappings the number of new classes (those not established by the constructor) number of known classes used to add "predefined" classes and by writable to copy "new" classes. add a class to the maps if it is not already present. class class for the specified id id for the specified class used by child copy constructors. constructor. conf the conf to set www.apache.org/licenses/license-2.0 utf8 is deprecated so we don't support it first write out the size of the class table and any classes that are "unknown" classes get the number of "unknown" classes then read in the class names and add them to our tables"
org.apache.hadoop.io.ArrayFile "a dense file-based mapping from integers to values. write a new array file. create the named file for values of the named class. create the named file for values of the named class. append a value to the file. provide access to an existing array file. construct an array reader for the named file. positions the reader before its nth value. read and return the next value in the file. returns the key associated with the most recent call to {@link #seek(long)}, {@link #next(writable)}, or {@link #get(long,writable)}. return the nth value in the file. www.apache.org/licenses/license-2.0 no public ctor add to map increment count"
org.apache.hadoop.io.ArrayPrimitiveWritable "this is a wrapper class. it wraps a writable implementation around an array of primitives (e.g., int[], long[], etc.), with optimized wire format, and without creating new objects per element. this is a wrapper class only; it does not make a copy of the underlying array. construct an empty instance, for use during writable read construct an instance of known type but no value yet for use with type-specific wrapper classes wrap an existing array of primitives - array of primitives get the original array. client must cast it back to type componenttype[] (or may use type-specific wrapper classes). - original array as object do not use this class. this is an internal class, purely for objectwritable to use as a label class for transparent conversions of arrays of primitives during wire protocol reads and writes. @see org.apache.hadoop.io.writable#write(java.io.dataoutput) @see org.apache.hadoop.io.writable#readfields(java.io.datainput) www.apache.org/licenses/license-2.0 componenttype is determined from the component type of the value array during a "set" operation. it must be primitive. declaredcomponenttype need not be declared, but if you do (by using the arrayprimitivewritable(class) constructor), it will provide typechecking for all "set" operations. must be an array of [length] empty constructor use for reads use for writes end internal subclass declaration write componenttype write length do the inner loop. walk the decision tree only once. boolean char byte short int long float double read and set the component type of the array read and set the length of the array construct and read in the array do the inner loop. walk the decision tree only once. boolean char byte short int long float double for efficient implementation, there's no way around the following massive code duplication."
org.apache.hadoop.io.ArrayWritable "a writable for arrays containing instances of a class. the elements of this writable must all be instances of the same class. if this writable will be the input for a reducer, you will need to create a subclass that sets the value to be of the proper type. for example:  public class intarraywritable extends arraywritable { public intarraywritable() { super(intwritable.class); } }  www.apache.org/licenses/license-2.0 construct values read a value store it in values write values"
org.apache.hadoop.io.BinaryComparable "interface supported by {@link org.apache.hadoop.io.writablecomparable} types supporting ordering/permutation by a representative set of bytes. return n st bytes 0..n-1 from {#getbytes()} are valid. return representative byte array for this instance. compare bytes from {#getbytes()}. @see org.apache.hadoop.io.writablecomparator#comparebytes(byte[],int,int,byte[],int,int) compare bytes from {#getbytes()} to those provided. return true if bytes from {#getbytes()} match. return a hash of the bytes returned from {#getbytes()}. @see org.apache.hadoop.io.writablecomparator#hashbytes(byte[],int) www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.BloomMapFile "this class extends {@link mapfile} and provides very much the same functionality. however, it uses dynamic bloom filters to provide quick membership test for keys, and it offers a fast version of {@link reader#get(writablecomparable, writable)} operation, especially in case of sparsely populated mapfile-s. checks if this mapfile has the indicated key. the membership test is performed using a bloom filter, so the result has always non-zero probability of false positives. key to check false iff key doesn't exist, true if key probably exists. @throws ioexception fast version of the {@link mapfile.reader#get(writablecomparable, writable)} method. first it checks the bloom filter for the existence of the key, and only if present it performs the real get operation. this yields significant performance improvements for get operations on sparsely populated files. retrieve the bloom filter used by this instance of the reader. bloom filter (see {@link filter}) www.apache.org/licenses/license-2.0 vector size should be -kn / (ln(1 - c^(1/k))) bits for single key, where  is the number of hash functions, n is the number of keys and c is the desired max. error rate. our desired error rate is by default 0.005, i.e. 0.5%"
org.apache.hadoop.io.BooleanWritable "a writablecomparable for booleans.   set the value of the booleanwritable returns the value of the booleanwritable     a comparator optimized for booleanwritable. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.BoundedByteArrayOutputStream "a byte array backed output stream with a limit. the limit should be smaller than the buffer capacity. the object can be reused through reset api and choose different limits in each round. create a boundedbytearrayoutputstream with the specified capacity the capacity of the underlying byte array create a boundedbytearrayoutputstream with the specified capacity and limit. the capacity of the underlying byte array the maximum limit upto which data can be written reset the limit new limit reset the buffer return the current limit returns the underlying buffer. data is only valid to {@link #size()}. returns the length of the valid data currently in the buffer. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.BytesWritable "a byte sequence that is usable as a key or value. it is resizable and distinguishes between the size of the seqeunce and the current capacity. the hash function is the front of the md5 of the buffer. the sort order is the same as memcmp. create a zero-size sequence. create a byteswritable using the byte array as the initial value. this array becomes the backing storage for the object. create a byteswritable using the byte array as the initial value and length as the length. use this constructor if the array is larger than the value it represents. this array becomes the backing storage for the object. the number of bytes to use from array. get a copy of the bytes that is exactly the length of the data. see {@link #getbytes()} for faster access to the underlying array. get the data backing the byteswritable. please use {@link #copybytes()} if you need the returned array to be precisely the length of the data. data is only valid between 0 and getlength() - 1. get the data from the byteswritable. @deprecated use {@link #getbytes()} instead. get the current size of the buffer. get the current size of the buffer. @deprecated use {@link #getlength()} instead. change the size of the buffer. the values in the old range are preserved and any new values are undefined. the capacity is changed if it is necessary. the new number of bytes get the capacity, which is the maximum size that could handled without resizing the backing storage. number of bytes change the capacity of the backing storage. the data is preserved. _cap the new capacity in bytes. set the byteswritable to the contents of the given newdata. the value to set this byteswritable to. set the value to a copy of the given byte range the new values to copy in the offset in newdata to start at the number of bytes to copy are the two byte sequences equal? generate the stream of bytes as hex pairs separated by ' '. a comparator optimized for byteswritable. compare the buffers in serialized form. www.apache.org/licenses/license-2.0 inherit javadoc clear the old data inherit javadoc if not the first, put a blank separator in if it is only one digit, add a leading 0. register this comparator"
org.apache.hadoop.io.ByteWritable "a writablecomparable for a single byte. set the value of this bytewritable. return the value of this bytewritable. returns true iff o is a bytewritable with the same value. compares two bytewritables. a comparator optimized for bytewritable. www.apache.org/licenses/license-2.0 register this comparator"
org.apache.hadoop.io.Closeable "@deprecated use java.io.closeable www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.BlockCompressorStream "a {@link org.apache.hadoop.io.compress.compressorstream} which works with 'block-based' based compression algorithms, as opposed to 'stream-based' compression algorithms. it should be noted that this wrapper does not guarantee that blocks will be sized for the compressor. if the {@link org.apache.hadoop.io.compress.compressor} requires buffering to effect meaningful compression, it is responsible for it. create a {@link blockcompressorstream}. stream compressor to be used size of buffer maximum 'overhead' of the compression algorithm with given buffersize create a {@link blockcompressorstream} with given output-stream and compressor. use default of 512 as buffersize and compressionoverhead of (1% of buffersize + 12 bytes) = 18 bytes (zlib algorithm). stream compressor to be used write the data provided to the compression codec, compressing no more than the buffer size less the compression overhead as specified during construction for each block. each block contains the uncompressed length for the block, followed by one or more length-prefixed blocks of compressed data. www.apache.org/licenses/license-2.0 the 'maximum' size of input data to be compressed, to account for the overhead of the compression algorithm. sanity checks adding this segment would exceed the maximum size. flush data if we have it. the data we're given exceeds the maximum size. any data we had have been flushed, so we write out this chunk in segments not exceeding the maximum size until it is exhausted. give data to the compressor compressor buffer size might be smaller than the maximum size, so we permit it to flush if required. write out the compressed chunk"
org.apache.hadoop.io.compress.BlockDecompressorStream "a {@link org.apache.hadoop.io.compress.decompressorstream} which works with 'block-based' based compression algorithms, as opposed to 'stream-based' compression algorithms. create a {@link blockdecompressorstream}. input stream decompressor to use size of buffer @throws ioexception create a {@link blockdecompressorstream}. input stream decompressor to use @throws ioexception www.apache.org/licenses/license-2.0 check if we are the beginning of a block get original data size eof if originalblocksize is 0 this will occur only when decompressing previous compressed empty file send the read data to the decompressor note the no. of decompressed bytes read from 'current' block get the size of the compressed chunk (always non-negative) read len bytes from underlying stream"
org.apache.hadoop.io.compress.bzip2.BZip2Constants "this package is based on the work done by keiron liddle, aftex software  to whom the ant project is very grateful for his great code. base class for both the compress and decompress classes. holds common arrays, and static data.  this interface is public for historical purposes. you should have no need to use it.  end of a bzip2 block end of bzip2 stream. this array really shouldn't be here. again, for historical purposes it is.  fixme: this array should be in a private or package private location, since it could be modified by malicious code.  www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor "this is a dummy compressor for bzip2. www.apache.org/licenses/license-2.0 do nothing do nothing"
org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor "this is a dummy decompressor for bzip2. www.apache.org/licenses/license-2.0 do nothing"
org.apache.hadoop.io.compress.bzip2.CBZip2InputStream "this package is based on the work done by keiron liddle, aftex software  to whom the ant project is very grateful for his great code. an input stream that decompresses from the bzip2 format (without the file header chars) to be read as any other stream.  the decompression requires large amounts of memory. thus you should call the {@link #close() close()} method as soon as possible, to force cbzip2inputstream to release the allocated memory. see {@link cbzip2outputstream cbzip2outputstream} for information about memory usage.   cbzip2inputstream reads bytes from the compressed source stream via the single byte {@link java.io.inputstream#read() read()} method exclusively. thus you should consider to use a buffered source stream.   this ant code was enhanced so that it can de-compress blocks of bzip2 data. current position in the stream is an important statistic for hadoop. for example in linerecordreader, we solely depend on the current position in the stream to know about the progess. the notion of position becomes complicated for compressed files. the hadoop splitting is done in terms of compressed file. but a compressed file deflates to a large amount of data. so we have handled this problem in the following way. on object creation time, we find the next block start delimiter. once such a marker is found, the stream stops there (we discard any read compressed data in this process) and the position is updated (i.e. the caller of this class will find out the stream location). at this point we are ready for actual reading (i.e. decompression) of data. the subsequent read calls give out data. the position is updated when the caller of this class has read off the current block + 1 bytes. in between the block reading, position is not updated. (we can only update the postion on block boundaries).   instances of this class are not threadsafe.  index of the last char in the block, so the block size == last + 1. index in zptr[] of original string after sorting. always: in the range 0 .. 9. the current block size is 100000 this number. a state machine to keep track of current state of the de-coder all memory intensive stuff. this field is initialized by initblock(). this method reports the processed bytes so far. please note that this statistic is only updated on block boundaries and only when the stream is initiated in byblock mode. this method keeps track of raw processed compressed bytes. count is the number of bytes to be added to raw processed bytes this method is called by the client of this class in case there are any corrections in the stream position. one common example is when client of this code removes starting bz characters from the compressed stream. count bytes are added to the reported bytes this method reads a byte from the compressed stream. whenever we need to read from the underlying compressed stream, this method should be called instead of directly calling the read method of the underlying compressed stream. this method does important record keeping to have the statistic that how many bytes have been read off the compressed stream. this method tries to find the marker (passed to it as the first parameter) in the stream. it can find bit patterns of length  although bzip2 headers are marked with the magic "bz" this constructor expects the next byte in the stream to be the first one after the magic. thus callers have to skip the first two bytes. otherwise this constructor will throw an exception.  @throws ioexception if the stream content is malformed or an i/o error occurs. @throws nullpointerexception if in == null returns the number of bytes between the current stream position and the immediate next bzip2 block marker. the inputstream number of bytes between current stream position and the next bzip2 block start marker. @throws ioexception in continous reading mode, this read method starts from the start of the compressed stream and end at the end of file by emitting un-compressed data. in this mode stream positioning is not announced and should be ignored. in byblock reading mode, this read method informs about the end of a bzip2 block by returning eob. at this event, the compressed stream position is also announced. this announcement tells that how much of the compressed stream has been de-compressed and read out of this class. in between eob events, the stream position is not updated. @throws ioexception if the stream content is malformed or an i/o error occurs. the return value greater than 0 are the bytes read. a value of -1 means end of stream while -2 represents end of block allocate data here instead in constructor, so we do not allocate it if the input file is empty. allocate data here instead in constructor, so we do not allocate it if the input file is empty. called by createhuffmandecodingtables() exclusively. receive the mapping table now the selectors undo the mtf values for the selectors. now the coding tables called by recvdecodingtables() exclusively. setting up the unzftab entries here is not strictly necessary, but it does save having to do it later in a separate pass, and so saves a block's worth of cache misses. this loop is hammered during decompression, hence avoid native method call overhead of system.arraycopy for very small ranges to copy. not a char and not eof freq table collected to save a pass over the data during decompression. initializes the {@link #tt} array. this method is called when the required length of the array is known. i don't initialize it at construction time to avoid unneccessary memory allocation when compressing small files. www.apache.org/licenses/license-2.0 start of block end of bzip2 stream the variable records the current advertised position of the stream. the following variable keep record of compressed bytes read. used by skiptonextmarker variables used by setup methods exclusively pick next marketbitlength bits in the stream i.e 9 >1 mb buffer report 'end of block' or 'end of stream' exactly when we are about to start a new block, we advertise the stream position. return -1 return -2 this.checkblockintegrity(); currblockno++; end of file '1' ')' 'y' '&' 's' 'y' currblockno++; a bad crc is considered a fatal error. make next blocks readable without error (repair feature, not yet documented, not tested) nearly all times v is zero, 4 in most other cases finally create the huffman tables (with blocksize 900k) 256 byte 256 byte 18002 byte 18002 byte 1024 byte 6192 byte 6192 byte 6192 byte 24 byte 1028 byte 512 byte 3096 byte 6 byte --------------- 60798 byte 3600000 byte 900000 byte --------------- 4560782 byte =============== tt.length should always be >= length, but theoretically it can happen, if the compressor mixed small and large blocks. normally only the last block will be smaller than others."
org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream "this package is based on the work done by keiron liddle, aftex software  to whom the ant project is very grateful for his great code. an output stream that compresses into the bzip2 format (without the file header chars) into another stream.  the compression requires large amounts of memory. thus you should call the {@link #close() close()} method as soon as possible, to force cbzip2outputstream to release the allocated memory.   you can shrink the amount of allocated memory and maybe raise the compression speed by choosing a lower blocksize, which in turn may cause a lower compression ratio. you can avoid unnecessary memory allocation by avoiding using a blocksize which is bigger than the size of the input.   you can compute the memory usage for compressing by the following formula:   &lt;code&gt;400k + (9 blocksize)&lt;/code&gt;.   to get the memory required for decompression by {@link cbzip2inputstream cbzip2inputstream} use   &lt;code&gt;65k + (5 blocksize)&lt;/code&gt;.         memory usage by blocksize   blocksize compression memory usage decompression memory usage   100k 1300k 565k   200k 2200k 1065k   300k 3100k 1565k   400k 4000k 2065k   500k 4900k 2565k   600k 5800k 3065k   700k 6700k 3565k   800k 7600k 4065k   900k 8500k 4565k    for decompression cbzip2inputstream allocates less memory if the bzipped input is smaller than one block.   instances of this class are not threadsafe.   todo: update to bzip2 1.0.1  the minimum supported blocksize  == 1. the maximum supported blocksize  == 9. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it. this constant is accessible by subclasses for historical purposes. if you don't know what it means then you don't need it.  if you are ever unlucky/improbable enough to get a stack overflow whilst sorting, increase the following constant and try again. in practice i have never seen the stack go above 27 elems, so the following limit seems very generous.  knuth's increments seem to work better than incerpi-sedgewick here. possibly because the number of elems to sort is usually small, typically &lt;= 20. this method is accessible by subclasses for historical purposes. if you don't know what it does then you don't need it. nodes and heap entries run from 1. entry 0 for both the heap and nodes is a sentinel. nodes and heap entries run from 1. entry 0 for both the heap and nodes is a sentinel. index of the last char in the block, so the block size == last + 1. index in fmap[] of original string after sorting. always: in the range 0 .. 9. the current block size is 100000 this number. used when sorting. if too many long comparisons happen, we stop sorting, randomise the block slightly, and try again. all memory intensive stuff. chooses a blocksize based on the given length of the data to compress. blocksize, between {@link #min_blocksize} and {@link #max_blocksize} both inclusive. for a negative inputlength this method returns max_blocksize always. the length of the data which will be compressed by cbzip2outputstream. constructs a new cbzip2outputstream with a blocksize of 900k.  attention: the caller is resonsible to write the two bzip2 magic bytes "bz" to the specified stream prior to calling this constructor.  the destination stream. @throws ioexception if an i/o error occurs in the specified stream. @throws nullpointerexception if out == null. constructs a new cbzip2outputstream with specified blocksize.  attention: the caller is resonsible to write the two bzip2 magic bytes "bz" to the specified stream prior to calling this constructor.  the destination stream. the blocksize as 100k units. @throws ioexception if an i/o error occurs in the specified stream. @throws illegalargumentexception if (blocksize  9). @throws nullpointerexception if out == null. @see #min_blocksize @see #max_blocksize overriden to close the stream. write `magic' bytes h indicating file-format == huffmanised, followed by a digit indicating blocksize100k. 20 is just a paranoia constant sort the block and establish posn of original string a 6-byte block header, the value chosen arbitrarily as 0x314159265359 :-). a 32 bit value does not really give a strong enough guarantee that the value will not appear by chance in the compressed datastream. worst-case probability of this event, for a 900k block, is about 2.0e-3 for 32 bits, 1.0e-5 for 40 bits and 4.0e-8 for 48 bits. for a compressed file of size 100gb -- about 100000 blocks -- only a 48-bit marker will do. nb: normal compression/ decompression donot rely on these statistical properties. they are only important when trying to recover blocks from damaged files. now the block's crc, so it is in a known place. now a single bit indicating randomisation. finally, block's contents proper. now another magic 48-bit number, 0x177245385090, to indicate the end of the last block. (sqrt(pi), if you want to know. i did want to use e, but it contains too much repetition -- 27 18 28 18 28 46 -- for me to feel statistically comfortable. call me paranoid.) returns the blocksize parameter specified at construction time. decide how many coding tables to use generate an initial set of coding tables iterate up to n_iters times to improve the tables. compute mtf values for the selectors. assign actual codes for the tables. transmit the mapping table. now the selectors. now the coding tables. and finally, the block data proper set group start & end marks. calculate the cost of this group as coded by each of the coding tables. find the coding table which is best for this group, and record its identity in the selector table. increment the symbol frequencies for the selected table. recompute the tables based on the accumulated frequencies. 10 11 this is the most hammered method of this class.  this is the version using unrolled loops. normally i never use such ones in java code. the unrolling has shown a noticable performance improvement on jre 1.4.2 (linux i586 / hotspot client). of course it depends on the jit compiler of the vm.  method "mainqsort3", file "blocksort.c", bzip2 1.0.2 in the various block-sized structures, live data runs from 0 to last+num_overshoot_bytes inclusive. first, set up the overshoot area for block. now ftab contains the first loc of every small bucket. calculate the running order, from smallest to largest big bucket. the main sorting loop. process big buckets, starting with the least full. complete the big bucket [ss] by quicksorting any unsorted small buckets [ss, j]. hopefully previous pointer-scanning phases have already completed many of the small buckets [ss, j], so we don't have to sort them at all. the ss big bucket is now done. record this fact, and update the quadrant descriptors. remember to update quadrants in the overshoot area too, if necessary. the "if (i  0) : this.nmtf; unrolled version of the else-block assert (ngroups = 1) : minlen; inlined: bsw(1, inuse[i16 + j] ? 1 : 0); write 8-bit inlined: bsw(1, 1); inlined: bsw(1, 0); bsbuffshadow |= 0  mj) && maingtu((a = fmap[j - h]) + d, vd, block, quadrant, lastshadow); j -= h) { fmap[j] = a; }  unrolled version: start inline maingtu following could be done in a loop, but unrolled it for performance: while x > 0 hammer end inline maingtu assert (this.origptr != -1) : this.origptr; set up the 2-byte frequency table complete the initial radix sort: step 1: step 2: now scan this big bucket so as to synthesise the sorted order for small buckets [t, ss] for all t != ss. step 3: handle 16 bit signed numbers make maps with blocksize 900k 256 byte 256 byte 1032 byte 18002 byte 18002 byte 256 byte 1548 byte 6192 byte 24 byte 12 byte 6192 byte 6 byte 16 byte 4000 byte 4000 byte 4000 byte 1024 byte 1024 byte 256 byte 1040 byte 2064 byte 2064 byte 262148 byte ------------ 333408 byte 900021 byte 3600000 byte 3600000 byte ------------ 8433529 byte ============"
org.apache.hadoop.io.compress.bzip2.CRC "this package is based on the work done by keiron liddle, aftex software  to whom the ant project is very grateful for his great code. a simple class the hold and calculate the crc for sanity checking of the data. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.bzip2.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.BZip2Codec "this class provides compressionoutputstream and compressioninputstream for compression and decompression. currently we dont have an implementation of the compressor and decompressor interfaces, so those methods of compressioncodec which have a compressor or decompressor type argument, throw unsupportedoperationexception. creates a new instance of bzip2codec creates compressionoutputstream for bzip2 the output stream bzip2 compressionoutputstream @throws java.io.ioexception throws io exception creates a compressor using given outputstream. @throws java.io.ioexception this functionality is currently not supported. 2dummycompressor.class this functionality is currently not supported. creates compressioninputstream to be used to read off uncompressed data. the inputstream compressioninputstream for bzip2 @throws java.io.ioexception throws ioexception this functionality is currently not supported. creates compressioninputstream to be used to read off uncompressed data in one of the two reading modes. i.e. continuous or blocked reading modes the inputstream the start offset into the compressed stream the end offset into the compressed stream controls whether progress is reported continuously or only at block boundaries. for bzip2 aligned at block boundaries this functionality is currently not supported. 2dummydecompressor.class this functionality is currently not supported. .bz2 is recognized as the default extension for compressed bzip2 files string telling the default bzip2 file extension this class is capable to de-compress bzip2 data in two modes; continous and byblock. byblock mode makes it possible to do decompression starting any arbitrary position in the stream. so this facility can easily be used to parallelize decompression of a large bzip2 file for performance reasons. (it is exactly done so for hadoop framework. see linerecordreader for an example). so one can break the file (of course logically) into chunks for parallel processing. these "splits" should be like default hadoop splits (e.g as in fileinputformat getsplit metod). so this code is designed and tested for fileinputformat's way of splitting only. this method updates compressed stream position exactly when the client of this code has read off at least one byte passed any bzip2 end of block marker. this mechanism is very helpful to deal with data level record boundaries. please see constructor and next methods of org.apache.hadoop.mapred.linerecordreader as an example usage of this feature. we elaborate it with an example in the following: assume two different scenarios of the bzip2 compressed stream, where [m] represent end of block, \n is line delimiter and . represent compressed data. ............[m]......\n....... ..........\n[m]......\n....... assume that end is right after [m]. in the first case the reading will stop at \n and there is no need to read one more line. (to see the reason of reading one more line in the next() method is explained in linerecordreader.) while in the second example linerecordreader needs to read one more line (till the second \n). now since bzip2codecs only update position at least one byte passed a maker, so it is straight forward to differentiate between the two cases mentioned. as the comments before read method tell that compressed stream is advertised when at least one byte passed eob have been read off. but there is an exception to this rule. when we construct the stream we advertise the position exactly at eob. in the following method shouldaddon boolean captures this exception. www.apache.org/licenses/license-2.0 find the position of first bzip2 start up marker bzip2 start of block markers are of 6 bytes. but the very first block also has "bzh9", making it 10 bytes. this is the common case. but at time stream might start without a leading bz. the following if clause handles the following case: assume the following scenario in bzip2 compressed stream where . represent compressed data. .....[48 bit block].....[48 bit block].....[48 bit block]... ........................[47 bits][1 bit].....[48 bit block]... ................................^[assume a byte alignment here] ........................................^^[current position of stream] .....................^^[we go back 10 bytes in stream and find a block marker] ........................................^^[we align at wrong position!] ...........................................................^^[while this pos is correct] class data starts here// class data ends here// the compressed bzip2 stream should start with the identifying characters bz. caller of cbzip2outputstream i.e. this class must write these characters. in the case that nothing is written to this stream, we still need to write out the header before closing, otherwise the stream won't be recognized by bzip2compressioninputstream. cannot write to out at this point because out might not be ready yet, as in sequencefile.writer implementation. in the case that nothing is written to this stream, we still need to write out the header before closing, otherwise the stream won't be recognized by bzip2compressioninputstream. end of class bzip2compressionoutputstream class data starts here// following state machine handles different states of compressed stream position hold : don't advertise compressed stream position advertise : read 1 more character and advertise stream position see more comments about it before updatepos method. class data ends here// we only strip header if it is start of file we are flexible enough to allow the compressed stream not to start with the header of bz. so it works fine either we have the header or not. in case of byblock mode, we also want to strip off remaining two character of the header. end of method this is the precise time to update compressed stream position to the client of this code. cannot read from bufferedin at this point because bufferedin might not be ready yet, as in sequencefile.reader implementation. end of bzip2compressioninputstream"
org.apache.hadoop.io.compress.CodecPool "a global compressor/decompressor pool used to save and reuse (possibly native) compression/decompression codecs. a global compressor pool used to save the expensive construction/destruction of (possibly native) decompression codecs. a global decompressor pool used to save the expensive construction/destruction of (possibly native) decompression codecs. get a {@link compressor} for the given {@link compressioncodec} from the pool or a new one. the compressioncodec for which to get the compressor the configuration object which contains confs for creating or reinit the compressor compressor for the given compressioncodec from the pool or a new one get a {@link decompressor} for the given {@link compressioncodec} from the pool or a new one. the compressioncodec for which to get the decompressor decompressor for the given compressioncodec the pool or a new one return the {@link compressor} to the pool. the compressor to be returned to the pool return the {@link decompressor} to the pool. the decompressor to be returned to the pool www.apache.org/licenses/license-2.0 check if an appropriate codec is available if the compressor can't be reused, don't pool it. if the decompressor can't be reused, don't pool it."
org.apache.hadoop.io.compress.CompressionCodec "this class encapsulates a streaming compression/decompression pair. create a {@link compressionoutputstream} that will write to the given {@link outputstream}. the location for the final output stream stream the create a {@link compressionoutputstream} that will write to the given {@link outputstream} with the given {@link compressor}. the location for the final output stream compressor to use stream the get the type of {@link compressor} needed by this {@link compressioncodec}. type of compressor needed by this codec. create a new {@link compressor} for use by this {@link compressioncodec}. new compressor for use by this codec create a {@link compressioninputstream} that will read from the given input stream. the stream to read compressed bytes from stream to read uncompressed bytes from @throws ioexception create a {@link compressioninputstream} that will read from the given {@link inputstream} with the given {@link decompressor}. the stream to read compressed bytes from decompressor to use stream to read uncompressed bytes from @throws ioexception get the type of {@link decompressor} needed by this {@link compressioncodec}. type of decompressor needed by this codec. create a new {@link decompressor} for use by this {@link compressioncodec}. new decompressor for use by this codec get the default filename extension for this kind of compression. extension including the '.' www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.CompressionCodecFactory "a factory that will find the correct codec for a given filename. a map from the reversed filename suffixes to the codecs. this is probably overkill, because the maps should be small, but it automatically supports finding the longest matching suffix. a map from the reversed filename suffixes to the codecs. this is probably overkill, because the maps should be small, but it automatically supports finding the longest matching suffix. a map from class names to the codecs print the extension map out as a string. get the list of codecs discovered via a java serviceloader, or listed in the configuration. codecs specified in configuration come later in the returned list, and are considered to override those from the serviceloader. the configuration to look in list of the {@link compressioncodec} classes sets a list of codec classes in the configuration. in addition to any classes specified using this method, {@link compressioncodec} classes on the classpath are discovered using a java serviceloader. the configuration to modify the list of classes to set find the codecs specified in the config value io.compression.codecs and register them. defaults to gzip and deflate. find the relevant compression codec for the given file based on its filename suffix. the filename to check codec object find the relevant compression codec for the codec's canonical class name. the canonical class name of the codec codec object find the relevant compression codec for the codec's canonical class name or by codec alias.  codec aliases are case insensitive.  the code alias is the short class name (without the package name). if the short class name ends with 'codec', then there are two aliases for the codec, the complete short class name and the short class name without the 'codec' ending. for example for the 'gzipcodec' codec class name the alias are 'gzip' and 'gzipcodec'. the canonical class name of the codec codec object find the relevant compression codec for the codec's canonical class name or by codec alias and returns its implemetation class.  codec aliases are case insensitive.  the code alias is the short class name (without the package name). if the short class name ends with 'codec', then there are two aliases for the codec, the complete short class name and the short class name without the 'codec' ending. for example for the 'gzipcodec' codec class name the alias are 'gzip' and 'gzipcodec'. the canonical class name of the codec codec class removes a suffix from a filename, if it has it. the filename to strip the suffix to remove shortened filename a little test program. www.apache.org/licenses/license-2.0 add codec classes discovered via service loading codec_providers is a lazy collection. synchronize so it is thread-safe. see hadoop-8406. add codec classes from configuration trying to get the codec by name in case the name was specified instead a class"
org.apache.hadoop.io.compress.CompressionInputStream "a compression input stream. implementations are assumed to be buffered. this permits clients to reposition the underlying input stream then call {@link #resetstate()}, without having to also synchronize client buffers. the input stream to be compressed. create a compression input stream that reads the decompressed bytes from the given stream. the input stream to be compressed. @throws ioexception read bytes from the stream. made abstract to prevent leakage to underlying stream. reset the decompressor to its initial state and discard any buffered data, as the underlying stream may have been repositioned. this method returns the current position in the stream. position in stream as a long this method is current not supported. @throws unsupportedoperationexception this method is current not supported. @throws unsupportedoperationexception www.apache.org/licenses/license-2.0 this way of getting the current position will not work for file size which can be fit in an int and hence can not be returned by available method."
org.apache.hadoop.io.compress.CompressionOutputStream "a compression output stream. the output stream to be compressed. create a compression output stream that writes the compressed bytes to the given stream. write compressed bytes to the stream. made abstract to prevent leakage to underlying stream. finishes writing compressed data to the output stream without closing the underlying stream. reset the compression to the initial state. does not reset the underlying stream. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.Compressor "specification of a stream-based 'compressor' which can be plugged into a {@link compressionoutputstream} to compress data. this is modelled after {@link java.util.zip.deflater} sets input data for compression. this should be called whenever #needsinput() returns true indicating that more input data is required. input data start offset length returns true if the input data buffer is empty and #setinput() should be called to provide more input. true if the input data buffer is empty and #setinput() should be called in order to provide more input. sets preset dictionary for compression. a preset dictionary is used when the history buffer can be predetermined. dictionary data bytes start offset length return number of uncompressed bytes input so far. return number of compressed bytes output so far. when called, indicates that compression should end with the current contents of the input buffer. returns true if the end of the compressed data output stream has been reached. true if the end of the compressed data output stream has been reached. fills specified buffer with compressed data. returns actual number of bytes of compressed data. a return value of 0 indicates that needsinput() should be called in order to determine if more input data is required. buffer for the compressed data start offset of the data size of the buffer actual number of bytes of compressed data. resets compressor so that a new set of input data can be processed. closes the compressor and discards any unprocessed input. prepare the compressor to be used in a new stream with settings defined in the given configuration configuration from which new setting are fetched www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.CompressorStream "allow derived classes to directly set the underlying stream. underlying output stream. www.apache.org/licenses/license-2.0 sanity checks"
org.apache.hadoop.io.compress.Decompressor "specification of a stream-based 'de-compressor' which can be plugged into a {@link compressioninputstream} to compress data. this is modelled after {@link java.util.zip.inflater} sets input data for decompression. this should be called if and only if {@link #needsinput()} returns true indicating that more input data is required. (both native and non-native versions of various decompressors require that the data passed in via b[] remain unmodified until the caller is explicitly notified--via {@link #needsinput()}--that the buffer may be safely modified. with this requirement, an extra buffer-copy can be avoided.) input data start offset length returns true if the input data buffer is empty and {@link #setinput(byte[], int, int)} should be called to provide more input. true if the input data buffer is empty and {@link #setinput(byte[], int, int)} should be called in order to provide more input. sets preset dictionary for compression. a preset dictionary is used when the history buffer can be predetermined. dictionary data bytes start offset length returns true if a preset dictionary is needed for decompression. true if a preset dictionary is needed for decompression returns true if the end of the decompressed data output stream has been reached. indicates a concatenated data stream when finished() returns true and {@link #getremaining()} returns a positive value. finished() will be reset with the {@link #reset()} method. true if the end of the decompressed data output stream has been reached. fills specified buffer with uncompressed data. returns actual number of bytes of uncompressed data. a return value of 0 indicates that {@link #needsinput()} should be called in order to determine if more input data is required. buffer for the compressed data start offset of the data size of the buffer actual number of bytes of compressed data. @throws ioexception returns the number of bytes remaining in the compressed data buffer. indicates a concatenated data stream if {@link #finished()} returns true and getremaining() returns a positive value. if {@link #finished()} returns true and getremaining() returns a zero value, indicates that the end of data stream has been reached and is not a concatenated data stream. number of bytes remaining in the compressed data buffer. resets decompressor and input and output buffers so that a new set of input data can be processed. if {@link #finished()}} returns true and {@link #getremaining()} returns a positive value, reset() is called before processing of the next data stream in the concatenated data stream. {@link #finished()} will be reset and will return false when reset() is called. closes the decompressor and discards any unprocessed input. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.DecompressorStream "allow derived classes to directly set the underlying stream. underlying input stream. @throws ioexception www.apache.org/licenses/license-2.0 first see if there was any leftover buffered input from previous stream; if not, attempt to refill buffer. if refill -> eof, we're all done; else reset, fix up input buffer, and get ready for next concatenated substream/"member". apparently the previous end-of-stream was also end-of-file: return success, as if we had never called getcompresseddata() looks like it's a concatenated stream: reset low-level zlib (or other engine) and buffers, then "resend" remaining input data this recopies note: this is the one place we do not want to save the number of bytes sent (nremaining here) into lastbytessent: since we are resending what we've already sent before, offset is nonzero in general (only way it could be zero is if it already equals nremaining), which would then screw up the offset calculation _next_ time around. iow, getremaining() is in terms of the original, zero-offset bufferload, so lastbytessent must be as well. cheesy ascii art:   +===============================================+ buffer: |1111111111|22222222222222222|333333333333| | +===============================================+ #1: | #2: | #3: (final substream: nremaining == 0; eof = true)  if lastbytessent is anything other than m, as shown, then "off" will be calculated incorrectly. note that the _caller_ is now required to call setinput() or throw sanity checks read 'n' bytes"
org.apache.hadoop.io.compress.DefaultCodec "www.apache.org/licenses/license-2.0 this may leak memory if called in a loop. the createcompressor() call may cause allocation of an untracked direct-backed buffer if native libs are being used (even if you close the stream). a compressor object should be reused between successive calls."
org.apache.hadoop.io.compress.DeflateCodec "alias class for defaultcodec to enable codec discovery by 'deflate' name. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.DoNotPool "this is a marker annotation that marks a compressor or decompressor type as not to be pooled. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.GzipCodec "this class creates gzip compressors/decompressors. a bridge that wraps around a deflateroutputstream to make it a compressionoutputstream. allow children types to put a different type in here. the deflater stream to use www.apache.org/licenses/license-2.0 always succeeds (or throws)"
org.apache.hadoop.io.compress.lz4.Lz4Compressor "a {@link compressor} based on the lz4 compression algorithm. http://code.google.com/p/lz4/ creates a new compressor. size of the direct buffer to be used. creates a new compressor with the default buffer size. sets input data for compression. this should be called whenever #needsinput() returns true indicating that more input data is required. input data start offset length if a write would exceed the capacity of the direct buffers, it is set aside to be loaded by this function while the compressed data are consumed. does nothing. returns true if the input data buffer is empty and #setinput() should be called to provide more input. true if the input data buffer is empty and #setinput() should be called in order to provide more input. when called, indicates that compression should end with the current contents of the input buffer. returns true if the end of the compressed data output stream has been reached. true if the end of the compressed data output stream has been reached. fills specified buffer with compressed data. returns actual number of bytes of compressed data. a return value of 0 indicates that needsinput() should be called in order to determine if more input data is required. buffer for the compressed data start offset of the data size of the buffer actual number of bytes of compressed data. resets compressor so that a new set of input data can be processed. prepare the compressor to be used in a new stream with settings defined in the given configuration configuration from which new setting are fetched return number of bytes given to this compressor since last reset. return number of bytes consumed by callers of compress since last reset. closes the compressor and discards any unprocessed input. www.apache.org/licenses/license-2.0 code.google.com/p/lz4/ hack - use this as a global lock in the jni layer initialize the native library ignore failure to load/initialize lz4 save data; now !needsinput note how much data is being fed to lz4 do nothing check if all uncompressed data has been consumed check if there is compressed data re-initialize the lz4's output direct-buffer no compressed data, so we should have !needsinput or !finished called without data; write nothing compress data lz4 consumes all buffer input set 'finished' if snapy has consumed all get atmost 'len' bytes"
org.apache.hadoop.io.compress.lz4.Lz4Decompressor "a {@link decompressor} based on the lz4 compression algorithm. http://code.google.com/p/lz4/ creates a new compressor. size of the direct buffer to be used. creates a new decompressor with the default buffer size. sets input data for decompression. this should be called if and only if {@link #needsinput()} returns true indicating that more input data is required. (both native and non-native versions of various decompressors require that the data passed in via b[] remain unmodified until the caller is explicitly notified--via {@link #needsinput()}--that the buffer may be safely modified. with this requirement, an extra buffer-copy can be avoided.) input data start offset length if a write would exceed the capacity of the direct buffers, it is set aside to be loaded by this function while the compressed data are consumed. does nothing. returns true if the input data buffer is empty and {@link #setinput(byte[], int, int)} should be called to provide more input. true if the input data buffer is empty and {@link #setinput(byte[], int, int)} should be called in order to provide more input. returns false. false. returns true if the end of the decompressed data output stream has been reached. true if the end of the decompressed data output stream has been reached. fills specified buffer with uncompressed data. returns actual number of bytes of uncompressed data. a return value of 0 indicates that {@link #needsinput()} should be called in order to determine if more input data is required. buffer for the compressed data start offset of the data size of the buffer actual number of bytes of compressed data. @throws ioexception returns 0. 0. resets decompressor and input and output buffers so that a new set of input data can be processed. www.apache.org/licenses/license-2.0 code.google.com/p/lz4/ hack - use this as a global lock in the jni layer initialize the native library ignore failure to load/initialize lz4 reinitialize lz4's output direct-buffer reinitialize lz4's input direct buffer note how much data is being fed to lz4 do nothing consume remaining compressed data? check if lz4 has consumed all input check if we have consumed all check if there is uncompressed data re-initialize the lz4's output direct buffer decompress data get atmost 'len' bytes never use this function in blockdecompressorstream. do nothing"
org.apache.hadoop.io.compress.lz4.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.Lz4Codec "this class creates lz4 compressors/decompressors. set the configuration to be used by this object. the configuration object. return the configuration used by this object. configuration object used by this objec. are the native lz4 libraries loaded & initialized? if loaded & initialized, otherwise false create a {@link compressionoutputstream} that will write to the given {@link outputstream}. the location for the final output stream stream the create a {@link compressionoutputstream} that will write to the given {@link outputstream} with the given {@link compressor}. the location for the final output stream compressor to use stream the get the type of {@link compressor} needed by this {@link compressioncodec}. type of compressor needed by this codec. create a new {@link compressor} for use by this {@link compressioncodec}. new compressor for use by this codec create a {@link compressioninputstream} that will read from the given input stream. the stream to read compressed bytes from stream to read uncompressed bytes from @throws ioexception create a {@link compressioninputstream} that will read from the given {@link inputstream} with the given {@link decompressor}. the stream to read compressed bytes from decompressor to use stream to read uncompressed bytes from @throws ioexception get the type of {@link decompressor} needed by this {@link compressioncodec}. type of decompressor needed by this codec. create a new {@link decompressor} for use by this {@link compressioncodec}. new decompressor for use by this codec get the default filename extension for this kind of compression. .lz4. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.snappy.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.snappy.SnappyCompressor "a {@link compressor} based on the snappy compression algorithm. http://code.google.com/p/snappy/ creates a new compressor. size of the direct buffer to be used. creates a new compressor with the default buffer size. sets input data for compression. this should be called whenever #needsinput() returns true indicating that more input data is required. input data start offset length if a write would exceed the capacity of the direct buffers, it is set aside to be loaded by this function while the compressed data are consumed. does nothing. returns true if the input data buffer is empty and #setinput() should be called to provide more input. true if the input data buffer is empty and #setinput() should be called in order to provide more input. when called, indicates that compression should end with the current contents of the input buffer. returns true if the end of the compressed data output stream has been reached. true if the end of the compressed data output stream has been reached. fills specified buffer with compressed data. returns actual number of bytes of compressed data. a return value of 0 indicates that needsinput() should be called in order to determine if more input data is required. buffer for the compressed data start offset of the data size of the buffer actual number of bytes of compressed data. resets compressor so that a new set of input data can be processed. prepare the compressor to be used in a new stream with settings defined in the given configuration configuration from which new setting are fetched return number of bytes given to this compressor since last reset. return number of bytes consumed by callers of compress since last reset. closes the compressor and discards any unprocessed input. www.apache.org/licenses/license-2.0 code.google.com/p/snappy/ hack - use this as a global lock in the jni layer save data; now !needsinput note how much data is being fed to snappy do nothing check if all uncompressed data has been consumed check if there is compressed data re-initialize the snappy's output direct-buffer no compressed data, so we should have !needsinput or !finished called without data; write nothing compress data snappy consumes all buffer input set 'finished' if snapy has consumed all get atmost 'len' bytes"
org.apache.hadoop.io.compress.snappy.SnappyDecompressor "a {@link decompressor} based on the snappy compression algorithm. http://code.google.com/p/snappy/ creates a new compressor. size of the direct buffer to be used. creates a new decompressor with the default buffer size. sets input data for decompression. this should be called if and only if {@link #needsinput()} returns true indicating that more input data is required. (both native and non-native versions of various decompressors require that the data passed in via b[] remain unmodified until the caller is explicitly notified--via {@link #needsinput()}--that the buffer may be safely modified. with this requirement, an extra buffer-copy can be avoided.) input data start offset length if a write would exceed the capacity of the direct buffers, it is set aside to be loaded by this function while the compressed data are consumed. does nothing. returns true if the input data buffer is empty and {@link #setinput(byte[], int, int)} should be called to provide more input. true if the input data buffer is empty and {@link #setinput(byte[], int, int)} should be called in order to provide more input. returns false. false. returns true if the end of the decompressed data output stream has been reached. true if the end of the decompressed data output stream has been reached. fills specified buffer with uncompressed data. returns actual number of bytes of uncompressed data. a return value of 0 indicates that {@link #needsinput()} should be called in order to determine if more input data is required. buffer for the compressed data start offset of the data size of the buffer actual number of bytes of compressed data. @throws ioexception returns 0. 0. resets decompressor and input and output buffers so that a new set of input data can be processed. www.apache.org/licenses/license-2.0 code.google.com/p/snappy/ hack - use this as a global lock in the jni layer reinitialize snappy's output direct-buffer reinitialize snappy's input direct buffer note how much data is being fed to snappy do nothing consume remaining compressed data? check if snappy has consumed all input check if we have consumed all check if there is uncompressed data re-initialize the snappy's output direct buffer decompress data get atmost 'len' bytes never use this function in blockdecompressorstream. do nothing"
org.apache.hadoop.io.compress.SnappyCodec "this class creates snappy compressors/decompressors. set the configuration to be used by this object. the configuration object. return the configuration used by this object. configuration object used by this objec. are the native snappy libraries loaded & initialized? create a {@link compressionoutputstream} that will write to the given {@link outputstream}. the location for the final output stream stream the create a {@link compressionoutputstream} that will write to the given {@link outputstream} with the given {@link compressor}. the location for the final output stream compressor to use stream the get the type of {@link compressor} needed by this {@link compressioncodec}. type of compressor needed by this codec. create a new {@link compressor} for use by this {@link compressioncodec}. new compressor for use by this codec create a {@link compressioninputstream} that will read from the given input stream. the stream to read compressed bytes from stream to read uncompressed bytes from @throws ioexception create a {@link compressioninputstream} that will read from the given {@link inputstream} with the given {@link decompressor}. the stream to read compressed bytes from decompressor to use stream to read uncompressed bytes from @throws ioexception get the type of {@link decompressor} needed by this {@link compressioncodec}. type of decompressor needed by this codec. create a new {@link decompressor} for use by this {@link compressioncodec}. new decompressor for use by this codec get the default filename extension for this kind of compression. .snappy. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.SplitCompressionInputStream "an inputstream covering a range of compressed data. the start and end offsets requested by a client may be modified by the codec to fit block boundaries or other algorithm-dependent requirements. after calling createinputstream, the values of start or end might change. so this method can be used to get the new value of start. changed value of start after calling createinputstream, the values of start or end might change. so this method can be used to get the new value of end. changed value of end www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.SplittableCompressionCodec "this interface is meant to be implemented by those compression codecs which are capable to compress / de-compress a stream starting at any arbitrary position. especially the process of de-compressing a stream starting at some arbitrary position is challenging. most of the codecs are only able to successfully de-compress a stream, if they start from the very beginning till the end. one of the reasons is the stored state at the beginning of the stream which is crucial for de-compression. yet there are few codecs which do not save the whole state at the beginning of the stream and hence can be used to de-compress stream starting at any arbitrary points. this interface is meant to be used by such codecs. such codecs are highly valuable, especially in the context of hadoop, because an input compressed file can be split and hence can be worked on by multiple machines in parallel. during decompression, data can be read off from the decompressor in two modes, namely continuous and blocked. few codecs (e.g. bzip2) are capable of compressing data in blocks and then decompressing the blocks. in blocked reading mode codecs inform 'end of block' events to its caller. while in continuous mode, the caller of codecs is unaware about the blocks and uncompressed data is spilled out like a continuous stream. create a stream as dictated by the readmode. this method is used when the codecs wants the ability to work with the underlying stream positions. the seekable input stream (seeks in compressed data) the start offset into the compressed stream. may be changed by the underlying codec. the end offset into the compressed stream. may be changed by the underlying codec. controls whether stream position is reported continuously from the compressed stream only only at block boundaries. a stream to read uncompressed bytes from www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor "a {@link decompressor} based on the popular gzip compressed file format. http://www.gzip.org/ the current state of the gzip decoder, external to the inflater context. (technically, the private variables localbuf through hasheadercrc are also part of the state, so this enum is merely the label for it.) immediately prior to or (strictly) within the 10-byte basic gzip header. immediately prior to or within the optional "extra field." immediately prior to or within the optional filename field. immediately prior to or within the optional comment field. immediately prior to or within the optional 2-byte header crc value. immediately prior to or within the main compressed (deflate) data stream. immediately prior to or (strictly) within the 4-byte uncompressed crc. immediately prior to or (strictly) within the 4-byte uncompressed size. immediately after the trailer (and potentially prior to the next gzip member/substream header), without reset() having been called. creates a new (pure java) gzip decompressor. {@inheritdoc} in our case, the input data includes both gzip header/trailer bytes (which we handle in executestate()) and deflate-stream bytes (which we hand off to inflater). note: this code assumes the data passed in via b[] remains unmodified until _we_ signal that it's safe to modify it (via needsinput()). the alternative would require an additional buffer-copy even for the bulk deflate stream, which is a performance hit we don't want to absorb. (decompressor now documents this requirement.) decompress the data (gzip header, deflate stream, gzip trailer) in the provided buffer. number of decompressed bytes placed into b from the caller's perspective, this is where the state machine lives. the code is written such that we never return from decompress() with data remaining in parse the gzip header (assuming we're in the appropriate state). in order to deal with degenerate cases (e.g., parse the gzip trailer (assuming we're in the appropriate state). in order to deal with degenerate cases (e.g., returns the total number of compressed bytes input so far, including gzip header/trailer bytes. total (non-negative) number of compressed bytes read so far returns the number of bytes remaining in the input buffer; normally called when finished() is true to determine amount of post-gzip-stream data. note that, other than the finished state with concatenated data after the end of the current gzip stream, this will never return a non-zero value unless called after {@link #setinput(byte[] b, int off, int len)} and before {@link #decompress(byte[] b, int off, int len)}. (that is, after {@link #decompress(byte[] b, int off, int len)} it always returns zero, except in finished state with concatenated data.) total (non-negative) number of unprocessed bytes in input returns true if the end of the gzip substream (single "member") has been reached. resets everything, including the input buffer, regardless of whether the current gzip substream is finished. check id bytes (throw if necessary), compression method (throw if not 8), and flag bits (set hasextrafield, hasfilename, hascomment, hasheadercrc). ignore mtime, xfl, os. caller must ensure we have at least 10 bytes (at the start of localbuf). flag bits (remainder are reserved and must be zero): bit 0 ftext bit 1 fhcrc (never implemented in gzip, at least through version 1.4.0; instead interpreted as "continuation of multi- part gzip file," which is unsupported through 1.4.0) bit 2 fextra bit 3 fname bit 4 fcomment [bit 5 encrypted] www.apache.org/licenses/license-2.0 www.gzip.org/ if read as le short int 'true' (nowrap) => inflater will handle raw deflate stream only fixme? inflater docs say: 'it is also necessary to provide an extra "dummy" byte as input. this is required by the zlib native library in order to support certain optimizations.' however, this does not appear to be true, and in any case, it's not entirely clear where the byte should go or what its value should be. perhaps it suffices to have some deflated bytes in the first buffer load? (but how else would one do it?) most common case see verify note: might be zero "executedeflatestreamstate()" hand off inflater may not have consumed all of previous bufferload (e.g., if data highly compressed or output buffer very small), in which case  now decompress it into b[] crc-32 is on _uncompressed_ data could save a copy of verify that bytesremaining  get numextrafieldbytesremaining, or already have 2 bytes & waiting to finish skipping specified length modifies exit early: used up entire buffer without hitting null exit early: used up entire buffer will reuse for crc-32 of uncompressed data switching to inflater now www.ietf.org/rfc/rfc1952.txt for the gzip spec. verify that the crc-32 of the decompressed stream matches the value stored in the gzip trailer localbuf was empty before we handed off to inflater, so we handle this exactly like header fields initially 0, but may need multiple calls verify that the mod-2^32 decompressed stream size matches the value stored in the gzip trailer initially 0, but may need multiple calls modifies should be strictly == could optionally emit info message if state != gzipstatelabel.finished alternatively, could call checkandskipbytes(len) for rest... returns true if saw null, false if ran out of buffer first; called _only_ during gzip-header processing (not trailer) (caller can check before/after state of this one doesn't update the crc and does support trailer processing but otherwise is same as its "checkand" sibling caller is responsible for not overrunning buffer caller is responsible for not overrunning buffer"
org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater "a wrapper around java.util.zip.deflater to make it conform to org.apache.hadoop.io.compress.compressor interface. reinit the compressor with the given configuration. it will reset the compressor's compression level and compression strategy. different from zlibcompressor, builtinzlibdeflater only support three kind of compression strategy: filtered, huffman_only and default_strategy. it will use default_strategy as default if the configured compression strategy is not supported. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater "a wrapper around java.util.zip.inflater to make it conform to org.apache.hadoop.io.compress.decompressor interface. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.zlib.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.compress.zlib.ZlibCompressor "a {@link compressor} based on the popular zlib compression algorithm. http://www.zlib.net/ the compression level for zlib library. compression level for no compression. compression level for fastest compression. compression level for best compression. default compression level. the compression level for zlib library. compression strategy best used for data consisting mostly of small values with a somewhat random distribution. forces more huffman coding and less string matching. compression strategy for huffman coding only. compression strategy to limit match distances to one (run-length encoding). compression strategy to prevent the use of dynamic huffman codes, allowing for a simpler decoder for special applications. default compression strategy. the type of header for compressed data. no headers/trailers/checksums. default headers/trailers/checksums. simple gzip headers/trailers. creates a new compressor with the default compression level. compressed data will be generated in zlib format. creates a new compressor, taking settings from the configuration. creates a new compressor using the specified compression level. compressed data will be generated in zlib format. compression level #compressionlevel compression strategy #compressionstrategy compression header #compressionheader size of the direct buffer to be used. prepare the compressor to be used in a new stream with settings defined in the given configuration. it will reset the compressor's compression level and compression strategy. configuration storing new settings returns the total number of compressed bytes output so far. total (non-negative) number of compressed bytes output so far returns the total number of uncompressed bytes input so far. total (non-negative) number of uncompressed bytes input so far www.apache.org/licenses/license-2.0 www.zlib.net/ hack - use this as a global lock in the jni layer initialize the native library ignore failure to load/initialize native-zlib reinitialize zlib's output direct buffer copy enough data from consume remaining compressed data? check if zlib has consumed all input compress should be invoked if keepuncompressedbuf true check if we have consumed all copy enough data from uncompresseddirectbuf is not full check if 'zlib' says its 'finished' and all compressed data has been consumed check if there is compressed data re-initialize the zlib's output direct buffer compress data check if zlib consumed all input buffer set keepuncompressedbuf properly zlib consumed all input buffer zlib did not consume all input buffer get atmost 'len' bytes"
org.apache.hadoop.io.compress.zlib.ZlibDecompressor "a {@link decompressor} based on the popular zlib compression algorithm. http://www.zlib.net/ the headers to detect from compressed data. no headers/trailers/checksums. default headers/trailers/checksums. simple gzip headers/trailers. autodetect gzip/zlib headers/trailers. creates a new decompressor. returns the total number of uncompressed bytes output so far. total (non-negative) number of uncompressed bytes output so far returns the total number of compressed bytes input so far. total (non-negative) number of compressed bytes input so far returns the number of bytes remaining in the input buffers; normally called when finished() is true to determine amount of post-gzip-stream data. total (non-negative) number of unprocessed bytes in input resets everything including the input buffers ( www.apache.org/licenses/license-2.0 www.zlib.net/ hack - use this as a global lock in the jni layer initialize the native library ignore failure to load/initialize native-zlib reinitialize zlib's output direct buffer reinitialize zlib's input direct buffer note how much data is being fed to zlib consume remaining compressed data? check if zlib has consumed all input check if we have consumed all check if 'zlib' says it's 'finished' and all compressed data has been consumed check if there is uncompressed data re-initialize the zlib's output direct buffer decompress data get at most 'len' bytes"
org.apache.hadoop.io.compress.zlib.ZlibFactory "a collection of factories to create the right zlib/gzip compressor/decompressor instances. check if native-zlib code is loaded & initialized correctly and can be loaded for this job. configuration true if native-zlib is loaded & initialized and can be loaded for this job, else false return the appropriate type of the zlib compressor. configuration appropriate type of the zlib compressor. return the appropriate implementation of the zlib compressor. configuration appropriate implementation of the zlib compressor. return the appropriate type of the zlib decompressor. configuration appropriate type of the zlib decompressor. return the appropriate implementation of the zlib decompressor. configuration appropriate implementation of the zlib decompressor. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.CompressedWritable "a base-class for writables which store themselves compressed and lazily inflate on field access. this is useful for large objects whose fields are not be altered during a map or reduce operation: leaving the field data compressed makes copying the instance from one file to another much faster. must be called by all methods which access fields to ensure that the data has been uncompressed. subclasses implement this instead of {@link #readfields(datainput)}. subclasses implement this instead of {@link #write(dataoutput)}. www.apache.org/licenses/license-2.0 if non-null, the compressed field data of this instance."
org.apache.hadoop.io.DataInputBuffer "a reusable {@link datainput} implementation that reads from an in-memory buffer. this saves memory over creating a new datainputstream and bytearrayinputstream each time data is read. typical usage is something like the following: datainputbuffer buffer = new datainputbuffer(); while (... loop condition ...) { byte[] data = ... get data ...; int datalength = ... get data length ...; buffer.reset(data, datalength); ... read buffer using datainput methods ... }  constructs a new empty buffer. resets the data that the buffer reads. resets the data that the buffer reads. returns the current position in the input. returns the length of the input. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.DataInputByteBuffer "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.DataOutputBuffer "a reusable {@link dataoutput} implementation that writes to an in-memory buffer. this saves memory over creating a new dataoutputstream and bytearrayoutputstream each time data is written. typical usage is something like the following: dataoutputbuffer buffer = new dataoutputbuffer(); while (... loop condition ...) { buffer.reset(); ... write buffer using dataoutput methods ... byte[] data = buffer.getdata(); int datalength = buffer.getlength(); ... write data to its ultimate destination ... }  constructs a new empty buffer. returns the current contents of the buffer. data is only valid to {@link #getlength()}. returns the length of the valid data currently in the buffer. resets the buffer to empty. writes bytes from a datainput directly into the buffer. write to a file stream www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.DataOutputByteBuffer "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.DataOutputOutputStream "outputstream implementation that wraps a dataoutput. construct an outputstream from the given dataoutput. if 'out' is already an outputstream, simply returns it. otherwise, wraps it in an outputstream. the dataoutput to wrap outputstream instance that outputs to 'out' www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.DefaultStringifier "defaultstringifier is the default implementation of the {@link stringifier} interface which stringifies the objects using base64 encoding of the serialized version of the objects. the {@link serializer} and {@link deserializer} are obtained from the {@link serializationfactory}.  defaultstringifier offers convenience methods to store/load objects to/from the configuration.  the class of the objects to stringify stores the item in the configuration with the given keyname.  the class of the item the configuration to store the object to be stored the name of the key to use @throws ioexception : forwards exceptions from the underlying {@link serialization} classes. restores the object from the configuration.  the class of the item the configuration to use the name of the key to use the class of the item object @throws ioexception : forwards exceptions from the underlying {@link serialization} classes. stores the array of items in the configuration with the given keyname.  the class of the item the configuration to use the objects to be stored the name of the key to use @throws indexoutofboundsexception if the items array is empty @throws ioexception : forwards exceptions from the underlying {@link serialization} classes. restores the array of objects from the configuration.  the class of the item the configuration to use the name of the key to use the class of the item object @throws ioexception : forwards exceptions from the underlying {@link serialization} classes. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.DoubleWritable "writable for double values. returns true iff o is a doublewritable with the same value. a comparator optimized for doublewritable. www.apache.org/licenses/license-2.0 register this comparator"
org.apache.hadoop.io.EnumSetWritable "a writable wrapper for enumset. construct a new enumsetwritable. if the value argument is null or its size is zero, the elementtype argument must not be null. if the argument value's size is bigger than zero, the argument elementtype is not be used. construct a new enumsetwritable. argument value should not be null or empty. reset the enumsetwritable with specified value and elementtype. if the value argument is null or its size is zero, the elementtype argument must not be null. if the argument value's size is bigger than zero, the argument elementtype is not be used. return the value of this enumsetwritable. returns true if o is an enumsetwritable with the same value, or both are null. returns the class of all the elements of the underlying enumsetwriable. it may return null. element class www.apache.org/licenses/license-2.0 other.value must not be null if we reach here"
org.apache.hadoop.io.FastByteComparisons "utility code to do optimized byte-array comparison. this is borrowed and slightly modified from guava's {@link unsignedbytes} class to be able to compare arrays that start at non-zero offsets. lexicographically compare two byte arrays. provides a lexicographical comparer implementation; either a java implementation or a faster implementation based on {@link unsafe}. uses reflection to gracefully fall back to the java implementation if {@code unsafe} isn't available. returns the unsafe-using comparer, or falls back to the pure-java implementation if unable to do so. the offset to the first element in a byte array. returns true if x1 is less than x2, when both values are treated as unsigned. lexicographically compare two arrays. 1 left operand 2 right operand 1 where to start comparing in the left buffer 2 where to start comparing in the right buffer 1 how much to compare from the left buffer 2 how much to compare from the right buffer 0 if equal,  ensure we really catch everything short circuit equal case bring writablecomparator code local used via reflection it doesn't matter what we throw; it's swallowed in getbestcomparer(). sanity check - this should never fail short circuit equal case use binary search the epilogue to cover the last (minlength % 8) elements."
org.apache.hadoop.io.file.tfile.BCFile "block compressed file, the underlying physical storage layer for tfile. bcfile provides the basic block level compression for the data block and meta blocks. it is separated from tfile as it may be used for other block-compressed file implementation. prevent the instantiation of bcfile objects. bcfile writer, the entry point for creating a new bcfile. call-back interface to register a block after a block is closed. register a block that is fully closed. the size of block in terms of uncompressed bytes. the start offset of the block. one byte after the end of the block. compressed block size is offsetend - offsetstart. intermediate class that maintain the state of a writable compression block. the compression algorithm to be used to for compression. @throws ioexception get the output stream for blockappender's consumption. output stream suitable for writing block data. get the current position in file. current byte offset in underlying file. @throws ioexception current size of compressed data. @return @throws ioexception finishing up the current block. access point to stuff data into a block. todo: change dataoutputstream to something else that tracks the size as long instead of int. currently, we will wrap around if the row block size is greater than 4gb. constructor the block register, which is called when the block is closed. the writable compression block state. get the raw size of the block. number of uncompressed bytes written through the blockappender so far. @throws ioexception expecting the size() of a block not exceeding 4gb. assuming the size() will wrap to negative integer if it exceeds 2gb. get the compressed size of the block in progress. number of compressed bytes written to the underlying fs file. the size may be smaller than actual need to compress the all data written due to internal buffering inside the compressor. @throws ioexception signaling the end of write to the block. the block register will be called for registering the finished block. constructor fs output stream. name of the compression algorithm, which will be used for all data blocks. @throws ioexception @see compression#getsupportedalgorithms close the bcfile writer. attempting to use the writer after calling close is not allowed and may lead to undetermined results. create a meta block and obtain an output stream for adding data into the block. there can only be one blockappender stream active at any time. regular blocks may not be create a meta block and obtain an output stream for adding data into the block. the meta block will be compressed with the same compression algorithm as data blocks. there can only be one blockappender stream active at any time. regular blocks may not be create a data block and obtain an output stream for adding data into the block. there can only be one blockappender stream active at any time. data blocks may not be callback to make sure a meta block is added to the internal list when its stream is closed. callback to make sure a data block is added to the internal list when it's being closed. bcfile reader, interface to read the file's data and meta blocks. intermediate class that maintain the state of a readable compression block. get the output stream for blockappender's consumption. output stream suitable for writing block data. access point to read a block. finishing reading the block. release all resources. get the name of the compression algorithm used to compress the block. of the compression algorithm. get the uncompressed size of the block. size of the block. get the compressed size of the block. size of the block. get the starting position of the block in the file. starting position of the block in the file. constructor fs input stream. length of the corresponding file @throws ioexception get the name of the default compression algorithm. name of the default compression algorithm. get version of bcfile file being read. of bcfile file being read. get version of bcfile api. of bcfile api. finishing reading the bcfile. release all resources. get the number of data blocks. number of data blocks. stream access to a meta block. meta block name input stream for reading the meta block. @throws ioexception @throws metablockdoesnotexist the meta block with the given name does not exist. stream access to a data block. 0-based data block index. input stream for reading the data block. @throws ioexception find the smallest block index whose starting offset is greater than or equal to the specified offset. index for all meta blocks. an entry describes a meta block in the metaindex. index of all compressed data blocks. magic number uniquely identifying a bcfile in the header/footer. block region. www.apache.org/licenses/license-2.0 the current version of bcfile impl, increment them (major or minor) made enough changes nothing the single meta block containing index of compressed data blocks index for meta blocks reusable buffers. !null only if using native hadoop compression the down stream is a special kind of stream that finishes a compression block upon flush. so we disable flush() here. add metabcfileindex to metaindex as the last meta block meta index and the trailing section are written out directly. do nothing index for meta blocks do not set rblkstate to null. people may access stats after calling close(). move the cursor to the beginning of the tail, containing: offset to the meta block index, version and magic read meta index read data:bcfile.index, the data block index nothing to be done now use a tree map, for getting a meta block entry by name for write for read, construct the map from the file for data blocks, each entry specifies a block's offset, compressed size and raw size for read, deserialized from a file for write ... total of 16 bytes check against ab_magic_bcfile, if not matching, throw an exception"
org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream "boundedrangefileinputstream abstracts a contiguous region of a hadoop fsdatainputstream as a regular input stream. one can create multiple boundedrangefileinputstream on top of the same fsdatainputstream and they would not interfere with each other. constructor the fsdatainputstream we connect to. begining offset of the region. length of the region. the actual length of the region may be smaller if (off_begin + length) goes beyond the end of fs input stream. we may skip beyond the end of the file. www.apache.org/licenses/license-2.0 invalidate the state of the stream."
org.apache.hadoop.io.file.tfile.ByteArray "adaptor class to wrap byte-array backed objects (including java byte array) as rawcomparable objects. constructing a bytearray from a {@link byteswritable}. wrap a whole byte array as a rawcomparable. the byte array buffer. wrap a partial byte array as a rawcomparable. the byte array buffer. the starting offset the length of the consecutive bytes to be wrapped. underlying buffer. offset in the buffer. size of the byte array. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.file.tfile.Chunk "several related classes to support chunk-encoded sub-streams on top of a regular stream. prevent the instantiation of class. decoding a chain of chunks encoded through chunkencoder or singlechunkencoder. constructor the source input stream which contains chunk-encoded data stream. have we reached the last chunk. if we have reached the last chunk. @throws java.io.ioexception how many bytes remain in the current chunk? bytes left in the current chunk. @throws java.io.ioexception reading the length of next chunk. @throws java.io.ioexception when no more data is available. check whether we reach the end of the stream. if the chunk encoded stream has more data to read (in which case available() will be greater than 0); true otherwise. @throws java.io.ioexception on i/o errors. this method never blocks the caller. returning 0 does not mean we reach the end of the stream. chunk encoder. encoding the output data into a chain of chunks in the following sequences: -len1, byte[len1], -len2, byte[len2], ... len_n, byte[len_n]. where len1, len2, ..., len_n are the lengths of the data chunks. non-terminal chunks have their lengths negated. non-terminal chunks cannot have length 0. all lengths are in the range of 0 to integer.max_value and are encoded in utils.vint format. the data output stream it connects to. the internal buffer that is only used when we do not know the advertised size. the number of valid bytes in the buffer. this value is always in the range 0 through buf.length; elements buf[0] through buf[count-1] contain valid byte data. constructor. the underlying output stream. write out a chunk. the chunk buffer. offset to chunk buffer for the beginning of chunk. is this the last call to flushbuffer? write out a chunk that is a concatenation of the internal buffer plus flush the internal buffer. is this the last call to flushbuffer? @throws java.io.ioexception if the input data do not fit in buffer, flush the output buffer and then write the data directly. in this way buffered streams will cascade harmlessly. encode the whole stream as a single chunk. expecting to know the size of the chunk up-front. the data output stream it connects to. the remaining bytes to be written. constructor. the underlying output stream. the total # of bytes to be written as a single chunk. @throws java.io.ioexception if an i/o error occurs. www.apache.org/licenses/license-2.0 nothing no need to wind forward the old input. always write out the length for the last chunk."
org.apache.hadoop.io.file.tfile.CompareUtils "prevent the instantiation of class. a comparator to compare anything that implements {@link rawcomparable} using a customized comparator. interface for all objects that has a single integer magnitude. www.apache.org/licenses/license-2.0 nothing"
org.apache.hadoop.io.file.tfile.Compression "compression related stuff. prevent the instantiation of class. compression algorithms. following statement is necessary to get around bugs in 0.18 where a compressor is referenced after returned back to the codec pool. following statement is necessary to get around bugs in 0.18 where a decompressor is referenced after returned back to the codec pool. www.apache.org/licenses/license-2.0 nothing that is okay set the internal buffer size to read from down stream. we require that all compression related settings are configured statically in the configuration object. data input buffer size to absorb small reads from application. data output buffer size to absorb small writes from application. somebody returns the compressor to codecpool but is still using it. somebody returns the decompressor to codecpool but is still using it."
org.apache.hadoop.io.file.tfile.MetaBlockAlreadyExists "exception - meta block with the same name already exists. constructor message. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.file.tfile.MetaBlockDoesNotExist "exception - no such meta block with the given name. constructor message. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.file.tfile.RawComparable "interface for objects that can be compared through {@link rawcomparator}. this is useful in places where we need a single object reference to specify a range of bytes in a byte array, such as {@link comparable} or {@link collections#binarysearch(java.util.list, object, comparator)} the actual comparison among rawcomparable's requires an external rawcomparator and it is applications' responsibility to ensure two rawcomparable are supposed to be semantically comparable with the same rawcomparator. get the underlying byte array. underlying byte array. get the offset of the first byte in the byte array. offset of the first byte in the byte array. get the size of the byte range in the byte array. size of the byte range in the byte array. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream "a simplified bufferedoutputstream with borrowed buffer, and allow www.apache.org/licenses/license-2.0 the borrowed buffer bytes used in buffer. constructor get the size of internal buffer being used."
org.apache.hadoop.io.file.tfile.TFile "a tfile is a container of key-value pairs. both keys and values are type-less bytes. keys are restricted to 64kb, value length is not restricted (practically limited to the available disk storage). tfile further provides the following features:  block compression. named meta data blocks. sorted or unsorted keys. seek by key or by file offset.  the memory footprint of a tfile includes the following:  some constant overhead of reading or writing a compressed block.  each compressed block requires one compression/decompression codec for i/o. temporary space to buffer the key. temporary space to buffer the value (for tfile.writer only). values are chunk encoded, so that we buffer at most one chunk of compression: gzip compression: lzo compression: none comparator: memcmp comparator prefix: java class make a raw comparator from a string name. comparator name rawcomparable comparator. get names of supported compression algorithms. the names are acceptable by tfile.writer. of strings, each represents a supported compression algorithm. currently, the following compression algorithms are supported.  "none" - no compression. "lzo" - lzo compression. "gz" - gzip compression.  tfile writer. writer states. the state always transits in circles: ready -> in_key -> end_key -> in_value -> ready. constructor output stream for writing. must be at position 0. minimum compressed block size in bytes. a compression block will not be closed until it reaches this size except for the last block. name of the compression algorithm. must be one of the strings returned by {@link tfile#getsupportedcompressionalgorithms()}. leave comparator as null or empty string if tfile is not sorted. otherwise, provide the string name for the comparison algorithm for keys. two kinds of comparators are supported.  algorithmic comparator: binary comparators that is language independent. currently, only "memcmp" is supported. language-specific comparator: binary comparators that can only be constructed in specific language. for java, the syntax is "jclass:", followed by the class name of the rawcomparator. currently, we only support rawcomparators that can be constructed through the default constructor (with no parameters). parameterized rawcomparators such as {@link writablecomparator} or {@link javaserializationcomparator} may not be directly used. one should write a wrapper class that inherits from such classes and use its default constructor to perform proper initialization.  the configuration object. @throws ioexception close the writer. resources will be released regardless of the exceptions being thrown. future close calls will have no effect. the underlying fsdataoutputstream is not closed. adding a new key-value pair to the tfile. this is synonymous to append(key, 0, key.length, value, 0, value.length) buffer for key. buffer for value. @throws ioexception adding a new key-value pair to tfile. buffer for key. offset in key buffer. length of key. buffer for value. offset in value buffer. length of value. @throws ioexception upon io errors.  if an exception is thrown, the tfile will be in an inconsistent state. the only legitimate call after that would be close helper class to register key after close call on key append stream. verify length. helper class to register value after close call on value append stream. obtain an output stream for writing a key into tfile. this may only be called when there is no active key appending stream or value appending stream. the expected length of the key. if length of the key is not known, set length = -1. otherwise, the application must write exactly as many bytes as specified here before calling close on the returned output stream. key appending output stream. @throws ioexception obtain an output stream for writing a value into tfile. this may only be called right after a key appending operation (the key append stream must be closed). the expected length of the value. if length of the value is not known, set length = -1. otherwise, the application must write exactly as many bytes as specified here before calling close on the returned output stream. advertising the value size up-front guarantees that the value is encoded in one chunk, and avoids intermediate chunk buffering. @throws ioexception obtain an output stream for creating a meta block. this function may not be called when there is a key append stream or value append stream active. no more key-value insertion is allowed after a meta data block has been added to tfile. name of the meta block. name of the compression algorithm to be used. must be one of the strings returned by {@link tfile#getsupportedcompressionalgorithms()}. dataoutputstream that can be used to write meta block data. closing the stream would signal the ending of the block. @throws ioexception @throws metablockalreadyexists the meta block with the same name already exists. obtain an output stream for creating a meta block. this function may not be called when there is a key append stream or value append stream active. no more key-value insertion is allowed after a meta data block has been added to tfile. data will be compressed using the default compressor as defined in writer's constructor. name of the meta block. dataoutputstream that can be used to write meta block data. closing the stream would signal the ending of the block. @throws ioexception @throws metablockalreadyexists the meta block with the same name already exists. check if we need to start a new data block. @throws ioexception close the current data block if necessary. force the closure regardless of the block size. @throws ioexception tfile reader. location representing a virtual position in the tfile. @see java.lang.comparable#compareto(java.lang.object) @see java.lang.object#clone() @see java.lang.object#hashcode() @see java.lang.object#equals(java.lang.object) constructor fs input stream of the tfile. the length of tfile. this is required because we have no easy way of knowing the actual size of the input file through the file input stream. @throws ioexception close the reader. the state of the reader object is undefined after close. calling close() for multiple times has no effect. get the begin location of the tfile. tfile is not empty, the location of the first key-value pair. otherwise, it returns end(). get the end location of the tfile. location right after the last key-value pair in tfile. get the string representation of the comparator. the tfile is not sorted by keys, an empty string will be returned. otherwise, the actual comparator string that is provided during the tfile creation time will be returned. is the tfile sorted? if tfile is sorted. get the number of key-value pair entries in tfile. number of key-value pairs in tfile lazily loading the tfile index. @throws ioexception get the first key in the tfile. first key in the tfile. @throws ioexception get the last key in the tfile. last key in the tfile. @throws ioexception get a comparator object to compare entries. it is useful when you want stores the entries in a collection (such as priorityqueue) and perform sorting or comparison among entries based on the keys without copying out the key. entry comparator.. provide a customized comparator for entries. this is useful if we have a collection of entry objects. however, if the entry objects come from different tfiles, get an instance of the rawcomparator that is constructed based on the string comparator representation. comparator that can compare rawcomparable's. stream access to a meta block.`` the name of the meta block. input stream. @throws ioexception on i/o error. @throws metablockdoesnotexist if the meta block with the name does not exist. if greater is true then returns the beginning location of the block containing the key strictly greater than input key. if greater is false then returns the beginning location of the block greater than equal to the input key the input key boolean flag @return @throws ioexception get the location pointing to the beginning of the first key-value pair in a compressed block whose byte offset in the tfile is greater than or equal to the specified offset. the get the recordnum for the first key-value pair in a compressed block whose byte offset in the tfile is greater than or equal to the specified offset. the get a sample key that is within a block whose starting offset is greater than or equal to the specified offset. the file offset. key that fits the requirement; or null if no such key exists (which could happen if the offset is close to the end of the tfile). @throws ioexception get a scanner than can scan the whole tfile. scanner object. a valid scanner is always returned even if the tfile is empty. @throws ioexception get a scanner that covers a portion of tfile based on byte offsets. the beginning byte offset in the tfile. the length of the region. actual coverage of the returned scanner tries to match the specified byte-region but always round up to the compression block boundaries. it is possible that the returned scanner contains zero key-value pairs even if length is positive. @throws ioexception get a scanner that covers a portion of tfile based on keys. begin key of the scan (inclusive). if null, scan from the first key-value entry of the tfile. end key of the scan (exclusive). if null, scan up to the last key-value entry of the tfile. actual coverage of the returned scanner will cover all keys greater than or equal to the beginkey and less than the endkey. @throws ioexception @deprecated use {@link #createscannerbykey(byte[], byte[])} instead. get a scanner that covers a portion of tfile based on keys. begin key of the scan (inclusive). if null, scan from the first key-value entry of the tfile. end key of the scan (exclusive). if null, scan up to the last key-value entry of the tfile. actual coverage of the returned scanner will cover all keys greater than or equal to the beginkey and less than the endkey. @throws ioexception get a scanner that covers a specific key range. begin key of the scan (inclusive). if null, scan from the first key-value entry of the tfile. end key of the scan (exclusive). if null, scan up to the last key-value entry of the tfile. actual coverage of the returned scanner will cover all keys greater than or equal to the beginkey and less than the endkey. @throws ioexception @deprecated use {@link #createscannerbykey(rawcomparable, rawcomparable)} instead. get a scanner that covers a specific key range. begin key of the scan (inclusive). if null, scan from the first key-value entry of the tfile. end key of the scan (exclusive). if null, scan up to the last key-value entry of the tfile. actual coverage of the returned scanner will cover all keys greater than or equal to the beginkey and less than the endkey. @throws ioexception create a scanner that covers a range of records. the recordnum for the first record (inclusive). the recordnum for the last record (exclusive). to scan the whole file, either specify endrecnum==-1 or endrecnum==getentrycount(). tfile scanner that covers the specified range of records. @throws ioexception the tfile scanner. the scanner has an implicit cursor, which, upon creation, points to the first key-value pair in the scan range. if the scan range is empty, the cursor will point to the end of the scan range.  use {@link scanner#atend()} to test whether the cursor is at the end location of the scanner.  use {@link scanner#advance()} to move the cursor to the next key-value pair (or end if none exists). use seekto methods ( {@link scanner#seekto(byte[])} or {@link scanner#seekto(byte[], int, int)}) to seek to any arbitrary location in the covered range (including backward seeking). use {@link scanner#rewind()} to seek back to the beginning of the scanner. use {@link scanner#seektoend()} to seek to the end of the scanner.  actual keys and values may be obtained through {@link scanner.entry} object, which is obtained through {@link scanner#entry()}. constructor the tfile reader object. begin byte-offset of the scan. end byte-offset of the scan. @throws ioexception the offsets will be rounded to the beginning of a compressed block whose offset is greater than or equal to the specified offset. constructor the tfile reader object. begin location of the scan. end location of the scan. @throws ioexception constructor the tfile reader object. begin key of the scan. if null, scan from the first  entry of the tfile. end key of the scan. if null, scan up to the last  entry of the tfile. @throws ioexception move the cursor to the first entry whose key is greater than or equal to the input key. synonymous to seekto(key, 0, key.length). the entry returned by the previous entry() call will be invalid. the input key if we find an equal key. @throws ioexception move the cursor to the first entry whose key is greater than or equal to the input key. the entry returned by the previous entry() call will be invalid. the input key offset in the key buffer. key buffer length. if we find an equal key; false otherwise. @throws ioexception move the cursor to the new location. the entry returned by the previous entry() call will be invalid. new cursor location. it must fall between the begin and end location of the scanner. @throws ioexception rewind to the first entry in the scanner. the entry returned by the previous entry() call will be invalid. @throws ioexception seek to the end of the scanner. the entry returned by the previous entry() call will be invalid. @throws ioexception move the cursor to the first entry whose key is greater than or equal to the input key. synonymous to lowerbound(key, 0, key.length). the entry returned by the previous entry() call will be invalid. the input key @throws ioexception move the cursor to the first entry whose key is greater than or equal to the input key. the entry returned by the previous entry() call will be invalid. the input key offset in the key buffer. key buffer length. @throws ioexception move the cursor to the first entry whose key is strictly greater than the input key. synonymous to upperbound(key, 0, key.length). the entry returned by the previous entry() call will be invalid. the input key @throws ioexception move the cursor to the first entry whose key is strictly greater than the input key. the entry returned by the previous entry() call will be invalid. the input key offset in the key buffer. key buffer length. @throws ioexception move the cursor to the next key-value pair. the entry returned by the previous entry() call will be invalid. if the cursor successfully moves. false when cursor is already at the end location and cannot be advanced. @throws ioexception load a compressed block for reading. expecting blockindex is valid. @throws ioexception close the scanner. release all resources. the behavior of using the scanner after calling close is not defined. the entry returned by the previous entry() call will be invalid. is cursor at the end location? if the cursor is at the end location. check whether we have already successfully obtained the key. it also initializes the valueinputstream. get an entry to access the key and value. entry object to access the key and value. @throws ioexception get the recordnum corresponding to the entry pointed by the cursor. recordnum corresponding to the entry pointed by the cursor. @throws ioexception internal api. comparing the key at cursor to entry to a &lt;key, value&gt; pair. get the length of the key. length of the key. copy the key and value in one shot into byteswritables. this is equivalent to getkey(key); getvalue(value); byteswritable to hold key. byteswritable to hold value @throws ioexception copy the key into byteswritable. the input byteswritable will be automatically resized to the actual key size. byteswritable to hold the key. @throws ioexception copy the value into byteswritable. the input byteswritable will be automatically resized to the actual value size. the implementation directly uses the buffer inside byteswritable for storing the value. the call does not require the value length to be known. @throws ioexception writing the key to the output stream. this method avoids copying key buffer from scanner into writing the value to the output stream. this method avoids copying value data from scanner into copy the key into copy the key into streaming access to the key. useful for desrializing the key into get the length of the value. isvaluelengthknown() must be tested true. length of the value. copy value into copy value into stream access to value. the value part of the key-value pair pointed by the current cursor is not cached and can only be examined once. calling any of the following functions more than once without moving the cursor will result in exception: {@link #getvalue(byte[])}, {@link #getvalue(byte[], int)}, {@link #getvaluestream}. input stream for reading the value. @throws ioexception check whether it is safe to call getvaluelength(). if value length is known before hand. values less than the chunk size will always have their lengths known before hand. values that are written out as a whole (with advertised length up-front) will always have their lengths known in read. compare the entry key to another key. synonymous to compareto(key, 0, key.length). the key buffer. result between the entry key with the input key. compare the entry key to another key. synonymous to compareto(new bytearray(buf, offset, length) the key buffer offset into the key buffer. the length of the key. result between the entry key with the input key. compare an entry with a rawcomparable object. this is useful when entries are stored in a collection, and we want to compare a compare whether this and other points to the same key value. advance cursor by n positions within the block. number of key-value pairs to skip in block. @throws ioexception advance cursor in block until we find a key that is greater than or equal to the input key. key to compare. advance until we find a key greater than the input key. if we find a equal key. @throws ioexception data structure representing "tfile.meta" meta block. data structure representing "tfile.index" meta block. for reading from file. @throws ioexception input key. id of the first block that contains key >= input key. or -1 if no such block exists. input key. id of the first block that contains key > input key. or -1 if no such block exists. for writing to file. tfile data index entry. we should try to make the memory footprint of each index entry as small as possible. dumping the tfile information. a list of tfile paths. www.apache.org/licenses/license-2.0 issues.apache.org/jira/browse/hadoop-3315>hadoop-3315. 64kb prevent the instantiation of tfiles nothing minimum compressed size for a block. meta blocks. reference to the underlying bcfile. current data block appender. buffers for caching the key. buffer used by chunk codec ready to start a new key-value pair insertion. in the middle of key insertion. key insertion complete, ready to insert value. in value insertion. error, // error encountered, cannot continue. tfile already closed. current state of writer. first try the normal finish. terminate upon the first exception. first, write out data:tfile.meta second, write out data:tfile.index avoiding flushing call to down stream. do nothing bump up the total record count in the whole file unknown length for each new block, get a new appender exceeded the size limit, do the compression and finish the block keep tracks of the last key of each data block, no padding for now close the appender the underlying bcfile reader. tfile index, it is loaded lazily. global begin and end locations. distance/offset from the beginning of the block first, read tfile meta set begin and end locations. the underlying tfile reader. current block (null if reaching end) flag to ensure value is only examined once. reusable buffer for keys. length of key, -1 means key is invalid. vlen == -1 if unknown. ensure the tfile index is loaded throughout the life of scanner. todo: remember the longest key in a tfile, and use it to replace max_key_size. check if what we are seeking is in the later part of the current block. sorry, we must seek to a different location first. going to a totally different block may temporarily go beyond the last record in the block (in which case the next if loop will always be true). last entry in tfile. last entry in block. attempt to read one more byte to determine whether we reached the end or not. ctor for writes set fileversion to api version when we create it. ctor for reads unsorted keys default comparator use its default ctor to create an instance end: class metatfilemeta size for the first key entry. not found not found count of  entries in the block. default entry, without any padding"
org.apache.hadoop.io.file.tfile.TFileDumper "dumping the information of a tfile. dump information about tfile. path string of the tfile printstream to output the information. the configuration object. @throws ioexception www.apache.org/licenses/license-2.0 namespace object not constructable. now output the properties table."
org.apache.hadoop.io.file.tfile.Utils "supporting utility classes used by tfile, and shared by prevent the instantiation of utils. encoding an integer into a variable-length encoding format. synonymous to utils#writevlong(out, n). output stream the integer to be encoded @throws ioexception @see utils#writevlong(dataoutput, long) encoding a long integer into a variable-length encoding format.  if n in [-32, 127): encode in one byte with the actual value. otherwise, if n in [-202^8, 202^8): encode in two bytes: byte[0] = n/256 - 52; byte[1]=n&0xff. otherwise, if n in [-162^16, 162^16): encode in three bytes: byte[0]=n/2^16 - 88; byte[1]=(n>>8)&0xff; byte[2]=n&0xff. otherwise, if n in [-82^24, 82^24): encode in four bytes: byte[0]=n/2^24 - 112; byte[1] = (n>>16)&0xff; byte[2] = (n>>8)&0xff; byte[3]=n&0xff. otherwise: if n in [-2^31, 2^31): encode in five bytes: byte[0]=-125; byte[1] = (n>>24)&0xff; byte[2]=(n>>16)&0xff; byte[3]=(n>>8)&0xff; byte[4]=n&0xff; if n in [-2^39, 2^39): encode in six bytes: byte[0]=-124; byte[1] = (n>>32)&0xff; byte[2]=(n>>24)&0xff; byte[3]=(n>>16)&0xff; byte[4]=(n>>8)&0xff; byte[5]=n&0xff if n in [-2^47, 2^47): encode in seven bytes: byte[0]=-123; byte[1] = (n>>40)&0xff; byte[2]=(n>>32)&0xff; byte[3]=(n>>24)&0xff; byte[4]=(n>>16)&0xff; byte[5]=(n>>8)&0xff; byte[6]=n&0xff; if n in [-2^55, 2^55): encode in eight bytes: byte[0]=-122; byte[1] = (n>>48)&0xff; byte[2] = (n>>40)&0xff; byte[3]=(n>>32)&0xff; byte[4]=(n>>24)&0xff; byte[5]=(n>>16)&0xff; byte[6]=(n>>8)&0xff; byte[7]=n&0xff; if n in [-2^63, 2^63): encode in nine bytes: byte[0]=-121; byte[1] = (n>>54)&0xff; byte[2] = (n>>48)&0xff; byte[3] = (n>>40)&0xff; byte[4]=(n>>32)&0xff; byte[5]=(n>>24)&0xff; byte[6]=(n>>16)&0xff; byte[7]=(n>>8)&0xff; byte[8]=n&0xff;  output stream the integer number @throws ioexception decoding the variable-length integer. synonymous to (int)utils#readvlong(in). input stream decoded integer @throws ioexception @see utils#readvlong(datainput) decoding the variable-length integer. suppose the value of the first byte is fb, and the following bytes are nb[].  if (fb >= -32), return (long)fb; if (fb in [-72, -33]), return (fb+52)if (fb in [-104, -73]), return (fb+88)if (fb in [-120, -105]), return (fb+112)if (fb in [-128, -121]), return interpret nb[fb+129] as a signed big-endian integer. input stream decoded long integer. @throws ioexception write a string as a vint n, followed by n bytes as in text format. @throws ioexception read a string as a vint n, followed by n bytes in text format. the input stream. string @throws ioexception a generic version class. we suggest applications built on top of tfile use this class to maintain version information in their meta blocks. a version number consists of a major version and a minor version. the suggested usage of major and minor version number is to increment major version number when the new storage format is not backward compatible, and increment the minor version otherwise. construct the version object by reading from the input stream. input stream @throws ioexception constructor. major version. minor version. write the objec to a dataoutput. the serialized format of the version is major version followed by minor version, both as big-endian short integers. the dataoutput object. @throws ioexception get the major version. version. get the minor version. minor version. get the size of the serialized version object. size of the version object. return a string representation of the version. test compatibility. the version object to test compatibility with. if both versions have the same major version number; false otherwise. compare this version with another version. lower bound binary search. find the index to the first element in the list that compares greater than or equal to key.  type of the input key. the list the input key. comparator for the key. index to the desired element if it exists; or list.size() otherwise. upper bound binary search. find the index to the first element in the list that compares greater than the input key.  type of the input key. the list the input key. comparator for the key. index to the desired element if it exists; or list.size() otherwise. lower bound binary search. find the index to the first element in the list that compares greater than or equal to key.  type of the input key. the list the input key. index to the desired element if it exists; or list.size() otherwise. upper bound binary search. find the index to the first element in the list that compares greater than the input key.  type of the input key. the list the input key. index to the desired element if it exists; or list.size() otherwise. www.apache.org/licenses/license-2.0 nothing how many bytes do we need to represent the number with sign bit? fall it through to firstbyte==-1, len=2. fall it through to firstbyte==0/-1, len=3. fall it through to firstbyte==0/-1, len=4."
org.apache.hadoop.io.FloatWritable "a writablecomparable for floats. set the value of this floatwritable. return the value of this floatwritable. returns true iff o is a floatwritable with the same value. compares two floatwritables. a comparator optimized for floatwritable. www.apache.org/licenses/license-2.0 register this comparator"
org.apache.hadoop.io.GenericWritable "a wrapper for writable instances.  when two sequence files, which have same key type but different value types, are mapped out to reduce, multiple value types is not allowed. in this case, this class can help you wrap instances with different types.   compared with objectwritable, this class is much more effective, because objectwritable will append the class declaration as a string into the output file in every key-value pair.   generic writable implements {@link configurable} interface, so that it will be configured by the framework. the configuration is passed to the wrapped objects implementing {@link configurable} interface before deserialization.  how to use it:  1. write your own class, such as genericobject, which extends genericwritable. 2. implements the abstract method gettypes(), defines the classes which will be wrapped in genericobject in application. attention: this classes defined in gettypes() method, must implement writable interface.  the code looks like this:  public class genericobject extends genericwritable { private static class[] classes = { classtype1.class, classtype2.class, classtype3.class, }; protected class[] gettypes() { return classes; } }  8, 2006 set the instance that is wrapped. return the wrapped instance. return all classes that may be wrapped. subclasses should implement this to return a constant array of classes. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.InputBuffer "a reusable {@link inputstream} implementation that reads from an in-memory buffer. this saves memory over creating a new inputstream and bytearrayinputstream each time data is read. typical usage is something like the following: inputbuffer buffer = new inputbuffer(); while (... loop condition ...) { byte[] data = ... get data ...; int datalength = ... get data length ...; buffer.reset(data, datalength); ... read buffer using inputstream methods ... }  @see datainputbuffer @see dataoutput constructs a new empty buffer. resets the data that the buffer reads. resets the data that the buffer reads. returns the current position in the input. returns the length of the input. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.IntWritable "a writablecomparable for ints. set the value of this intwritable. return the value of this intwritable. returns true iff o is a intwritable with the same value. compares two intwritables. a comparator optimized for intwritable. www.apache.org/licenses/license-2.0 register this comparator"
org.apache.hadoop.io.IOUtils "an utility class for i/o related functionality. copies from one stream to another. inputstrem to read from outputstream to write to the size of the buffer whether or not close the inputstream and outputstream at the end. the streams are closed in the finally clause. copies from one stream to another. inputstrem to read from outputstream to write to the size of the buffer copies from one stream to another. closes the input and output streams at the end. inputstrem to read from outputstream to write to the configuration object copies from one stream to another. inputstream to read from outputstream to write to the configuration object whether or not close the inputstream and outputstream at the end. the streams are closed in the finally clause. copies count bytes from one stream to another. inputstream to read from outputstream to write to number of bytes to copy whether to close the streams @throws ioexception if bytes can not be read or written utility wrapper for reading from {@link inputstream}. it catches any errors thrown by the underlying stream (either io or decompression-related), and re-throws as an ioexception. - inputstream to be read from - buffer the data is read into - offset within buf - amount of data to be read of bytes read reads len bytes in a loop. inputstream to read from the buffer to fill offset from the buffer the length of bytes to read @throws ioexception if it could not read requested number of bytes for any reason (including eof) similar to readfully(). skips bytes in a loop. the inputstream to skip bytes from number of bytes to skip. @throws ioexception if it could not skip requested number of bytes for any reason (including eof) close the closeable objects and ignore any {@link ioexception} or null pointers. must only be used for cleanup in exception handlers. the log to record problems to at debug level. can be null. the objects to close closes the stream ignoring {@link ioexception}. must only be called in cleaning up from exception handlers. the stream to close closes the socket ignoring {@link ioexception} the socket to close the /dev/null of outputstreams. write a bytebuffer to a writablebytechannel, handling short writes. the writablebytechannel to write to the input buffer @throws ioexception on i/o error write a bytebuffer to a filechannel at a given offset, handling short writes. the filechannel to write to the input buffer the offset in the file to start writing at @throws ioexception on i/o error www.apache.org/licenses/license-2.0 skip may return 0 even if we're not at eof. luckily, we can use the read() method to figure out if we're at the end."
org.apache.hadoop.io.LongWritable "a writablecomparable for longs. set the value of this longwritable. return the value of this longwritable. returns true iff o is a longwritable with the same value. compares two longwritables. a comparator optimized for longwritable. a decreasing comparator optimized for longwritable. www.apache.org/licenses/license-2.0 register default comparator"
org.apache.hadoop.io.MapFile "a file-based map from keys to values. a map is a directory containing two files, the data file, containing all keys and values in the map, and a smaller index file, containing a fraction of the keys. the fraction is determined by {@link writer#getindexinterval()}. the index file is read entirely into memory. thus key implementations should try to keep themselves small. map files are the name of the index file. the name of the data file. writes a new map. what's the position (in bytes) we wrote when we got the last index what was size when we last wrote an index. set to min_value to ensure that we have an index at position zero -- midkey will throw an exception if this is not the case create the named map for keys of the named class. @deprecated use writer(configuration, path, option...) instead. create the named map for keys of the named class. @deprecated use writer(configuration, path, option...) instead. create the named map for keys of the named class. @deprecated use writer(configuration, path, option...) instead. create the named map for keys of the named class. @deprecated use writer(configuration, path, option...) instead. create the named map using the named key comparator. @deprecated use writer(configuration, path, option...) instead. create the named map using the named key comparator. @deprecated use writer(configuration, path, option...) instead. create the named map using the named key comparator. @deprecated use writer(configuration, path, option...)} instead. create the named map using the named key comparator. @deprecated use writer(configuration, path, option...) instead. the number of entries that are added before an index entry is added. sets the index interval. @see #getindexinterval() sets the index interval and stores it in conf @see #getindexinterval() close the map. append a key/value pair to the map. the key must be greater or equal to the previous key added to the map. provide access to an existing map. number of index entries to skip between each entry. zero by default. setting this to values larger than zero can facilitate opening large map files using less memory. returns the class of keys in this file. returns the class of values in this file. construct a map reader for the named map. @deprecated construct a map reader for the named map using the named comparator. @deprecated override this method to specialize the type of {@link sequencefile.reader} returned. re-positions the reader before its first key. get the key at approximately the middle of the file. or null if the file is empty. reads the final key from the file. key to read into positions the reader at the named key, or if none such exists, at the first entry after the named key. returns true iff the named key exists in this map. positions the reader at the named key, or if none such exists, at the first entry after the named key. 0 - exact match found before parameter is set. - if true, and key does not exist, position file at entry that falls just before key. otherwise, position file at record that sorts just after. 0 - exact match found key and val. returns true if such a pair exists and false when at the end of the map return the value for the named key, or null if none exists. finds the record that is the closest match to the specified key. returns key or if it does not exist, at the first entry after the named key. - - key that we're trying to find - - data value if key is found - - the key that was the closest match or null if eof. finds the record that is the closest match to the specified key. - key that we're trying to find - data value if key is found - if true, and key does not exist, return the first entry that falls just before the key. otherwise, return the record that sorts just after. - the key that was the closest match or null if eof. close the map. renames an existing map directory. deletes the named map file. this method attempts to fix a corrupt mapfile by re-creating its index. filesystem directory containing the mapfile data and index key class (has to be a subclass of writable) value class (has to be a subclass of writable) do not perform any changes, just report what needs to be done of valid entries in this mapfile, or -1 if no fixing was needed @throws exception www.apache.org/licenses/license-2.0 no public ctor the following fields are used only for checking key order our options are a superset of sequence file writer options only write an index if we've changed positions. in a block compressed file, this means we write an entry at the start of each block point to current eof append key/value to data check that keys are well-ordered update lastkey with a copy of key by writing and reading write new key read into lastkey the data, on disk whether the index reader was closed the index, in memory open the data open the index read the index entirely into memory check order to make sure comparator is compatible skip this entry reset skip don't read an index that is the same as the previous one. block compressed map files used to do this (multiple entries would point at the same block) save position make sure index is valid skip to last indexed entry start at the beginning scan to eof restore position make sure index is read seeked before before next indexed but after last seeked do nothing decode insertion point belongs before first entry use beginning of file else use index if we're looking for the key before, we need to keep track of the position we got the current key as well as the position of the key before it. at or beyond desired we're on the first record of this index block and we've already passed the search key. therefore we must be at the beginning of the file, so seek to the beginning of this block and return c we have a previous record to back up to now that we've rewound, the search key must be greater than this key key found key not found. if we didn't get an exact match, and we ended up in the wrong direction relative to the query key, return null since we must be at the beginning or end of the file. there's nothing we can do to fix this! no fixing needed truncated data file. swallow it. copy all entries"
org.apache.hadoop.io.MapWritable "a writable map. default constructor. copy constructor. the map to copy from www.apache.org/licenses/license-2.0 writable write out the number of entries in the map then write out each key/value pair first clear the map. otherwise we will just accumulate entries every time this method is called. read the number of entries in the map then read each key/value pair"
org.apache.hadoop.io.MD5Hash "a writable for md5 hash values. constructs an md5hash. constructs an md5hash from a hex string. constructs an md5hash with a specified value. constructs, reads and returns an instance. copy the contents of another instance into this instance. returns the digest bytes. construct a hash value for a byte array. create a thread local md5 digester construct a hash value for the content from the inputstream. construct a hash value for a byte array. construct a hash value for a string. construct a hash value for a string. construct a half-sized version of this md5. fits in a long return a 32-bit digest of the md5. first 4 bytes of the md5 returns true iff o is an md5hash whose digest contains the same values. returns a hash code value for this object. only uses the first 4 bytes, since md5s are evenly distributed. compares this object with the specified object for order. a writablecomparator optimized for md5hash keys. returns a string representation of this object. sets the digest value from a hex string. www.apache.org/licenses/license-2.0 javadoc from writable javadoc from writable register this comparator"
org.apache.hadoop.io.MultipleIOException "encapsulate a list of {@link ioexception} into an {@link ioexception} require by {@link java.io.serializable} constructor is private, use {@link #createioexception(list)}. underlying exceptions a convenient method to create an {@link ioexception}. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.nativeio.Errno "enum representing posix errno values. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.nativeio.NativeIO "jni wrappers for various native io-related calls not available in java. these functions should generally be used alongside a fallback to another more portable mechanism. no further special treatment. expect random page references. expect sequential page references. will need these pages. don't need these pages. data will be accessed once. wait upon writeout of all pages in the range before performing the write. initiate writeout of all those dirty pages in the range which are not presently under writeback. wait upon writeout of all pages in the range after performing the write. return true if the jni-based native io extensions are available. wrapper around open(2) wrapper around fstat(2) wrapper around chmod(2) wrapper around posix_fadvise(2) wrapper around sync_file_range(2) initialize the jni method id and class id cache call posix_fadvise on the given file descriptor. see the manpage for this syscall for more information. on systems where this call is not available, does nothing. @throws nativeioexception if there is an error with the syscall call sync_file_range on the given file descriptor. see the manpage for this syscall for more information. on systems where this call is not available, does nothing. @throws nativeioexception if there is an error with the syscall result type of the fstat call type of file named pipe (fifo) character special directory block special regular symbolic link socket whiteout set set group id on execution save swapped text even after use read permission, owner write permission, owner execute/search permission, owner returns the file stat for a file descriptor. file descriptor. file descriptor file stat. @throws ioexception thrown if there was an io error while obtaining the file stat. a version of renameto that throws a descriptive exception when it fails. the source path the destination path @throws nativeioexception on failure. a version of renameto that throws a descriptive exception when it fails. the source path the destination path @throws nativeioexception on failure. www.apache.org/licenses/license-2.0 flags for open() call from bits/fcntl.h flags for posix_fadvise() from bits/fcntl.h this can happen if the installed - in this case we can continue without native io after warning mode constants"
org.apache.hadoop.io.nativeio.NativeIOException "an exception generated by a call to the native io code. these exceptions simply wrap errno result codes. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.nativeio.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.NullWritable "singleton writable with no data. returns the single instance of this class. a comparator &quot;optimized&quot; for nullwritable. compare the buffers in serialized form. www.apache.org/licenses/license-2.0 no public ctor register this comparator"
org.apache.hadoop.io.ObjectWritable "a polymorphic writable that writes an instance with it's class name. handles arrays, strings and primitive types without a writable wrapper. return the instance, or null if none. return the class this is meant to be. reset the instance. write a {@link writable}, {@link string}, primitive type, or an array of the preceding. write a {@link writable}, {@link string}, primitive type, or an array of the preceding. - set true for rpc and internal or intra-cluster usages. set false for inter-cluster, file, and other persisted output usages, to preserve the ability to interchange files with other clusters that may not be running the same version of software. sometime in ~2013 we can consider removing this parameter and always using the compact format. read a {@link writable}, {@link string}, primitive type, or an array of the preceding. read a {@link writable}, {@link string}, primitive type, or an array of the preceding. try to instantiate a protocol buffer of the given message class from the given input stream. the class of the generated protocol buffer the input stream to read from instantiated message instance @throws ioexception if an io problem occurs find and load the class with given name classname by first finding it in the specified conf. if the specified conf is null, try load it directly. www.apache.org/licenses/license-2.0 null special case: must come before writing out the declaredclass. if this is an eligible array of primitives, wrap it in an arrayprimitivewritable$internal wrapper class. always write declared non-primitive or non-compact array string primitive type boolean char byte short int long float double void enum writable primitive types boolean char byte short int long float double void array read and unwrap arrayprimitivewritable$internal array. always allow the read, even if write is disabled by allowcompactarrays. string enum writable null store values we can use the built-in parsedelimitedfrom and not have to re-copy the data have to read it into a buffer first, since protobuf doesn't deal with the datainput interface directly. read the size delimiter that writedelimitedto writes this is a bug in hadoop - protobufs should all have this static method"
org.apache.hadoop.io.OutputBuffer "a reusable {@link outputstream} implementation that writes to an in-memory buffer. this saves memory over creating a new outputstream and bytearrayoutputstream each time data is written. typical usage is something like the following: outputbuffer buffer = new outputbuffer(); while (... loop condition ...) { buffer.reset(); ... write buffer using outputstream methods ... byte[] data = buffer.getdata(); int datalength = buffer.getlength(); ... write data to its ultimate destination ... }  @see dataoutputbuffer @see inputbuffer constructs a new empty buffer. returns the current contents of the buffer. data is only valid to {@link #getlength()}. returns the length of the valid data currently in the buffer. resets the buffer to empty. writes bytes from a inputstream directly into the buffer. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.RawComparator " a {@link comparator} that operates directly on byte representations of objects.   @see deserializercomparator compare two objects in binary. b1[s1:l1] is the first object, and b2[s2:l2] is the second object. 1 the first byte array. 1 the position index in b1. the object under comparison's starting index. 1 the length of the object in b1. 2 the second byte array. 2 the position index in b2. the object under comparison's starting index. 2 the length of the object under comparison in b2. integer result of the comparison. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.ReadaheadPool "manages a pool of threads which can issue readahead requests on file descriptors. return the singleton instance for the current process. issue a request to readahead on the given file descriptor. a textual identifier that will be used in error messages (e.g. the file name) the file descriptor to read ahead the current offset at which reads are being issued the configured length to read ahead the maximum offset that will be readahead (useful if, for example, only some segment of the file is requested by the submit a request to readahead on the given file descriptor. a textual identifier used in error messages, etc. the file descriptor to readahead the offset at which to start the readahead the number of bytes to read object representing this pending request an outstanding readahead request that has been submitted to the pool. this request may be pending or may have been completed. cancels the request for readahead. this should be used if the reader no longer needs the requested data, before closing the related file descriptor. it is safe to use even if the readahead request has already been fulfilled. requested offset requested length www.apache.org/licenses/license-2.0 trigger each readahead when we have reached the halfway mark in the previous readahead. this gives the system time to satisfy the readahead before we start reading the data. cancel any currently pending readahead, to avoid piling things up in the queue. each reader should have at most one outstanding request in the queue. we've reached the end of the stream there's a very narrow race here that the file will close right at this instant. but if that happens, we'll likely receive an ebadf error below, and see that it's canceled, ignoring the error. it's also possible that we'll end up requesting readahead on some other fd, which may be wasted work, but won't cause a problem. no big deal - the reader canceled the request and closed the file. we could attempt to remove it from the work queue, but that would add complexity. in practice, the work queues remain very short, so removing canceled requests has no gain."
org.apache.hadoop.io.retry.DefaultFailoverProxyProvider "an implementation of {@link failoverproxyprovider} which does nothing in the event of failover, and always returns the same proxy object. www.apache.org/licenses/license-2.0 nothing to do."
org.apache.hadoop.io.retry.FailoverProxyProvider "an implementer of this interface is capable of providing proxy objects for use in ipc communication, and potentially modifying these objects or creating entirely new ones in the event of certain types of failures. the determination of whether or not to fail over is handled by {@link retrypolicy}. get the proxy object which should be used until the next failover event occurs. proxy object to invoke methods upon called whenever the associated {@link retrypolicy} determines that an error warrants failing over. the proxy object which was being used before this failover event return a reference to the interface this provider's proxy objects actually implement. if any of the methods on this interface are annotated as being {@link idempotent}, then this fact will be passed to the {@link retrypolicy#shouldretry(exception, int, int, boolean)} method on error, for use in determining whether or not failover should be attempted. interface implemented by the proxy objects returned by {@link failoverproxyprovider#getproxy()} www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.retry.Idempotent "used to mark certain methods of an interface as being idempotent, and therefore warrant being retried on failover. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.retry.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.retry.RetryInvocationHandler "the number of times the associated proxyprovider has ever been failed over. www.apache.org/licenses/license-2.0 the number of times this method invocation has been failed over. the number of times this invocation handler has ever been failed over, before this method invocation attempt. used to prevent concurrent failed method invocations from triggering multiple failover attempts. retry or failover avoid logging the failover if this is the first call on this proxy object, and we successfully achieve the failover without any flip-flopping make sure that concurrent failed method invocations only cause a single actual fail over. rpcinvocationhandler"
org.apache.hadoop.io.retry.RetryPolicies " a collection of useful implementations of {@link retrypolicy}.   try once, and fail by re-throwing the exception. this corresponds to having no retry mechanism in place.   keep trying forever.   keep trying a limited number of times, waiting a fixed time between attempts, and then fail by re-throwing the exception.   keep trying for a maximum time, waiting a fixed time between attempts, and then fail by re-throwing the exception.   keep trying a limited number of times, waiting a growing amount of time between attempts, and then fail by re-throwing the exception. the time between attempts is sleeptime mutliplied by the number of tries so far.   keep trying a limited number of times, waiting a growing amount of time between attempts, and then fail by re-throwing the exception. the time between attempts is sleeptime mutliplied by a random number in the range of [0, 2 to the number of retries)   set a default policy with some explicit handlers for specific exceptions.   a retry policy for remoteexception set a default policy with some explicit handlers for specific exceptions.  retry up to maxretries. the actual sleep time of the n-th retry is f(n, sleeptime), where f is a function provided by the subclass implementation. the object of the subclasses should be immutable; otherwise, the subclass must override hashcode(), equals(..) and tostring(). given pairs of number of retries and sleep time (n0, t0), (n1, t1), ..., the first n0 retries sleep t0 milliseconds on average, the following n1 retries sleep t1 milliseconds on average, and so on. for all the sleep, the actual sleep time is randomly uniform distributed in the close interval [0.5t, 1.5t], where t is the sleep time specified. the objects of this class are immutable. pairs of numretries and sleepseconds given the current number of retry, search the corresponding pair. corresponding pair, or null if the current number of retry > maximum number of retry. parse the given string as a multiplelinearrandomretry object. the format of the string is "t_1, n_1, t_2, n_2, ...", where t_i and n_i are the i-th pair of sleep time and number of retires. note that the white spaces in the string are ignored. parsed object, or null if the parsing fails. parse the i-th element as an integer. -1 if the parsing fails or the parsed value time increasing exponentially as a function of retries, +/- 0%-50% of that value, chosen randomly. the base amount of time to work with the number of retries that have so occurred so far value at which to cap the base sleep time amount of time to sleep www.apache.org/licenses/license-2.0 no more retries. calculate sleep time and return. 0.5 <= ratio <=1.5 parse the i-th sleep-time parse fails parse the i-th number-of-retries parse fails calculatesleeptime may overflow. retry immediately if this is our first failover, sleep otherwise"
org.apache.hadoop.io.retry.RetryPolicy " specifies a policy for retrying method failures. implementations of this interface should be immutable.  returned by {@link retrypolicy#shouldretry(exception, int, int, boolean)}.  determines whether the framework should retry a method for the given exception, and the number of retries that have been made for that operation so far.  the exception that caused the method to fail the number of times the method has been retried the number of times the method has failed over to a different backend implementation true if the method is idempotent and so can reasonably be retried on failover when we don't know if the previous attempt reached the server or not true if the method should be retried, false if the method should not be retried but shouldn't fail with an exception (only for void methods) @throws exception the re-thrown exception e indicating that the method failed and should not be retried further www.apache.org/licenses/license-2.0 a few common retry policies, with no delays."
org.apache.hadoop.io.retry.RetryProxy " a factory for creating retry proxies.   create a proxy for an interface of an implementation class using the same retry policy for each method in the interface.  the interface that the retry will implement the instance whose methods should be retried the policy for retrying method call failures retry proxy create a proxy for an interface of implementations of that interface using the given {@link failoverproxyprovider} and the same retry policy for each method in the interface. the interface that the retry will implement provides implementation instances whose methods should be retried the policy for retrying or failing over method call failures retry proxy create a proxy for an interface of an implementation class using the a set of retry policies specified by method name. if no retry policy is defined for a method then a default of {@link retrypolicies#try_once_then_fail} is used. the interface that the retry will implement the instance whose methods should be retried a map of method names to retry policies retry proxy create a proxy for an interface of implementations of that interface using the given {@link failoverproxyprovider} and the a set of retry policies specified by method name. if no retry policy is defined for a method then a default of {@link retrypolicies#try_once_then_fail} is used. the interface that the retry will implement provides implementation instances whose methods should be retried map of method names to retry policies retry proxy www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.retry.RetryUtils "return the default retry policy set in conf. if the value retrypolicyenabledkey is set to false in conf, use try_once_then_fail. otherwise, get the multiplelinearrandomretry policy specified in the conf and then (1) use multiplelinearrandomretry for - remoteexceptiontoretry, or - ioexception other than remoteexception, or - serviceexception; and (2) use try_once_then_fail for - non-remoteexceptiontoretry remoteexception, or - non-ioexception. conf property key for enabling retry default retrypolicyenabledkey conf value conf property key for retry policy spec default retrypolicyspeckey conf value the particular remoteexception to retry default retry policy. return the multiplelinearrandomretry policy specified in the conf, or null if the feature is disabled. if the policy is specified in the conf but the policy cannot be parsed, the default policy is returned. retry policy spec: n pairs of sleep-time and number-of-retries "s1,n1,s2,n2,..." conf property key for enabling retry default retrypolicyenabledkey conf value conf property key for retry policy spec default retrypolicyspeckey conf value multiplelinearrandomretry policy specified in the conf, or null if the feature is disabled. www.apache.org/licenses/license-2.0 no retry unwrap serviceexception see (1) and (2) in the javadoc of this method. non-ioexception"
org.apache.hadoop.io.SecureIOUtils "this class provides secure apis for opening and creating files on the local disk. the main issue this class tries to handle is that of symlink traversal.  an example of such an attack is:   malicious ensure that we are set up to run with the appropriate native support code. if security is disabled, and the support code is unavailable, this class still tries its best to be secure, but is vulnerable to some race condition attacks. if security is enabled but the support code is unavailable, throws a runtimeexception since we don't want to run insecurely. open the given file for read access, verifying the expected same as openforread() except that it will run even if security is off. this is used by unit tests. open the specified file for write access, ensuring that it does not exist. the file that we want to create we want to have on the file (if security is enabled) @throws alreadyexistsexception if the file already exists @throws ioexception if any other error occurred signals that an attempt to create a file at a given pathname has failed because another file already existed at that path. www.apache.org/licenses/license-2.0 pre-cache an instance of the raw filesystem since we sometimes do secure io in a shutdown hook, where this call could fail. secureio just skips security checks in the case that security is disabled if we can't do real security, do a racy exists check followed by an open and chmod use the native wrapper around open(2)"
org.apache.hadoop.io.SequenceFile "sequencefiles are flat files consisting of binary key/value pairs. sequencefile provides {@link writer}, {@link reader} and {@link sorter} classes for writing, reading and sorting respectively. there are three sequencefile writers based on the {@link compressiontype} used to compress key/value pairs:   writer : uncompressed records.   recordcompresswriter : record-compressed files, only compress values.   blockcompresswriter : block-compressed files, both keys & values are collected in 'blocks' separately and compressed. the size of the 'block' is configurable.  the actual compression algorithm used to compress key and/or values can be specified by using the appropriate {@link compressioncodec}. the recommended way is to use the static createwriter methods provided by the sequencefile to chose the preferred format. the {@link reader} acts as the bridge and can read any of the above sequencefile formats. sequencefile formats essentially there are 3 different formats for sequencefiles depending on the compressiontype specified. all of them share a common header described below. sequencefile header   version - 3 bytes of magic header seq, followed by 1 byte of actual version number (e.g. seq4 or seq6)   keyclassname -key class   valueclassname - value class   compression - a boolean which specifies if compression is turned on for keys/values in this file.   blockcompression - a boolean which specifies if block-compression is turned on for keys/values in this file.   compression codec - compressioncodec class which is used for compression of keys and/or values (if compression is enabled).   metadata - {@link metadata} for this file.   sync - a sync marker to denote end of the header.   uncompressed sequencefile format   header   record  record length key length key value    a sync-marker every few 100 bytes or so.   record-compressed sequencefile format   header   record  record length key length key compressed value    a sync-marker every few 100 bytes or so.   block-compressed sequencefile format   header   record block  uncompressed number of records in the block compressed key-lengths block-size compressed key-lengths block compressed keys block-size compressed keys block compressed value-lengths block-size compressed value-lengths block compressed values block-size compressed values block    a sync-marker every block.   the compressed blocks of key lengths and value lengths consist of the actual lengths of individual keys/values encoded in zerocompressedinteger format. @see compressioncodec the number of bytes between sync points. the compression type used to compress key/value pairs in the {@link sequencefile}. @see sequencefile.writer do not compress records. compress values only, each separately. compress sequences of records together in blocks. get the compression type for the reduce outputs the job config to look in kind of compression to use set the default compression type for sequence files. the configuration to modify the new compression type (none, block, record) create a new writer with the given options. the configuration to use the options to create the file with new writer @throws ioexception construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. the compression type. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. the compression type. the progressable object to track progress. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. the compression type. the compression codec. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. the compression type. the compression codec. the progressable object to track progress. the metadata of the file. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. buffer size for the underlaying outputstream. replication factor for the file. block size for the file. the compression type. the compression codec. the progressable object to track progress. the metadata of the file. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. buffer size for the underlaying outputstream. replication factor for the file. block size for the file. create parent directory if non-existent the compression type. the compression codec. the metadata of the file. the handle to the constructed sequencefile writer. @throws ioexception construct the preferred type of sequencefile writer. the context for the specified file. the configuration. the name of the file. the 'key' type. the 'value' type. the compression type. the compression codec. the metadata of the file. gives the semantics of create: overwrite, append etc. file creation options; see {@link createopts}. the handle to the constructed sequencefile writer. @throws ioexception construct the preferred type of sequencefile writer. the configured filesystem. the configuration. the name of the file. the 'key' type. the 'value' type. the compression type. the compression codec. the progressable object to track progress. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of 'raw' sequencefile writer. the configuration. the stream on top which the writer is to be constructed. the 'key' type. the 'value' type. the compression type. the compression codec. the metadata of the file. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. construct the preferred type of 'raw' sequencefile writer. the configuration. the stream on top which the writer is to be constructed. the 'key' type. the 'value' type. the compression type. the compression codec. the handle to the constructed sequencefile writer. @throws ioexception @deprecated use {@link #createwriter(configuration, writer.option...)} instead. the interface to 'raw' values of sequencefiles. writes the uncompressed bytes to the outstream. : stream to write uncompressed bytes into. @throws ioexception write compressed bytes to outstream. note: that it will not compress the bytes if they are not compressed. : stream to write compressed bytes into. size of stored data. the class encapsulating with the metadata of a file. the metadata of a file is a list of attribute name/value pairs of text type. write key/value pairs to a sequence-format file. @deprecated only used for backwards-compatibility in the createwriter methods that take filesystem. @deprecated only used for backwards-compatibility in the createwriter methods that take filesystem. construct a uncompressed writer from a set of options. the configuration to use the options used when creating the writer @throws ioexception if it fails create the named file. @deprecated use {@link sequencefile#createwriter(configuration, writer.option...)} instead. create the named file with write-progress reporter. @deprecated use {@link sequencefile#createwriter(configuration, writer.option...)} instead. create the named file with write-progress reporter. @deprecated use {@link sequencefile#createwriter(configuration, writer.option...)} instead. write and flush the file header. initialize. returns the class of keys in this file. returns the class of values in this file. returns the compression codec of data in this file. create a sync point flush all currently written data to the file system @deprecated use {@link #hsync()} or {@link #hflush()} instead returns the configuration of this file. close the file. append a key/value pair. append a key/value pair. returns the current length of the output file. this always returns a synchronized position. in other words, immediately after calling {@link sequencefile.reader#seek(long)} with a position returned by this method, {@link sequencefile.reader#next(writable)} may be called. however the key may be earlier in the file than key last written when this method was called (e.g., with block-compression, it may be the first key in the block that was being written when this method was called). write key/compressed-value pairs to a sequence-format file. append a key/value pair. append a key/value pair. write compressed key/value blocks to a sequence-format file. workhorse to check and write out compressed data/lengths compress and flush contents to dfs close the file. append a key/value pair. append a key/value pair. get the configured buffer size reads key/value pairs from a sequence-format file. a tag interface for all of the reader options create an option to specify the path name of the sequence file. the path to read new option create an option to specify the stream with the sequence file. the stream to read. new option create an option to specify the starting byte to read. the number of bytes to skip over new option create an option to specify the number of bytes to read. the number of bytes to read new option create an option with the buffer size for reading the given pathname. the number of bytes to buffer new option construct a reader by opening a file from the given file system. the file system used to open the file. the file being read. configuration @throws ioexception @deprecated use reader(configuration, option...) instead. construct a reader by the given input stream. an input stream. unused the starting position. the length being read. configuration @throws ioexception @deprecated use reader(configuration, reader.option...) instead. common work of the constructors. override this method to specialize the type of {@link fsdatainputstream} returned. the file system used to open the file. the file being read. the buffer size used to read the file. the length being read if it is >= 0. otherwise, the length is not available. opened stream. @throws ioexception initialize the {@link reader} true if we are constructing a temporary reader {@link sequencefile.sorter.clonefileattributes}, and hence do not initialize every component; false otherwise. @throws ioexception close the file. returns the name of the key class. returns the class of keys in this file. returns the name of the value class. returns the class of values in this file. returns true if values are compressed. returns true if records are block-compressed. returns the compression codec of data in this file. get the compression type for this file. compression type returns the metadata object of the file returns the configuration used for this file. read a compressed buffer read the next 'compressed' block position vallenin/valin to the 'value' corresponding to the 'current' key get the 'value' corresponding to the last read 'key'. : the 'value' to be read. @throws ioexception get the 'value' corresponding to the last read 'key'. : the 'value' to be read. @throws ioexception read the next key in the file into key, skipping its value. true if another entry exists, and false at end of file. read the next key/value pair in the file into key and val. returns true if such a pair exists and false when at end of file read and return the next record length, potentially skipping over a sync block. length of the next record or -1 if there is no next record @throws ioexception read the next key/value pair in the file into buffer. returns the length of the key read, or -1 if at end of file. the length of the value may be computed by calling buffer.getlength() before and after calls to this method. @deprecated call {@link #nextraw(dataoutputbuffer,sequencefile.valuebytes)}. read 'raw' records. - the buffer into which the key is read - the 'raw' value the total record length or -1 for end of file @throws ioexception read 'raw' keys. - the buffer into which the key is read the key length or -1 for end of file @throws ioexception read the next key in the file, skipping its value. return null at end of file. read 'raw' values. - the 'raw' value the value length @throws ioexception disables sync. often invoked for tmp files set the current byte position in the input file. the position passed must be a position returned by {@link sequencefile.writer#getlength()} when writing this file. to seek to an arbitrary position, use {@link sequencefile.reader#sync(long)}. seek to the next sync mark past a given position. returns true iff the previous call to next passed a sync mark. return the current byte position in the input file. returns the name of the file. sorts key/value pairs in a sequence-format file. for best performance, applications should make sure that the {@link writable#readfields(datainput)} implementation of their keys is very efficient. in particular, it should avoid allocating memory. sort and merge files containing the named classes. sort and merge using an arbitrary {@link rawcomparator}. sort and merge using an arbitrary {@link rawcomparator}. set the number of streams to merge at once. get the number of streams to merge at once. set the total amount of buffer memory, in bytes. get the total amount of buffer memory, in bytes. set the progressable object in order to report progress. perform a file sort from a set of input files into an output file. the files to be sorted the sorted output file should the input files be deleted as they are read? perform a file sort from a set of input files and return an iterator. the files to be sorted the directory where temp files are the backwards compatible interface to sort. the input file to sort the sorted output file set the progressable object in order to report progress the interface to iterate over raw keys/values of sequencefiles. gets the current raw key @throws ioexception gets the current raw value @throws ioexception sets up the current key and value (for getkey and getvalue) if there exists a key/value, false otherwise @throws ioexception closes the iterator so that the underlying streams can be closed @throws ioexception gets the progress object; this has a float (0.0 - 1.0) indicating the bytes processed by the iterator so far merges the list of segments of type segmentdescriptor the list of segmentdescriptors the directory to write temporary files into @throws ioexception merges the contents of files passed in path[] using a max factor value that is already set the array of path names true if the input files should be deleted when unnecessary the directory to write temporary files into @throws ioexception merges the contents of files passed in path[] the array of path names true if the input files should be deleted when unnecessary the factor that will be used as the maximum merge fan-in the directory to write temporary files into @throws ioexception merges the contents of files passed in path[] the array of path names the directory for creating temp files during merge true if the input files should be deleted when unnecessary @throws ioexception clones the attributes (like compression of the input file and creates a corresponding writer the path of the input file whose attributes should be cloned the path of the output file the progressable to report status during the file write @throws ioexception writes records from rawkeyvalueiterator into a file represented by the passed writer the rawkeyvalueiterator the writer merge the provided files. the array of input path names the final output file @throws ioexception sort calls this to generate the final merged output used by mergepass to merge the output of the sort the name of the input file containing sorted segments the offsets of the sorted segments the relative directory to store intermediate results in @throws ioexception this class implements the core of the merge logic a queue of file segments to merge the file segments to merge a relative local directory to save intermediate files in the reference to the progressable object this is the single level merge that is called multiple times depending on the factor size and the number of segments @throws ioexception return (& remove) the requested number of segment descriptors from the sorted map. this class defines a merge segment. this class can be subclassed to provide a customized cleanup method implementation. in this implementation, cleanup closes the file handle and deletes the file constructs a segment the offset of the segment in the file the length of the segment the path name of the file containing the segment do the sync checks whether to delete the files when no longer needed fills up the rawkey object with the key returned by the reader if there is a key returned; false, otherwise @throws ioexception fills up the passed rawvalue with the value corresponding to the key read earlier length of the value @throws ioexception returns the stored rawkey closes the underlying reader the default cleanup. subclasses can override this with a custom cleanup this class provisions multiple segments contained within a single file constructs a segment the offset of the segment in the file the length of the segment the path name of the file containing the segment the parent segmentcontainer that holds the segment the default cleanup. subclasses can override this with a custom cleanup the class that defines a container for segments to be merged. primarily required to delete temp files as soon as all the contained segments have been looked at this constructor is there primarily to serve the sort routine that generates a single output file with an associated index file www.apache.org/licenses/license-2.0 no public ctor "length" of sync entries number of bytes in hash escape + hash uncompressedbytes compressedbytes any arbitrary constant will do insert a globally unique 16-byte value every few entries, so that one can seek into the middle of a file and then synchronize with record starts and ends by scanning for this value. position of last sync 16 random bytes check consistency of options write the sync bytes flush header mark the start of the sync write sync update lastsyncpos flush contents to file system close the underlying stream iff we own it... time to emit sync append the 'key' append the 'value' write the record out sync total record length key portion length data total record length key portion length key value class writer append the 'key' compress 'value' and append it write the record out sync total record length key portion length data sync total record length key portion length 'key' data 'value' data recordcompressionwriter no. of records write 'keys' and lengths write 'values' and lengths flush the file-stream reset internal states save key/value into respective buffers added another key/value pair compress and flush? save key/value data in relevant buffers added another key/value pair compress and flush? blockcompressionwriter only used directly look up the options, these are null if not set check for consistency figure out the real values really set up if it wrapped around, use the max set 'version' key class name val class name if version > 2 is compressed? if version >= 4 is block-compressed? if version >= 5 setup the compression codec if version >= 6 if version > 1 read sync bytes record end of header initialize... not if this we are constructing a temporary reader return the decompressors to the pool close the input-stream read data into a temporary buffer set up 'buffer' connected to the input-stream reset the codec check if we need to throw away a whole block of 'values' due to 'lazy decompression' reset internal states process sync read synccheck check it read number of records in this block read key lengths and keys read value lengths and values check if this is the first value in the 'block' to be read read the value lengths and values calculate the no. of bytes to skip note: 'current' key has already been read! skip to the 'val' corresponding to 'current' key position stream to 'current' value get the value read another compressed 'value' sanity check position stream to 'current' value get the value read another compressed 'value' sanity check reset syncseen sanity check read another compressed 'key' process a sync entry read synccheck check it re-read length unsupported for block-compressed sequence files checksum failure reset syncseen read 'key' read raw 'value' reset syncseen read 'key' reset syncseen sanity check read another compressed 'key' position stream to current value trigger block read seek directly to first record note the sync marker "seen" in the header skip escape position before sync checksum failure the implementation of merge sort when merging or sorting bytes merged per pass outfile will basically be used as prefix for temp files in the cases where sort outputs multiple sorted segments. for the single segment case, the outputfile itself will contain the sorted data for that segment make the sortpass run it close it initialize read a record into buffer note: attempt to re-use 'rawvalue' as far as possible update pointers buffer is full -- sort & flush it indicate we're making progress disable sync on temp files write in sorted order save the segment length sequencefile.sorter.sortpass pass in object to report progress, if present get the segments from innames outfile will basically be used as prefix for temp files for the intermediate merge outputs get the segments from innames pass in object to report progress, if present get the segments from indexin we create a segmentcontainer so that we can track segments belonging to inname and delete inname as soon as we see that we have looked at all the contained segments during the merge process & hence don't need them anymore handle to the progress reporting object a treemap used to store the segments sorted by size (segment offset and segment path name is used to break ties between segments of same sizes) indicate we're making progress close inputs minsegment is non-null for all invocations of next except the first one. for the first invocation, the priority queue is ready for use but for the subsequent invocations, first adjust the queue current position in stream save the raw key reference load the raw value. re-use the existing rawvalue buffer end position after reading value current position in stream end position after reading key create the mergestreams from the sorted map and dump the final output to a file get the factor for this pass of merge extract the smallest 'factor' number of segment pointers from the treemap. call cleanup on the empty segments (no key/value data) count the fact that we read some bytes in calling nextrawkey() we ignore this segment for the merge if we have the desired number of segments or looked at all available segments, we break feed the streams to the priority queue if we have lesser number of segments remaining, then just return the iterator, else do another single level merge calculate the length of the remaining segments. required for calculating the merge progress being paranoid reset factor to what it originally was we want to spread the creation of temp files on multiple disks if available under the space constraints disable sync for temp files we finished one single level merge; now clean up the priority queue put the segment back in the treemap we are worried about only the first pass merge factor. so reset the factor to what it originally was hadoop-591 sequencefile.sorter.mergequeue the start of the segment in the file the length of the segment the path name of the file containing the segment set to true for temp files this will hold the current key delete input segment files? sometimes we ignore syncs especially for temp merge files sequencefile.sorter.segmentdescriptor sequencefile.sorter.linkedsegmentsdescriptor track the no. of segment cleanups # of segments contained input file from where segments are the list of segments read from the file get the segments from indexin sequencefile.sorter.segmentcontainer sequencefile.sorter sequencefile"
org.apache.hadoop.io.serializer.avro.AvroReflectSerializable "tag interface for avro 'reflect' serializable classes. classes implementing this interface can be serialized/deserialized using {@link avroreflectserialization}. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.avro.AvroReflectSerialization "serialization for avro reflect classes. for a class to be accepted by this serialization, it must either be in the package list configured via avro.reflect.pkgs or implement {@link avroreflectserializable} interface. key to configure packages that contain classes to be serialized and deserialized using this class. multiple packages can be specified using comma-separated list. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.avro.AvroSerialization "base class for providing serialization to avro types. return an avro schema instance for the given class. create and return avro datumwriter for the given class. create and return avro datumreader for the given class. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization "serialization for avro specific classes. this serialization is to be used for classes generated by avro's 'specific' compiler. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.Deserializer " provides a facility for deserializing objects of type  from an {@link inputstream}.   deserializers are stateful, but must not buffer the input since other producers may read from the input between calls to {@link #deserialize(object)}.   prepare the deserializer for reading.  deserialize the next object from the underlying input stream. if the object t is non-null then this deserializer may set its internal state to the next object read from the input stream. otherwise, if the object t is null a new deserialized object will be close the underlying input stream and clear up any resources. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.DeserializerComparator " a {@link rawcomparator} that uses a {@link deserializer} to deserialize the objects to be compared so that the standard {@link comparator} can be used to compare them.   one may optimize compare-intensive operations by using a custom implementation of {@link rawcomparator} that operates directly on byte representations.   www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.JavaSerialization " an experimental {@link serialization} for java {@link serializable} classes.  @see javaserializationcomparator www.apache.org/licenses/license-2.0 no header ignore passed-in object no header clear (class) back-references"
org.apache.hadoop.io.serializer.JavaSerializationComparator " a {@link rawcomparator} that uses a {@link javaserialization} {@link deserializer} to deserialize objects that are then compared via their {@link comparable} interfaces.   @see javaserialization www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.Serialization " encapsulates a {@link serializer}/{@link deserializer} pair.   allows clients to test whether this {@link serialization} supports the given class. {@link serializer} for the given class. {@link deserializer} for the given class. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.SerializationFactory " a factory for {@link serialization}s.   serializations are found by reading the io.serializations property from conf, which is a comma-delimited list of classnames.  www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.Serializer " provides a facility for serializing objects of type  to an {@link outputstream}.   serializers are stateful, but must not buffer the output since other producers may write to the output between calls to {@link #serialize(object)}.   prepare the serializer for writing. serialize t to the underlying output stream. close the underlying output stream and clear up any resources. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.serializer.WritableSerialization "a {@link serialization} for {@link writable}s that delegates to {@link writable#write(java.io.dataoutput)} and {@link writable#readfields(java.io.datainput)}. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.SetFile "a file-based set of keys. write a new set file. create the named set for keys of the named class. @deprecated pass a configuration too create a set naming the element class and compression type. create a set naming the element comparator and compression type. append a key to a set. the key must be strictly greater than the previous key added to the set. provide access to an existing set file. construct a set reader for the named set. construct a set reader for the named set using the named comparator. read the next key in a set into key. returns true if such a key exists and false when at the end of the set. read the matching key from a set into key. returns key, or null if no match exists. www.apache.org/licenses/license-2.0 no public ctor javadoc inherited"
org.apache.hadoop.io.ShortWritable "a writablecomparable for shorts. set the value of this shortwritable. return the value of this shortwritable. read the short value write short value returns true iff o is a shortwritable with the same value. hash code compares two shortwritable. short values in string format a comparator optimized for shortwritable. www.apache.org/licenses/license-2.0 register this comparator"
org.apache.hadoop.io.SortedMapWritable "a writable sortedmap. default constructor. copy constructor. the map to copy from www.apache.org/licenses/license-2.0 returning null means we use the natural ordering of the keys read the number of entries in the map then read each key/value pair write out the number of entries in the map then write out each key/value pair"
org.apache.hadoop.io.Stringifier "stringifier interface offers two methods to convert an object to a string representation and restore the object given its string representation.  the class of the objects to stringify converts the object to a string representation the object to convert string representation of the object @throws ioexception if the object cannot be converted restores the object from its string representation. the string representation of the object object @throws ioexception if the object cannot be restored closes this object. @throws ioexception if an i/o error occurs www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.Text "this class stores text using standard utf8 encoding. it provides methods to serialize, deserialize, and compare texts at byte level. the type of length is integer and is serialized using zero-compressed format. in addition, it provides methods for string traversal without converting the byte array to a string. also includes utilities for serializing/deserialing a string, coding/decoding a string, checking if a byte array contains valid utf8 code, calculating the length of an encoded string. construct from a string. construct from another text. construct from a byte array. get a copy of the bytes that is exactly the length of the data. see {@link #getbytes()} for faster access to the underlying array. returns the raw bytes; however, only data up to {@link #getlength()} is valid. please use {@link #copybytes()} if you need the returned array to be precisely the length of the data. returns the number of bytes in the byte array returns the unicode scalar value (32-bit integer value) for the character at position. note that this method avoids using the converter or doing string instatiation unicode scalar value at position or -1 if the position is invalid or points to a trailing byte finds any occurence of what in the backing buffer, starting as position start. the starting position is measured in bytes and the return value is in terms of byte position in the buffer. the backing buffer is not converted to a string for this operation. position of the first occurence of the search string in the utf-8 buffer or -1 if not found set to contain the contents of a string. set to a utf8 byte array copy a text. set the text to range of bytes 8 the data to copy from the first position of the new string the number of bytes of the new string append a range of bytes to the end of the given text 8 the data to copy from the first position to append from utf8 the number of bytes to append clear the string to empty. note: for performance reasons, this call does not clear the underlying byte array that is retrievable via {@link #getbytes()}. in order to free the byte-array memory, call {@link #set(byte[])} with an empty byte array (for example, new byte[0]). sets the capacity of this text object to at least len bytes. if the current buffer is longer, then the capacity and existing content of the buffer are unchanged. if len is larger than the current capacity, the text object's capacity is increased to match. the number of bytes we need should the old data be kept convert text back to string @see java.lang.object#tostring() deserialize skips over one text in the input. serialize write this object to out length uses zero-compressed encoding @see writable#write(dataoutput) returns true iff o is a text with the same contents. a writablecomparator optimized for text keys. converts the provided byte array to a string using the utf-8 encoding. if the input is malformed, replace by a default value. converts the provided byte array to a string using the utf-8 encoding. if replace is true, then malformed input is replaced with the substitution character, which is u+fffd. otherwise the method throws a malformedinputexception. converts the provided string to bytes using the utf-8 encoding. if the input is malformed, invalid chars are replaced by a default value. : bytes stores at bytebuffer.array() and length is bytebuffer.limit() converts the provided string to bytes using the utf-8 encoding. if replace is true, then malformed input is replaced with the substitution character, which is u+fffd. otherwise the method throws a malformedinputexception. : bytes stores at bytebuffer.array() and length is bytebuffer.limit() read a utf8 encoded string from in read a utf8 encoded string with a maximum size write a utf8 encoded string to out write a utf8 encoded string with a maximum size to out check if a byte array contains valid utf-8 8 byte array @throws malformedinputexception if the byte array contains invalid utf-8 check to see if a byte array is valid utf-8 8 the array of bytes the offset of the first byte in the array the length of the byte sequence @throws malformedinputexception if the byte array contains invalid bytes magic numbers for utf-8. these are the number of bytes that follow a given lead byte. trailing bytes have the value -1. the values 4 and 5 are presented in this table, even though valid utf-8 cannot include the five and six byte sequences. returns the next code point at the current position in the buffer. the buffer's position will be incremented. any mark set on this buffer will be changed by this method! remember, illegal utf-8 remember, illegal utf-8 for the given string, returns the number of utf-8 bytes required to encode the string. text to encode of utf-8 bytes required to encode www.apache.org/licenses/license-2.0 too long duh. matching first byte save position in loop save position in target src expired first no match not found can't get here register this comparator / static utilities from here down set decoder back to its default value: report //// states for validateutf8 check for ascii too long! longest valid utf-8 is 4 bytes (lead + three) or if < 0 we got a trail byte in the lead byte position switch (length) falls through to regular trail-byte test!! switch (state) trail bytes trailing byte! surrogate pair? valid pair invalid pair rewind one ch < 0x10000, that is, the largest char value"
org.apache.hadoop.io.TwoDArrayWritable "a writable for 2d arrays containing a matrix of instances of a class. www.apache.org/licenses/license-2.0 construct matrix construct values construct value read a value store it in values write values"
org.apache.hadoop.io.UTF8 "a writablecomparable for strings that uses the utf8 encoding. also includes utilities for efficiently reading and writing utf-8. note that this decodes utf-8 but actually encodes cesu-8, a variant of utf-8: see http://en.wikipedia.org/wiki/cesu-8 @deprecated replaced by text construct from a given string. construct from a given string. the raw bytes. the number of bytes in the encoded string. set to contain the contents of a string. set to contain the contents of a string. skips over one utf8 in the input. compare two utf8s. convert to a string. convert to a string, checking for valid utf8. converted string @throws utfdataformatexception if the underlying bytes contain invalid utf8 data. returns true iff o is a utf8 with the same contents. a writablecomparator optimized for utf8 keys. convert a string to a utf-8 encoded byte array. @see string#getbytes(string) convert a utf-8 encoded byte array back into a string. @throws ioexception if the byte array is invalid utf8 read a utf-8 encoded string. @see datainput#readutf() write a utf-8 encoded string. @see dataoutput#writeutf(string) returns the number of bytes required to write this. www.apache.org/licenses/license-2.0 en.wikipedia.org/wiki/cesu-8 set(""); maybe too long compute length double-check length grow buffer avoid sync'd allocations grow buffer register this comparator / static utilities from here down / these are probably not used much anymore, and might be removed... avoid sync'd allocations 0b0xxxxxxx: 1-byte sequence 0b110xxxxx: 2-byte sequence 0b1110xxxx: 3-byte sequence 0b11110xxx: 4-byte sequence the utf8 standard describes 5-byte and 6-byte sequences, but these are no longer allowed as of 2003 (see rfc 3629) only show the next 6 bytes max in the error code - in case the buffer is large, this will prevent an exceedingly large message. maybe too long double-check length"
org.apache.hadoop.io.VersionedWritable "a base class for writables that provides version checking. this is useful when a class may evolve, so that instances written by the old version of the class may still be processed by the new version. to handle this situation, {@link #readfields(datainput)} implementations should catch {@link versionmismatchexception}. return the version number of the current implementation. www.apache.org/licenses/license-2.0 javadoc from writable store version javadoc from writable read version"
org.apache.hadoop.io.VersionMismatchException "thrown by {@link versionedwritable#readfields(datainput)} when the version of an object being read does not match the current implementation version as returned by {@link versionedwritable#getversion()}. returns a string representation of this object. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.VIntWritable "a writablecomparable for integer values stored in variable-length format. such values take between one and five bytes. smaller values take fewer bytes. @see org.apache.hadoop.io.writableutils#readvint(datainput) set the value of this vintwritable. return the value of this vintwritable. returns true iff o is a vintwritable with the same value. compares two vintwritables. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.VLongWritable "a writablecomparable for longs in a variable-length format. such values take between one and five bytes. smaller values take fewer bytes. @see org.apache.hadoop.io.writableutils#readvlong(datainput) set the value of this longwritable. return the value of this longwritable. returns true iff o is a vlongwritable with the same value. compares two vlongwritables. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.Writable "a serializable object which implements a simple, efficient, serialization protocol, based on {@link datainput} and {@link dataoutput}. any key or value type in the hadoop map-reduce framework implements this interface. implementations typically implement a static read(datainput) method which constructs a new instance, calls {@link #readfields(datainput)} and returns the instance. example:  public class mywritable implements writable { // some data private int counter; private long timestamp; public void write(dataoutput out) throws ioexception { out.writeint(counter); out.writelong(timestamp); } public void readfields(datainput in) throws ioexception { counter = in.readint(); timestamp = in.readlong(); } public static mywritable read(datainput in) throws ioexception { mywritable w = new mywritable(); w.readfields(in); return w; } }  serialize the fields of this object to out. dataouput to serialize this object into. @throws ioexception deserialize the fields of this object from in. for efficiency, implementations should attempt to re-use storage in the existing object where possible. datainput to deseriablize this object from. @throws ioexception www.apache.org/licenses/license-2.0 some data"
org.apache.hadoop.io.WritableComparable "a {@link writable} which is also {@link comparable}. writablecomparables can be compared to each other, typically via comparators. any type which is to be used as a key in the hadoop map-reduce framework should implement this interface. note that hashcode() is frequently used in hadoop to partition keys. it's important that your implementation of hashcode() returns the same result across different instances of the jvm. note also that the default hashcode() implementation in object does not satisfy this property. example:  public class mywritablecomparable implements writablecomparable { // some data private int counter; private long timestamp; public void write(dataoutput out) throws ioexception { out.writeint(counter); out.writelong(timestamp); } public void readfields(datainput in) throws ioexception { counter = in.readint(); timestamp = in.readlong(); } public int compareto(mywritablecomparable o) { int thisvalue = this.value; int thatvalue = o.value; return (thisvalue &lt; thatvalue ? -1 : (thisvalue==thatvalue ? 0 : 1)); } public int hashcode() { final int prime = 31; int result = 1; result = prime result + counter; result = prime result + (int) (timestamp ^ (timestamp &gt;&gt;&gt; 32)); return result } }  www.apache.org/licenses/license-2.0 some data"
org.apache.hadoop.io.WritableComparator "a comparator for {@link writablecomparable}s. this base implemenation uses the natural ordering. to define alternate orderings, override {@link #compare(writablecomparable,writablecomparable)}. one may optimize compare-intensive operations by overriding {@link #compare(byte[],int,int,byte[],int,int)}. static utility methods are provided to assist in optimized implementations of this method. get a comparator for a {@link writablecomparable} implementation. force initialization of the static members. as of java 5, referencing a class doesn't force it to initialize. since this class requires that the classes be initialized to declare their comparators, we force that initialization to happen. the class to initialize register an optimized comparator for a {@link writablecomparable} implementation. comparators registered with this method must be thread-safe. construct for a {@link writablecomparable} implementation. returns the writablecomparable implementation class. construct a new {@link writablecomparable} instance. optimization hook. override this to make sequencefile.sorter's scream. the default implementation reads the data into two {@link writablecomparable}s (using {@link writable#readfields(datainput)}, then calls {@link #compare(writablecomparable,writablecomparable)}. compare two writablecomparables.  the default implementation uses the natural ordering, calling {@link comparable#compareto(object)}. lexicographic order of binary data. compute hash for binary data. compute hash for binary data. parse an unsigned short from a byte array. parse an integer from a byte array. parse a float from a byte array. parse a long from a byte array. parse a double from a byte array. reads a zero-compressed encoded long from a byte array and returns it. byte array with decode long starting index @throws java.io.ioexception long reads a zero-compressed encoded integer from a byte array and returns it. byte array with the encoded integer start index @throws java.io.ioexception integer www.apache.org/licenses/license-2.0 registry force the static initializers to run look to see if it is defined now if not, use the generic one parse key1 parse key2 compare them"
org.apache.hadoop.io.WritableFactories "factories for non-public writables. defining a factory permits {@link objectwritable} to be able to construct instances of non-public classes. define a factory for a class. define a factory for a class. create a new instance of a class with a defined factory. create a new instance of a class with a defined factory. www.apache.org/licenses/license-2.0 singleton"
org.apache.hadoop.io.WritableFactory "a factory for a class of writable. @see writablefactories return a new instance. www.apache.org/licenses/license-2.0"
org.apache.hadoop.io.WritableName "utility to permit renaming of writable implementation classes without invalidiating files that contain their class name. set the name that a class should be known as to something other than the class name. add an alternate name for a class. return the name for a class. default is {@link class#getname()}. return the class for a name. default is {@link class#forname(string)}. www.apache.org/licenses/license-2.0 define important types no public ctor"
org.apache.hadoop.io.WritableUtils "debug only! once we have confidence, can lose this. ugly utility, maybe someone else can do this better write a string as a network int n, followed by n bytes alternative to 16 bit read/writeutf. encoding standard is... ? read a string as a network int n, followed by n bytes alternative to 16 bit read/writeutf. encoding standard is... ? write a string array as a nework int n, followed by int n byte array strings. could be generalised using introspection. write a string array as a nework int n, followed by int n byte array of compressed strings. handles also null arrays and null values. could be generalised using introspection. write a string array as a nework int n, followed by int n byte array strings. could be generalised using introspection. actually this bit couldn't... write a string array as a nework int n, followed by int n byte array strings. could be generalised using introspection. handles null arrays and null values. test utility method display byte array. make a copy of a writable object using serialization to a buffer. the object to copy copied object make a copy of the writable object using serialiation to a buffer the object to copy from the object to copy into, which is destroyed @throws ioexception @deprecated use reflectionutils.cloneinto instead. serializes an integer to a binary stream with zero-compressed encoding. for -120  enum type datainput to read from class type of enum represented by string read from datainput @throws ioexception writes string value of enum to dataoutput. dataoutput stream enum value @throws ioexception skip len number of bytes in input streamin input stream number of bytes to skip @throws ioexception when skipped less number of bytes convert writables to a byte array read a string, but check it for sanity. the format consists of a vint followed by the given number of bytes. the stream to read from the largest acceptable length of the encoded string bytes as a string @throws ioexception if reading from the datainput fails @throws illegalargumentexception if the encoded byte size for string is negative or larger than maxsize. only the vint is read. www.apache.org/licenses/license-2.0 could/should use readfully(buffer,0,length)? could/should use readfully(buffer,0,length)? unchecked cast from class to class take one's complement' take one's complement' find the number of bytes with non-leading zeros find the number of data bytes + length byte"
org.apache.hadoop.ipc.Client "a client for an ipc service. ipc calls take a single {@link writable} as a parameter, and return a {@link writable} as their value. a service runs on a port and is defined by a parameter class and a value class. @see server executor on which ipc calls' parameters are sent. deferring the sending of parameters to a separate thread isolates them from thread interruptions in the calling code. set the ping interval value in configuration configuration the ping interval get the ping interval from configuration; if not set in the configuration, return the default value. configuration ping interval the time after which a rpc will timeout. if ping is not enabled (via ipc.client.ping), then the timeout value is the same as the pinginterval. if ping is enabled, then there is no timeout value. configuration timeout period in milliseconds. -1 if no timeout value is set set the connection timeout value in configuration configuration the socket connect timeout value increment this client's reference count decrement this client's reference count return if this client has no reference if this client has no reference; false otherwise class that represents an rpc call indicate when the call is complete and the value or error are available. notifies by default. set the exception when there is an error. notify the caller the call is done. exception thrown by the call; either local or remote set the return value when there is no error. notify the caller the call is done. return value of the rpc call. thread that reads responses and notifies callers. each connection owns a socket connected to a remote address. calls are multiplexed through this socket: responses may be delivered out of order. update lastactivity with the current time. add a call to this connection's call queue and notify a listener; synchronized. returns false if called during shutdown. to add if the call was added. this class sends a ping to the remote side when timeout on reading. if no failure is detected, it retries until at least a byte is read. constructor process timeout exception if the connection is not going to be closed or is not configured to have a rpc timeout, send a ping. (if rpctimeout is not set to be 0, then rpc should timeout. otherwise, throw the timeout exception. read a byte from the stream. send a ping if timeout on read. retries if no failure is detected until a byte is read. @throws ioexception for any io problem other than socket timeout read bytes into a buffer starting from offset off send a ping if timeout on read. retries if no failure is detected until a byte is read. total number of bytes read; -1 if the connection is closed. update the server address if the address corresponding to the host name has changed. if an addr change was detected. @throws ioexception when the hostname cannot be resolved. bind the socket to the host specified in the principal name of the client, to ensure server matching address of the client connection to host name in principal passed. check for an address change and update the local reference. reset the failure counter if the address was changed if multiple clients with the same principal try to connect to the same server at the same time, the server assumes a replay attack is in progress. this is a feature of kerberos. in order to work around this, what is done is that the client backs off randomly and tries to initiate the connection again. the other problem is to do with ticket expiry. to handle that, a relogin is attempted. connect to the server and set up the i/o streams. it then sends a header to the server and starts the connection thread that waits for responses. handle connection failures due to timeout on connect if the current number of retries is equal to the max number of retries, stop retrying and throw the exception; otherwise backoff 1 second and try connecting again. this method is only called from inside setupiostreams(), which is synchronized. hence the sleep is synchronized; the locks will be retained. current number of retries max number of retries allowed failure reason @throws ioexception if max number of retries is reached write the connection header - this is sent when connection is established +----------------------------------+ | "hrpc" 4 bytes | +----------------------------------+ | version (1 bytes) | +----------------------------------+ | authmethod (1 byte) | +----------------------------------+ | ipcserializationtype (1 byte) | +----------------------------------+ write the connection context header for each connection out is not synchronized because only the first thread does this. wait till someone signals us to start reading rpc response or it is idle too long, it is marked as to be closed, or the client is marked as not running. return true if it is time to read a response; false otherwise. send a ping to the server if the time elapsed since last i/o activity is equal to or greater than the ping interval initiates a call by sending the parameter to the remote server. note: this is not called from the connection thread, but by other threads. receive a response. because only one receiver, so no synchronization on in. close the connection. cleanup all calls and mark them as done construct an ipc client whose values are of the given {@link writable} class. construct an ipc client with the default socketfactory return the socket factory of this client client's socket factory stop all threads related to this client. no further calls may be made using this client. same as {@link #call(rpc.rpckind, writable, connectionid)} for rpc_builtin make a call, passing param, to the ipc server running at address, returning the value. throws exceptions if there are network problems or if the remote code threw an exception. @deprecated use {@link #call(rpc.rpckind, writable, connectionid)} instead make a call, passing param, to the ipc server running at address with the ticket credentials, returning the value. throws exceptions if there are network problems or if the remote code threw an exception. @deprecated use {@link #call(rpc.rpckind, writable, connectionid)} instead make a call, passing param, to the ipc server running at address which is servicing the protocol protocol, with the ticket credentials and rpctimeout as timeout, returning the value. throws exceptions if there are network problems or if the remote code threw an exception. @deprecated use {@link #call(rpc.rpckind, writable, connectionid)} instead same as {@link #call(rpc.rpckind, writable, inetsocketaddress, class, make a call, passing param, to the ipc server running at address which is servicing the protocol protocol, with the ticket credentials, rpctimeout as timeout and conf as conf for this connection, returning the value. throws exceptions if there are network problems or if the remote code threw an exception. same as {link {@link #call(rpc.rpckind, writable, connectionid)} except the rpckind is rpc_builtin make a call, passing rpcrequest, to the ipc server defined by remoteid, returning the rpc respond. - contains serialized method and method parameters - the target rpc server @returns the rpc response throws exceptions if there are network problems or if the remote code threw an exception. get a connection from the pool, or create a new one and add it to the pool. connections to a given connectionid are reused. we could avoid this allocation for each rpc by having a connectionsid object and with set() method. we need to manage the refs for keys in hashmap properly. for now its ok. this class holds the address and the max connection retries on socket time outs returns a connectionid object. remote address for the connection. protocol for rpc. ugi timeout configuration object connectionid instance @throws ioexception www.apache.org/licenses/license-2.0 class of call values counter for call ids if client runs how to create sockets call id the serialized rpc request - rpcpayload null if rpc has error exception, null if success rpc enginekind true when call is done notify caller server ip:port server's krb5 principal name connection id authentication method connected socket connections will be culled if it was idle for maxidletime msecs if t then disable nagle's algorithm do we need to send ping message how often sends ping to the server in msecs currently active calls last i/o activity time indicate if the connection is closed close reason this only happens in lazy tests make sure relogin only in case it is the login or super do a fresh lookup with the old host name. if host name is a valid local address then bind socket to it rpctimeout overwrites pinginterval try re-login have granularity of milliseconds we are sleeping with the connection lock held but since this connection instance is being used for connecting to the server in question, it is okay sasl connect is successful. let's set up sasl i/o streams. fall back to simple auth because server told us so. update last activity time start the receiver thread after the socket connection has been set up close the current connection set socket to null so that the next call to setupiostreams can start the process of connect all over again. throw the exception if the maximum number of retries is reached otherwise back off and retry write out the header, version and authentication method write out the connectionheader write out the payload length idle connection closed or stopped get stopped but there are still pending requests wait here for work - read or close connection this truly is unexpected, since we catch ioexception in receiveresponse -- this is only to be really sure that we don't leave a client hanging forever. serialize the call to be sent. this is done from the actual caller thread, rather than the send_params_executor thread, so that if the serialization throws an error, it is reported properly. this also parallelizes the serialization.  format of a call on the wire: 0) length of rest below (1 + 2) 1) payloadheader - is serialized delimited hence contains length 2) the payload - the rpcrequest  items '1' and '2' are prepared here. total length payloadheader + rpcrequest exception at this point would leave the connection in an unrecoverable state (eg half a call left on the wire). so, close the connection, killing any outstanding calls the buffer is just an in-memory buffer, but it is still polite to close early cause should only be a runtimeexception as the runnable above catches ioexception read value close the connection release the resources first thing to do;take the connection out of the connection list close the streams and therefore the socket clean up all calls clean up calls anyway log the info cleanup calls local exception wake up all connections wait until all connections are closed send the parameter wait for the result save the fact that we were interrupted set the interrupt flag now that we are done waiting local exception for unit testing only the client is stopped we don't invoke the method below inside "synchronized (connections)" block above. the reason for that is if the server happens to be slow, it will take longer to establish a connection and that will slow the entire system down. connections will be culled if it was idle for maxidletime msecs the max. no. of retries for socket connections on time out exceptions if t then disable nagle's algorithm do we need to send ping message how often sends ping to the server in msecs"
org.apache.hadoop.ipc.ClientCache "cache a client using its socket factory as the hash key construct & cache an ipc client with the construct & cache an ipc client with the default socketfactory and default valueclass if no cached client exists. configuration ipc client construct & cache an ipc client with the stop a rpc client connection a rpc client is closed only when its reference count becomes zero. www.apache.org/licenses/license-2.0 construct & cache client. the configuration is only used for timeout, and clients have connection pools. so we can either (a) lose some connection pooling and leak sockets, or (b) use the same timeout for all configurations. since the ipc is usually intended globally, not per-job, we choose (a)."
org.apache.hadoop.ipc.IpcException "ipc exception is thrown by ipc layer when the ipc connection cannot be established. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.metrics.package-info "rpc related metrics. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.metrics.RpcDetailedMetrics "this class is for maintaining rpc method related statistics and publishing them through the metrics interfaces. initialize the metrics for jmx with protocol methods the protocol class add an rpc processing time sample of the rpc call the processing time shutdown the instrumentation for the process www.apache.org/licenses/license-2.0 @override // some instrumentation interface @override // some instrumentation interface"
org.apache.hadoop.ipc.metrics.RpcMetrics "this class is for maintaining the various rpc statistics and publishing them through the metrics interfaces. one authentication failure event one authentication success event one one shutdown the instrumentation for the process increment sent bytes by count to increment increment received bytes by count to increment add an rpc queue time sample the queue time add an rpc processing time sample the processing time www.apache.org/licenses/license-2.0 public instrumentation methods that could be extracted to an abstract class if we decide to do custom instrumentation classes a la jobtrackerinstrumenation. the methods with //@override comment are candidates for abstract methods in a abstract instrumentation class. @override @override @override @override @override @override @override @override @override"
org.apache.hadoop.ipc.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.protobuf.HadoopRpcProtos "generated by the protocol buffer compiler. do not edit! source: hadoop_rpc.proto required string methodname = 1; optional bytes request = 2; required string declaringclassprotocolname = 3; required uint64 clientprotocolversion = 4; use hadooprpcrequestproto.newbuilder() to construct. required string methodname = 1; optional bytes request = 2; required string declaringclassprotocolname = 3; required uint64 clientprotocolversion = 4; construct using org.apache.hadoop.ipc.protobuf.hadooprpcprotos.hadooprpcrequestproto.newbuilder() required string methodname = 1; optional bytes request = 2; required string declaringclassprotocolname = 3; required uint64 clientprotocolversion = 4; @@protoc_insertion_point(builder_scope:hadoop.common.hadooprpcrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.hadooprpcrequestproto) @@protoc_insertion_point(outer_class_scope)"
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos "generated by the protocol buffer compiler. do not edit! source: ipcconnectioncontext.proto optional string effective optional string real use optional string effective optional string real construct using org.apache.hadoop.ipc.protobuf.ipcconnectioncontextprotos. optional string effective optional string real @@protoc_insertion_point(builder_scope:hadoop.common. @@protoc_insertion_point(class_scope:hadoop.common. optional .hadoop.common. optional string protocol = 3; use ipcconnectioncontextproto.newbuilder() to construct. optional .hadoop.common. optional string protocol = 3; construct using org.apache.hadoop.ipc.protobuf.ipcconnectioncontextprotos.ipcconnectioncontextproto.newbuilder() optional .hadoop.common. optional string protocol = 3; @@protoc_insertion_point(builder_scope:hadoop.common.ipcconnectioncontextproto) @@protoc_insertion_point(class_scope:hadoop.common.ipcconnectioncontextproto) @@protoc_insertion_point(outer_class_scope)"
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos "generated by the protocol buffer compiler. do not edit! source: protocolinfo.proto required string protocol = 1; use getprotocolversionsrequestproto.newbuilder() to construct. required string protocol = 1; construct using org.apache.hadoop.ipc.protobuf.protocolinfoprotos.getprotocolversionsrequestproto.newbuilder() required string protocol = 1; @@protoc_insertion_point(builder_scope:hadoop.common.getprotocolversionsrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.getprotocolversionsrequestproto) required string rpckind = 1; repeated uint64 versions = 2; use protocolversionproto.newbuilder() to construct. required string rpckind = 1; repeated uint64 versions = 2; construct using org.apache.hadoop.ipc.protobuf.protocolinfoprotos.protocolversionproto.newbuilder() required string rpckind = 1; repeated uint64 versions = 2; @@protoc_insertion_point(builder_scope:hadoop.common.protocolversionproto) @@protoc_insertion_point(class_scope:hadoop.common.protocolversionproto) repeated .hadoop.common.protocolversionproto protocolversions = 1; use getprotocolversionsresponseproto.newbuilder() to construct. repeated .hadoop.common.protocolversionproto protocolversions = 1; construct using org.apache.hadoop.ipc.protobuf.protocolinfoprotos.getprotocolversionsresponseproto.newbuilder() repeated .hadoop.common.protocolversionproto protocolversions = 1; @@protoc_insertion_point(builder_scope:hadoop.common.getprotocolversionsresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.getprotocolversionsresponseproto) required string protocol = 1; required string rpckind = 2; use getprotocolsignaturerequestproto.newbuilder() to construct. required string protocol = 1; required string rpckind = 2; construct using org.apache.hadoop.ipc.protobuf.protocolinfoprotos.getprotocolsignaturerequestproto.newbuilder() required string protocol = 1; required string rpckind = 2; @@protoc_insertion_point(builder_scope:hadoop.common.getprotocolsignaturerequestproto) @@protoc_insertion_point(class_scope:hadoop.common.getprotocolsignaturerequestproto) repeated .hadoop.common.protocolsignatureproto protocolsignature = 1; use getprotocolsignatureresponseproto.newbuilder() to construct. repeated .hadoop.common.protocolsignatureproto protocolsignature = 1; construct using org.apache.hadoop.ipc.protobuf.protocolinfoprotos.getprotocolsignatureresponseproto.newbuilder() repeated .hadoop.common.protocolsignatureproto protocolsignature = 1; @@protoc_insertion_point(builder_scope:hadoop.common.getprotocolsignatureresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.getprotocolsignatureresponseproto) required uint64 version = 1; repeated uint32 methods = 2; use protocolsignatureproto.newbuilder() to construct. required uint64 version = 1; repeated uint32 methods = 2; construct using org.apache.hadoop.ipc.protobuf.protocolinfoprotos.protocolsignatureproto.newbuilder() required uint64 version = 1; repeated uint32 methods = 2; @@protoc_insertion_point(builder_scope:hadoop.common.protocolsignatureproto) @@protoc_insertion_point(class_scope:hadoop.common.protocolsignatureproto) @@protoc_insertion_point(outer_class_scope)"
org.apache.hadoop.ipc.protobuf.RpcPayloadHeaderProtos "generated by the protocol buffer compiler. do not edit! source: rpcpayloadheader.proto @@protoc_insertion_point(enum_scope:hadoop.common.rpckindproto) @@protoc_insertion_point(enum_scope:hadoop.common.rpcpayloadoperationproto) @@protoc_insertion_point(enum_scope:hadoop.common.rpcstatusproto) optional .hadoop.common.rpckindproto rpckind = 1; optional .hadoop.common.rpcpayloadoperationproto rpcop = 2; required uint32 callid = 3; use rpcpayloadheaderproto.newbuilder() to construct. optional .hadoop.common.rpckindproto rpckind = 1; optional .hadoop.common.rpcpayloadoperationproto rpcop = 2; required uint32 callid = 3; construct using org.apache.hadoop.ipc.protobuf.rpcpayloadheaderprotos.rpcpayloadheaderproto.newbuilder() optional .hadoop.common.rpckindproto rpckind = 1; optional .hadoop.common.rpcpayloadoperationproto rpcop = 2; required uint32 callid = 3; @@protoc_insertion_point(builder_scope:hadoop.common.rpcpayloadheaderproto) @@protoc_insertion_point(class_scope:hadoop.common.rpcpayloadheaderproto) required uint32 callid = 1; required .hadoop.common.rpcstatusproto status = 2; optional uint32 serveripcversionnum = 3; use rpcresponseheaderproto.newbuilder() to construct. required uint32 callid = 1; required .hadoop.common.rpcstatusproto status = 2; optional uint32 serveripcversionnum = 3; construct using org.apache.hadoop.ipc.protobuf.rpcpayloadheaderprotos.rpcresponseheaderproto.newbuilder() required uint32 callid = 1; required .hadoop.common.rpcstatusproto status = 2; optional uint32 serveripcversionnum = 3; @@protoc_insertion_point(builder_scope:hadoop.common.rpcresponseheaderproto) @@protoc_insertion_point(class_scope:hadoop.common.rpcresponseheaderproto) @@protoc_insertion_point(outer_class_scope)"
org.apache.hadoop.ipc.ProtobufHelper "helper methods for protobuf related rpc implementation return the ioexception thrown by the remote server wrapped in serviceexception as cause. serviceexception that wraps io exception thrown by the server wrapped in serviceexception or a new ioexception that wraps the unexpected serviceexception. www.apache.org/licenses/license-2.0 hidden constructor for class with only static helper methods"
org.apache.hadoop.ipc.ProtobufRpcEngine "rpc engine for for protobuf based rpcs. this constructor takes a connectionid, instead of creating a new one. this is the client side invoker of rpc method. it only throws serviceexception, since the invocation proxy expects only serviceexception to be thrown by the method in case protobuf service. serviceexception has the following causes:  exceptions encountered on the client side in this method are set as cause in serviceexception as is. exceptions from the server are wrapped in remoteexception and are set as cause in serviceexception  note that the client calling protobuf rpc methods, must handle serviceexception by getting the cause from the serviceexception. if the cause is remoteexception, then unwrap it to get the exception thrown by the server. writable wrapper for protocol buffer requests writable wrapper for protocol buffer responses construct an rpc server. the class of protocol the protocolimpl whose methods will be called the configuration to use the address to bind on to listen for connection the port to listen for connections on the number of method handler threads to run whether each call should be logged a config parameter that can be used to restrict the range of ports used when port is 0 (an ephemeral port) protobuf invoker for {@link rpcinvoker} this is a server side method, which is invoked over rpc. on success the return response has protobuf response payload. on failure, the exception name and the stack trace are return in the resposne. see {@link hadooprpcresponseproto} in this method there three types of exceptions possible and they are returned in response as follows.   exceptions encountered in this method that are returned as {@link rpcserverexception}   exceptions thrown by the service is wrapped in serviceexception. in that this method returns in response the exception thrown by the service.  other exceptions thrown by the service. they are returned as it is.  www.apache.org/licenses/license-2.0 register the rpcrequest deserializer for writablerpcengine rpccontroller + message for protobuf, {@code protocol} used when creating client side proxy is the interface extending blockinginterface, which has the annotations such as protocolname etc.  using method.getdeclaringclass(), as in writableengine to get at the protocol interface will return blockinginterface, from where the annotation protocolname and version cannot be obtained.  hence we simply use the protocol class used to create the proxy. for pb this may limit the use of mixins on client side. rpcinvocationhandler no match for protocol and version protocol supported but not the version that client wants"
org.apache.hadoop.ipc.ProtocolInfo "the protocol name that is used when a client and server connect. by default the class name of the protocol interface is the protocol name. why override the default name (i.e. the class name)? one use case overriding the default name (i.e. the class name) is when there are multiple implementations of the same protocol, each with say a different version/serialization. in hadoop this is used to allow multiple server and client adapters for different versions of the same protocol service. www.apache.org/licenses/license-2.0 the name of the protocol (i.e. rpc service) default means not defined use old way"
org.apache.hadoop.ipc.ProtocolMetaInfoPB "protocol to get versions and signatures for supported protocols from the server. note: this extends the protocolbuffer service based interface to add annotations. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB "this class serves the requests for protocol versions and signatures by looking them up in the server registry. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.ProtocolMetaInterface "this interface is implemented by the client side translators and can be used to obtain information about underlying protocol e.g. to check if a method is supported on the server side. checks whether the given method name is supported by the server. it is assumed that all method names are unique for a protocol. the name of the method if method is supported, otherwise false. @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.ProtocolProxy "a class wraps around a server's proxy, containing a list of its supported methods. a list of methods with a value of null indicates that the client and server have the same protocol. constructor protocol class its proxy if false proxy will never fetch server methods and ismethodsupported will always return true. if true, server methods will be fetched for the first call to ismethodsupported. get the proxy check if a method is supported by the server or not a method's name in string format a method's parameter types if the method is supported by the server www.apache.org/licenses/license-2.0 client & server have the same protocol"
org.apache.hadoop.ipc.ProtocolSignature "default constructor constructor server version hash codes of the methods supported by server calculate a method's hash code considering its method name, returning type, and its parameter types a method hash code convert an array of method into an array of hash codes of hash codes get the hash code of an array of methods methods are sorted before hashcode is calculated. so the returned value is irrelevant of the method order in the array. an array of methods hash code get the hash code of an array of hashcodes hashcodes are sorted before hashcode is calculated. so the returned value is irrelevant of the hashcode order in the array. an array of methods hash code a cache that maps a protocol's name to its signature & finger print return a protocol's signature and finger print from cache a protocol class protocol version signature and finger print get a server protocol's signature client protocol methods hashcode server protocol version protocol server's protocol signature get a server protocol's signature server implementation server protocol client's version client's protocol's hash code server protocol's signature @throws ioexception if any error occurs www.apache.org/licenses/license-2.0 register a ctor an array of method hash codes try to get the finger print & signature from the cache check if the client side protocol matches the one on the server side null indicates a match"
org.apache.hadoop.ipc.ProtocolTranslator "an interface implemented by client-side protocol translators to get the underlying proxy object the translator is operating on. return the proxy object underlying this protocol translator. proxy object underlying this protocol translator. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.RemoteException "for java.io.serializable if this remote exception wraps up one of the lookuptypes then return this exception.  unwraps any ioexception. the desired exception class. , which is either the lookupclass exception or this. instantiate and return the exception wrapped up by this remote exception.  this unwraps any throwable that has a constructor taking a string as a parameter. otherwise it returns this. throwable create remoteexception from attributes www.apache.org/licenses/license-2.0 cannot instantiate lookupclass, just return this wrapped up exception is not in lookuptypes, just return this cannot instantiate the original exception, just return this"
org.apache.hadoop.ipc.RPC "a simple rpc mechanism. a protocol is a java interface. all parameters and return types must be one of:  a primitive type, boolean, byte, char, short, int, long, float, double, or void; or a {@link string}; or a {@link writable}; or an array of the above types  all methods in the protocol should throw only ioexception. no field data of the protocol instance is transmitted. process a client call on the server side the server within whose context this rpc call is made - the protocol name (the class of the client proxy used to make calls to the rpc server. - deserialized time at which the call received (for metrics) call's return @throws ioexception get all superinterfaces that extend versionedprotocol super interfaces that extend versionedprotocol get all interfaces that the given protocol implements or extends which are assignable from versionedprotocol. get the protocol name. if the protocol class has a protocolannotation, then get the protocol name from the annotation; otherwise the class name is the protocol name. get the protocol version from protocol class. if the protocol class has a protocolannotation, then get the protocol name from the annotation; otherwise the class name is the protocol name. set a protocol to use a non-default rpcengine. configuration to use the protocol interface the rpcengine impl a version mismatch for the rpc protocol. create a version mismatch exception the name of the protocol mismatch the client's version of the protocol the server's version of the protocol get the interface name java class name (eg. org.apache.hadoop.mapred.intertrackerprotocol) get the client's preferred version get the server's agreed to version. get a proxy connection to a remote server protocol class client version remote address configuration to use proxy @throws ioexception if the far end through a remoteexception get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server protocol class client version remote address configuration to use protocol proxy @throws ioexception if the far end through a remoteexception get a proxy connection to a remote server protocol class client version remote address configuration to use time in milliseconds before giving up proxy @throws ioexception if the far end through a remoteexception get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server protocol class client version remote address configuration to use time in milliseconds before giving up protocol proxy @throws ioexception if the far end through a remoteexception get a proxy connection to a remote server protocol class client version remote address configuration to use timeout for each rpc time in milliseconds before giving up proxy @throws ioexception if the far end through a remoteexception get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server protocol class client version remote address configuration to use timeout for each rpc time in milliseconds before giving up proxy @throws ioexception if the far end through a remoteexception construct a client-side proxy object that implements the named protocol, talking to a server at the named address.  get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server protocol class client version remote address configuration to use socket factory protocol proxy @throws ioexception if the far end through a remoteexception construct a client-side proxy object that implements the named protocol, talking to a server at the named address.  get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server protocol class client version remote address construct a client-side proxy that implements the named protocol, talking to a server at the named address.  protocol client's version server address security ticket configuration socket factory max time for each rpc; 0 means no timeout proxy @throws ioexception if any error occurs get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server protocol client's version server address security ticket configuration socket factory max time for each rpc; 0 means no timeout proxy @throws ioexception if any error occurs construct a client-side proxy object with the default socketfactory  proxy instance @throws ioexception returns the server address for a given proxy. return the connection id of the given object. if the provided object is in fact a protocol translator, we'll get the connection id of the underlying proxy object. the proxy object to get the connection id of. connection id for the provided proxy object. get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server protocol proxy @throws ioexception stop the proxy. proxy must either implement {@link closeable} or must have associated {@link rpcinvocationhandler}. the rpc proxy object to be stopped @throws hadoopillegalargumentexception if the proxy does not implement {@link closeable} interface or does not have closeable {@link invocationhandler} construct a server for a protocol implementation instance listening on a port and address. @deprecated please use {@link builder} to build the {@link server} construct a server for a protocol implementation instance listening on a port and address. @deprecated please use {@link builder} to build the {@link server} construct a server for a protocol implementation instance. @deprecated please use {@link builder} to build the {@link server} construct a server for a protocol implementation instance. @deprecated please use {@link builder} to build the {@link server} construct a server for a protocol implementation instance. @deprecated please use {@link builder} to build the {@link server} @deprecated please use {@link builder} to build the {@link server} construct a server for a protocol implementation instance. @deprecated please use {@link builder} to build the {@link server} class to construct instances of rpc server with specific options. mandatory field mandatory field default: 0.0.0.0 default: 0 default: 1 default: -1 default: -1 default: false default: null default: null build the rpc server. @throws ioexception on error @throws hadoopillegalargumentexception when mandatory fields are not set an rpc server. store a map of protocol and version to its implementation the key in map the value in map add a protocol to the existing server. - the protocol class - the impl of the protocol that will be called server (for convenience) www.apache.org/licenses/license-2.0 used for built in calls by tests use writablerpcengine use protobufrpcengine used for array size todo make it private no public ctor cache of rpcengines by protocol return the rpcengine configured to handle a protocol namenode has not been started namenode is busy perhaps a vip is failing over check if timed out wait for retry ignore if you see this error on a mock object in a unit test you're developing, make sure to use mockitoutil.mockprotocol() to create your mock. use impl class for protocol initialize for all rpc kinds register protocol and its impl for rpc calls will be useful later."
org.apache.hadoop.ipc.RpcClientException "indicates an exception in the rpc client constructs exception with the specified detail message. detailed message. constructs exception with the specified detail message and cause. message. that cause this exception the cause (can be retried by the {@link #getcause()} method). (a null value is permitted, and indicates that the cause is nonexistent or unknown.) www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.RpcClientUtil "this class maintains a cache of protocol versions and corresponding protocol signatures, keyed by server address, protocol and rpc kind. the cache is lazily populated. returns whether the given method is supported or not. the protocol signatures are fetched and cached. the connection id for the proxy provided is re-used. proxy which provides an existing connection id. protocol for which the method check is required. the rpckind for which the method check is required. the version at the client. name of the method. if the method is supported, false otherwise. @throws ioexception www.apache.org/licenses/license-2.0 object object assuming unique method names. the proxy returned re-uses the underlying connection. this is a special mechanism for protocolmetainfopb. don't do this for any other protocol, it might cause a security hole."
org.apache.hadoop.ipc.RpcEngine "an rpc implementation. construct a client-side proxy object.  construct a server for a protocol implementation instance. the class of protocol to use the instance of protocol whose methods will be called the configuration to use the address to bind on to listen for connection the port to listen for connections on the number of method handler threads to run the number of reader threads to run the size of the queue per hander thread whether each call should be logged the secret manager to use to validate incoming requests. a config parameter that can be used to restrict the range of ports used when port is 0 (an ephemeral port) server instance @throws ioexception on any error returns a proxy for protocolmetainfopb, which uses the given connection id. , connectionid to be used for the proxy. , configuration. , socket factory. object. @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.RpcException "indicates an exception during the execution of remote procedure call. constructs exception with the specified detail message. detailed message. constructs exception with the specified detail message and cause. message. that cause this exception the cause (can be retried by the {@link #getcause()} method). (a null value is permitted, and indicates that the cause is nonexistent or unknown.) www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.RpcInvocationHandler "this interface must be implemented by all invocationhandler implementations. returns the connection id associated with the invocationhandler instance. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.RpcServerException "indicates an exception on the rpc server constructs exception with the specified detail message. detailed message. constructs exception with the specified detail message and cause. message. the cause (can be retried by the {@link #getcause()} method). (a null value is permitted, and indicates that the cause is nonexistent or unknown.) www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.Server "an abstract ipc service. ipc calls take a single {@link writable} as a parameter, and return a {@link writable} as their value. a service runs on a port and is defined by a parameter class and a value class. @see client the first four bytes of hadoop rpc connections serialization type for connectioncontext and rpcpayloadheader if the an http response to send back if we detect an http request to our ipc port. initial and max size of response buffer register a rpc kind and the class to deserialize the rpc request. called by static initializers of rpckind engines - this class is used to deserialze the the rpc request. - use to process the calls on ss. returns the server instance called under or null. may be called under {@link #call(writable, long)} implementations, and under {@link writable} methods of paramters and return values. permits applications to access the server context. this is set to call object before handler invokes an rpc and reset after the call returns. returns the remote side ip address when invoked inside an rpc returns null incase of an error. returns remote address as a string when invoked inside an rpc. returns null in case of an error. return true if the invocation was through an rpc. a convenience method to bind to a given address and report better exceptions if the address is not a valid host. the socket to bind the address to bind to the number of connections allowed in the queue @throws bindexception if the address can't be bound @throws unknownhostexception if the address isn't a valid host name @throws ioexception other random errors from bind returns a handle to the rpcmetrics (required in tests) metrics refresh the service returns a handle to the service a call queued for handling. listens on the socket. creates jobs for the handler threads this gets reader into the state that waits for the new channel to be registered with readselector. if it was waiting in select() the thread will be woken up, otherwise whenever select() is called it will return even if there is nothing to read and wait in while(adding) for finishadd call cleanup connections from connectionlist. choose a random range to scan and also have a limit on the number of the connections that will be cleanedup per run. the criteria for cleanup is the time for which the connection was idle. if 'force' is true then all connections will be looked at for the cleanup. the listener/reader might have closed the socket. we don't explicitly cancel the key, so not sure if this will ever fire. this warning could be removed. reads calls from a connection and queues them for handling. return true if the connection has no outstanding rpc decrement the outstanding rpc count increment the outstanding rpc count read at most one rpc. if the header is not read completely yet then iterate until we read first rpc or until there is no data left. try to set up the response to indicate that the client version is incompatible with the server. this can contain special-case code to speak enough of past ipc protocols to pass back an exception to the caller. the version the caller is using @throws ioexception reads the connection context following the connection header handles queued calls . constructs a server listening on the named port and address. parameters passed must be of the named class. the handlercount determines the number of handler threads that will be used to process calls. if queuesizeperhandler or numreaders are not -1 they will be used instead of parameters from configuration. otherwise the configuration will be picked up. if rpcrequestclass is null then the rpcrequestclass must have been registered via {@link #registerprotocolengine(rpcpayloadheader.rpckind, class, rpc.rpcinvoker)} this parameter has been retained for compatibility with existing tests and usage. setup response for the ipc call. buffer to serialize the response into {@link call} to which we are setting up the response of the ipc call return value for the ipc call, if the call was successful error class, if the the call failed error message, if the call failed @throws ioexception setup response for the ipc call on fatal error from a client that is using old version of hadoop. the response is serialized using the previous protocol's response layout. buffer to serialize the response into {@link call} to which we are setting up the response return value for the ipc call, if the call was successful error class, if the the call failed error message, if the call failed @throws ioexception sets the socket buffer size used for responding to rpcs starts the service. must be called before any calls will be handled. stops the service. no new calls will be handled after this is called. wait for the server to be stopped. does not wait for all subthreads to finish. see {@link #stop()}. return the socket (ip+port) on which the rpc server is listening to. socket (ip+port) on which the rpc server is listening to. called for each call. @deprecated use {@link #call(rpcpayloadheader.rpckind, string, writable, long)} instead called for each call.  get the port on which the ipc server is listening for incoming connections. this could be an ephemeral port too, in which case we return the real port on which the server has bound. on which ipc server is listening the number of open rpc conections number of open rpc connections the number of rpc calls in the queue. number of rpc calls in the queue. the maximum size of the rpc call queue of this server. maximum size of the rpc call queue. the number of reader threads for this server. number of reader threads. when the read or write buffer size is larger than this limit, i/o will be done in chunks of this size. most rpc requests and responses would be be smaller. this is a wrapper around {@link writablebytechannel#write(bytebuffer)}. if the amount of data is large, it writes to channel in smaller chunks. this is to avoid jdk from creating many direct buffers as the size of buffer increases. this also minimizes extra copies in nio layer as a result of multiple write operations required to write a large buffer. @see writablebytechannel#write(bytebuffer) this is a wrapper around {@link readablebytechannel#read(bytebuffer)}. if the amount of data is large, it writes to channel in smaller chunks. this is to avoid jdk from creating many direct buffers as the size of bytebuffer increases. there should not be any performance degredation. @see readablebytechannel#read(bytebuffer) helper for {@link #channelread(readablebytechannel, bytebuffer)} and {@link #channelwrite(writablebytechannel, bytebuffer)}. only one of readch or writech should be non-null. @see #channelread(readablebytechannel, bytebuffer) @see #channelwrite(writablebytechannel, bytebuffer) www.apache.org/licenses/license-2.0 add new serialization type to the end without affecting the enum order 1 : introduce ping and server does not throw away rpcs 3 : introduce the protocol into the rpc connection header 4 : introduced sasl security layer 5 : introduced use of {@link arrayprimitivewritable$internal} in objectwritable to efficiently transmit arrays of primitives 6 : made rpc payload header explicit 7 : changed ipc connection header to use protocol buffers 8 : sasl server always sends a final response port we listen on number of handler threads number of read threads class used for deserializing the rpc request the maximum idle time after which a client may be disconnected the number of idle connections after which we will start cleaning up idle connections the max number of connections to nuke during a cleanup if t then disable nagle's algorithm true while server runs queued calls maintain a list of client connections ignored the client's call id serialized rpc request from client connection to client time received when response is null time served when response is not null the response for this call the accept channel the selector that we use for the server the address we bind at the last time when a cleanup connec- -tion (for idle connections) ran the minimum interval between two cleanup runs create a new server socket and set to non blocking mode bind the server socket to the local host and port could be an ephemeral port create a selector; register accepts on the server socket with the selector. unexpected -- log it we can run out of memory if we have too many threads log the event and sleep for a minute and give some thread(s) a chance to finish clean up all connections so that the (count < 0) block is executed the method that will return the next reader to work with simplistic implementation of round robin for now sends responses of rpc back to clients. connections waiting to register 15mins create a selector last check for old calls. if a channel is being registered, wait.  if there were some calls that have not been sent out for a long time, discard them.  get the list of channels from list of keys.  we can run out of memory if we have too many threads log the event and sleep for a minute and give some thread(s) a chance to finish   remove calls that have been pending in the responsequeue for a long time.  processes one response. returns true if there are no more pending data for this channel.  there is more data for this channel.  if there are no items for this channel, then we are done  no more data for this channel.  extract the first call   send as much data as we can in the non-blocking fashion  clear out the response buffer so it can be collected last call fully processes. no more data for this channel. more calls pending to be sent.  if we were unable to write the entire response out, then insert in selector queue.  set the serve time when the response has to be sent later wakeup the thread blocked on select, only then can the call to channel.register() complete. its ok. channel might be closed else where. everything went off well error. no more data for this channel.  enqueue a response from the application.  call waiting to be enqueued. call done enqueueing. connection header is read? if connection context that follows connection header is read number of outstanding rpcs cache the remote host & port info so that even if the socket is disconnected, we can say where it used to connect to.  fake 'call' for failed fake 'call' for sasl context setup attempting send final response for success write status every connection is expected to send the header. check if it looks like the with an http get - this is a common error, so we can send back a simple string indicating as much. warning is ok since this is not supposed to happen. this may create a sasl server, or switch us into simple covers the !usesasl too ping message client has already sent the initial sasl message and we should ignore it. both client and server should fall back to simple auth from now on. no sasl for simple we should never be able to get here versions 3 and greater can interpret this exception response in the same manner hadoop 0.18.3 call id error  now we check if this is a proxy different from the ' this is not allowed if not allowed to doas if token authentication is used effective for simple auth or kerberos auth the read all rpcs contained in the inbuf, even partial ones ping message if we know the rpc kind, get its class so that we can deserialize (note it would make more sense to have the handler deserialize but we continue with this original design. read the rpc request queue the call; maybe blocked here increment the rpc count if auth method is digest, the token was obtained by the real  authentication pop the queue; maybe blocked here make the call as the the call with the subject make the call these exception types indicate something is probably wrong on the server side, as opposed to just a normal exceptional result. don't log the whole stack trace of these exceptions. way too noisy! remove redundant error class name from the beginning of the stack trace setupresponse() needs to be sync'ed together with responder.doresponse() since setupresponse may use sasl to encrypt response data and sasl enforces its own message ordering. discard the large buf and reset it back to smaller size to free up heap unexpected -- log it configure supported authentications start the listener here and let it bind to the port create the responder here get the security type from the conf. implicitly include token support if a secret manager is provided, or fail if token is the conf value but there is no secret manager call back to same function - this is ok since the buffer is reset at the top, and since status is changed to error it won't infinite loop. write call id write fatal_status synchronization may be needed since there can be multiple handler threads using saslserver to wrap responses. should not be more than 64kb."
org.apache.hadoop.ipc.StandbyException "thrown by a remote server when it is up, but is not the active server in a set of servers in which only a subset may be active. www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.UnexpectedServerException "indicates that the rpc server encountered an undeclared exception from the service constructs exception with the specified detail message. detailed message. constructs exception with the specified detail message and cause. message. that cause this exception the cause (can be retried by the {@link #getcause()} method). (a null value is permitted, and indicates that the cause is nonexistent or unknown.) www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.VersionedProtocol "superclass of all protocols that use hadoop rpc. subclasses of this interface are also supposed to have a static final long versionid field. return protocol version corresponding to protocol interface. the classname of the protocol interface the version of the protocol that the client speaks version that the server will speak @throws ioexception if any io error occurs return protocol version corresponding to protocol interface. the classname of the protocol interface the version of the protocol that the client speaks the hashcode of client protocol methods server protocol signature containing its version and a list of its supported methods @see protocolsignature#getprotocolsignature(versionedprotocol, string, long, int) for a default implementation www.apache.org/licenses/license-2.0"
org.apache.hadoop.ipc.WritableRpcEngine "an rpcengine implementation for writable data. whether or not this class has been initialized. initialize this class if it isn't already. register the rpcrequest deserializer for writablerpcengine a method invocation, including the method name and its parameters. the name of the method invoked. the parameter classes. the parameter instances. returns the rpc version used by the client. close the ipc client that's responsible for this invoker's rpcs construct a client-side proxy object that implements the named protocol, talking to a server at the named address.  construct a server for a protocol implementation instance listening on a port and address. an rpc server. construct an rpc server. the instance whose methods will be called the configuration to use the address to bind on to listen for connection the port to listen for connections on @deprecated use #server(class, object, configuration, string, int) construct an rpc server. class the instance whose methods will be called the configuration to use the address to bind on to listen for connection the port to listen for connections on construct an rpc server. the instance whose methods will be called the configuration to use the address to bind on to listen for connection the port to listen for connections on the number of method handler threads to run whether each call should be logged @deprecated use server#server(class, object, configuration, string, int, int, int, int, boolean, secretmanager) construct an rpc server. - the protocol being registered can be null for compatibility with old usage (see below for details) the protocol impl that will be called the configuration to use the address to bind on to listen for connection the port to listen for connections on the number of method handler threads to run whether each call should be logged in order to remain compatible with the old usage where a single target protocolimpl is suppled for all protocol interfaces, and the protocolimpl is derived from the protocolclass(es) we register all interfaces extended by the protocolimpl www.apache.org/licenses/license-2.0 writablerpcversion should be updated if there is a change in format of the rpc messages. 2l - added declared class to invocation this could be different from static writablerpcversion when received at server, if client is using a different version. called when deserializing an invocation versionedprotocol is exempted from version check. for unit testing only derive protocol from impl register protocol class and its super interfaces verify rpc version client is using a different version of writablerpc versionprotocol methods are often used by client to figure out which version of protocol to use.  versioned protocol methods should go the protocolname protocol rather than the declaring class of the method since the the declaring class is versionedprotocol which is not registered directly. send the call to the highest protocol version find the right impl for the protocol based on client version. no match for protocol and version protocol supported but not the version that client wants invoke the protocol method"
org.apache.hadoop.jmx.JMXJsonServlet "this servlet is based off of the jmxproxyservlet from tomcat 7.0.14. it has been rewritten to be read only and to output in a json format so it is not really that close to the original. provides read only web access to jmx.  this servlet generally will be placed under the /jmx url for each httpserver. it provides read only access to jmx metrics. the optional qry parameter may be used to query only a subset of the jmx beans. this query functionality is provided through the {@link mbeanserver#querynames(objectname, javax.management.queryexp)} method.  for example http://.../jmx?qry=hadoop: will return all hadoop metrics exposed through jmx.  the optional get parameter is used to query an specific attribute of a jmx bean. the format of the url is http://.../jmx?get=mxbeanname::attributename  for example  http://../jmx?get=hadoop:service=namenode,name=namenodeinfo::clusterid  will return the cluster id of the namenode mxbean.  if the qry or the get parameter is not formatted correctly then a 400 bad request http response code will be returned.  if a resouce such as a mbean or attribute can not be found, a 404 sc_not_found http response code will be returned.  the return format is json and in the form   { "beans" : [ { "name":"bean-name" ... } ] }   the servlet attempts to convert the the jmxbeans into json. each bean's attributes will be converted to a json object member. if the attribute is a boolean, a number, a string, or an array it will be converted to the json equivalent. if the value is a {@link compositedata} then it will be converted to a json object with the keys as the name of the json member and the value is converted following these same rules. if the value is a {@link tabulardata} then it will be converted to an array of the {@link compositedata} elements that it contains. all other objects will be converted to a string and output as such. the bean's name and modelertype will be returned for all beans. optional paramater "callback" should be used to deliver jsonp response. mbean server. initialize this servlet. process a get request for the specified resource. the servlet request we are processing the servlet response we are creating www.apache.org/licenses/license-2.0 .../jmx?qry=hadoop: will return .../jmx?get=mxbeanname::attributename ../jmx?get=hadoop:service=namenode,name=namenodeinfo::clusterid ----------------------------------------------------- instance variables --------------------------------------------------------- public methods retrieve the mbean server "callback" parameter implies jsonp outpout query per mbean attribute query per mbean --------------------------------------------------------- private methods if the modelertype attribute was not found, the class name is used instead. the code inside the attribute getter threw an exception so log it, and fall back on the class name for some reason even with an mbeanexception available to them runtime exceptionscan still find their way through, so treat them the same as mbeanexception this happens when the code inside the jmx bean (setter?? from the java docs) threw an exception, so log it and fall back on the class name ignored for some reason the bean was not found so don't output it this is an internal error, something odd happened with reflection so log it and don't output the bean. this happens when the code inside the jmx bean threw an exception, so log it and don't output the bean. unsupportedoperationexceptions happen in the normal course of business, so no need to log them as errors all the time. runtimeerrorexception happens when an unexpected failure occurs in getattribute for example https://issues.apache.org/jira/browse/daemon-120 ignored the attribute was not found, which should never happen because the bean just told us that it has this attribute, but if this happens just don't output the attribute. the code inside the attribute getter threw an exception so log it, and skip outputting the attribute for some reason even with an mbeanexception available to them runtime exceptions can still find their way through, so treat them the same as mbeanexception this happens when the code inside the jmx bean (setter?? from the java docs) threw an exception, so log it and skip outputting the attribute ignored the mbean itself was not found, which should never happen because we just accessed it (perhaps something unregistered in-between) but if this happens just don't output the attribute."
org.apache.hadoop.jmx.package-info "this package provides access to jmx primarily through the {@link org.apache.hadoop.jmx.jmxjsonservlet} class. www.apache.org/licenses/license-2.0"
org.apache.hadoop.log.EventCounter "a log4j appender that simply counts logging events in three levels: fatal, error and warn. the class name is used in log4j.properties @deprecated use {@link org.apache.hadoop.log.metrics.eventcounter} instead www.apache.org/licenses/license-2.0 the logging system is not started yet."
org.apache.hadoop.log.Log4Json "this offers a log layout for json, with some test entry points. it's purpose is to allow log4j to generate events that are easy for other programs to parse, but which are somewhat human-readable. some features.  every event is a standalone json clause time is published as a time_t event since 1/1/1970 -this is the fastest to generate. an iso date is generated, but this is cached and will only be accurate to within a second the stack trace is included as an array  a simple log event will resemble the following  {"name":"test","time":1318429136789,"date":"2011-10-12 15:18:56,789","level":"info","thread":"main","message":"test message"}  an event with an error will contain data similar to that below (which has been reformatted to be multi-line).  { "name":"testexception", "time":1318429136789, "date":"2011-10-12 15:18:56,789", "level":"info", "thread":"quoted\"", "message":"new line\n and {}", "exceptionclass":"java.net.noroutetohostexception", "stack":[ "java.net.noroutetohostexception: that box caught fire 3 years ago", "\tat org.apache.hadoop.log.testlog4json.testexception(testlog4json.java:49)", "\tat sun.reflect.nativemethodaccessorimpl.invoke0(native method)", "\tat sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)", "\tat sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)", "\tat java.lang.reflect.method.invoke(method.java:597)", "\tat junit.framework.testcase.runtest(testcase.java:168)", "\tat junit.framework.testcase.runbare(testcase.java:134)", "\tat junit.framework.testresult$1.protect(testresult.java:110)", "\tat junit.framework.testresult.runprotected(testresult.java:128)", "\tat junit.framework.testresult.run(testresult.java:113)", "\tat junit.framework.testcase.run(testcase.java:124)", "\tat junit.framework.testsuite.runtest(testsuite.java:232)", "\tat junit.framework.testsuite.run(testsuite.java:227)", "\tat org.junit.internal.runners.junit38classrunner.run(junit38classrunner.java:83)", "\tat org.apache.maven.surefire.junit4.junit4testset.execute(junit4testset.java:59)", "\tat org.apache.maven.surefire.suite.abstractdirectorytestsuite.executetestset(abstractdirectorytestsuite.java:120)", "\tat org.apache.maven.surefire.suite.abstractdirectorytestsuite.execute(abstractdirectorytestsuite.java:145)", "\tat org.apache.maven.surefire.surefire.run(surefire.java:104)", "\tat sun.reflect.nativemethodaccessorimpl.invoke0(native method)", "\tat sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)", "\tat sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)", "\tat java.lang.reflect.method.invoke(method.java:597)", "\tat org.apache.maven.surefire.booter.surefirebooter.runsuitesinprocess(surefirebooter.java:290)", "\tat org.apache.maven.surefire.booter.surefirebooter.main(surefirebooter.java:1017)" ] }  jackson factories are thread safe when constructing parsers and generators. they are not thread safe in configure methods; if there is to be any configuration it must be done in a static intializer block. mime type of json convert an event to json the event -must not be null string value @throws ioexception on problems generating the json convert an event to json the destination writer the event -must not be null writer @throws ioexception on problems generating the json build a json entry from the parameters. this is public for testing. destination logger name time_t value level string name of the thread rendered message nullable thrown information writer @throws ioexception on any problem this appender does not ignore throwables , always do nothing for use in tests incoming json to parse node tree @throws ioexception on any parsing problems www.apache.org/licenses/license-2.0 this really should not happen, and rather than throw an exception which may hide the real problem, the log class is printed in json format. the classname is used to ensure valid json is returned without playing escaping games there is some throwable info, but if the log event has been sent over the wire, there may not be a throwable inside it, just a summary."
org.apache.hadoop.log.LogLevel "change log level in runtime. a command line implementation a servlet implementation www.apache.org/licenses/license-2.0 " + args[1] + "/loglevel?log=" + args[2]); " + args[1] + "/loglevel?log=" + args[2] do the"
org.apache.hadoop.log.metrics.EventCounter "a log4j appender that simply counts logging events in three levels: fatal, error and warn. the class name is used in log4j.properties www.apache.org/licenses/license-2.0 depends on the api, == might not work see hadoop-7055 for details"
org.apache.hadoop.metrics.ContextFactory "contextfactory.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. factory class for creating metricscontext objects. to obtain an instance of this class, use the static getfactory() method. creates a new instance of contextfactory returns the value of the named attribute, or null if there is no attribute of that name. the attribute name attribute value returns the names of all the factory's attributes. attribute names sets the named factory attribute to the specified value, creating it if it did not already exist. if the value is null, this is the same as calling removeattribute. the attribute name the new attribute value removes the named attribute if it exists. the attribute name returns the named metricscontext instance, constructing it if necessary using the factory's current configuration attributes.  when constructing the instance, if the factory property contextname.class exists, its value is taken to be the name of the class to instantiate. otherwise, the default is to create an instance of org.apache.hadoop.metrics.spi.nullcontext, which is a dummy "no-op" context which will cause all metric data to be discarded. the name of the context named metricscontext returns all metricscontexts built by this factory. returns a "null" context - one which does nothing. returns the singleton contextfactory instance, constructing it if necessary.  when the instance is constructed, this method checks if the file hadoop-metrics.properties exists on the class path. if it exists, it must be in the format defined by java.util.properties, and all the properties in the file are set as attributes on the newly www.apache.org/licenses/license-2.0 used only when contexts, or the contextfactory itself, cannot be  for (string attributename : attributemap.keyset()) { make a copy to avoid race conditions with creating new contexts. for (object propertynameobj : properties.keyset()) {"
org.apache.hadoop.metrics.file.FileContext "filecontext.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. metrics context for writing metrics to a file. this class is configured by setting contextfactory attributes which in turn are usually configured through a properties file. all the attributes are prefixed by the contextname. for example, the properties file might contain:  mycontextname.filename=/tmp/metrics.log mycontextname.period=5  @see org.apache.hadoop.metrics2.sink.filesink for metrics 2.0. configuration attribute names creates a new instance of filecontext returns the configured file name, or null. starts or restarts monitoring, by opening in append-mode, the file specified by the filename attribute, if specified. otherwise the data will be written to standard output. stops monitoring, closing the file. @see #close() emits a metrics record to a file. flushes the output writer, forcing updates to disk. www.apache.org/licenses/license-2.0 file for metrics to be written to"
org.apache.hadoop.metrics.ganglia.GangliaContext "gangliacontext.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. context for sending metrics to ganglia. creates a new instance of gangliacontext method to close the datagram socket puts a string into the buffer by first writing the size of the string as an int, followed by the bytes of the string, padded if necessary to a multiple of 4. pads the buffer with zero bytes up to the nearest multiple of 4. puts an integer into the buffer as 4 bytes, big-endian. www.apache.org/licenses/license-2.0 as per libgmond.c setup so that the records have the proper leader names so they are unambiguous at the ganglia level, and this prevents a lot of rework emit each metric in turn metric_ see gmetric.c"
org.apache.hadoop.metrics.ganglia.GangliaContext31 "gangliacontext.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. context for sending metrics to ganglia version 3.1.x. 3.1.1 has a slightly different wire portal compared to 3.0.x. num of the entries in extra_value field for ganglia 3.1.x group attribute group value www.apache.org/licenses/license-2.0 take the hostname from the dns class. the following xdr recipe was done through a careful reading of gm_protocol.x in ganglia 3.1 and carefully examining the output of the gmetric utility with strace. first we send out a metadata message metric_id = metadata_msg hostname metric name spoof = false metric type metric name units slope tmax, the maximum time between metrics dmax, the maximum data value now we send out a message with the actual value. technically, we only need to send out the metadata message once for each metric, but i don't want to have to record which metrics we did and did not send. we are sending a string value hostname metric name spoof = false format field metric value"
org.apache.hadoop.metrics.jvm.EventCounter "a log4j appender that simply counts logging events in three levels: fatal, error and warn. www.apache.org/licenses/license-2.0 the logging system is not started yet."
org.apache.hadoop.metrics.jvm.JvmMetrics "singleton class which reports java virtual machine metrics to the metrics api. any application can create an instance of this class in order to emit java vm metrics. creates a new instance of jvmmetrics this will be called periodically (with the period being configuration dependent). www.apache.org/licenses/license-2.0 garbage collection counters logging event counters threadinfo is null if the thread is not alive or doesn't exist"
org.apache.hadoop.metrics.jvm.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.MetricsContext "metricscontext.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. the main interface to the metrics package. default period in seconds at which data is sent to the metrics system. initialize this context. the given name for this context the creator of this context returns the context name. context name starts or restarts monitoring, the emitting of metrics records as they are updated. stops monitoring. this does not free any data that the implementation may have buffered for sending at the next timer event. it is ok to call startmonitoring() again after calling this. @see #close() returns true if monitoring is currently in progress. stops monitoring and also frees any buffered data, returning this object to its initial state. creates a new metricsrecord instance with the given recordname. throws an exception if the metrics implementation is configured with a fixed set of record names and recordname is not in that set. the name of the record @throws metricsexception if recordname conflicts with configuration data registers a callback to be called at regular time intervals, as determined by the implementation-class specific configuration. object to be run periodically; it should updated some metrics records and then return removes a callback, if it exists. object to be removed from the callback list returns the timer period. retrieves all the records managed by this metricscontext. useful for monitoring systems that are polling-based. non-null map from all record names to the records managed. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.MetricsException "metricsexception.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. general-purpose, unchecked metrics exception. creates a new instance of metricsexception creates a new instance of metricsexception an error message www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.MetricsRecord "metricsrecord.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. a named and optionally tagged set of records to be sent to the metrics system.  a record name identifies the kind of data to be reported. for example, a program reporting statistics relating to the disks on a computer might use a record name "diskstats". a record has zero or more tags. a tag has a name and a value. to continue the example, the "diskstats" record might use a tag named "diskname" to identify a particular disk. sometimes it is useful to have more than one tag, so there might also be a "disktype" with value "ide" or "scsi" or whatever. a record also has zero or more metrics. these are the named values that are to be reported to the metrics system. in the "diskstats" example, possible metric names would be "diskpercentfull", "diskpercentbusy", "kbreadpersecond", etc. the general procedure for using a metricsrecord is to fill in its tag and metric values, and then call update() to pass the record to the client library. metric data is not immediately sent to the metrics system each time that update() is called. an internal table is maintained, identified by the record name. this table has columns corresponding to the tag and the metric names, and rows corresponding to each unique set of tag values. an update either modifies an existing row in the table, or adds a new row with a set of tag values that are different from all the other rows. note that if there are no tags, then there can be at most one row in the table.  once a row is added to the table, its data will be sent to the metrics system on every timer period, whether or not it has been updated since the previous timer period. if this is inappropriate, for example if metrics were being reported by some transient object in an application, the remove() method can be used to remove the row and thus stop the data from being sent. note that the update() method is atomic. this means that it is safe for different threads to be updating the same metric. more precisely, it is ok for different threads to call update() on metricsrecord instances with the same set of tag names and tag values. different threads should not use the same metricsrecord instance at the same time. returns the record name. record name sets the named tag to the specified value. the tagvalue may be null, which is treated the same as an empty string. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration removes any tag of the specified name. name of a tag sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration updates the table of buffered data which is to be sent periodically. if the tag values match an existing row, that row is updated; otherwise, a new row is added. removes, from the buffered data table, all rows having tags that equal the tags that have been set on this record. for example, if there are no tags on this record, all rows for this record name would be removed. or, if there is a single tag on this record, then just rows containing a tag with the same name and value would be removed. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.MetricsServlet "a servlet to print out metrics data. by default, the servlet returns a textual representation (no promises are made for parseability), and a helper class to hold a tagmap and metricmap. converts to json by providing an array. collects all metric data, and returns a map: contextname -> recordname -> [ (tag->tagvalue), (metric->metricvalue) ]. the values are either string or number. the final value is implemented as a list of tagsmetricspair. prints metrics data in a multi-line text form. www.apache.org/licenses/license-2.0 uses jetty's built-in json support to convert the map into json. prints tag values in the form "{key=value,key=value}:" now print metric values, one per line"
org.apache.hadoop.metrics.MetricsUtil "utility class to simplify creation and reporting of hadoop metrics. for examples of usage, see namenodemetrics. @see org.apache.hadoop.metrics.metricsrecord @see org.apache.hadoop.metrics.metricscontext @see org.apache.hadoop.metrics.contextfactory don't allow creation of a new instance of metrics utility method to return the named context. if the desired context cannot be utility method to create and return new metrics record instance within the given context. this record is tagged with the host name. the context name of the record returns the host name. if the host name is unobtainable, logs the exception and returns "unknown". www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.AbstractMetricsContext "abstractmetricscontext.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. the main class of the service provider interface. this class should be extended in order to integrate the metrics api with a specific metrics client library.  this class implements the internal table of metric data, and the timer on which data is to be sent to the metrics system. subclasses must override the abstract emitrecord method in order to transmit the data.  returns true if this tagmap contains every tag in other. creates a new instance of abstractmetricscontext initializes the context. convenience method for subclasses to access factory attributes. returns an attribute-value map derived from the factory attributes by finding all factory attributes that begin with contextname.tablename. the returned map consists of those attributes with the contextname and tablename stripped off. returns the context name. returns the factory by which this context was starts or restarts monitoring, the emitting of metrics records. stops monitoring. this does not free buffered data. @see #close() returns true if monitoring is currently in progress. stops monitoring and frees buffered data, returning this object to its initial state. creates a new abstractmetricsrecord instance with the given recordname. throws an exception if the metrics implementation is configured with a fixed set of record names and recordname is not in that set. the name of the record @throws metricsexception if recordname conflicts with configuration data subclasses should override this if they subclass metricsrecordimpl. the name of the record registers a callback to be called at time intervals determined by the configuration. object to be run periodically; it should update some metrics records removes a callback, if it exists. object to be removed from the callback list starts timer if it is not already started stops timer if it is running timer callback. emits the records. retrieves all the records managed by this metricscontext. useful for monitoring systems that are polling-based. non-null collection of all monitoring records. sends a record to the metrics system. called each period after all records have been emitted, this method does nothing. subclasses may override it in order to perform some kind of flush. called by metricsrecordimpl.update(). creates or updates a row in the internal table of metric data. adds two numbers, coercing the second to the type of the first. called by metricsrecordimpl.remove(). removes all matching rows in the internal table of metric data. a row matches if it has the same tag names and values as record, but it may also have additional tags. returns the timer period. sets the timer period if a period is set in the attribute passed in, override the default with it. www.apache.org/licenses/license-2.0 either key does not exist here, or the value is different run all the registered updates without holding a lock on this context clone tags should never happen"
org.apache.hadoop.metrics.spi.CompositeContext "return true if all subcontexts are monitoring. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.MetricsRecordImpl "metricsrecordimpl.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. an implementation of metricsrecord. keeps a back-pointer to the context from which it was creates a new instance of filerecord returns the record name. record name sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration sets the named tag to the specified value. name of the tag new value of the tag @throws metricsexception if the tagname conflicts with the configuration removes any tag of the specified name. sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration sets the named metric to the specified value. name of the metric new value of the metric @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration increments the named metric by the specified value. name of the metric incremental value @throws metricsexception if the metricname or the type of the metricvalue conflicts with the configuration updates the table of buffered data which is to be sent periodically. if the tag values match an existing row, that row is updated; otherwise, a new row is added. removes the row, if it exists, in the buffered data table having tags that equal the tags that have been set on this record. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.MetricValue "metricvalue.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. a number that is either an absolute or an incremental amount. creates a new instance of metricvalue www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.NoEmitMetricsContext "a metricscontext that does not emit data, but, unlike nullcontextwithupdate, does save it for retrieval with getallrecords(). this is useful if you want to support {@link metricsservlet}, but not emit metrics in any other way. creates a new instance of nullcontextwithupdatethread do-nothing version of emitrecord www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.NullContext "nullcontext.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. null metrics context: a metrics context which does nothing. used as the default context, so that no performance data is emitted if no configuration data is found. creates a new instance of nullcontext do-nothing version of startmonitoring do-nothing version of emitrecord do-nothing version of update do-nothing version of remove www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.NullContextWithUpdateThread "a null context which has a thread calling periodically when monitoring is started. this keeps the data sampled correctly. in all other respects, this is like the null context: no data is emitted. this is suitable for monitoring systems like jmx which reads the metrics when someone reads the data from jmx. the default impl of start and stop monitoring: is the abstractmetricscontext is good enough. creates a new instance of nullcontextwithupdatethread do-nothing version of emitrecord do-nothing version of update do-nothing version of remove www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.OutputRecord "outputrecord.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. represents a record of metric data to be sent to a metrics system. creates a new instance of outputrecord returns the set of tag names returns a tag object which is can be a string, integer, short or byte. tag value, or null if there is no such tag returns the set of metric names. returns the metric object which can be a float, integer, short or byte. returns a copy of this record's tags. returns a copy of this record's metrics. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.spi.Util "util.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. static utility methods this class is not intended to be instantiated parses a space and/or comma separated sequence of server specifications of the form hostname or hostname:port. if the specs string is null, defaults to localhost:defaultport. list of inetsocketaddress objects. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.Updater "updater.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. call-back interface. see metricscontext.registerupdater(). timer-based call-back from the metric library. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.util.MBeanUtil "this util class provides a method to register an mbean using our standard naming convention as described in the doc for {link {@link #registermbean(string, string, object)} register the mbean using our standard mbeanname format "hadoop:service=,name=" where the  and  are the supplied parameters - the mbean to register named used to register the mbean www.apache.org/licenses/license-2.0 ignore if instance already exists ignore"
org.apache.hadoop.metrics.util.MetricsBase "this is base class for all metrics www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase "this abstract base class facilitates creating dynamic mbeans automatically from metrics. the metrics constructors registers metrics in a registry. different categories of metrics should be in differnt classes with their own registry (as in namenodemetrics and datanodemetrics). then the mbean can be www.apache.org/licenses/license-2.0 has metrics and registry for each of the metrics there are 3 different attributes the min and max can be reset. note the special attributes (avg_time, min_time, ..) are derived from metrics rather than check for the suffix we store them in a map. right now we support only one fixed operation (if it applies)"
org.apache.hadoop.metrics.util.MetricsIntValue "the metricsintvalue class is for a metric that is not time varied but changes only when it is set. each time its value is set, it is published only once at the next update call. constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered a description of {@link #no_description} is used set the value get value value last set push the metric to the mr. the metric is pushed only if it was updated since last push note this does not push to jmx (jmx gets the info via {@link #get()} www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.util.MetricsLongValue "the metricslongvalue class is for a metric that is not time varied but changes only when it is set. each time its value is set, it is published only once at the next update call. constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered a description of {@link #no_description} is used set the value get value value last set push the metric to the mr. the metric is pushed only if it was updated since last push note this does not push to jmx (jmx gets the info via {@link #get()} www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.util.MetricsRegistry "this is the registry for metrics. related set of metrics should be declared in a holding class and registered in a registry for those metrics which is also stored in the the holding class. of metrics in the registry add a new metrics to the registry - the name - the metrics @throws illegalargumentexception if a name is already registered metrics if there is one registered by the supplied name. returns null if none is registered list of metrics names list of metrics www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.util.MetricsTimeVaryingInt "the metricstimevaryingint class is for a metric that naturally varies over time (e.g. number of files constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered - the description constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered a description of {@link #no_description} is used inc metrics for incr vlaue - number of operations inc metrics by one push the delta metrics to the mr. the delta is since the last push/interval. note this does not push to jmx (jmx gets the info via {@link #previousintervalvalue} the value at the previous interval interval value the value at the current interval interval value www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.util.MetricsTimeVaryingLong "the metricstimevaryinglong class is for a metric that naturally varies over time (e.g. number of files constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered a description of {@link #no_description} is used inc metrics for incr vlaue - number of operations inc metrics by one push the delta metrics to the mr. the delta is since the last push/interval. note this does not push to jmx (jmx gets the info via {@link #previousintervalvalue} the value at the previous interval interval value the value at the current interval interval value www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics.util.MetricsTimeVaryingRate "the metricstimevaryingrate class is for a rate based metric that naturally varies over time (e.g. time taken to create a file). the rate is averaged at each interval heart beat (the interval is set in the metrics config file). this class also keeps track of the min and max rates along with a method to reset the min-max. constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered constructor - create a new metric the name of the metrics to be used to publish the metric - where the metrics object will be registered a description of {@link #no_description} is used increment the metrics for numops operations - number of operations - time for numops operations increment the metrics for one operation for one operation push the delta metrics to the mr. the delta is since the last push/interval. note this does not push to jmx (jmx gets the info via {@link #getpreviousintervalaveragetime()} and {@link #getpreviousintervalnumops()} the number of operations in the previous interval - ops in prev interval the average rate of an operation in the previous interval - the average rate. the min time for a single operation since the last reset {@link #resetminmax()} time for an operation the max time for a single operation since the last reset {@link #resetminmax()} time for an operation reset the min max values www.apache.org/licenses/license-2.0 total time or average time update min max"
org.apache.hadoop.metrics.util.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.AbstractMetric "the immutable metric construct the metric about the metric get the value of the metric value of the metric get the type of the metric type of the metric accept a visitor interface of the metric www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.annotation.Metric "annotation interface for a single metric shorthand for optional name and description {description} or {name, description} description of the metric sample name for mutablestat/rate/rates value name for mutablestat/rate/rates to create a metric snapshot even if unchanged. type (counter|gauge) of the metric www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.annotation.Metrics "annotation interface for a group of metrics (record) name of the metrics optional description of metrics context name for a group of metrics www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.annotation.package-info "annotation interfaces for metrics instrumentation www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.filter.AbstractPatternFilter "base class for pattern based filters compile a string pattern in to a pattern object the string pattern to compile compiled pattern object www.apache.org/licenses/license-2.0 accept if whitelisted reject if blacklisted reject if no match in whitelist only mode accept if any include tag pattern matches reject if any exclude tag pattern matches reject if no match in whitelist only mode accept if whitelisted reject if blacklisted reject if no match in whitelist only mode"
org.apache.hadoop.metrics2.filter.GlobFilter "a glob pattern filter for metrics. the class name is used in metrics config files www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.filter.package-info "builtin metrics filters (to be used in metrics config files) www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.filter.RegexFilter "a regex pattern filter for metrics www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.AbstractMetricsRecord "www.apache.org/licenses/license-2.0 should make sense most of the time when the record is used as a key"
org.apache.hadoop.metrics2.impl.MBeanInfoBuilder "helper class to build mbeaninfo from metrics records www.apache.org/licenses/license-2.0 read-only, non-is no ops/ctors/notifications"
org.apache.hadoop.metrics2.impl.MetricCounterInt "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricCounterLong "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricGaugeDouble "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricGaugeFloat "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricGaugeInt "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricGaugeLong "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricsBuffer "an immutable element for the sink queues. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricsBufferBuilder "builder for the immutable metrics buffers www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricsCollectorImpl "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricsConfig "metrics configuration for metricssystemimpl load configuration from a list of files until the first successful load the configuration object the list of filenames to try the configuration object return sub configs for instance specified in the config. assuming format specified as follows: [type].[instance].[option] = [value] note, '' is a special default instance, which is excluded in the result. of the instance a map with [instance] as key and config object as value will poke parents for defaults to lookup the value or null www.apache.org/licenses/license-2.0 seconds seconds back off factor default to an empty configuration pluginloader.close(); // jdk7 is saner don't create filter instances without out options glob filter is assumed if pattern is specified but class is not."
org.apache.hadoop.metrics2.impl.MetricsConfigException "the metrics configuration runtime exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricsRecordFiltered "www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.MetricsRecordImpl "construct a metrics record {@link metricinfo} of the record of the record of the record of the record www.apache.org/licenses/license-2.0 usually the first tag already unmodifiable from metricsrecordbuilderimpl#tags"
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter "an adapter class for metrics sink and associated filters www.apache.org/licenses/license-2.0 ok millis don't keep complaining ad infinitum"
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter "an adapter class for metrics source and associated filter and jmx impl www.apache.org/licenses/license-2.0 hack to avoid most of the "innocuous" races. temporarilly advance the expiry while updating the cache in case regular interval update is not running get all the metrics to populate the sink caches"
org.apache.hadoop.metrics2.impl.MetricsSystemImpl "a base class for metrics system singletons construct the metrics system for the system construct the system but not initializing (read config etc.) it. initialized the metrics system with a prefix. the system will look for configs with the prefix metrics system object itself requests an immediate publish of all metrics from sources to sinks. sample all the sources for a snapshot of metrics/tags the metrics buffer containing the snapshot publish a metrics snapshot to all the sinks the metrics snapshot to publish indicates that we should publish metrics immediately instead of using a separate thread. www.apache.org/licenses/license-2.0 things that are changed by init()/start()/stop() seconds number of timer invocations period for mini cluster mode prefix could be null for default ctor, which requires init later in mini cluster mode configuration errors (e.g., typos) should not be fatal. we can always start the metrics system later via jmx. in mini cluster mode be friendly to non-metrics tests we want to re-register the source to pick up new config when the metrics system restarts. we want to re-register the sink to pick up new config when the metrics system restarts. these are not considered fatal. sink can be registered later on already shutdown"
org.apache.hadoop.metrics2.impl.MsInfo "metrics system related metrics info instances www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.package-info "a metrics system implementation www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.impl.SinkQueue "a half-blocking (nonblocking for producers, blocking for consumers) queue for metrics sinks. new elements are dropped when the queue is full to preserve "interesting" elements at the onset of queue filling events consume one element, will block if queue is empty only one consumer at a time is allowed the consumer callback object consume all the elements, will block if queue is empty the consumer callback object @throws interruptedexception dequeue one element from head of the queue, will block if queue is empty the first element @throws interruptedexception www.apache.org/licenses/license-2.0 a fixed size circular buffer to minimize garbage head position tail position number of elements can take forever can take forever hint to gc"
org.apache.hadoop.metrics2.lib.DefaultMetricsFactory "experimental interface to extend metrics dynamically www.apache.org/licenses/license-2.0 the singleton"
org.apache.hadoop.metrics2.lib.DefaultMetricsSystem "the default metrics system singleton convenience method to initialize the metrics system for the metrics system configuration metrics system instance metrics system object shutdown the metrics system www.apache.org/licenses/license-2.0 the singleton"
org.apache.hadoop.metrics2.lib.Interns "helpers to create interned metrics info get a metric info object interned metric info object get a metrics tag of the tag of the tag interned metrics tag get a metrics tag of the tag of the tag of the tag interned metrics tag www.apache.org/licenses/license-2.0 a simple intern cache with two keys (to avoid creating new (combined) key objects for lookup) sanity limits in case of misuse/abuse. distinct per name sanity limits distinct per name"
org.apache.hadoop.metrics2.lib.MethodMetric "metric generated from a method, mostly used by annotation www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MetricsAnnotations "metrics annotation helpers. make an metrics source from an annotated object. the annotated object. metrics source www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MetricsInfoImpl "making implementing metric info a little easier www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MetricsRegistry "an optional metrics registry class for creating and maintaining a collection of metricsmutables, making writing metrics source easier. construct the registry with a record name of the record of the metrics construct the registry with a metadata object the info object for the metrics record/group info object of the metrics registry get a metric by name of the metric metric object get a tag by name of the tag tag object create a mutable integer counter of the metric metric description initial value new counter object create a mutable integer counter metadata of the metric initial value new counter object create a mutable long integer counter of the metric metric description initial value new counter object create a mutable long integer counter metadata of the metric initial value new counter object create a mutable integer gauge of the metric metric description initial value new gauge object create a mutable integer gauge metadata of the metric initial value new gauge object create a mutable long integer gauge of the metric metric description initial value new gauge object create a mutable long integer gauge metadata of the metric initial value new gauge object create a mutable metric that estimates quantiles of a stream of values of the metric metric description of the metric (e.g., "ops") of the metric (e.g., "time" or "latency") rollover interval of estimator in seconds new quantile estimator object create a mutable metric with stats of the metric metric description of the metric (e.g., "ops") of the metric (e.g., "time" or "latency") produce extended stat (stdev, min/max etc.) if true. new mutable stat metric object create a mutable metric with stats of the metric metric description of the metric (e.g., "ops") of the metric (e.g., "time" or "latency") new mutable metric object create a mutable rate metric of the metric new mutable metric object create a mutable rate metric of the metric of the metric new mutable rate metric object create a mutable rate metric (for throughput measurement) of the metric description produce extended stat (stdev/min/max etc.) if true new mutable rate metric object add sample to a stat metric by name. of the metric of the snapshot to add set the metrics context tag of the context registry itself as a convenience add a tag to the metrics of the tag of the tag of the tag registry (for keep adding tags) add a tag to the metrics of the tag of the tag of the tag existing tag if true registry (for keep adding tags) add a tag to the metrics metadata of the tag of the tag existing tag if true registry (for keep adding tags etc.) sample all the mutable metrics and put the snapshot in the builder to contain the metrics snapshot get all the metrics even if the values are not changed. www.apache.org/licenses/license-2.0 default is a rate metric"
org.apache.hadoop.metrics2.lib.MetricsSourceBuilder "helper class to build metrics source object from annotations www.apache.org/licenses/license-2.0 get the registry if it already exists. create a new registry according to annotation skip fields already set"
org.apache.hadoop.metrics2.lib.MutableCounter "the mutable counter (monotonically increasing) metric interface increment the metric value by 1. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableCounterInt "a mutable int counter for implementing metrics sources increment the value by a delta of the increment www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableCounterLong "a mutable long counter increment the value by a delta of the increment www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableGauge "the mutable gauge metric interface increment the value of the metric by 1 decrement the value of the metric by 1 www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableGaugeInt "a mutable int gauge increment by delta of the increment decrement by delta of the decrement set the value of the metric to set www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableGaugeLong "a mutable long gauge increment by delta of the increment decrement by delta of the decrement set the value of the metric to set www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableMetric "the mutable metric interface get a snapshot of the metric the metrics record builder if true, snapshot unchanged metrics as well get a snapshot of metric if changed the metrics record builder set the changed flag in mutable operations clear the changed flag in the snapshot operations true if metric is changed since last snapshot/snapshot www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableMetricsFactory "override to handle custom mutable metrics for fields of the metric of the field new metric object or null override to handle custom mutable metrics for methods the metrics source object to return the metric of the method new metric object or null www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableQuantiles "watches a stream of long values, maintaining online estimates of specific quantiles with provably low error bounds. this is particularly useful for accurate high-percentile (e.g. 95th, 99th) latency metrics. instantiates a new {@link mutablequantiles} for a metric that rolls itself over on the specified time interval. of the metric long-form textual description of the metric type of items in the stream (e.g., "ops") type of the values rollover interval (in seconds) of the estimator runnable used to periodically roll over the internal {@link samplequantiles} every interval. www.apache.org/licenses/license-2.0 construct the metricsinfos for the quantiles, converting to percentiles if snapshot is null, we failed to update since the window was empty"
org.apache.hadoop.metrics2.lib.MutableRate "a convenient mutable metric for throughput measurement www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableRates "helper class to manage a group of mutable rate metrics initialize the registry with all the methods in a protocol so they all show up in the first snapshot. convenient for jmx implementations. the protocol class add a rate sample for a rate metric of the rate metric time www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.MutableStat "a mutable metric with stats. useful for keeping throughput/latency stats. construct a sample statistics metric of the metric of the metric of the metric (e.g. "ops") of the metric (e.g. "time", "latency") create extended stats (stdev, min/max etc.) by default. construct a snapshot stat metric with extended stat off by default of the metric of the metric of the metric (e.g. "ops") of the metric (e.g. "time", "latency") add a number of samples and their sum to the running stat number of samples of the samples add a snapshot to the metric of the metric reset the all time min max of the metric www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.package-info "a collection of library classes for implementing metrics sources www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.lib.UniqueNames "generates predictable and www.apache.org/licenses/license-2.0 handle collisons, assume to be rare cases, eg: people explicitly passed in name-\d+ names."
org.apache.hadoop.metrics2.MetricsCollector "the metrics collector interface add a metrics record of the record a metrics record builder for the record add a metrics record of the record a metrics record builder for the record www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsException "a general metrics exception wrapper construct the exception with a message for the exception construct the exception with a message and a cause for the exception of the exception construct the exception with a cause of the exception www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsFilter "the metrics filter interface whether to accept the name to filter on true to accept; false otherwise. whether to accept the tag to filter on true to accept; false otherwise whether to accept the tags to filter on true to accept; false otherwise whether to accept the record to filter on true to accept; false otherwise. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsInfo "interface to provide immutable meta info for metrics name of the metric/tag description of the metric/tag www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsPlugin "the plugin interface for the metrics framework initialize the plugin the configuration object for the plugin www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsRecord "an immutable snapshot of metrics with a timestamp get the timestamp of the metrics the timestamp record name description of the record context name of the record get the tags of the record note: returning a collection instead of iterable as we need to use tags as keys (hence collection#hashcode etc.) in maps unmodifiable collection of tags get the metrics of the record immutable iterable interface for metrics www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsRecordBuilder "the metrics record builder interface add a metrics tag metadata of the tag of the tag add an immutable metrics tag object a pre-made tag object (potentially save an object construction) add a pre-made immutable metric object the pre-made metric to save an object construction set the context tag of the context add an integer metric metadata of the metric of the metric add an long metric metadata of the metric of the metric add a integer gauge metric metadata of the metric of the metric add a long gauge metric metadata of the metric of the metric add a float gauge metric metadata of the metric of the metric add a double gauge metric metadata of the metric of the metric parent metrics collector object syntactic sugar to add multiple records in a collector in a one liner. parent metrics collector object www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsSink "the metrics sink interface put a metrics record in the sink the record to put flush any buffered metrics www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsSource "the metrics source interface get metrics from the source to contain the resulting metrics snapshot if true, return all metrics even if unchanged. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsSystem "the metrics system interface register a metrics source  the actual type of the source object object to register of the source. must be unique or null (then extracted from the annotations of the source object.) the description of the source (or null. see above.) source object @exception metricsexception register a metrics source (deriving name and description from the object)  the actual type of the source object object to register the source object @exception metricsexception of the metrics source metrics source (potentially wrapped) object register a metrics sink  the type of the sink to register of the sink. must be unique. the description of the sink sink @exception metricsexception register a callback interface for jmx events the callback object implementing the mbean interface. requests an immediate publish of all metrics from sources to sinks. this is a "soft" request: the expectation is that a best effort will be done to synchronously snapshot the metrics from all the sources and put them in all the sinks (including flushing the sinks) before returning to the caller. if this can't be accomplished in reasonable time it's ok to return to the caller before everything is done. shutdown the metrics system completely (usually during server shutdown.) the metricssystemmxbean will be unregistered. if shutdown completed the metrics system callback interface (needed for proxies.) called before start() called after start() called before stop() called after stop() convenient abstract class for implementing callback interface www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsSystemMXBean "the jmx interface to the metrics system start the metrics system @throws metricsexception stop the metrics system @throws metricsexception start metrics mbeans @throws metricsexception stop metrics mbeans. note, it doesn't stop the metrics system control mbean, i.e this interface. @throws metricsexception current config avoided getconfig, as it'll turn into a "config" attribute, which doesn't support multiple line values in jconsole. @throws metricsexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsTag "immutable tag for metrics (for grouping on host/queue/ construct the tag with name, description and value of the tag of the tag info object of the tag get the value of the tag the value www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricsVisitor "a visitor interface for metrics callback for integer value gauges the metric info of the metric callback for long value gauges the metric info of the metric callback for float value gauges the metric info of the metric callback for double value gauges the metric info of the metric callback for integer value counters the metric info of the metric callback for long value counters the metric info of the metric www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.MetricType "a monotonically increasing metric that can be used to calculate throughput an arbitrary varying metric www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.package-info "metrics 2.0  overview getting started configuration metrics filtering metrics instrumentation strategy migration from previous system  overview this package provides a framework for metrics instrumentation and publication.  the framework provides a variety of ways to implement metrics instrumentation easily via the simple {@link org.apache.hadoop.metrics2.metricssource} interface or the even simpler and more concise and declarative metrics annotations. the consumers of metrics just need to implement the simple {@link org.apache.hadoop.metrics2.metricssink} interface. producers register the metrics sources with a metrics system, while consumers register the sinks. a default metrics system is provided to marshal metrics from sources to sinks based on (per source/sink) configuration options. all the metrics are also published and queryable via the standard jmx mbean interface. this document targets the framework www.apache.org/licenses/license-2.0 wiki.apache.org/hadoop/hadoop-6728-metricsv2">design called once per application"
org.apache.hadoop.metrics2.sink.FileSink "a metrics sink that writes to a file www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink "this the base class for ganglia sink classes using metrics2. lot of the code has been derived from org.apache.hadoop.metrics.ganglia.gangliacontext. as per the documentation, sink implementations doesn't have to worry about thread safety. hence the code wasn't written for thread safety and should be modified in case the above assumption changes in the future. output of "gmetric --help" showing allowable values -t, --type=string either string|int8|uint8|int16|uint16|int32|uint32|float|double -u, --units=string unit of measure for the value e.g. kilobytes, celcius (default='') -s, --slope=string either zero|positive|negative|both (default='both') -x, --tmax=int the maximum time in seconds between gmetric calls (default='60') used for visiting metrics ganglia slope values which equal the ordinal define enum for various type of conf (non-javadoc) @see org.apache.hadoop.metrics2.metricsplugin#init(org.apache.commons.configuration .subsetconfiguration) (non-javadoc) @see org.apache.hadoop.metrics2.metricssink#flush() lookup gangliaconf from cache. if not found, return default values up gangliaconf hostname puts a string into the buffer by first writing the size of the string as an int, followed by the bytes of the string, padded if necessary to a multiple of 4. the string to be written to buffer at offset location puts an integer into the buffer as 4 bytes, big-endian. sends ganglia metrics to the configured hosts @throws ioexception reset the buffer for the next metric to be built sparse metrics are supported used only by unit test the datagramsocket to set. www.apache.org/licenses/license-2.0 as per libgmond.c 0 1 2 3 take the hostname from the dns class. load the gannglia servers from properties extract the ganglia conf per metrics see if sparsemetrics is supported. default is false nothing to do as we are not buffering data load the configurations for a conf type pads the buffer with zero bytes up to the nearest multiple of 4. reset the buffer for the next metric to be built"
org.apache.hadoop.metrics2.sink.ganglia.GangliaConf "class which is used to store ganglia properties units the units to set slope the slope to set dmax the dmax to set tmax the tmax to set www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor "since implementations of metric are not public, hence use a visitor to figure out the type and slope of the metric. counters have "positive" slope. type of a visited metric slope of a visited metric. slope is positive for counters and null for others www.apache.org/licenses/license-2.0 metricgaugeint.class ==> "int32" set to null as cannot figure out from metric metricgaugelong.class ==> "float" set to null as cannot figure out from metric metricgaugefloat.class ==> "float" set to null as cannot figure out from metric metricgaugedouble.class ==> "double" set to null as cannot figure out from metric metriccounterint.class ==> "int32" counters have positive slope metriccounterlong.class ==> "float" counters have positive slope"
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30 "this code supports ganglia 3.0 the method sends metrics to ganglia servers. the method has been taken from org.apache.hadoop.metrics.ganglia.gangliacontext30 with minimal changes in order to keep it in sync. the group name of the metric the metric name the type of the metric the value of the metric the gangliaconf for this metric the slope for this metric @throws ioexception www.apache.org/licenses/license-2.0 a key with a null value means all the context is always skipped here because it is always added the hostname is always skipped to avoid case-mismatches from different dnses. the method handles both cases whether ganglia support dense publish of metrics of sparse (only on change) publish of metrics reset the buffer to the beginning for sending dense metrics, update metrics cache and get the updated data visit the metric to identify the ganglia type and slope send metric to ganglia reset the length of the buffer for next iteration we support sparse updates we got metrics. so send the latest visit the metric to identify the ganglia type and slope send metric to ganglia reset the length of the buffer for next iteration calculate the slope from properties and metric if slope has been specified in properties, use that slope not specified in properties, use derived from metric metric_ send the metric to ganglia hosts"
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31 "this code supports ganglia 3.1 the method sends metrics to ganglia servers. the method has been taken from org.apache.hadoop.metrics.ganglia.gangliacontext31 with minimal changes in order to keep it in sync. the group name of the metric the metric name the type of the metric the value of the metric the gangliaconf for this metric the slope for this metric @throws ioexception num of the entries in extra_value field for ganglia 3.1.x group attribute group value www.apache.org/licenses/license-2.0 the following xdr recipe was done through a careful reading of gm_protocol.x in ganglia 3.1 and carefully examining the output of the gmetric utility with strace. first we send out a metadata message metric_id = metadata_msg hostname metric name spoof = false metric type metric name units slope tmax, the maximum time between metrics dmax, the maximum data value send the metric to ganglia hosts now we send out a message with the actual value. technically, we only need to send out the metadata message once for each metric, but i don't want to have to record which metrics we did and did not send. we are sending a string value hostname metric name spoof = false format field metric value send the metric to ganglia hosts"
org.apache.hadoop.metrics2.sink.package-info "builtin metrics sinks www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.source.JvmMetrics "jvm and logging related metrics. mostly used by various servers as a part of the metrics they export. www.apache.org/licenses/license-2.0 race protection"
org.apache.hadoop.metrics2.source.JvmMetricsInfo "jvm and logging related metrics info instances www.apache.org/licenses/license-2.0 record infoã metrics"
org.apache.hadoop.metrics2.util.Contracts "additional helpers (besides guava preconditions) for programming by contract check an argument for false conditions  type of the argument the argument to check the boolean expression for the condition the error message if {@code expression} is false argument for convenience check an argument for false conditions the argument to check the boolean expression for the condition the error message if {@code expression} is false argument for convenience check an argument for false conditions the argument to check the boolean expression for the condition the error message if {@code expression} is false argument for convenience check an argument for false conditions the argument to check the boolean expression for the condition the error message if {@code expression} is false argument for convenience check an argument for false conditions the argument to check the boolean expression for the condition the error message if {@code expression} is false argument for convenience www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.util.MBeans "this util class provides a method to register an mbean using our standard naming convention as described in the doc for {link {@link #register(string, string, object)} register the mbean using our standard mbeanname format "hadoop:service=,name=" where the  and  are the supplied parameters - the mbean to register named used to register the mbean www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.util.MetricsCache "a metrics cache for sinks that don't support sparse updates. cached record lookup a tag value name of the tag tag value lookup a metric value name of the metric metric value lookup a metric instance name of the metric metric instance entry set of the tags of the record @deprecated use metricsentryset() instead set of metrics set of metrics construct a metrics cache limit of the number records per record name update the cache and return the current cached record the update record cache tag values (for later lookup by name) if true updated cache record update the cache and return the current cache record the update record updated cache record get the cached record of the record of the record cached record or null www.apache.org/licenses/license-2.0 mostly for some sinks that include tags as part of a dense schema"
org.apache.hadoop.metrics2.util.package-info "general helpers for implementing source and sinks www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.util.Quantile "specifies a quantile (with error bounds) to be watched by a {@link samplequantiles} object. www.apache.org/licenses/license-2.0"
org.apache.hadoop.metrics2.util.SampleQuantiles "implementation of the cormode, korn, muthukrishnan, and srivastava algorithm for streaming calculation of targeted high-percentile epsilon-approximate quantiles. this is a generalization of the earlier work by greenwald and khanna (gk), which essentially allows different error bounds on the targeted quantiles, which allows for far more efficient calculation of high-percentiles. see: cormode, korn, muthukrishnan, and srivastava "effective computation of biased quantiles over data streams" in icde 2005 greenwald and khanna, "space-efficient online computation of quantile summaries" in sigmod 2001 total number of items in stream current list of sampled items, maintained in sorted order with error bounds buffers incoming items to be inserted in batch. items are inserted into the buffer linearly. when the buffer fills, it is flushed into the samples array in its entirety. array of quantiles that we care about, along with desired error. specifies the allowable error for this rank, depending on which quantiles are being targeted. this is the f(r_i, n) function from the ckms paper. it's basically how wide the range of this rank can be. the index in the list of samples add a new value from the stream. merges items from buffer into the samples array in one pass. this is more efficient than doing an insert on every item. try to remove extraneous items from the set of sampled items. this checks if an item is unnecessary based on the desired error bounds, and merges it with the adjacent item if it is. get the estimated value at the specified quantile. queried quantile, e.g. 0.50 or 0.99. value at that quantile. get a snapshot of the current values of all the tracked quantiles. of the tracked quantiles. if no items are added to the estimator, returns null. returns the number of items that the estimator has processed total number of items processed returns the number of samples kept by the estimator current number of samples resets the estimator, clearing out all previously inserted items describes a measured value passed to the estimator, tracking additional metadata required by the ckms algorithm. value of the sampled item (e.g. a measured latency value) difference between the lowest possible rank of the previous item, and the lowest possible rank of this item. the sum of the g of all previous items yields this item's lower bound. difference between the item's greatest possible rank and lowest possible rank. www.apache.org/licenses/license-2.0 base case: no samples if we found that bigger item, back up so we insert ourselves before it we use different indexes for the edge comparisons, because of the above if statement that adjusts the iterator remove prev. it.remove() kills the last thing returned. it.next() is now equal to next, skip it back forward again edge case of wanting max value flush the buffer first for best results"
org.apache.hadoop.metrics2.util.SampleStat "helper to compute running sample stats construct a new running sample stat copy the values to other (saves object creation and gc.) the destination to hold our values add a sample the running stat. the sample number self add some sample and a partial sum to the running stat. note, min/max is not evaluated using this method. number of samples the partial sum self the total number of samples the arithmetic mean of the samples the variance of the samples the standard deviation of the samples the minimum value of the samples the maximum value of the samples helper to keep running min/max www.apache.org/licenses/license-2.0 we want to reuse the object, sometimes. the welford method for numerical stability float.max_value is used rather than double.max_value, even though the min and max variables are of type double. float.max_value is big enough, and using double.max_value makes ganglia core due to buffer overflow. the same reasoning applies to the min_value counterparts."
org.apache.hadoop.metrics2.util.Servers "util.java licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. helpers to handle server addresses this class is not intended to be instantiated parses a space and/or comma separated sequence of server specifications of the form hostname or hostname:port. if the specs string is null, defaults to localhost:defaultport. server specs (see description) the default port if not specified list of inetsocketaddress objects. www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.AbstractDNSToSwitchMapping "this is a base class for dns to switch mappings.  it is not mandatory to derive {@link dnstoswitchmapping} implementations from it, but it is strongly recommended, as it makes it easy for the hadoop developers to add new methods to this base class that are automatically picked up by all implementations.  this class does not extend the configured base class, and should not be changed to do so, as it causes problems for subclasses. the constructor of the configured calls the {@link #setconf(configuration)} method, which will call into the subclasses before they have been fully constructed. create an unconfigured instance create an instance, caching the configuration file. this constructor does not call {@link #setconf(configuration)}; if a subclass extracts information in that method, it must call it explicitly. the configuration predicate that indicates that the switch mapping is known to be single-switch. the base class returns false: it assumes all mappings are multi-rack. subclasses may override this with methods that are more aware of their topologies.  this method is used when parts of hadoop need know whether to apply single rack vs multi-rack policies, such as during block placement. such algorithms behave differently if they are on multi-switch systems.  if the mapping thinks that it is on a single switch get a copy of the map (for diagnostics) clone of the map or null for none known generate a string listing the switch mapping implementation, the mapping for every known node and the number of nodes and unique switches known about -each entry to a separate line. string that can be presented to the ops team or used in debug messages. query for a {@link dnstoswitchmapping} instance being on a single switch.  this predicate simply assumes that all mappings not derived from this class are multi-switch. the mapping to query if the base class says it is single switch, or the mapping is not derived from this class. www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.CachedDNSToSwitchMapping "a cached implementation of dnstoswitchmapping that takes an raw dnstoswitchmapping and stores the resolved network location in a cache. the following calls to a resolved network location will get its location from the cache. the uncached mapping cache a raw dns mapping the raw mapping to cache a list of hostnames to probe for being cached hosts from 'names' that have not been cached previously caches the resolved host:rack mappings. the two list parameters must be of equal size. a list of hosts that were uncached a list of resolved host entries where the element at index(i) is the resolved value for the entry in uncachedhosts[i] a list of hostnames to look up (can be be empty) cached resolution of the list of hostnames/addresses. or null if any of the names are not currently in the cache get the (host x switch) map. copy of the cached map of hosts to rack delegate the switch topology query to the raw mapping, via {@link abstractdnstoswitchmapping#ismappingsingleswitch(dnstoswitchmapping)} iff the raw mapper is considered single-switch. www.apache.org/licenses/license-2.0 find out all names without cached resolved location cache the result construct the result normalize all input names to be in the form of ip addresses resolve the uncached hosts cache them now look up the entire list in the cache"
org.apache.hadoop.net.ConnectTimeoutException "thrown by {@link netutils#connect(java.net.socket, java.net.socketaddress, int)} if it times out while connecting to the remote host. www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.DNS "a class that provides direct and reverse lookup functionalities, allowing the querying of specific network interfaces or nameservers. the cached hostname -initially null. returns the hostname associated with the specified ip address by the provided nameserver. loopback addresses the address to reverse lookup the host name of a reachable dns server host name associated with the provided ip @throws namingexception if a namingexception is encountered for the given subinterface name (eg eth0:0) or null if no interface with the given name can be found network interface to get addresses for containing addresses for each subinterface of nif, see below for the rationale for using an ordered set like {@link dns#getips(string, boolean), but returns all ips associated with the given interface and its subinterfaces. returns all the ips associated with the provided interface, if any, in textual form. the name of the network interface or sub-interface to query (eg eth0 or eth0:0) or the string "default" whether to return ips associated with subinterfaces of the given interface string vector of all the ips associated with the provided interface. the local host ip is returned if the interface name "default" is specified or there is an i/o error looking for the given interface. @throws unknownhostexception if the given interface is invalid returns the first available ip address associated with the provided network interface or the local host ip if "default" is given. the name of the network interface or subinterface to query (e.g. eth0 or eth0:0) or the string "default" ip address in text form, the local host ip is returned if the interface name "default" is specified @throws unknownhostexception if the given interface is invalid returns all the host names associated by the provided nameserver with the address bound to the specified network interface the name of the network interface or subinterface to query (e.g. eth0 or eth0:0) the dns host name string vector of all host names associated with the ips tied to the specified interface @throws unknownhostexception if the given interface is invalid determine the local hostname; retrieving it from cache if it is known if we cannot determine our host name, return "localhost" local hostname or "localhost" get the ipaddress of the local host as a string. this will be a loop back value if the local host address cannot be determined. if the loopback address of "localhost" does not resolve, then the system's network is in such a state that nothing is going to work. a message is logged at the error level and a null pointer returned, a pointer which will trigger failures later on the application ipaddress of the local host or null for a serious problem. returns all the host names associated by the default nameserver with the address bound to the specified network interface the name of the network interface to query (e.g. eth0) list of host names associated with ips bound to the network interface @throws unknownhostexception if one is encountered while querying the default interface returns the default (first) host name associated by the provided nameserver with the address bound to the specified network interface the name of the network interface to query (e.g. eth0) the dns host name default host names associated with ips bound to the network interface @throws unknownhostexception if one is encountered while querying the default interface returns the default (first) host name associated by the default nameserver with the address bound to the specified network interface the name of the network interface to query (e.g. eth0). must not be null. default host name associated with ips bound to the network interface @throws unknownhostexception if one is encountered while querying the default interface www.apache.org/licenses/license-2.0  builds the reverse ip lookup form this is formed by reversing the ip numbers and appending in-addr.arpa  " // use "dns:///" if the default nameserver is to be used nb: using a linkedhashset to preserve the order for callers that depend on a particular element being 1st in the array. for example, getdefaultip always returns the first element. at this point, deep trouble"
org.apache.hadoop.net.DNSToSwitchMapping "an interface that must be implemented to allow pluggable dns-name/ip-address to rackid resolvers. resolves a list of dns-names/ip-addresses and returns back a list of switch information (network paths). one-to-one correspondence must be maintained between the elements in the lists. consider an element in the argument list - x.y.com. the switch information that is returned must be a network path of the form /foo/rack, where / is the root, and 'foo' is the switch where 'rack' is connected. note the hostname/ip-address is not part of the returned path. the network topology of the cluster would determine the number of components in the network path.  if a name cannot be resolved to a rack, the implementation should return {@link networktopology#default_rack}. this is what the bundled implementations do, though it is not a formal requirement the list of hosts to resolve (can be empty) of resolved network paths. if names is empty, the returned list is also empty www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.NetUtils "text to point text included in wrapped exceptions if the host is null: {@value} base url of the hadoop wiki: {@value} get the socket factory for the given class according to its configuration parameter hadoop.rpc.socket.factory.class.&lt;classname&gt;. when no such parameter exists then fall back on the default socket factory as configured by hadoop.rpc.socket.factory.class.default. if this default socket factory is not configured, then fall back on the jvm default socket factory. the configuration the class (usually a {@link versionedprotocol}) socket factory get the default socket factory as specified by the configuration parameter hadoop.rpc.socket.factory.default the configuration default socket factory as specified in the configuration or the jvm default socket factory if the configuration does not contain a default socket factory property. get the socket factory corresponding to the given proxy uri. if the given proxy uri corresponds to an absence of configuration parameter, returns null. if the uri is malformed raises an exception. the property which is the class name of the socketfactory to instantiate; assumed non null and non empty. socket factory as defined in the property value. util method to build socket addr from either: : ://:/ util method to build socket addr from either:  : ://:/ create an inetsocketaddress from the given target string and default port. if the string cannot be parsed correctly, the configname parameter is used as part of the exception message, allowing the create a socket address with the given host and port. the hostname might be replaced with another host that was set via {@link #addstaticresolution(string, string)}. the value of hadoop.security.token.service.use_ip will determine whether the standard java host resolver is used, or if the fully qualified resolver is used. the hostname or ip use to instantiate the object the port number resolve the uri's hostname and add the default port if not in the uri to resolve if none is given adds a static resolution for host. this can be used for setting up hostnames with names that are fake to point to a well known host. for e.g. in some testcases we require to have daemons with different hostnames running on the same machine. in order to create connections to these daemons, one can set up mappings from those hostnames to "localhost". {@link netutils#getstaticresolution(string)} can be used to query for the actual hostname. retrieves the resolved name for the passed host. the resolved name must have been set earlier using {@link netutils#addstaticresolution(string, string)} resolution this is used to get all the resolutions that were added using {@link netutils#addstaticresolution(string, string)}. the return value is a list each element of which contains an array of string of the form string[0]=hostname, string[1]=resolved-hostname list of resolutions returns inetsocketaddress that a client can use to connect to the server. server.getlisteneraddress() is not correct when the server binds to "0.0.0.0". this returns "hostname:port" of the server, or "127.0.0.1:port" when the getlisteneraddress() returns "0.0.0.0:port". address that a client can use to connect to the server. returns an inetsocketaddress that a client can use to connect to the given listening address. of a listener address that a client can use to connect to the server. same as getinputstream(socket, socket.getsotimeout()).  @see #getinputstream(socket, long) return a {@link socketinputwrapper} for the socket and set the given timeout. if the socket does not have an associated channel, then its socket timeout will be set to the specified value. otherwise, a {@link socketinputstream} will be same as getoutputstream(socket, 0). timeout of zero implies write will wait until data is available. from documentation for {@link #getoutputstream(socket, long)} :  returns outputstream for the socket. if the socket has an associated socketchannel then it returns a {@link socketoutputstream} with the given timeout. if the socket does not have a channel, {@link socket#getoutputstream()} is returned. in the later case, the timeout argument is ignored and the write will wait until data is available. any socket returns outputstream for the socket. if the socket has an associated socketchannel then it returns a {@link socketoutputstream} with the given timeout. if the socket does not have a channel, {@link socket#getoutputstream()} is returned. in the later case, the timeout argument is ignored and the write will wait until data is available. any socket this is a drop-in replacement for {@link socket#connect(socketaddress, int)}. in the case of normal sockets that don't have associated channels, this just invokes socket.connect(endpoint, timeout). if socket.getchannel() returns a non-null channel, connect is implemented using hadoop's selectors. this is done mainly to avoid sun's connect implementation from creating thread-local selectors, since hadoop does not have control on when these are closed and could end up taking all the available file descriptors. @see java.net.socket#connect(java.net.socketaddress, int) the remote address timeout in milliseconds like {@link netutils#connect(socket, socketaddress, int)} but also takes a local address and port to bind the socket to. the remote address the local address to bind the socket to timeout in milliseconds given a string representation of a host, return its ip address in textual presentation. a string representation of a host: either a textual representation its ip address or its host name ip address in the string format given a collection of string representation of hosts, return a list of corresponding ip addresses in the textual representation. a collection of string representations of hosts list of corresponding ip addresses in the string format @see #normalizehostname(string) performs a sanity check on the list of hostnames/ips to verify they at least appear to be valid. - list of hostnames/ips @throws unknownhostexception attempt to obtain the host name of the given string which contains an ip address and an optional port. string of form ip[:port] name or null if the name can not be determined return hostname without throwing exception. compose a "host:port" string from the address. checks if {@code host} is a local host name and return {@link inetaddress} corresponding to that address. the specified host valid local {@link inetaddress} or null @throws socketexception if an i/o error occurs given an inetaddress, checks to see if the address is a local address, by comparing the address with all the interfaces on the node. address to check if it is local node's address if the address corresponds to the local node take an ioexception , the local host port and remote host port details and return an ioexception with the input exception as the cause and also include the host details. the new exception provides the stack trace of the place where the exception is thrown and some extra diagnostics information. if the exception is bindexception or connectexception or unknownhostexception or sockettimeoutexception, return a new one of the same type; otherwise return an ioexception. target host (nullable) target port local host (nullable) local port the caught exception. exception to throw get the host details as a string destinatioon host (nullable) destination port local host (nullable) string describing the destination host:port and the local host quote a hostname if it is not null the hostname; nullable quoted hostname or {@link #unknown_host} if the hostname is null if the given string is a subnet specified using cidr notation, false otherwise add all addresses associated with the given nif in the given subnet to the given list. return an inetaddress for each interface that matches the given subnet specified using cidr notation. subnet specified using cidr notation whether to return ips associated with subinterfaces @throws illegalargumentexception if subnet is invalid return a free port number. there is no guarantee it will remain free, so it should be used immediately. @returns a free port for binding a local socket www.apache.org/licenses/license-2.0 wiki.apache.org/hadoop/"; :/ :/ "); "+target); if there is a static entry for the host, make the returned address look like the original given host skip if there is no short out if already canonical with a port reconstruct the uri with the canonical host and port cache the canonicalized hostnames; the cache currently isn't expired, but the canonicals will only change if the host's resolver configuration changes check if the host has already been canonicalized slight race condition, but won't hurt shouldn't get here unless the host doesn't have a loopback iface let the default implementation handle it. there is a very rare case allowed by the tcp specification, such that if we are trying to connect to an endpoint on the local machine, and we end up choosing an ephemeral port equal to the destination port, we will actually end up getting connected to ourself (ie any data we send just comes right back). this is only possible if the target daemon is down, so we'll treat it like connection refused. the first check supports url formats (e.g. hdfs://, etc.). java.net.uri requires a schema, so we add a dummy one if it doesn't have one already. " + name); pattern for matching ip[:port] not a local address check if the address is any local or loop back check if the address is defined on any interface connection refused; include the host:port in the error nb: adding addresses even if the nif is not up could not get a free port. return default port 0."
org.apache.hadoop.net.NetworkTopology "the class represents a cluster of computer with a tree hierarchical network topology. for example, a cluster may be consists of many data centers filled with racks of computers. in a network topology, leaves represent data nodes (computers) and inner nodes represent switches/routers that manage traffic in/out of data centers or racks. innernode represents a switch/router of a data center or rack. different from a leaf node, it has non-null children. construct an innernode from a path-like string construct an innernode from its name and its network location construct an innernode from its name, its network location, its parent, and its level children number of children this node has judge if this node represents a rack if it has no child or its children are not innernodes judge if this node is an ancestor of node n a node if this node is an ancestor of n judge if this node is the parent of node n a node if this node is the parent of n return a child name of this node who is an ancestor of node n add node n to the subtree of this node node to be added if the node is added; false otherwise remove node n from the subtree of this node node to be deleted if the node is deleted; false otherwise given a node's string representation, return a reference to the node string location of the form /rack/node if the node is not found or the childnode is there but not an instance of {@link innernode} get leafindex leaf of this subtree if it is not in the excludednode an indexed leaf of the node an excluded node (can be null) @return the root cluster map depth of all leaf nodes rack counter the lock used to manage access add a leaf node update node counter & rack counter if necessary node to be added; can be null @exception illegalargumentexception if add a node to a leave or node to be added is not a leaf remove a node update node counter and rack counter if necessary node to be removed; can be null check if the tree contains node node a node if node is already in the tree; false otherwise given a string representation of a node, return its reference a path-like string representation of a node reference to the node; null if the node is not in the tree total number of racks total number of leaf nodes return the distance between two nodes it is assumed that the distance from one node to its parent is 1 the distance between two nodes is calculated by summing up their distances to their closest common ancestor. 1 one node 2 another node distance between node1 and node2 which is zero if they are the same or {@link integer#max_value} if node1 or node2 do not belong to the cluster check if two nodes are on the same rack 1 one node (can be null) 2 another node (can be null) if node1 and node2 are on the same rack; false otherwise @exception illegalargumentexception when either node1 or node2 is null, or node1 or node2 do not belong to the cluster randomly choose one node from scope if scope starts with ~, choose one from the all nodes except for the ones in scope; otherwise, choose one from scope range of nodes from which a node will be chosen chosen node return the number of leaves in scope but not in excludednodes if scope starts with ~, return the number of nodes that are not in scope and excludednodes; a path string that may start with ~ a list of nodes of available nodes convert a network tree to a string swap two array items sort nodes array by their distances to reader it linearly scans the array, if a local node is found, swap it with the first element of the array. if a local rack node is found, swap it with the first element following the local node. if neither local node or local rack node is found, put a random replica location at position 0. it leaves the rest nodes untouched. the node that wishes to read a block from one of the nodes the list of nodes containing data for the reader www.apache.org/licenses/license-2.0 this node is the parent of n; add n directly find the next ancestor node create a new innernode add n to the subtree of the next ancestor node this node is the parent of n; remove n directly find the next ancestor node: the parent node remove n from the parent node if the parent node has no children, remove the parent node too end of remove non-existing node check if the excluded node a leaf calculate the total number of excluded leaf nodes children are leaves excluded node is a leaf node excluded node is one of the children so adjust the leaf index range check not the excludednode the leaf is in the child subtree go to the next child it is the excluedednode skip it and set the excludednode to be null end of innernode the number of nodes in both scope & excludednodes print the number of racks print the number of leaves print nodes scan the array to find the local node & local rack node local node swap the local node and the node at position 0 local rack swap the local rack node and the node at position tempindex put a random node at position 0 if it is not a local/local-rack node"
org.apache.hadoop.net.Node "the interface defines a node in a network topology. a node may be a leave representing a data node or an inner node representing a datacenter or rack. each data has a name and its location in the network is decided by a string with syntax similar to a file name. for example, a data node's name is hostname:port# and if it's located at rack "orange" in datacenter "dog", the string representation of its network location is /dog/orange string representation of this node's network location set this node's network location the location node's name node's parent set this node's parent the parent node's level in the tree. e.g. the root of a tree returns 0 and its children return 1 set this node's level in the tree the level www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.NodeBase "a base class that implements interface node path separator {@value} path separator as a string {@value} string representation of root {@value} default constructor construct a node from its path a concatenation of this node's location, the path seperator, and its name construct a node from its name and its location this node's name (can be null, must not contain {@link #path_separator}) this node's location construct a node from its name and its location this node's name (can be null, must not contain {@link #path_separator}) this node's location this node's parent node this node's level in the tree set this node's name and location the (nullable) name -which cannot contain the {@link #path_separator} the location node's name node's network location set this node's network location the location get the path of a node a non-null node path of a node node's path as its string representation normalize a path by stripping off any trailing {@link #path_separator} path to normalize. normalised path if pathis null or empty {@link #root} is returned @throws illegalargumentexception if the first character of a non empty path is not {@link #path_separator} node's parent set this node's parent the parent node's level in the tree. e.g. the root of a tree returns 0 and its children return 1 set this node's level in the tree the level www.apache.org/licenses/license-2.0 host:port# string representation of this node's location which level of the tree the node resides its parent"
org.apache.hadoop.net.ScriptBasedMapping "this class implements the {@link dnstoswitchmapping} interface using a script configured via the {@link commonconfigurationkeys#net_topology_script_file_name_key} option.  it contains a static class rawscriptbasedmapping that performs the work: reading the configuration parameters, executing any defined script, handling errors and such like. the outer class extends {@link cacheddnstoswitchmapping} to cache the delegated queries.  this dns mapper's {@link #issingleswitch()} predicate returns true if and only if a script is defined. minimum number of arguments: {@value} default number of arguments: {@value} key to the script filename {@value} key to the argument count that the script supports {@value} text used in the {@link #tostring()} method if there is no string {@value} create an instance with the default configuration.  calling {@link #setconf(configuration)} will trigger a re-evaluation of the configuration settings and so be used to set up the mapping script. create an instance from the given configuration configuration get the cached mapping and convert it to its real type inner raw script mapping. {@inheritdoc}  this will get called in the superclass constructor, so a check is needed to ensure that the raw mapping is defined before trying to relaying a null configuration. this is the uncached script mapping that is fed into the cache managed by the superclass {@link cacheddnstoswitchmapping} set the configuration and extract the configuration parameters of interest the new configuration constructor. the mapping is not ready to use until {@link #setconf(configuration)} has been called build and execute the resolution command. the command is executed in the directory specified by the system property " declare that the mapper is single-switched if a script was not named in the configuration. iff there is no script www.apache.org/licenses/license-2.0 max hostnames per call of the script invalid number of entries returned by the script an error occurred. return null to signify this. (exn was already logged in runresolvecommand)"
org.apache.hadoop.net.SocketInputStream "this implements an input stream that can have a timeout while reading. this sets non-blocking flag on the socket channel. so after create this object, read() on {@link socket#getinputstream()} and write() on {@link socket#getoutputstream()} for the associated socket will throw illegalblockingmodeexception. please use {@link socketoutputstream} for writing. create a new input stream with the given timeout. if the timeout is zero, it will be treated as infinite timeout. the socket's channel will be configured to be non-blocking. channel for reading, should also be a {@link selectablechannel}. the channel will be configured to be non-blocking. timeout in milliseconds. must not be negative. @throws ioexception same as socketinputstream(socket.getchannel(), timeout):  create a new input stream with the given timeout. if the timeout is zero, it will be treated as infinite timeout. the socket's channel will be configured to be non-blocking. @see socketinputstream#socketinputstream(readablebytechannel, long) should have a channel associated with it. timeout timeout in milliseconds. must not be negative. @throws ioexception same as socketinputstream(socket.getchannel(), socket.getsotimeout()) : create a new input stream with the given timeout. if the timeout is zero, it will be treated as infinite timeout. the socket's channel will be configured to be non-blocking. @see socketinputstream#socketinputstream(readablebytechannel, long) should have a channel associated with it. @throws ioexception allocation can be removed if required. probably no need to optimize or encourage single byte read. close the channel since socket.getinputstream().close() closes the socket. returns underlying channel used by inputstream. this is useful in certain cases like channel for {@link filechannel#transferfrom(readablebytechannel, long, long)}. waits for the underlying channel to be ready for reading. the timeout specified for this stream applies to this wait. @throws sockettimeoutexception if select on the channel times out. @throws ioexception if any other i/o error occurs. www.apache.org/licenses/license-2.0 unexpected readablebytechannel interface"
org.apache.hadoop.net.SocketInputWrapper "a wrapper stream around a socket which allows setting of its timeout. if the socket has a channel, this uses non-blocking io via the package-private {@link socketinputstream} implementation. otherwise, timeouts are managed by setting the underlying socket timeout itself. set the timeout for reads from this stream. note: the behavior here can differ subtly depending on whether the underlying socket has an associated channel. in particular, if there is no channel, then this call will affect the socket timeout for all readers of this socket. if there is a channel, then this call will affect the timeout only for this stream. as such, it is recommended to only create one {@link socketinputwrapper} instance per socket. the new timeout, 0 for no timeout @throws socketexception if the timeout cannot be set underlying readablebytechannel implementation. @throws illegalstateexception if this socket does not have a channel www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.SocketIOWithTimeout "this supports input and output streams for a socket channels. these streams can have a timeout. a timeout value of 0 implies wait for ever. we should have a value of timeout that implies zero wait.. i.e. read or write returns immediately. this will set channel to non-blocking. utility function to check if channel is ok. mainly to throw ioexception instead of runtime exception in case of mismatch. this mismatch can occur for many runtime reasons. most common reason is that original socket does not have a channel. so making this an ioexception rather than a runtimeexception. performs actual io operations. this is not expected to block. of bytes (or some equivalent). 0 implies underlying channel is drained completely. we will wait if more io is required. @throws ioexception performs one io and returns number of bytes read or written. it waits up to the specified timeout. if the channel is not read before the timeout, sockettimeoutexception is thrown. buffer for io selection ops used for waiting. suggested values: selectionkey.op_read while reading and selectionkey.op_write while writing. of bytes read or written. negative implies end of stream. @throws ioexception for now only one thread is allowed. if the contract is similar to {@link socketchannel#connect(socketaddress)} with a timeout. @see socketchannel#connect(socketaddress) - this should be a {@link selectablechannel} @throws ioexception this is similar to {@link #doio(bytebuffer, int)} except that it does not perform any i/o. it just waits for the channel to be ready for i/o as specified in ops. selection ops used for waiting @throws sockettimeoutexception if select on the channel times out. @throws ioexception if any other i/o error occurs. this maintains a pool of selectors. these selectors are closed once they are idle (unused) for a few seconds. waits on the channel with the given timeout using one of the cached selectors. it also removes any cached selectors that are idle for a few seconds. @return @throws ioexception sometimes select() returns 0 much before timeout for unknown reasons. so select again if required. takes one selector from end of lru list of free selectors. if there are no selectors awailable, it creates a new selector. also invokes trimidleselectors(). @throws ioexception puts selector back at the end of lru list of free selectos. also invokes trimidleselectors(). closes selectors that are idle for idle_timeout (10 sec). it does not traverse the whole list, just over the one that have crossed the timeout. www.apache.org/licenses/license-2.0 this is intentionally package private. set non-blocking or should we just return 0? successful io or an error. now wait for socket to be ready. unexpected ioexception. otherwise the socket should be ready for io. does not reach here. we might have to call finishconnect() more than once for some channels (with javadoc for socketchannel.connect() says channel should be closed. lifo 10 seconds. clear the canceled key. don't put the selector back. pick the list : rarely there is more than one provider in use. log.info("creating new providerinfo : " + provider.tostring());"
org.apache.hadoop.net.SocketOutputStream "this implements an output stream that can have a timeout while writing. this sets non-blocking flag on the socket channel. so after creating this object , read() on {@link socket#getinputstream()} and write() on {@link socket#getoutputstream()} on the associated socket will throw llegalblockingmodeexception. please use {@link socketinputstream} for reading. create a new ouput stream with the given timeout. if the timeout is zero, it will be treated as infinite timeout. the socket's channel will be configured to be non-blocking. channel for writing, should also be a {@link selectablechannel}. the channel will be configured to be non-blocking. timeout in milliseconds. must not be negative. @throws ioexception same as socketoutputstream(socket.getchannel(), timeout): create a new ouput stream with the given timeout. if the timeout is zero, it will be treated as infinite timeout. the socket's channel will be configured to be non-blocking. @see socketoutputstream#socketoutputstream(writablebytechannel, long) should have a channel associated with it. timeout timeout in milliseconds. must not be negative. @throws ioexception if we need to, we can optimize this allocation. probably no need to optimize or encourage single byte writes. unlike read, write can not inform close the channel since socket.getouputstream().close() closes the socket. returns underlying channel used by this stream. this is useful in certain cases like channel for {@link filechannel#transferto(long, long, writablebytechannel)} waits for the underlying channel to be ready for writing. the timeout specified for this stream applies to this wait. @throws sockettimeoutexception if select on the channel times out. @throws ioexception if any other i/o error occurs. transfers data from filechannel using {@link filechannel#transferto(long, long, writablebytechannel)}. updates waitforwritabletime and transfertotime with the time spent blocked on the network and the time spent transferring data from disk to network respectively. similar to readfully(), this waits till requested amount of data is transfered. filechannel to transfer data from. position within the channel where the transfer begins number of bytes to transfer. nanoseconds spent waiting for the socket to become writable nanoseconds spent transferring data @throws eofexception if end of input file is reached before requested number of bytes are transfered. @throws sockettimeoutexception if this channel blocks transfer longer than timeout for this stream. @throws ioexception includes any exception thrown by {@link filechannel#transferto(long, long, writablebytechannel)}. ideally we should wait after transferto returns 0. but because of a bug in jre on linux (http://bugs.sun.com/view_bug.do?bug_id=5103988), which throws an exception instead of returning 0, we wait for the channel to be writable before writing to it. if you ever see ioexception with message "resource temporarily unavailable" thrown here, please let us know. once we move to java se 7, wait should be moved to correct place. call {@link #transfertofully(filechannel, long, int, mutablerate, mutablerate)} with null waitforwritabletime and transfertotime www.apache.org/licenses/license-2.0 writablebytechannle interface bugs.sun.com/view_bug.do?bug_id=5103988), check if end of file is reached. otherwise assume the socket is full. waitforwritable(); // see comment above."
org.apache.hadoop.net.SocksSocketFactory "specialized socketfactory to create sockets with a socks proxy default empty constructor (for use with the reflection api). constructor with a supplied proxy the proxy to use to create sockets set the proxy of this socket factory as described in the string parameter the proxy address using the format "host:port" www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.StandardSocketFactory "specialized socketfactory to create sockets with a socks proxy default empty constructor (for use with the reflection api). note: this returns an nio socket so that it has an associated socketchannel. as of now, this unfortunately makes streams returned by socket.getinputstream() and socket.getoutputstream() unusable (because a blocking read on input stream blocks write on output stream and vice versa). so www.apache.org/licenses/license-2.0"
org.apache.hadoop.net.TableMapping " simple {@link dnstoswitchmapping} implementation that reads a 2 column text file. the columns are separated by whitespace. the first column is a dns or ip address and the second column specifies the rack where the address maps.   this class uses the configuration parameter {@code net.topology.table.file.name} to locate the mapping file.   calls to {@link #resolve(list)} will look up the address as defined in the mapping file. if no entry corresponding to the address is found, the value {@code /default-rack} is returned.  www.apache.org/licenses/license-2.0"
org.apache.hadoop.package-info "generated by src/saveversion.sh svn.apache.org/repos/asf/hadoop/common/branches/branch-2.0.3-alpha/hadoop-common-project/hadoop-common","
org.apache.hadoop.record.BinaryRecordInput "@deprecated replaced by avro. get a thread-local record input for the supplied datainput. data input stream record input corresponding to the supplied datainput. creates a new instance of binaryrecordinput creates a new instance of binaryrecordinput www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. no-op no-op no-op no-op"
org.apache.hadoop.record.BinaryRecordOutput "@deprecated replaced by avro. get a thread-local record output for the supplied dataoutput. data output stream record output corresponding to the supplied dataoutput. creates a new instance of binaryrecordoutput creates a new instance of binaryrecordoutput www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.Buffer "a byte sequence that is used as a java native type for buffer. it is resizable and distinguishes between the count of the seqeunce and the current capacity. @deprecated replaced by avro. number of valid bytes in this.bytes. backing store for buffer. create a zero-count sequence. create a buffer using the byte array as the initial value. this array becomes the backing storage for the object. create a buffer using the byte range as the initial value. copy of this array becomes the backing storage for the object. offset into byte array length of data use the specified bytes array as underlying sequence. byte sequence copy the specified byte array to the buffer. replaces the current buffer. byte array to be assigned offset into byte array length of data get the data from the buffer. data is only valid between 0 and getcount() - 1. get the current count of the buffer. get the capacity, which is the maximum count that could handled without resizing the backing storage. number of bytes change the capacity of the backing storage. the data is preserved if newcapacity >= getcount(). the new capacity in bytes. reset the buffer to 0 size change the capacity of the backing store to be the same as the current count of buffer. append specified bytes to the buffer. byte array to be appended offset into byte array length of data append specified bytes to the buffer byte array to be appended define the sort order of the buffer. the other buffer if this is bigger than other, 0 if they are equal, and negative if this is smaller than other. convert the byte buffer to a string an specific character encoding valid java character set name www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. inherit javadoc inherit javadoc inheric javadoc inherit javadoc"
org.apache.hadoop.record.compiler.ant.RccTask "hadoop record compiler ant task  this task takes the given record definition files and compiles them into java or c++ files. it is then up to the creates a new instance of rcctask sets the output language option "java"/"c++" sets the record definition file attribute record definition file given multiple files (via fileset), set the error handling behavior true will throw build exception in case of failure (default) sets directory where output files will be generated output directory adds a fileset that can consist of one or more files set of record definition files invoke the hadoop record compiler on each record definition file www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.CGenerator "c code generator front-end for hadoop record i/o. generate c code. this method only creates the requested file(s) and spits-out file-level elements (such as include statements etc.) record-level code is generated by jrecord. for (iterator iter = rlist.iterator(); iter.hasnext();) { iter.next().gencppcode(hh, cc); } www.apache.org/licenses/license-2.0 "+name.touppercase().replace('.','_')+"__\n");"
org.apache.hadoop.record.compiler.CodeBuffer "a wrapper around stringbuffer that automatically does indentation @deprecated replaced by avro. creates a new instance of codebuffer www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.CodeGenerator "codegenerator is a factory and a base class for hadoop record i/o translators. different translators register creation methods with this factory. www.apache.org/licenses/license-2.0"
org.apache.hadoop.record.compiler.Consts "const definitions for record i/o compiler @deprecated replaced by avro. cannot create a new instance www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. prefix to use for variables in generated classes other vars used in generated classes"
org.apache.hadoop.record.compiler.CppGenerator "c++ code generator front-end for hadoop record i/o. generate c++ code. this method only creates the requested file(s) and spits-out file-level elements (such as include statements etc.) record-level code is generated by jrecord. www.apache.org/licenses/license-2.0 "+filename.touppercase().replace('.','_')+"__\n");"
org.apache.hadoop.record.compiler.generated.ParseException "generated by:javacc: do not edit this line. parseexception.java version 3.0 this exception is thrown when parse errors are encountered. you can explicitly create objects of this exception type by calling the method generateparseexception in the generated parser. you can modify this class to customize your error reporting mechanisms so long as you retain the public fields. @deprecated replaced by avro. this constructor is used by the method "generateparseexception" in the generated parser. calling this constructor generates a new object of this type with the fields "currenttoken", "expectedtokensequences", and "tokenimage" set. the boolean flag "specialconstructor" is also set to true to indicate that this constructor was used to create this object. this constructor calls its super class with the empty string to force the "tostring" method of parent class "throwable" to print the error message in the form: parseexception:  the following constructors are for use by you for whatever purpose you can think of. constructing the exception in this manner makes the exception behave in the normal way - i.e., as documented in the class "throwable". the fields "errortoken", "expectedtokensequences", and "tokenimage" do not contain relevant information. the javacc generated code does not use these constructors. this variable determines which constructor was used to create this object and thereby affects the semantics of the "getmessage" method (see below). this is the last token that has been consumed successfully. if this object has been each entry in this array is an array of integers. each array of integers represents a sequence of tokens (by their ordinal values) that is expected at this point of the parse. this is a reference to the "tokenimage" array of the generated parser within which the parse error occurred. this array is defined in the generated ...constants interface. this method has the standard behavior when this object has been the end of line string for this machine. used to convert raw characters to their escaped version when these raw version cannot be used as part of an ascii string literal. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.generated.Rcc "generated by:javacc: do not edit this line. rcc.java @deprecated replaced by avro. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.generated.RccConstants "generated by:javacc: do not edit this line. rccconstants.java @deprecated replaced by avro. \"", "\" www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. \"","
org.apache.hadoop.record.compiler.generated.RccTokenManager "generated by:javacc: do not edit this line. rcctokenmanager.java @deprecated replaced by avro. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.generated.SimpleCharStream "generated by:javacc: do not edit this line. simplecharstream.java version 4.0 an implementation of interface charstream, where the stream is assumed to contain only ascii characters (without unicode processing). @deprecated replaced by avro. method to adjust line and column numbers for the start of a token. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.generated.Token "generated by:javacc: do not edit this line. token.java version 3.0 describes the input token stream. @deprecated replaced by avro. an integer that describes the kind of this token. this numbering system is determined by javaccparser, and a table of these numbers is stored in the file ...constants.java. beginline and begincolumn describe the position of the first character of this token; endline and endcolumn describe the position of the last character of this token. the string image of the token. a reference to the next regular (non-special) token from the input stream. if this is the last token from the input stream, or if the token manager has not read tokens beyond this one, this field is set to null. this is true only if this token is also a regular token. otherwise, see below for a description of the contents of this field. this field is used to access special tokens that occur prior to this token, but after the immediately preceding regular (non-special) token. if there are no such special tokens, this field is set to null. when there are more than one such special token, this field refers to the last of these special tokens, which in turn refers to the next previous special token through its specialtoken field, and so on until the first special token (whose specialtoken field is null). the next fields of special tokens refer to other special tokens that immediately follow it (without an intervening regular token). if there is no such token, this field is null. returns the image. returns a new token object, by default. however, if you want, you can create and return subclass objects based on the value of ofkind. simply add the cases to the switch for all those special cases. for example, if you have a subclass of token called idtoken that you want to create if ofkind is id, simlpy add something like : case myparserconstants.id : return new idtoken(); to the following switch statement. then you can cast matchedtoken variable to the appropriate type and use it in your lexical actions. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.generated.TokenMgrError "generated by:javacc: do not edit this line. tokenmgrerror.java version 3.0 @deprecated replaced by avro. ordinals for various reasons why an error of this type can be thrown. lexical error occured. an attempt wass made to create a second instance of a static token manager. tried to change to an invalid lexical state. detected (and bailed out of) an infinite loop in the token manager. indicates the reason why the exception is thrown. it will have one of the above 4 values. replaces unprintable characters by their espaced (or unicode escaped) equivalents in the given string returns a detailed message for the error when it is thrown by the token manager to indicate a lexical error. parameters : eofseen : indicates if eof caused the lexicl error curlexstate : lexical state in which this error occured errorline : line number when the error occured errorcolumn : column number when the error occured errorafter : prefix that was seen before this error occured curchar : the offending character note: you can customize the lexical error message by modifying this method. you can also modify the body of this method to customize your error messages. for example, cases like loop_detected and invalid_lexical_state are not of end- constructors of various flavors follow. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JavaGenerator "java code generator front-end for hadoop record i/o. generate java code for records. this method is only a front-end to jrecord, since one file is generated for each record. possibly full pathname to the file included files (as jfile) list of records defined within this file output directory www.apache.org/licenses/license-2.0"
org.apache.hadoop.record.compiler.JBoolean "@deprecated replaced by avro. creates a new instance of jboolean www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. in binary format, boolean is written as byte. true = 1, false = 0 in binary format, boolean is written as byte. true = 1, false = 0"
org.apache.hadoop.record.compiler.JBuffer "code generator for "buffer" type. @deprecated replaced by avro. creates a new instance of jbuffer www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JByte "code generator for "byte" type. @deprecated replaced by avro. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JCompType "abstract base class for all the "compound" types such as ustring, buffer, vector, map, and record. @deprecated replaced by avro. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JDouble "@deprecated replaced by avro. creates a new instance of jdouble www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JField "a thin wrappper around record field. @deprecated replaced by avro. creates a new instance of jfield www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JFile "container for the hadoop record ddl. the main components of the file are filename, list of included files, and records defined in that file. @deprecated replaced by avro. possibly full name of the file ordered list of included files ordered list of records declared in this file creates a new instance of jfile possibly full pathname to the file included files (as jfile) list of records defined within this file strip the other pathname components and return the basename generate record code in given language. language should be all lowercase. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JFloat "@deprecated replaced by avro. creates a new instance of jfloat www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JInt "code generator for "int" type @deprecated replaced by avro. creates a new instance of jint www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JLong "code generator for "long" type @deprecated replaced by avro. creates a new instance of jlong www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JMap "@deprecated replaced by avro. creates a new instance of jmap www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JRecord "@deprecated replaced by avro. int ct = 0; for (iterator> i = fields.iterator(); i.hasnext();) { ct++; jfield jf = i.next(); javatype type = jf.gettype(); string name = jf.getname(); if (ct != 1) { cb.append("else "); } type.genrtifieldcondition(cb, name, ct); } if (ct != 0) { cb.append("else {\n"); cb.append("rtifilterfields[i] = 0;\n"); cb.append("}\n"); } creates a new instance of jrecord www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. ignore, if we'ev already set the type filter for this record we set the rti filter here for each typeinfo in the filter, we see if there's a similar one in the record. since we store typeinfos in arraylists, thsi search is o(n squared). we do it faster if we also store a map (of typeinfo to index), but since setuprtifields() is called only once when deserializing, we're sticking with the former, as the code is easier. we may already have done this\n"); create the pkg directory not a directory file generated by hadoop record compiler. do not edit.\n"); type information declarations static init for type information field definitions default constructor constructor getter/setter for type info set rtifilter for nested structs. to prevent setting up the type filter for the same struct more than once, we use a hash map to keep track of what we've set. setuprtifields() getters/setters for member variables serialize() deserializewithoutfilter() deserialize() if we're here, we need to read based on version info\n"); compareto() equals() clone() we set the rti filter here type info vars end record "+name+"\n"); end namespace "+ns[i]+"\n"); initialize type info vars setuptypeinfo() settypefilter() set rtifilter for nested structs. we may end up with multiple lines that do the same thing, if the same struct is nested in more than one field, but that's ok. settypefilter() setuprtifields() serialize() deserializewithoutfilter() deserialize() if we're here, we need to read based on version info\n"); operator < precompute signature"
org.apache.hadoop.record.compiler.JString "@deprecated replaced by avro. creates a new instance of jstring www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.compiler.JType "abstract base class for all types supported by hadoop record i/o. @deprecated replaced by avro. void genrtifieldcondition(codebuffer cb, string fname, int ct) { cb.append("if ((tinfo.fieldid.equals(\"" + fname + "\")) && (typeval ==" + " org.apache.hadoop.record.meta." + gettypeidbytestring() + ")) {\n"); cb.append("rtifilterfields[i] = " + ct + ";\n"); cb.append("}\n"); } void genrtinestedfieldcondition(codebuffer cb, string varname, int ct) { cb.append("if (" + varname + ".getelementtypeid().gettypeval() == " + "org.apache.hadoop.record.meta." + gettypeidbytestring() + ") {\n"); cb.append("rtifilterfields[i] = " + ct + ";\n"); cb.append("}\n"); } www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. points to typeid.riotype do nothing by default do nothing by default"
org.apache.hadoop.record.compiler.JVector "@deprecated replaced by avro. creates a new instance of jvector www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.CsvRecordInput "@deprecated replaced by avro. creates a new instance of csvrecordinput www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.CsvRecordOutput "@deprecated replaced by avro. creates a new instance of csvrecordoutput www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.Index "interface that acts as an iterator for deserializing maps. the deserializer returns an instance that the record uses to read vectors and maps. an example of usage is as follows:  index idx = startvector(...); while (!idx.done()) { .... // read element of a vector idx.incr(); }  @deprecated replaced by avro. www.apache.org/licenses/license-2.0 read element of a vector hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.meta.FieldTypeInfo "represents a type information for a field, which is made up of its id (name) and its type (a typeid object). @deprecated replaced by avro. construct a filedtypeinfo with the given field name and the type get the field's typeid object get the field's id (name) two fieldtypeinfos are equal if ach of their fields matches we use a basic hashcode implementation, since this class will likely not be used as a hashmap key www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. first check if fieldid matches now see if typeid matches first check if fieldid matches now see if typeid matches"
org.apache.hadoop.record.meta.MapTypeID "represents typeid for a map @deprecated replaced by avro. get the typeid of the map's key element get the typeid of the map's value element two map typeids are equal if their constituent elements have the same type we use a basic hashcode implementation, since this class will likely not be used as a hashmap key www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.meta.RecordTypeInfo "a record's type information object which can read/write itself. type information for a record comprises metadata about the record, as well as a collection of type information for each field in the record. @deprecated replaced by avro. create an empty recordtypeinfo object. create a recordtypeinfo object representing a record with the given name name of the record private constructor return the name of the record set the name of the record add a field. name of the field type id of the field return a collection of field type infos return the type info of a nested record. we only consider nesting to one level. name of the nested record serialize the type information for a record deserialize the type information for a record this class doesn't implement comparable as it's not meant to be used for anything besides de/serializing. so we always throw an exception. not implemented. always returns 0 if another recordtypeinfo is passed in. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. a recordtypeinfo is really just a wrapper around structtypeid a recordtypeinfo object is just a collection of typeinfo objects for each of its fields. private arraylist typeinfos = new arraylist(); we keep a hashmap of struct/record names and their type information, as we need it to set filters when reading nested structs. this map is used during deserialization. private map structrtis = new hashmap(); write out any header, version info, here read in any header, version info name"
org.apache.hadoop.record.meta.StructTypeID "represents typeid for a struct @deprecated replaced by avro. create a structtypeid based on the recordtypeinfo of some record return the structtypeid, if any, of the given field writes rest of the struct (excluding type value). as an optimization, this method is directly called by rti for the top level record so that we don't write out the byte indicating that this is a struct (since top level records are always structs). deserialize ourselves. called by rti. www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. walk through the list, searching. not the most efficient way, but this in intended to be used rarely, so we keep it simple. as an optimization, we can keep a hashmap of record name to its rti, for later. number of elements generic reader: reads the next typeinfo object from stream and returns it generic reader: reads the next typeid object from stream and returns it shouldn't be here"
org.apache.hadoop.record.meta.TypeID "represents typeid for basic types. @deprecated replaced by avro. constants representing the idl types we support constant classes for the basic types, so we can share them. create a typeid object get the type value. one of the constants in riotype. serialize the typeid object two base typeids are equal if they refer to the same type we use a basic hashcode implementation, since this class will likely not be used as a hashmap key www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. see 'effectve java' by joshua bloch"
org.apache.hadoop.record.meta.Utils "various utility functions for hadooop record i/o platform. @deprecated replaced by avro. cannot create a new instance of utils read/skip bytes from stream based on a type www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. read past each field in the struct shouldn't be here"
org.apache.hadoop.record.meta.VectorTypeID "represents typeid for vector. @deprecated replaced by avro. two vector typeids are equal if their constituent elements have the same type we use a basic hashcode implementation, since this class will likely not be used as a hashmap key www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.Record "abstract class that is extended by generated classes. @deprecated replaced by avro. serialize a record with tag (ususally field name) record output destination record tag (used only in tagged serialization e.g. xml) deserialize a record with a tag (usually field name) record input source record tag (used only in tagged serialization e.g. xml) serialize a record without a tag record output destination deserialize a record without a tag record input source www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. inheric javadoc inherit javadoc inherit javadoc inherit javadoc"
org.apache.hadoop.record.RecordComparator "a raw record comparator base class @deprecated replaced by avro. construct a raw {@link record} comparison implementation. register an optimized comparator for a {@link record} implementation. record classs for which a raw comparator is provided raw comparator instance for class c www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro. inheric javadoc"
org.apache.hadoop.record.RecordInput "interface that all the deserializers have to implement. @deprecated replaced by avro. read a byte from serialized record. used by tagged serialization formats (such as xml) read from serialized record. read a boolean from serialized record. used by tagged serialization formats (such as xml) read from serialized record. read an integer from serialized record. used by tagged serialization formats (such as xml) read from serialized record. read a long integer from serialized record. used by tagged serialization formats (such as xml) read from serialized record. read a single-precision float from serialized record. used by tagged serialization formats (such as xml) read from serialized record. read a double-precision number from serialized record. used by tagged serialization formats (such as xml) read from serialized record. read a utf-8 encoded string from serialized record. used by tagged serialization formats (such as xml) read from serialized record. read byte array from serialized record. used by tagged serialization formats (such as xml) read from serialized record. check the mark for start of the serialized record. used by tagged serialization formats (such as xml) check the mark for end of the serialized record. used by tagged serialization formats (such as xml) check the mark for start of the serialized vector. used by tagged serialization formats (such as xml) that is used to count the number of elements. check the mark for end of the serialized vector. used by tagged serialization formats (such as xml) check the mark for start of the serialized map. used by tagged serialization formats (such as xml) that is used to count the number of map entries. check the mark for end of the serialized map. used by tagged serialization formats (such as xml) www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.RecordOutput "interface that alll the serializers have to implement. @deprecated replaced by avro. write a byte to serialized record. byte to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization write a boolean to serialized record. boolean to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization write an integer to serialized record. integer to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization write a long integer to serialized record. long to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization write a single-precision float to serialized record. float to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization write a double precision floating point number to serialized record. double to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization write a unicode string to serialized record. string to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization write a buffer to serialized record. buffer to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization mark the start of a record to be serialized. record to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization mark the end of a serialized record. record to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization mark the start of a vector to be serialized. vector to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization mark the end of a serialized vector. vector to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization mark the start of a map to be serialized. map to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization mark the end of a serialized map. map to be serialized used by tagged serialization formats (such as xml) @throws ioexception indicates error in serialization www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.Utils "various utility functions for hadooop record i/o runtime. @deprecated replaced by avro. cannot create a new instance of utils @return @return @return @throws java.io.ioexception @return @return @throws java.io.ioexception @return @return converts a csv-serialized representation of buffer to a new buffer csv-serialized representation of buffer @throws java.io.ioexception buffer parse a float from a byte array. parse a double from a byte array. reads a zero-compressed encoded long from a byte array and returns it. byte array with decode long starting index @throws java.io.ioexception long reads a zero-compressed encoded integer from a byte array and returns it. byte array with the encoded integer start index @throws java.io.ioexception integer reads a zero-compressed encoded long from a stream and return it. input stream @throws java.io.ioexception long reads a zero-compressed encoded integer from a stream and returns it. input stream @throws java.io.ioexception integer get the encoded length if an integer is stored in a variable-length format encoded length serializes a long to a binary stream with zero-compressed encoding. for -112 avro. codepoints expand to 4 bytes max for the most commmon case, i.e. ascii, numchars = utf8len"
org.apache.hadoop.record.XmlRecordInput "xml deserializer. @deprecated replaced by avro. creates a new instance of xmlrecordinput www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.record.XmlRecordOutput "xml serializer. @deprecated replaced by avro. creates a new instance of xmlrecordoutput www.apache.org/licenses/license-2.0 hadoop.apache.org/avro/">avro."
org.apache.hadoop.security.AccessControlException "an exception class for access control related issues. default constructor is needed for unwrapping from {@link org.apache.hadoop.ipc.remoteexception}. constructs an {@link accesscontrolexception} with the specified detail message. the detail message. constructs a new exception with the specified cause and a detail message of (cause==null ? null : cause.tostring()) (which typically contains the class and detail message of cause). cause the cause (which is saved for later retrieval by the {@link #getcause()} method). (a null value is permitted, and indicates that the cause is nonexistent or unknown.) www.apache.org/licenses/license-2.0 required by {@link java.io.serializable}."
org.apache.hadoop.security.AnnotatedSecurityInfo "constructs securityinfo from annotations provided in protocol interface. www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.AuthenticationFilterInitializer "initializes hadoop-auth authenticationfilter which provides support for kerberos http spnego authentication.  it enables anonymous access, simple/speudo and kerberos http spnego authentication for hadoop jobtracker, namenode, datanodes and tasktrackers.  refer to the core-default.xml file, after the comment 'http authentication' for details on the configuration options. all related configuration properties have 'hadoop.http.authentication.' as prefix. initializes hadoop-auth authenticationfilter.  propagates to hadoop-auth authenticationfilter configuration all hadoop configuration properties prefixed with "hadoop.http.authentication." the filter container configuration for run-time parameters www.apache.org/licenses/license-2.0 setting the cookie path to root '/' so it is used for all resources. resolve _host into bind address"
org.apache.hadoop.security.authorize.AccessControlList "class representing a configured access control list. this constructor exists primarily for accesscontrollist to be writable. construct a new acl from a string representation of the same. the string is a a comma separated list of build acl from the given string, format of the string is checks whether acl string contains wildcard check this acl string for wildcard if acl string contains wildcard false otherwise add add group to the names of groups allowed for this service. the group name remove remove group from the names of groups allowed for this service. the group name get the names of get the names of cleanup list, remove empty strings, trim leading/trailing spaces clean this list add list to a set add list to this set add items of this list to the set returns descriptive way of returns the access control list as a string that can be used for building a new instance by sending it to the constructor of {@link accesscontrollist}. serializes the accesscontrollist object deserializes the accesscontrollist object returns comma-separated concatenated single string of the set ' returns comma-separated concatenated single string of the set 'groups' separated list of groups returns comma-separated concatenated single string of all strings of the given set set of strings to concatenate www.apache.org/licenses/license-2.0 register a ctor indicates an acl string that represents access to all set of set of groups which are granted access whether all"
org.apache.hadoop.security.authorize.AuthorizationException "an exception class for constructs a new exception with the specified cause and a detail message of (cause==null ? null : cause.tostring()) (which typically contains the class and detail message of cause). cause the cause (which is saved for later retrieval by the {@link #getcause()} method). (a null value is permitted, and indicates that the cause is nonexistent or unknown.) www.apache.org/licenses/license-2.0 do not provide the stack-trace do not provide the stack-trace do not provide the stack-trace do not provide the stack-trace"
org.apache.hadoop.security.authorize.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.authorize.PolicyProvider "{@link policyprovider} provides the {@link service} definitions to the security {@link policy} in effect for hadoop. configuration key for the {@link policyprovider} implementation. a default {@link policyprovider} without any defined services. get the {@link service} definitions from the {@link policyprovider}. {@link service} definitions www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.authorize.ProxyUsers "reread the conf and get new values for "hadoop.proxy refresh configuration returns configuration key for effective return configuration key for super  return true if the configuration specifies the special configuration value "", indicating that any group or host list is allowed to use this configuration. www.apache.org/licenses/license-2.0 list of groups and hosts per proxy load server side configuration; remove alle existing stuff get all the new keys for groups now hosts"
org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol "protocol which is used to refresh the version 1: initial version refresh the service-level www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.authorize.Service "an abstract definition of service as related to service level get the configuration key for the service. configuration key for the service get the protocol for the service {@link class} for the protocol www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.authorize.ServiceAuthorizationManager "an configuration key for controlling service-level  www.apache.org/licenses/license-2.0 get client principal key to verify (if available) get the system property 'hadoop.policy.file' make a copy of the original config, and load the policy file parse the config file flip to the newly parsed permissions package-protected for use in tests."
org.apache.hadoop.security.Credentials "a class that provides the facilities of reading and writing secret keys and tokens. create an empty credentials instance create a copy of the given credentials to copy returns the key bytes for the alias the alias for the key for this alias returns the token object for the alias the alias for the token for this alias add a token in the storage (in memory) the alias for the key the token object return all the tokens in the in-memory map of tokens in the in-memory map of keys in the in-memory map set the key for an alias the alias for the key the key bytes convenience method for reading a token storage file, and loading the tokens therein in the passed ugi @throws ioexception convenience method for reading a token storage file, and loading the tokens therein in the passed ugi @throws ioexception convenience method for reading a token storage file directly from a datainputstream stores all the keys to dataoutput @throws ioexception loads all the keys @throws ioexception copy all of the credentials from one credential object into another. existing secrets and tokens are overwritten. the credentials to copy copy all of the credentials from one credential object into another. existing secrets and tokens are not overwritten. the credentials to copy www.apache.org/licenses/license-2.0 write out tokens first now write out secret keys"
org.apache.hadoop.security.GroupMappingServiceProvider "an interface for the implementation of a get all various group memberships of a given refresh the cache of groups and caches the group www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.Groups "a get the group memberships of a given refresh all add groups to cache list of groups to add to cache class to hold the cached groups create and initialize group cache returns time of last cache update of last cache update get list of cached groups groups get the groups being used to map get the groups being used to map www.apache.org/licenses/license-2.0 return cached value if available if cache has a value and it hasn't expired create and cache"
org.apache.hadoop.security.HadoopKerberosName "this class implements parsing and handling of kerberos principal names. in particular, it splits them apart and translates them down into local operating system names. create a name from the full kerberos principal name. set the static configuration to get the rules.  important: this method does a nop if the rules have been set already. if there is a need to reset the rules, the {@link kerberosname#setrules(string)} method should be invoked directly. the new configuration @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.JniBasedUnixGroupsMapping "a jni-based implementation of {@link groupmappingserviceprovider} that invokes libc calls to get the group memberships of a given www.apache.org/licenses/license-2.0 does nothing in this provider of does nothing in this provider of"
org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback "www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping "a jni-based implementation of {@link groupmappingserviceprovider} that invokes libc calls to get the group memberships of a given gets unix groups and netgroups for the refresh the netgroup cache add a group to cache, only netgroups are cached list of group names to add to cache calls jni function to get www.apache.org/licenses/license-2.0 parent gets unix groups better safe than sorry (should never happen) unix group, not caching jni code does not expect '@' at the begining of the group name"
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback "www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.KerberosInfo "indicates kerberos related information to be used key for getting server's kerberos principal name from configuration www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.LdapGroupsMapping "an implementation of {@link groupmappingserviceprovider} which connects directly to an ldap server for determining group membership. this provider should be used only if it is necessary to map url of the ldap server should ssl be used to connect to the server file path to the location of the ssl keystore to use password for the keystore  password for the bind base distinguished name to use for searches any additional filters to apply when searching for any additional filters to apply when finding relevant groups ldap attribute to use for determining group membership ldap attribute to use for identifying a group's name returns list of groups for a caches groups, no need to do that for this provider adds groups to cache, no need to do that for this provider unused www.apache.org/licenses/license-2.0 search for the set up the initial environment for ldap connectivity set up ssl security, if necessary does nothing in this provider of does nothing in this provider of if there is no password file defined, we'll assume that we should do an anonymous bind"
org.apache.hadoop.security.NetgroupCache "class that caches the netgroups and inverts group-to- get netgroups for a given get the list of cached netgroups of cached groups returns true if a given netgroup is cached check if this group is cached if group is cached, false otherwise clear the cache add group to cache name of the group to add to cache list of www.apache.org/licenses/license-2.0 at the beginning to avoid race update add to at the end to avoid race"
org.apache.hadoop.security.proto.SecurityProtos "generated by the protocol buffer compiler. do not edit! source: security.proto required bytes identifier = 1; required bytes password = 2; required string kind = 3; required string service = 4; use tokenproto.newbuilder() to construct. required bytes identifier = 1; required bytes password = 2; required string kind = 3; required string service = 4; construct using org.apache.hadoop.security.proto.securityprotos.tokenproto.newbuilder() required bytes identifier = 1; required bytes password = 2; required string kind = 3; required string service = 4; @@protoc_insertion_point(builder_scope:hadoop.common.tokenproto) @@protoc_insertion_point(class_scope:hadoop.common.tokenproto) required string renewer = 1; use getdelegationtokenrequestproto.newbuilder() to construct. required string renewer = 1; construct using org.apache.hadoop.security.proto.securityprotos.getdelegationtokenrequestproto.newbuilder() required string renewer = 1; @@protoc_insertion_point(builder_scope:hadoop.common.getdelegationtokenrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.getdelegationtokenrequestproto) optional .hadoop.common.tokenproto token = 1; use getdelegationtokenresponseproto.newbuilder() to construct. optional .hadoop.common.tokenproto token = 1; construct using org.apache.hadoop.security.proto.securityprotos.getdelegationtokenresponseproto.newbuilder() optional .hadoop.common.tokenproto token = 1; @@protoc_insertion_point(builder_scope:hadoop.common.getdelegationtokenresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.getdelegationtokenresponseproto) required .hadoop.common.tokenproto token = 1; use renewdelegationtokenrequestproto.newbuilder() to construct. required .hadoop.common.tokenproto token = 1; construct using org.apache.hadoop.security.proto.securityprotos.renewdelegationtokenrequestproto.newbuilder() required .hadoop.common.tokenproto token = 1; @@protoc_insertion_point(builder_scope:hadoop.common.renewdelegationtokenrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.renewdelegationtokenrequestproto) required uint64 newexpirytime = 1; use renewdelegationtokenresponseproto.newbuilder() to construct. required uint64 newexpirytime = 1; construct using org.apache.hadoop.security.proto.securityprotos.renewdelegationtokenresponseproto.newbuilder() required uint64 newexpirytime = 1; @@protoc_insertion_point(builder_scope:hadoop.common.renewdelegationtokenresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.renewdelegationtokenresponseproto) required .hadoop.common.tokenproto token = 1; use canceldelegationtokenrequestproto.newbuilder() to construct. required .hadoop.common.tokenproto token = 1; construct using org.apache.hadoop.security.proto.securityprotos.canceldelegationtokenrequestproto.newbuilder() required .hadoop.common.tokenproto token = 1; @@protoc_insertion_point(builder_scope:hadoop.common.canceldelegationtokenrequestproto) @@protoc_insertion_point(class_scope:hadoop.common.canceldelegationtokenrequestproto) use canceldelegationtokenresponseproto.newbuilder() to construct. construct using org.apache.hadoop.security.proto.securityprotos.canceldelegationtokenresponseproto.newbuilder() @@protoc_insertion_point(builder_scope:hadoop.common.canceldelegationtokenresponseproto) @@protoc_insertion_point(class_scope:hadoop.common.canceldelegationtokenresponseproto) @@protoc_insertion_point(outer_class_scope)"
org.apache.hadoop.security.RefreshUserMappingsProtocol "protocol use version 1: initial version. refresh refresh super www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.SaslInputStream "a saslinputstream is composed of an inputstream and a saslserver (or saslclient) so that read() methods return data that are read in from the underlying inputstream but have been additionally processed by the saslserver (or saslclient) object. the saslserver (or saslclient) object must be fully initialized before being used by a saslinputstream. should we wrap the communication channel? data read from the underlying input stream before being processed by sasl buffer holding data that have been processed by sasl, but have not been read out read more data and get them processed  entry condition: ostart = ofinish  exit condition: ostart  return (ofinish-ostart) (we have this many bytes for you), 0 (no data now, but could have more later), or -1 (absolutely no more data) disposes of any system resources or security-sensitive information sasl might be using. @exception saslexception if a sasl error occurs. constructs a saslinputstream from an inputstream and a saslserver  note: if the specified inputstream or saslserver is null, a nullpointerexception may be thrown later when they are used. the inputstream to be processed an initialized saslserver object constructs a saslinputstream from an inputstream and a saslclient  note: if the specified inputstream or saslclient is null, a nullpointerexception may be thrown later when they are used. the inputstream to be processed an initialized saslclient object reads the next byte of data from this input stream. the value byte is returned as an int in the range 0 to 255. if no byte is available because the end of the stream has been reached, the value -1 is returned. this method blocks until input data is available, the end of the stream is detected, or an exception is thrown.  next byte of data, or -1 if the end of the stream is reached. @exception ioexception if an i/o error occurs. reads up to b.length bytes of data from this input stream into an array of bytes.  the read method of inputstream calls the read method of three arguments with the arguments b, 0, and b.length. the buffer into which the data is read. total number of bytes read into the buffer, or -1 is there is no more data because the end of the stream has been reached. @exception ioexception if an i/o error occurs. reads up to len bytes of data from this input stream into an array of bytes. this method blocks until some input is available. if the first argument is null, up to len bytes are read and discarded. the buffer into which the data is read. the start offset of the data. the maximum number of bytes read. total number of bytes read into the buffer, or -1 if there is no more data because the end of the stream has been reached. @exception ioexception if an i/o error occurs. skips n bytes of input from the bytes that can be read from this input stream without blocking.  fewer bytes than requested might be skipped. the actual number of bytes skipped is equal to n or the result of a call to {@link #available() available}, whichever is smaller. if n is less than zero, no bytes are skipped.  the actual number of bytes skipped is returned. the number of bytes to be skipped. actual number of bytes skipped. @exception ioexception if an i/o error occurs. returns the number of bytes that can be read from this input stream without blocking. the available method of inputstream returns 0. this method should be overridden by subclasses. number of bytes that can be read from this input stream without blocking. @exception ioexception if an i/o error occurs. closes this input stream and releases any system resources associated with the stream.  the close method of saslinputstream calls the close method of its underlying input stream. @exception ioexception if an i/o error occurs. tests if this input stream supports the mark and reset methods, which it does not. false, since this class does not support the mark and reset methods. www.apache.org/licenses/license-2.0 position of the next "new" byte position of the last "new" byte whether or not this stream is open using saslserver using saslclient we loop for new data as we are blocking we loop for new data as we are blocking"
org.apache.hadoop.security.SaslOutputStream "a sasloutputstream is composed of an outputstream and a saslserver (or saslclient) so that write() methods first process the data before writing them out to the underlying outputstream. the saslserver (or saslclient) object must be fully initialized before being used by a sasloutputstream. constructs a sasloutputstream from an outputstream and a saslserver  note: if the specified outputstream or saslserver is null, a nullpointerexception may be thrown later when they are used. the outputstream to be processed an initialized saslserver object constructs a sasloutputstream from an outputstream and a saslclient  note: if the specified outputstream or saslclient is null, a nullpointerexception may be thrown later when they are used. the outputstream to be processed an initialized saslclient object disposes of any system resources or security-sensitive information sasl might be using. @exception saslexception if a sasl error occurs. writes the specified byte to this output stream. the byte. @exception ioexception if an i/o error occurs. writes b.length bytes from the specified byte array to this output stream.  the write method of sasloutputstream calls the write method of three arguments with the three arguments b, 0, and b.length. the data. @exception nullpointerexception if b is null. @exception ioexception if an i/o error occurs. writes len bytes from the specified byte array starting at offset off to this output stream. the data. the start offset in the data. the number of bytes to write. @exception ioexception if an i/o error occurs. flushes this output stream @exception ioexception if an i/o error occurs. closes this output stream and releases any system resources associated with this stream. @exception ioexception if an i/o error occurs. www.apache.org/licenses/license-2.0 processed data ready to be written out buffer holding one byte of incoming data using saslserver using saslclient"
org.apache.hadoop.security.SaslPlainServer "www.apache.org/licenses/license-2.0 [ authz, authn, password ] authz = authn"
org.apache.hadoop.security.SaslRpcClient "a utility class that encapsulates sasl logic for rpc client create a saslrpcclient for an authentication method the requested authentication method token to use if needed by the authentication method do client side sasl authentication with server via the given inputstream and outputstream inputstream to use outputstream to use if connection is set up, or false if needs to switch to simple auth. @throws ioexception get a sasl wrapped inputstream. can be called only after saslconnect() has been called. the inputstream to wrap sasl wrapped inputstream @throws ioexception get a sasl wrapped outputstream. can be called only after saslconnect() has been called. the outputstream to wrap sasl wrapped outputstream @throws ioexception release resources used by wrapped saslclient www.apache.org/licenses/license-2.0 read status shouldn't happen ignore further exceptions during cleanup"
org.apache.hadoop.security.SaslRpcServer "a utility class for dealing with sasl on rpc server splitting fully qualified kerberos name into parts authentication method the code for this method. return the object represented by the code. return the sasl mechanism name read from in write to out callbackhandler for sasl digest-md5 mechanism callbackhandler for sasl gssapi kerberos mechanism www.apache.org/licenses/license-2.0 realm is ignored may throw exception"
org.apache.hadoop.security.SecurityInfo "interface used by rpc to get the security information for a given protocol. get the kerberosinfo for a given protocol. interface class configuration get the tokeninfo for a given protocol. interface class configuration object. instance www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.SecurityUtil "for use only by tests and initialization find the original tgt within the current subject's credentials. cross-realm tgt's of the form "krbtgt/two.com@one.com" may be present. tgt from the current subject @throws ioexception if tgt can't be found tgs must have the server principal of the form "krbtgt/foo@foo". or false check whether the server principal is the tgs's principal the original tgt (the ticket that is obtained when a kinit is done) or false convert kerberos principal name pattern to valid kerberos principal names. it replaces hostname pattern with hostname, which should be fully-qualified domain name. if hostname is null or "0.0.0.0", it uses dynamically looked-up fqdn of the current host instead. the kerberos principal name conf value to convert the fully-qualified domain name used for substitution kerberos principal name @throws ioexception if the client address cannot be determined convert kerberos principal name pattern to valid kerberos principal names. this method is similar to {@link #getserverprincipal(string, string)}, except 1) the reverse dns lookup from addr to hostname is done only when necessary, 2) param addr can't be null (no default behavior of using local hostname when addr is null). kerberos principal name pattern to convert inetaddress of the host used for substitution kerberos principal name @throws ioexception if the client address cannot be determined login as a principal specified in config. substitute $host in login as a principal specified in config. substitute $host in create the service name for a delegation token of the service is used if the uri lacks a port token service, or null if no get the host name from the principal name of format /host@realm. principal name of format as described above name if the the string conforms to the above format, else null test setup method to register additional providers. a list of high priority providers to use look up the kerberosinfo for a given protocol. it searches all known securityinfo providers. the protocol class to get the information for configuration object kerberosinfo or null if it has no kerberosinfo defined look up the tokeninfo for a given protocol. it searches all known securityinfo providers. the protocol class to get the information for. configuration object tokeninfo or null if it has no kerberosinfo defined decode the given token's service field into an inetaddress from which to obtain the service for the service set the given token's service to the format expected by the rpc client a delegation token the socket for the rpc connection construct the service key for a token inetsocketaddress of remote connection with a token "ip:port" or "host:port" depending on the value of hadoop.security.token.service.use_ip construct the service key for a token of remote connection with a token "ip:port" or "host:port" depending on the value of hadoop.security.token.service.use_ip perform the given action as the daemon's login perform the given action as the daemon's login perform the given action as the daemon's current open a (if need be) secure connection to a url in a secure environment that is using spnego to authenticate its urls. all namenode and secondary namenode urls that are protected via spnego should be accessed via this method. to authenticate via spnego. connection that has been authenticated via spnego @throws ioexception if unable to authenticate via spnego resolves a host subject to the security requirements determined by hadoop.security.token.service.use_ip. host or ip to resolve resolved host @throws unknownhostexception if the host doesn't exist uses standard java host resolution this an alternate resolver with important properties that the standard java resolver lacks: 1) the hostname is fully qualified. this avoids security issues if not all hosts in the cluster do not share the same search domains. it also prevents other hosts from performing unnecessary dns searches. in contrast, inetaddress simply returns the host as given. 2) the inetaddress is instantiated with an exact host and ip to prevent further unnecessary lookups. inetaddress may perform an unnecessary reverse lookup for an ip. 3) a call to gethostname() will always return the qualified hostname, or more importantly, the ip if instantiated with an ip. this avoids unnecessary dns timeouts if the host is not resolvable. 4) point 3 also ensures that if the host is re-resolved, ex. during a connection re-attempt, that a reverse lookup to host and forward lookup to ip is not performed since the reverse/forward mappings may not always return the same ip. if the client initiated a connection with an ip, then that ip is all that should ever be contacted. note: this resolver is only used if: hadoop.security.token.service.use_ip=false create an inetaddress with a fully qualified hostname of the given hostname. inetaddress does not qualify an incomplete hostname that is resolved via the domain search list. {@link inetaddress#getcanonicalhostname()} will fully qualify the hostname, but it always return the a record whereas the given hostname may be a cname. a hostname or ip address with the fully qualified hostname or ip @throws unknownhostexception if host does not exist www.apache.org/licenses/license-2.0 this will need to be replaced someday when there is a suitable replacement controls whether buildtokenservice will use an ip or host/ip as given by the token#tostring() prints service host has no ip address use ipv4 address as-is use ipv6 address as-is a rooted host ends with a dot, ex. "host." rooted hosts never use the search path, so only try an exact lookup the host contains a dot (domain), ex. "host.domain" try an exact host lookup, then fallback to search list it's a simple host with no dots, ex. "host" try the search list, then fallback to exact host unresolvable! inetaddress will use the search list unless the host is rooted with a trailing dot. the trailing dot will disable any use of the search path in a lower level resolver. see rfc 1535. can't leave the hostname as rooted or other parts of the system malfunction, ex. kerberos principals are lacking proper host equivalence for rooted/non-rooted hostnames ignore, caller will throw if necessary already qualified? implemented as a separate method to facilitate unit testing"
org.apache.hadoop.security.ShellBasedUnixGroupsMapping "a simple shell-based implementation of {@link groupmappingserviceprovider} that exec's the groups shell command to fetch the group memberships of a given returns list of groups for a caches groups, no need to do that for this provider adds groups to cache, no need to do that for this provider unused get the current www.apache.org/licenses/license-2.0 does nothing in this provider of does nothing in this provider of if we didn't get the group - just return empty list;"
org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping "a simple shell-based implementation of {@link groupmappingserviceprovider} that exec's the groups shell command to fetch the group memberships of a given get unix groups (parent) and netgroups for given refresh the netgroup cache add a group to cache, only netgroups are cached list of group names to add to cache gets calls shell to get www.apache.org/licenses/license-2.0 parent get unix groups better safe than sorry (should never happen) unix group, not caching returns a string similar to this: group ( , get rid of spaces, makes splitting much easier remove netgroup name at the beginning of the string split string into  get rid of everything before first and after last comma voila! got shell command does not expect '@' at the begining of the group name if we didn't get the group - just return empty list;"
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory "{@link keystoresfactory} implementation that reads the certificates from keystore files.  if the trust certificates keystore file changes, the {@link trustmanager} is refreshed with the new trust certificate entries (using a {@link reloadingx509trustmanager} trustmanager). default format of the keystore files. reload interval in milliseconds. resolves a property name to its client/server version if applicable.  note: this method is public for testing purposes. client/server mode. property name template. resolved property name. sets the configuration for the factory. the configuration for the factory. returns the configuration of the factory. configuration of the factory. initializes the keystores of the factory. if the keystores are to be used in client or server mode. @throws ioexception thrown if the keystores could not be initialized due to an io error. @throws generalsecurityexception thrown if the keystores could not be initialized due to a security error. releases any resources being used. returns the keymanagers for owned certificates. keymanagers for owned certificates. returns the trustmanagers for trusted certificates. trustmanagers for trusted certificates. www.apache.org/licenses/license-2.0 certificate store trust store"
org.apache.hadoop.security.ssl.KeyStoresFactory "interface that gives access to {@link keymanager} and {@link trustmanager} implementations. initializes the keystores of the factory. if the keystores are to be used in client or server mode. @throws ioexception thrown if the keystores could not be initialized due to an io error. @throws generalsecurityexception thrown if the keystores could not be initialized due to an security error. releases any resources being used. returns the keymanagers for owned certificates. keymanagers for owned certificates. returns the trustmanagers for trusted certificates. trustmanagers for trusted certificates. www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.ssl.ReloadingX509TrustManager "a {@link trustmanager} implementation that reloads its configuration when the truststore file on disk changes. creates a reloadable trustmanager. the trustmanager reloads itself if the underlying trustore file has changed. type of truststore file, typically 'jks'. local path to the truststore file. password of the truststore file. interval to check if the truststore file has changed, in milliseconds. @throws ioexception thrown if the truststore could not be initialized due to an io error. @throws generalsecurityexception thrown if the truststore could not be initialized due to a security error. starts the reloader thread. stops the reloader thread. returns the reload check interval. reload check interval, in milliseconds. www.apache.org/licenses/license-2.0 nop"
org.apache.hadoop.security.ssl.SSLFactory "factory that creates sslengine and sslsocketfactory instances using hadoop configuration information.  this sslfactory uses a {@link reloadingx509trustmanager} instance, which reloads public keys if the truststore file changes.  this factory is used to configure https in hadoop http based endpoints, both client and server. creates an sslfactory. sslfactory mode, client or server. hadoop configuration from where the sslfactory configuration will be read. initializes the factory. @throws generalsecurityexception thrown if an ssl initialization error happened. @throws ioexception thrown if an io error happened while reading the ssl configuration. releases any resources being used. returns the sslfactory keystoresfactory instance. sslfactory keystoresfactory instance. returns a configured sslengine. configured sslengine. @throws generalsecurityexception thrown if the ssl engine could not be initialized. @throws ioexception thrown if and io error occurred while loading the server keystore. returns a configured sslserversocketfactory. configured sslsocketfactory. @throws generalsecurityexception thrown if the sslsocketfactory could not be initialized. @throws ioexception thrown if and io error occurred while loading the server keystore. returns a configured sslsocketfactory. configured sslsocketfactory. @throws generalsecurityexception thrown if the sslsocketfactory could not be initialized. @throws ioexception thrown if and io error occurred while loading the server keystore. returns the hostname verifier it should be used in httpsurlconnections. hostname verifier. returns if client certificates are required or not. client certificates are required or not. if the given {@link httpurlconnection} is an {@link httpsurlconnection} configures the connection with the {@link sslsocketfactory} and {@link hostnameverifier} of this sslfactory, otherwise does nothing. the {@link httpurlconnection} instance to configure. configured {@link httpurlconnection} instance. @throws ioexception if an io error occurred. www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.ssl.SSLHostnameVerifier "$headurl$ $revision$ $date$ ==================================================================== licensed to the apache software foundation (asf) under one or more contributor license agreements. see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the "license"); you may not use this file except in compliance with the license. you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an "as is" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. ==================================================================== this software consists of voluntary contributions made by many individuals on behalf of the apache software foundation. for more information on the apache software foundation, please see . copied from the not-yet-commons-ssl project at http://juliusdavies.ca/commons-ssl/ this project is not yet in apache, but it is apache 2.0 licensed. interface for checking if a hostname matches the names stored inside the server's x.509 certificate. correctly implements javax.net.ssl.hostnameverifier, but that interface is not recommended. instead we added several check() methods that take sslsocket, or x509certificate, or ultimately (they all end up calling this one), string. (it's easier to supply junit with strings instead of mock sslsession objects!) our check() methods throw exceptions if the name is invalid, whereas javax.net.ssl.hostnameverifier just returns true/false.  we provide the hostnameverifier.default, hostnameverifier.strict, and hostnameverifier.allow_all implementations. we also provide the more specialized hostnameverifier.default_and_localhost, as well as hostnameverifier.strict_ie6. but feel free to define your own implementations!  inspired by sebastian hauer's original strictsslprotocolsocketfactory in the httpclient "contrib" repository. checks to see if the supplied hostname matches any of the supplied cns or "dns" subject-alts. most implementations only look at the first cn, and ignore any additional cns. most implementations do look at all of the "dns" subject-alts. the cns or subject-alts may contain wildcards according to rfc 2818. cn fields, in order, as extracted from the x.509 certificate. subject-alt fields of type 2 ("dns"), as extracted from the x.509 certificate. the array of hostnames to verify. @throws sslexception if verification failed. the default hostnameverifier works the same way as curl and firefox.  the hostname must match either the first cn, or any of the subject-alts. a wildcard can occur in the cn, and in any of the subject-alts.  the only difference between default and strict is that a wildcard (such as ".foo.com") with default matches all subdomains, including "a.b.foo.com". the default_and_localhost hostnameverifier works like the default one with one additional relaxation: a host of "localhost", "localhost.localdomain", "127.0.0.1", "::1" will always pass, no matter what is in the server's certificate. the strict hostnameverifier works the same way as java.net.url in sun java 1.4, sun java 5, sun java 6. it's also pretty close to ie6. this implementation appears to be compliant with rfc 2818 for dealing with wildcards.  the hostname must match either the first cn, or any of the subject-alts. a wildcard can occur in the cn, and in any of the subject-alts. the one divergence from ie6 is how we only check the first cn. ie6 allows a match against any of the cns present. we decided to follow in sun java 1.4's footsteps and only check the first cn.  a wildcard such as ".foo.com" matches only subdomains in the same level, for example "a.foo.com". it does not match deeper subdomains such as "a.b.foo.com". the strict_ie6 hostnameverifier works just like the strict one with one minor variation: the hostname can match against any of the cn's in the server's certificate, not just the first one. this behaviour is identical to ie6's behaviour. the allow_all hostnameverifier essentially turns hostname verification off. this implementation is a no-op, and never throws the sslexception. this contains a list of 2nd-level domains that aren't allowed to have wildcards when combined with country-codes. for example: [.co.uk].  the [.co.uk] problem is an interesting one. should we just hope that ca's would never foolishly allow such a certificate to happen? looks like we're the only implementation guarding against this. firefox, curl, sun java 1.4, 5, 6 don't bother with this check. the javax.net.ssl.hostnameverifier contract. 'hostname' we used to create our socket sslsession with the remote server if the host matched the one in the certificate. if you're looking at the 2 lines of code above because you're running into a problem, you probably have two options: #1. clean up the certificate chain that your server is presenting (e.g. edit "/etc/apache2/server.crt" or wherever it is your server's certificate chain is defined). or #2. upgrade to an ibm 1.5.x or greater jvm, or switch to a non-ibm jvm. counts the number of dots "." in a string. string to count dots from of dots sebastian hauer's original strictsslprotocolsocketfactory used getname() and had the following comment: parses a x.500 distinguished name for the value of the "common name" field. this is done a bit sloppy right now and should probably be done a bit more according to rfc 2253. i've noticed that tostring() seems to do a better job than getname() on these x500principal objects, so i'm hoping that addresses sebastian's concern. for example, getname() gives me this: 1.2.840.113549.1.9.1=#16166a756c6975736461766965734063756362632e636f6d whereas tostring() gives me this: emailaddress=juliusdavies@cucbc.com looks like tostring() even works with non-ascii domain names! i tested it with "&#x82b1;&#x5b50;.co.jp" and it worked fine. extracts the array of subjectalt dns names from an x509certificate. returns null if there aren't any.  note: java doesn't appear able to extract international characters from the subjectalts. it can only extract international characters from the cn field.  (or maybe the version of openssl i'm using to test isn't storing the international characters correctly in the subjectalts?). x509certificate of subjectalt dns names stored in the certificate. www.apache.org/licenses/license-2.0 www.apache.org/>. juliusdavies.ca/commons-ssl/ allow everything - so never blowup. just in case developer forgot to manually sort the array. :-) in our experience this only happens under ibm 1.4.x when spurious (unrelated) certificates show up in the server' chain. hopefully this will unearth the real problem: if ssl.getinputstream().available() didn't cause an exception, maybe at least now the session is available? if it's still null, probably a starthandshake() will unearth the real problem. okay, if we still haven't managed to cause an exception, might as well go for the npe. or maybe we're okay now? didn't trigger anything interesting? okay, just throw original. build up lists of allowed hosts for logging/debugging purposes. build the list of names we're going to check. our default and strict implementations of the hostnameverifier only use the first cn provided. all other cns are ignored. (firefox, wget, curl, sun java 1.4, 5, 6 all work this way). stringbuffer for building the error message. don't trim the cn, though! store cn in stringbuffer in case we need to report an error. the cn better have at least two dots if it wants wildcard action. it also can't be [.co.uk] or [.co.jp] or [.org.uk], etc... if we're in strict mode, then [.foo.com] is not allowed to match [a.b.foo.com] we only bother analyzing the characters after the final dot in the name. look for the '.' in the 3rd-last position: trim off the [.] and the [.xx]. and test against the sorted array of bad 2lds: should probably log.debug() this? if type is 2, then we've got a dnsname"
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier "get the www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager "cache of currently valid tokens, mapping from delegationtokenidentifier to delegationtokeninformation. protected by this object lock. sequence number to create delegationtokenidentifier. protected by this object lock. access to allkeys is protected by this object lock access to currentid is protected by this object lock. access to currentkey is protected by this object lock if the delegation token update thread holds this lock, it will not get interrupted. should be called before this object is used reset all data structures and mutable state. add a previously used master key to cache (when nn restarts), should be called before activate(). update the current master key this is called once by startthreads before tokenremoverthread is create a new currentkey with an estimated expiry date. update the current master key for generating delegation tokens it should be called only by tokenremoverthread. set final expiry date for retiring currentkey currentkey might have been removed by removeexpiredkeys(), if updatemasterkey() isn't called at expected interval. add it back to allkeys just in case. verifies that the given identifier and password are valid and match. token identifier. password in the token. @throws invalidtoken renew a delegation token. the token to renew the full principal name of the cancel a token by removing it from cache. of the canceled token @throws invalidtoken for invalid token @throws accesscontrolexception if the convert the byte[] to a secret key the byte[] to create the secret key from secret key class to encapsulate a token's renew date and password. returns renew date returns password remove expired delegation tokens from cache is secretmgr running if secret mgr is running www.apache.org/licenses/license-2.0 a safety check log must be invoked outside the lock on 'this' 5 seconds"
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector "look through tokens to find the first delegation token that matches the service and return it. www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.delegation.DelegationKey "key used for generating and verifying delegation tokens default constructore required for writable   www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.delegation.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.SecretManager "the server-side secret manager for each token type.  the type of the token identifier the token was invalid and the message explains why. create the password for the given identifier. identifier may be modified inside this method. the identifier to use new password retrieve the password for the given token identifier. should check the date or registry to make sure the token hasn't expired or been revoked. returns the relevant password. the identifier to validate password to use @throws invalidtoken the token was invalid create an empty token identifier. newly no-op if the secret manager is available for reading tokens, throw a standbyexception otherwise. @throws standbyexception if the secret manager is not available to read tokens the name of the hashing algorithm. the length of the random keys to use. a thread local store for the macs. key generator to use. generate a new random secret key. new key compute hmac of the identifier using the secret key and return the output as password the bytes of the identifier the secret key bytes of the generated password convert the byte[] to a secret key the byte[] to create a secret key from secret key www.apache.org/licenses/license-2.0 default to being available for read."
org.apache.hadoop.security.token.Token "the client-side form of the token. construct a token given a token identifier and a secret manager for the type of the token identifier. the token identifier the secret manager construct a token from the components. the token identifier the token's password the kind of token the service for this token default constructor clone a token. the token to clone get the token identifier's byte representation token identifier's byte representation get the token identifier object, or null if it could not be constructed (because the class could not be loaded, for example). token identifier, or null @throws ioexception get the token password/secret token password/secret get the token kind kind of the token set the token kind. this is only intended to be used by services that wrap another service's token, such as hftp wrapping hdfs. get the service on which the token is supposed to be used service name set the service on which the token is supposed to be used the service name generate a string with the url-quoted base64 encoded serialized form of the writable. the object to serialize encoded string @throws ioexception modify the writable to the value from the newvalue the object to read into the string with the url-safe base64 encoded bytes @throws ioexception encode this token as a url safe string encoded string @throws ioexception decode the given url safe string into this token. the encoded string @throws ioexception is this token managed so that it can be renewed or cancelled? , if it can be renewed and cancelled. renew this delegation token new expiration time @throws ioexception @throws interruptedexception cancel this delegation token @throws ioexception @throws interruptedexception a trivial renewer for token kinds that aren't managed. sub-classes need to implement getkind for their token kind. www.apache.org/licenses/license-2.0 if not the first, put a blank separator in if it is only one digit, add a leading 0. handle in the finally block define the kind for this renewer"
org.apache.hadoop.security.token.TokenIdentifier "an identifier that identifies a token, may contain public information about a token, including its kind (or type). get the token kind kind of the token get the ugi with the get the bytes for the token identifier bytes of the identifier www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.TokenInfo "indicates token related information to be used the type of tokenselector to be used www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.TokenRenewer "this is the interface for plugins that handle tokens. does this renewer handle this kind of token? the kind of the token if this renewer can renew it is the given token managed? only managed tokens may be renewed or cancelled. the token being checked if the token may be renewed or cancelled @throws ioexception renew the given token. new expiration time @throws ioexception @throws interruptedexception cancel the given token @throws ioexception @throws interruptedexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.token.TokenSelector "select token of type t from tokens for use with named service  t extends tokenidentifier www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.User "save the full and short name of the get the full name of the get the returns login object set the login object set the last login time. the number of milliseconds since the beginning of time get the time of the last login. number of milliseconds since the beginning of time. www.apache.org/licenses/license-2.0"
org.apache.hadoop.security.UserGroupInformation "percentage of the ticket window to use before we renew ticket. ugimetrics maintains ugi activity statistics and publishes them through the metrics interfaces. a login module that looks at the kerberos, unix, or windows principal and adds the corresponding metrics to track ugi activity are the static variables that depend on configuration initialized? the auth method to use server-side groups fetching service the configuration to use leave 10 minutes between relogin attempts. environment variable pointing to the token cache file a method to initialize the fields that depend on a configuration. must be called before usekerberos or groups is used. initialize ugi and related classes. the configuration to use set the configuration values for ugi. the configuration to use set the static configuration for ugi. in particular, set the security authentication mechanism and the group look up service. the configuration to use determine if information about the logged in return the os login module class name return the os principal class a jaas configuration that defines the login modules that we want to use for login. represents a javax.security configuration that is create a checks if logged in using kerberos if the subject logged via keytab or has a kerberos tgt return the current find the most appropriate create a get the currently logged in is this get the kerberos tgt spawn a thread to do periodic renewals of kerberos credentials log a re-login a re-login a re-login a log a did the login happen via keytab or false create a existing types of authentications' methods create a proxy get real this class is used for storing the groups for testing. it stores a local map that has the translation of create a ugi for testing hdfs and mapreduce the full create a proxy get the get the add a tokenidentifier to this ugi. the tokenidentifier has typically been authenticated by the rpc layer as belonging to the get the set of tokenidentifiers belonging to this ugi set of tokenidentifiers belonging to this ugi add a token to this ugi token to be added on successful add of new token add a named token to this ugi name of the token token to be added on successful add of new token obtain the collection of tokens associated with this obtain the tokens in credentials form associated with this add the given credentials to this get the group names for this return the sets the authentication method in the subject sets the authentication method in the subject get the authentication method from the subject in the subject, null if not present. get the authentication method from the real returns the authentication method of a ugi. if the authentication method is proxy, returns the authentication method of the real compare the subjects to see if they are equal to each other. return the hash of the subject. get the underlying subject from this ugi. subject that represents this run the given action as the run the given action as the a test method to print out the current www.apache.org/licenses/license-2.0 if we already have a if we are using kerberos, try it out if we don't have a kerberos if use the os if we found the give the configuration on how to translate kerberos names if we haven't set up testing groups, use the configuration to find it all non-static fields must be read-only caches that come from the subject. temporarily switch the thread's contextclassloader to match this class's classloader, so that we can properly load hadooploginmodule from the jaas libraries. if the hadoop_proxy_ is specified, create a proxy load the token storage file and put all of the tokens into the  cycle (hadoop-9212). if this is to become stable, should probably logout the currently logged in ugi if it's different spawn thread only if we have kerb credentials return if tgt is valid and is not going to expire soon. register most recent relogin attempt clear up the kerberos state. but the tokens are not cleared! as per the java kerberos login module code, only the kerberos credentials are cleared login and also update the subject field of this instance to have the new credentials (pass it to the logincontext constructor) register most recent relogin attempt clear up the kerberos state. but the tokens are not cleared! as per the java kerberos login module code, only the kerberos credentials are cleared login and also update the subject field of this instance to have the new credentials (pass it to the logincontext constructor) currently we support only one auth per method, but eventually a subtype is needed to differentiate, ex. if digest is token or ldap make sure that the testing object is setup add the make sure that the testing object is setup add the would be nice if action included a descriptive tostring()"
org.apache.hadoop.tools.GetGroupsBase "base class for the hdfs and mr implementations of tools which fetch and display the groups that create an instance of this tool using the given configuration. used exclusively for testing. the configuration to use. the printstream to write to, instead of system.out get the groups for the must be overridden by subclasses to get the address where the {@link get get a client of the {@link get www.apache.org/licenses/license-2.0"
org.apache.hadoop.tools.GetUserMappingsProtocol "protocol implemented by the name node and job tracker which maps version 1: initial version. get the groups which are mapped to the given www.apache.org/licenses/license-2.0"
org.apache.hadoop.tools.package-info "www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.AsyncDiskService "this class is a container of multiple thread pools, each for a volume, so that we can schedule async disk operations easily. examples of async disk operations are deletion of files. we can move the files to a "to_be_deleted" folder before asychronously deleting it, to make sure the caller can run it faster. create a asyncdiskservices with a set of volumes (specified by their root directories). the asyncdiskservices uses one threadpool per volume to do the async disk operations. the roots of the file system volumes. execute the task sometime in the future, using threadpools. gracefully start the shut down of all threadpools. wait for the termination of the thread pools. the number of milliseconds to wait true if all thread pools are terminated without time limit @throws interruptedexception shut down all threadpools immediately. www.apache.org/licenses/license-2.0 threadpool core pool size threadpool maximum pool size threadpool keep-alive time for threads over core pool size create one threadpool per volume this can reduce the number of running threads"
org.apache.hadoop.util.bloom.BloomFilter "implements a bloom filter, as defined by bloom in 1970.  the bloom filter is a data structure that was introduced in 1970 and that has been adopted by the networking research community in the past decade thanks to the bandwidth efficiencies that it offers for the transmission of set membership information between networked hosts. a sender encodes the information into a bit vector, the bloom filter, that is more compact than a conventional representation. computation and space costs for construction are linear in the number of elements. the receiver uses the filter to test whether various elements are members of the set. though the filter will occasionally return a false positive, it will never return a false negative. when creating the filter, the sender can choose its desired point in a trade-off between the false positive rate and the size.  originally the bit vector. default constructor - use with readfields constructor the vector size of this filter. the number of hash function to consider. type of the hashing function (see {@link org.apache.hadoop.util.hash.hash}). of the the bloomfilter of bytes needed to hold bit vector www.one-lab.org) www.apache.org/licenses/license-2.0 www.one-lab.org">european commission one-lab project 034819. portal.acm.org/citation.cfm?id=362692&dl=acm&coll=portal">space/time trade-offs in hash coding with allowable errors writable end class"
org.apache.hadoop.util.bloom.CountingBloomFilter "implements a counting bloom filter, as defined by fan et al. in a ton 2000 paper.  a counting bloom filter is an improvement to standard a bloom filter as it allows dynamic additions and deletions of set membership information. this is achieved through the use of a counting vector instead of a bit vector.  originally storage for the counting buckets we are using 4bit buckets, so each bucket can count to 15 default constructor - use with readfields constructor the vector size of this filter. the number of hash function to consider. type of the hashing function (see {@link org.apache.hadoop.util.hash.hash}). returns the number of 64 bit words it would take to hold vectorsize buckets removes a specified key from this counting bloom filter.  invariant: nothing happens if the specified key does not belong to this counter bloom filter. the key to remove. this method calculates an approximate count of the key, i.e. how many times the key was added to the filter. this allows the filter to be used as an approximate key -&gt; count map. note: due to the bucket size of this filter, inserting the same key more than 15 times will cause an overflow at all filter positions associated with this key, and it will significantly increase the error rate for this and other keys. for this reason the filter can only be used to store small count values 0 &lt;= n &lt;&lt; 15. key to be tested 0 if the key is not present. otherwise, a positive value v will be returned such that v == count with probability equal to the error rate of this filter, and v &gt; count otherwise. additionally, if the filter experienced an underflow as a result of {@link #delete(key)} operation, the return value may be lower than the count with the probability of the false negative rate of such filter. www.one-lab.org) www.apache.org/licenses/license-2.0 www.one-lab.org">european commission one-lab project 034819. portal.acm.org/citation.cfm?id=343571.343572">summary cache: a scalable wide-area web cache sharing protocol find the bucket div 16 (mod 16) 4 only increment if the count in the bucket is less than bucket_max_value increment by 1 find the bucket div 16 (mod 16) 4 only decrement if the count in the bucket is between 0 and bucket_max_value decrement by 1 find the bucket div 16 (mod 16) 4 find the bucket div 16 (mod 16) 4 div 16 (mod 16) 4 writable"
org.apache.hadoop.util.bloom.DynamicBloomFilter "implements a dynamic bloom filter, as defined in the infocom 2006 paper.  a dynamic bloom filter (dbf) makes use of a s m bit matrix but each of the s rows is a standard bloom filter. the creation process of a dbf is iterative. at the start, the dbf is a 1 m bit matrix, i.e., it is composed of a single standard bloom filter. it assumes that nr elements are recorded in the initial bit vector, where nr  (n is the cardinality of the set a to record in the filter).  as the size of a grows during the execution of the application, several keys must be inserted in the dbf. when inserting a key into the dbf, one must first get an active bloom filter in the matrix. a bloom filter is active when the number of recorded keys, nr, is strictly less than the current cardinality of a, n. if an active bloom filter is found, the key is inserted and nr is incremented by one. on the other hand, if there is no active bloom filter, a new one is threshold for the maximum number of key to record in a dynamic bloom filter row. the number of keys recorded in the current standard active bloom filter. the matrix of bloom filter. zero-args constructor for the serialization. constructor.  builds an empty dynamic bloom filter. the number of bits in the vector. the number of hash function to consider. type of the hashing function (see {@link org.apache.hadoop.util.hash.hash}). the threshold for the maximum number of keys to record in a dynamic bloom filter row. adds a new row to this dynamic bloom filter. returns the active standard bloom filter in this dynamic bloom filter. the active standard bloom filter. null otherwise. www.one-lab.org) www.apache.org/licenses/license-2.0 www.one-lab.org">european commission one-lab project 034819. www.cse.fau.edu/~jie/research/publications/publication_files/infocom2006.pdf">theory and network applications of dynamic bloom filters writable"
org.apache.hadoop.util.bloom.Filter "defines the general behavior of a filter.  a filter is a data structure which aims at offering a lossy summary of a set a. the key idea is to map entries of a (also called keys) into several positions in a vector through the use of several hash functions.  typically, a filter will be implemented as a bloom filter (or a bloom filter extension).  it must be extended in order to define the real behavior. @see key the general behavior of a key @see hashfunction a hash function the vector size of this filter. the hash function used to map a key to several positions in the vector. the number of hash function to consider. type of hashing function to use. constructor. the vector size of this filter. the number of hash functions to consider. type of the hashing function (see {@link hash}). adds a key to this filter. the key to add. determines wether a specified key belongs to this filter. the key to test. true if the specified key belongs to this filter. false otherwise. peforms a logical and between this filter and a specified filter.  invariant: the result is assigned to this filter. the filter to and with. peforms a logical or between this filter and a specified filter.  invariant: the result is assigned to this filter. the filter to or with. peforms a logical xor between this filter and a specified filter.  invariant: the result is assigned to this filter. the filter to xor with. performs a logical not on this filter.  the result is assigned to this filter. adds a list of keys to this filter. the list of keys. adds a collection of keys to this filter. the collection of keys. adds an array of keys to this filter. the array of keys. www.one-lab.org) www.apache.org/licenses/license-2.0 negative to accommodate for old format end add() end add() end add() writable interface old unversioned format end class"
org.apache.hadoop.util.bloom.HashFunction "implements a hash object that returns a certain number of hashed values. @see key the general behavior of a key being stored in a filter @see filter the general behavior of a filter the number of hashed values. the maximum highest returned value. hashing algorithm to use. constructor.  builds a hash function that must obey to a given maximum number of returned values and a highest value. the maximum highest returned value. the number of resulting hashed values. type of the hashing function (see {@link hash}). clears this hash function. a noop hashes a specified key into several integers. the specified key. array of hashed values. www.one-lab.org) www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.bloom.Key "the general behavior of a key that must be stored in a filter. @see filter the general behavior of a filter byte value of key the weight associated to this key.  invariant: if it is not specified, each instance of key will have a default weight of 1.0 default constructor - use with readfields constructor.  builds a key with a default weight. the byte value of this key. constructor.  builds a key with a specified weight. the value of this key. the weight associated to this key.  [] the value of this key. the weight associated to this key. increments the weight of this key with a specified value. the increment. increments the weight of this key by one. www.one-lab.org) www.apache.org/licenses/license-2.0 writable comparable"
org.apache.hadoop.util.bloom.RemoveScheme "defines the different remove scheme for retouched bloom filters.  originally random selection.  the idea is to randomly select a bit to reset. minimumfn selection.  the idea is to select the bit to reset that will generate the minimum number of false negative. maximumfp selection.  the idea is to select the bit to reset that will remove the maximum number of false positive. ratio selection.  the idea is to select the bit to reset that will, at the same time, remove the maximum number of false positve while minimizing the amount of false negative generated. www.one-lab.org) www.apache.org/licenses/license-2.0 www.one-lab.org">european commission one-lab project 034819."
org.apache.hadoop.util.bloom.RetouchedBloomFilter "implements a retouched bloom filter, as defined in the conext 2006 paper.  it allows the removal of selected false positives at the cost of introducing random false negatives, and with the benefit of eliminating some random false positives at the same time.  originally keylist vector (or elementlist vector, as defined in the paper) of false positives. keylist vector of keys recorded in the filter. ratio vector. default constructor - use with readfields constructor the vector size of this filter. the number of hash function to consider. type of the hashing function (see {@link org.apache.hadoop.util.hash.hash}). adds a false positive information to this retouched bloom filter.  invariant: if the false positive is null, nothing happens. the false positive key to add. adds a collection of false positive information to this retouched bloom filter. the collection of false positive. adds a list of false positive information to this retouched bloom filter. the list of false positive. adds an array of false positive information to this retouched bloom filter. the array of false positive. performs the selective clearing for a given key. the false positive key to remove from this retouched bloom filter. the selective clearing scheme to apply. chooses the bit position that minimizes the number of false negative generated. the different bit positions. position that minimizes the number of false negative generated. chooses the bit position that maximizes the number of false positive removed. the different bit positions. position that maximizes the number of false positive removed. chooses the bit position that minimizes the number of false negative generated while maximizing. the number of false positive removed. the different bit positions. position that minimizes the number of false negative generated while maximizing. clears a specified bit in the bit vector and keeps up-to-date the keylist vectors. the position of the bit to clear. removes a given key from this filer. the key to remove. the counting vector associated to the key. computes the ratio a/fp. creates and initialises the various vectors. www.one-lab.org) www.apache.org/licenses/license-2.0 www.one-lab.org">european commission one-lab project 034819. www-rp.lip6.fr/site_npa/site_rp/_publications/740-rbf_cameraready.pdf">retouched bloom filters: allowing networked applications to trade off selected false positives against false negatives update key list update false positive list update ratio update bit vector writable"
org.apache.hadoop.util.Daemon "a thread that has called {@link thread#setdaemon(boolean) } with true. provide a factory for named daemon threads, for use in executorservices constructors construct a daemon thread. construct a daemon thread. construct a daemon thread to be part of a specified thread group. www.apache.org/licenses/license-2.0 always a daemon"
org.apache.hadoop.util.DataChecksum "this class provides inteface and utilities for processing checksums for dfs data transfers. the checksum types type corresponding to the id. creates a datachecksum from header_len bytes from arr[offset]. of the type in the array or null in case of an error. this constructucts a datachecksum by reading header_len bytes from input stream in writes the checksum header to the output stream out. writes the current checksum to the stream. if reset is true, then resets the checksum. of bytes written. will be equal to getchecksumsize(); writes the current checksum to a buffer. if reset is true, then resets the checksum. of bytes written. will be equal to getchecksumsize(); compares the checksum located at buf[offset] with the current checksum. if the checksum matches and false otherwise. verify that the given checksums match the given data. the 'mark' of the bytebuffer parameters may be modified by this function,. but the position is maintained. the directbytebuffer pointing to the data to verify. the directbytebuffer pointing to a series of stored checksums the name of the file being read, for error-reporting the file position to which the start of 'data' corresponds @throws checksumexception if the checksums do not match implementation of chunked verification specifically on byte arrays. this is to avoid the copy when dealing with bytebuffers that have array backing. calculate checksums for the given data. the 'mark' of the bytebuffer parameters may be modified by this function, but the position is maintained. the directbytebuffer pointing to the data to checksum. the directbytebuffer into which checksums will be stored. enough space must be available in this buffer to put the checksums. implementation of chunked calculation specifically on byte arrays. this is to avoid the copy when dealing with bytebuffers that have array backing. this just provides a dummy implimentation for checksum class this is used when there is no checksum available or required for data www.apache.org/licenses/license-2.0 misc constants / 1 byte type and 4 byte len checksum types this cannot be used to create datachecksum this cannot be used to create datachecksum like readint(): writing in buffer just like dataoutput.writeint() accessors type byte, bytesperchecksum int checksum interface. just a wrapper around member summer. dummy interface"
org.apache.hadoop.util.DiskChecker "class that provides utility functions for checking disk problem the semantics of mkdirswithexistscheck method is different from the mkdirs method provided in the sun's java.io.file class in the following way: while creating the non-existent parent directories, this method checks for the existence of those directories if the mkdir fails at any point (since that directory might have just been create the directory if it doesn't exist and check that dir is readable, writable and executable @throws diskerrorexception create the directory or check permissions if it already exists. the semantics of mkdirswithexistsandpermissioncheck method is different from the mkdirs method provided in the sun's java.io.file class in the following way: while creating the non-existent parent directories, this method checks for the existence of those directories if the mkdir fails at any point (since that directory might have just been create the local directory if necessary, check permissions and also ensure it can be read from and written into. local filesystem directory permission @throws diskerrorexception @throws ioexception www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.ExitUtil "facilitates hooking process termination for tests and debugging. disable the use of system.exit for testing. if terminate has been called first exitexception thrown, null if none thrown yet reset the tracking of process termination. this is for use in unit tests where one test in the suite expects an exit but others do not. terminate the current process. note that terminate is the only method that should be used to terminate the daemon processes. exit code message used to create the exitexception @throws exitexception if system.exit is disabled for test purposes like {@link terminate(int, string)} but uses the given throwable to initialize the exitexception. throwable used to create the exitexception @throws exitexception if system.exit is disabled for test purposes like {@link terminate(int, string)} without a message. @throws exitexception if system.exit is disabled for test purposes www.apache.org/licenses/license-2.0 either we set this member or we actually called system#exit"
org.apache.hadoop.util.GenericOptionsParser "genericoptionsparser is a utility to parse command line arguments generic to the hadoop framework. genericoptionsparser recognizes several standarad command line arguments, enabling applications to easily specify a namenode, a jobtracker, additional configuration resources etc. generic options the supported generic options are:  -conf &lt;configuration file&gt; specify a configuration file -d &lt;property=value&gt; use value for given property -fs &lt;local|namenode:port&gt; specify a namenode -jt &lt;local|jobtracker:port&gt; specify a job tracker -files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster -libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath. -archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines.  the general command line syntax is:  bin/hadoop command [genericoptions] [commandoptions]  generic command line arguments might modify configuration  objects, given to constructors. the functionality is implemented using commons cli. examples:  $ bin/hadoop dfs -fs darwin:8020 -ls /data list /data directory in dfs with namenode darwin:8020 $ bin/hadoop dfs -d fs.default.name=darwin:8020 -ls /data list /data directory in dfs with namenode darwin:8020 $ bin/hadoop dfs -conf core-site.xml -conf hdfs-site.xml -ls /data list /data directory in dfs with multiple conf files specified. $ bin/hadoop job -d mapred.job.tracker=darwin:50020 -submit job.xml submit a job to job tracker darwin:50020 $ bin/hadoop job -jt darwin:50020 -submit job.xml submit a job to job tracker darwin:50020 $ bin/hadoop job -jt local -submit job.xml submit a job to local runner $ bin/hadoop jar -libjars testlib.jar -archives test.tgz -files file.txt inputjar args job submission with libjars, files and archives  @see tool @see toolrunner create an options parser with the given options to parse the args. the options the command line arguments @throws ioexception create an options parser to parse the args. the command line arguments @throws ioexception create a genericoptionsparser to parse only the generic hadoop arguments. the array of string arguments other than the generic arguments can be obtained by {@link #getremainingargs()}. the configuration to modify. command-line arguments. @throws ioexception create a genericoptionsparser to parse given options as well as generic hadoop options. the resulting commandline object can be obtained by {@link #getcommandline()}. the configuration to modify options built by the caller returns an array of strings containing only application-specific arguments. of strings containing the un-parsed arguments or empty array if commandline was not defined. get the modified configuration configuration that has the modified parameters. returns the commons-cli commandline object to process the parsed arguments. note: if the object is specify properties of each generic option modify configuration according if libjars are set in the conf, parse the libjars. urls @throws ioexception takes input as a comma separated list of files and verifies if they exist. it defaults for file:/// if the files specified do not have a scheme. it returns the paths uri converted defaulting to file:///. so an input of /home/ parse the print the usage message for generic command-line options supported. stream to print the usage message to. www.apache.org/licenses/license-2.0 file with security tokens setting libjars in client classpath tokensfile check if the local file exists / /. /home/ default to the local file system check if the file exists or not first check if the file exists in this file system we need to recreate this filesystem object to copy these files to the file system jobtracker is running on."
org.apache.hadoop.util.GenericsUtil "contains utility methods for dealing with java generics. returns the class object (of type class&lt;t&gt;) of the argument of type t.  the type of the argument the object to get it class class&lt;t&gt; converts the given list&lt;t&gt; to a an array of t[]. the class object of the items in the list the list to convert converts the given list&lt;t&gt; to a an array of t[]. the list to convert @throws arrayindexoutofboundsexception if the list is empty. use {@link #toarray(class, list)} if the list may be empty. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.hash.Hash "this class represents a common api for hashing functions. constant to denote invalid hash type. constant to denote {@link jenkinshash}. constant to denote {@link murmurhash}. this utility method converts string representation of hash function name to a symbolic constant. currently two function types are supported, "jenkins" and "murmur". hash function name of the predefined constants this utility method converts the name of the configured hash type to a symbolic constant. configuration of the predefined constants get a singleton instance of hash function of a given type. predefined hash type function instance, or null if type is invalid get a singleton instance of hash function of a type defined in the configuration. current configuration hash type, or null if type is invalid calculate a hash using all bytes from the input argument, and a seed of -1. input bytes value calculate a hash using all bytes from the input argument, and a provided seed value. input bytes seed value value calculate a hash using bytes from 0 to length, and the provided seed value input bytes length of the valid bytes to consider seed value value www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.hash.JenkinsHash "produces 32-bit hash for hash table lookup. lookup3.c, by bob jenkins, may 2006, public domain. you can use this free for any purpose. it's in the public domain. it has no warranty.  @see lookup3.c @see hash functions (and how this function compares to others such as crc, md?, etc @see has update on the dr. dobbs article taken from hashlittle() -- hash a variable-length key into a 32-bit value the key (the unaligned variable-length array of bytes) number of bytes to include in hash can be any integer value 32-bit value. every bit of the key affects every bit of the return value. two keys differing by one or two bits will have totally different hash values. the best hash table sizes are powers of 2. there is no need to do mod a prime (mod is sooo slow!). if you need less than 32 bits, use a bitmask. for example, if you need only 10 bits, do h = (h & hashmask(10)); in which case, the hash table should have hashsize(10) elements. if you are hashing n strings byte[][] k, do it like this: for (int i = 0, h = 0; i by bob jenkins, 2006. bob_jenkins@burtleburtle.net. you may use this code any way you wish, private, educational, or commercial. it's free. use for hash table lookup, or anything where one collision in 2^^32 is acceptable. do not use for cryptographic purposes. mix -- mix 3 32-bit values reversibly. this is reversible, so any information in (a,b,c) before mix() is still in (a,b,c) after mix(). if four pairs of (a,b,c) inputs are run through mix(), or through mix() in reverse, there are at least 32 bits of the output that are sometimes the same for one pair and different for another pair. this was tested for: - pairs that differed by one bit, by two bits, in any combination of top bits of (a,b,c), or in any combination of bottom bits of (a,b,c). - "differ" is defined as +, -, ^, or ~^. for + and -, i transformed the output delta to a gray code (a^(a>>1)) so a string of 1's (as is commonly produced by subtraction) look like a single 1-bit difference. - the base values were pseudorandom, all zero but one bit set, or all zero plus a counter that starts at zero. some k values for my "a-=c; a^=rot(c,k); c+=b;" arrangement that satisfy this are 4 6 8 16 19 4 9 15 3 18 27 15 14 9 3 7 17 3 well, "9 15 3 18 27 15" didn't quite get 32 bits diffing for "differ" defined as + with a one-bit base and a two-bit delta. i used http://burtleburtle.net/bob/hash/avalanche.html to choose the operations, constants, and arrangements of the variables. this does not achieve avalanche. there are input bits of (a,b,c) that fail to affect some output bits of (a,b,c), especially of a. the most thoroughly mixed value is c, but it doesn't really even achieve avalanche in c. this allows some parallelism. read-after-writes are good at doubling the number of bits affected, so the goal of mixing pulls in the opposite direction as the goal of parallelism. i did what i could. rotates seem to cost as much as shifts on every machine i could lay my hands on, and rotates are much kinder to the top and bottom bits, so i used rotates. #define mix(a,b,c) \ { \ a -= c; a ^= rot(c, 4); c += b; \ b -= a; b ^= rot(a, 6); a += c; \ c -= b; c ^= rot(b, 8); b += a; \ a -= c; a ^= rot(c,16); c += b; \ b -= a; b ^= rot(a,19); a += c; \ c -= b; c ^= rot(b, 4); b += a; \ } mix(a,b,c); final -- final mixing of 3 32-bit values (a,b,c) into c pairs of (a,b,c) values differing in only a few bits will usually produce values of c that look totally different. this was tested for - pairs that differed by one bit, by two bits, in any combination of top bits of (a,b,c), or in any combination of bottom bits of (a,b,c). - "differ" is defined as +, -, ^, or ~^. for + and -, i transformed the output delta to a gray code (a^(a>>1)) so a string of 1's (as is commonly produced by subtraction) look like a single 1-bit difference. - the base values were pseudorandom, all zero but one bit set, or all zero plus a counter that starts at zero. these constants passed: 14 11 25 16 4 14 24 12 14 25 16 4 14 24 and these came close: 4 8 15 26 3 22 24 10 8 15 26 3 22 24 11 8 15 26 3 22 24 #define final(a,b,c) \ { c ^= b; c -= rot(b,14); \ a ^= c; a -= rot(c,11); \ b ^= a; b -= rot(a,25); \ c ^= b; c -= rot(b,16); \ a ^= c; a -= rot(c,4); \ b ^= a; b -= rot(a,14); \ c ^= b; c -= rot(b,24); \ } compute the hash of the specified file name of file to compute hash of. @throws ioexception www.apache.org/licenses/license-2.0 burtleburtle.net/bob/c/lookup3.c">lookup3.c www.ddj.com/184410284">hash functions (and how this burtleburtle.net/bob/hash/doobs.html">has update on the we use longs because we don't have unsigned ints burtleburtle.net/bob/hash/avalanche.html to choose -------------------------------- last block: affect all 32 bits of (c) all the case statements fall through"
org.apache.hadoop.util.hash.MurmurHash "this is a very fast, non-cryptographic hash suitable for general hash-based lookup. see http://murmurhash.googlepages.com/ for more details. the c version of murmurhash 2.0 found at that site was ported to java by andrzej bialecki (ab at getopt org). www.apache.org/licenses/license-2.0 murmurhash.googlepages.com/ for more details. avoid calculating modulo"
org.apache.hadoop.util.HeapSort "an implementation of the core algorithm of heapsort. sort the given range of items using heap sort. {@inheritdoc} www.apache.org/licenses/license-2.0 build heap w/ reverse comparator, then write in-place from end"
org.apache.hadoop.util.HostsFileReader "www.apache.org/licenses/license-2.0 keeps track of which datanodes/tasktrackers are allowed to connect to the namenode/jobtracker. everything from now on is a comment might need to add canonical name switch the new hosts that are to be included switch the excluded hosts"
org.apache.hadoop.util.IdGenerator "generic id generator used for generating various types of number sequences. increment and then return the next value. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.IndexedSortable "interface for collections capable of being sorted by {@link indexedsorter} algorithms. compare items at the given addresses consistent with the semantics of {@link java.util.comparator#compare(object, object)}. swap items at the given addresses. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.IndexedSorter "interface for sort algorithms accepting {@link indexedsortable} items. a sort algorithm implementing this interface may only {@link indexedsortable#compare} and {@link indexedsortable#swap} items for a range of indices to effect a sort across that range. sort the items accessed through the given indexedsortable over the given range of logical indices. from the perspective of the sort algorithm, each index between l (inclusive) and r (exclusive) is an addressable entry. @see indexedsortable#compare @see indexedsortable#swap same as {@link #sort(indexedsortable,int,int)}, but indicate progress periodically. @see #sort(indexedsortable,int,int) www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.LineReader "a class that provides a line reader from an input stream. depending on the constructor used, lines will either be terminated by:  one of the following: '\n' (lf) , '\r' (cr), or '\r\n' (cr+lf). or, a custom byte sequence delimiter  in both cases, eof also terminates an otherwise unterminated line. create a line reader that reads from the given stream using the default buffer-size (64k). the input stream @throws ioexception create a line reader that reads from the given stream using the given buffer-size. the input stream size of the read buffer @throws ioexception create a line reader that reads from the given stream using the io.file.buffer.size specified in the given configuration. input stream configuration @throws ioexception create a line reader that reads from the given stream using the default buffer-size, and using a custom delimiter of array of bytes. the input stream the delimiter create a line reader that reads from the given stream using the given buffer-size, and using a custom delimiter of array of bytes. the input stream size of the read buffer the delimiter @throws ioexception create a line reader that reads from the given stream using the io.file.buffer.size specified in the given configuration, and using a custom delimiter of array of bytes. input stream configuration the delimiter @throws ioexception close the underlying stream. @throws ioexception read one line from the inputstream into the given text. the object to store the given line (without newline) the maximum number of bytes to store into str; the rest of the line is silently discarded. the maximum number of bytes to consume in this call. this is only a hint, because if the line cross this threshold, we allow it to happen. it can overshoot potentially by as much as one buffer length. number of bytes read including the (longest) newline found. @throws ioexception if the underlying stream throws read a line terminated by one of cr, lf, or crlf. we're reading data from in, but the head of the stream may be already buffered in buffer, so we have several cases: 1. no newline characters are in the buffer, so we need to copy everything and read another buffer from the stream. 2. an unambiguously terminated line is in buffer, so we just copy to str. 3. ambiguously terminated line is in buffer, i.e. buffer ends in cr. in this case we copy everything up to cr to str, but we also need to see what follows cr: if it's lf, then we need consume lf as well, so next call to readline will read from after that. we use a flag prevcharcr to signal if previous character was cr and, if it happens to be at the end of the buffer, delay consuming it until we have a chance to look at the char that follows. read a line terminated by a custom delimiter. we're reading data from inputstream, but the head of the stream may be already captured in the previous buffer, so we have several cases: 1. the buffer tail does not contain any character sequence which matches with the head of delimiter. we count it as a ambiguous byte count = 0 2. the buffer tail contains a x number of characters, that forms a sequence, which matches with the head of delimiter. we count ambiguous byte count = x // eg: a segment of input file is as follows " record 1792: i found this bug very interesting and i have completely read about it. record 1793: this bug can be solved easily record 1794: this ." delimiter = "record"; supposing:- string at the end of buffer = "i found this bug very interesting and i have completely re" there for next buffer = "ad about it. record 179 ...." the matching characters in the input buffer tail and delimiter head = "re" therefore, ambiguous byte count = 2 // 2.1 if the following bytes are the remaining characters of the delimiter, then we have to capture only up to the starting position of delimiter. that means, we need not include the ambiguous characters in str. 2.2 if the following bytes are not the remaining characters of the delimiter ( as mentioned in the example ), then we have to include the ambiguous characters in str. read from the inputstream into the given text. the object to store the given line the maximum number of bytes to store into str. number of bytes read including the newline @throws ioexception if the underlying stream throws read from the inputstream into the given text. the object to store the given line number of bytes read including the newline @throws ioexception if the underlying stream throws www.apache.org/licenses/license-2.0 the number of bytes of real data in the buffer the current position in the buffer the line delimiter tracks str.getlength(), as an optimization length of terminating newline true of prev char was cr starting from where we left off the last time account for cr from previous read eof search for newline at next invocation proceed from following byte cr + notlf, we are at notlf cr at the end of the buffer eg: a segment of input file is as follows  tracks str.getlength(), as an optimization to capture the ambiguous characters count start from previous end position eof appending the ambiguous characters (refer case 2.2) to be consumed in next"
org.apache.hadoop.util.MergeSort "an implementation of the core algorithm of mergesort. www.apache.org/licenses/license-2.0 reusable intwritables the comparator that the algo should use insertion sort on smallest arrays recursively sort halves of dest into src if list is already sorted, just copy from src to dest. this is an optimization that results in faster sorts for nearly ordered lists. merge sorted halves (now in src) into dest"
org.apache.hadoop.util.NativeCodeLoader "a helper to load the native hadoop code i.e. libhadoop.so. this handles the fallback to either the bundled libhadoop-linux-i386-32.so or the default java implementations where appropriate. check if native-hadoop code is loaded for this platform. true if native-hadoop is loaded, else false returns true only if this build was compiled with support for snappy. return if native hadoop libraries, if present, can be used for this job. configuration true if native hadoop libraries, if present, can be used for this job; false otherwise. set if native hadoop libraries, if present, can be used for this job. configuration can native hadoop libraries be loaded www.apache.org/licenses/license-2.0 try to load native hadoop library and set fallback flag appropriately ignore failure to load"
org.apache.hadoop.util.NativeCrc32 "wrapper around jni support code to do checksum computation natively. return true if the jni-based native crc extensions are available. verify the given buffers of data and checksums, and throw an exception if any checksum is invalid. the buffers given to this function should have their position initially at the start of the data, and their limit set at the end of the data. the position, limit, and mark are not modified. the chunk size (eg 512 bytes) the datachecksum type constant the directbytebuffer pointing at the beginning of the stored checksums the directbytebuffer pointing at the beginning of the data to check the position in the file where the data buffer starts the name of the file being verified @throws checksumexception if there is an invalid checksum www.apache.org/licenses/license-2.0 copy the constants over from datachecksum so that javah will pick them up and make them available in the native code header."
org.apache.hadoop.util.NativeLibraryChecker "a tool to test native library availability, www.apache.org/licenses/license-2.0 lz4 is linked within libhadoop return 1 to indicated check failed"
org.apache.hadoop.util.Options "this class allows generic access to variable length type-safe parameter lists. find the first option of the required class.  the static class to find  the parent class of the array the dynamic class to find the list of options to look through first option that matches @throws ioexception prepend some new options to the old options  the type of options the old options the new options new array of options www.apache.org/licenses/license-2.0 copy the new options to the front of the array now copy the old options"
org.apache.hadoop.util.PlatformName "a helper class for getting build-info of the java-vm. the complete platform 'name' to identify the platform as per the java-vm. get the complete platform as per the java-vm. the complete platform as per the java-vm. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.PrintJarMainClass "a micro-application that prints the main class name out of a jar file.  www.apache.org/licenses/license-2.0 ignore it"
org.apache.hadoop.util.PriorityQueue "a priorityqueue maintains a partial ordering of its elements such that the least element can always be found in constant time. put()'s and pop()'s require log(size) time. determines the ordering of objects in this priority queue. subclasses must define this one method. subclass constructors must call this. adds an object to a priorityqueue in log(size) time. if one tries to add more objects than maxsize from initialize a runtimeexception (arrayindexoutofbound) is thrown. adds element to the priorityqueue in log(size) time if either the priorityqueue is not full, or not lessthan(element, top()). if element is added, false otherwise. returns the least element of the priorityqueue in constant time. removes and returns the least element of the priorityqueue in log(size) time. should be called when the object at top changes values. still log(n) worst case, but it's at least twice as fast to  { pq.top().change(); pq.adjusttop(); }  instead of  { o = pq.pop(); o.change(); pq.push(o); }  returns the number of elements currently stored in the priorityqueue. removes all entries from the priorityqueue. www.apache.org/licenses/license-2.0 save first value move last to first permit gc of objects adjust heap save bottom node shift parents down install saved node save top node find smaller child shift up child install saved node"
org.apache.hadoop.util.ProgramDriver "a driver that is used to run programs added to it a description of a program based on its class and a human-readable description. @date april 2006 create a description of an example program. the class with the main for the example program a string to display to the invoke the example application with the given arguments the arguments for the application @throws throwable the exception thrown by the invoked method this is the method that adds the classed to the repository the name of the string you want the class instance to be called with the class that you want to add to the repository the description of the class @throws nosuchmethodexception @throws securityexception this is a driver for the example programs. it looks at the first command line argument and tries to find an example program with that name. if it is found, it calls the main method in that class with the rest of the command line arguments. the argument from the www.apache.org/licenses/license-2.0 make sure they gave us a program name. and that it is good. remove the leading argument and call main"
org.apache.hadoop.util.Progress "utility to assist with generation of progress reports. applications build a hierarchy of {@link progress} instances, each modelling a phase of execution. the root is constructed with {@link #progress()}. nodes for sub-phases are creates a new root node. adds a named node to the tree. adds a node to the tree. gives equal weightage to all phases adds a new phase. caller needs to set progress weightage adds a named node with a specified progress weightage to the tree. adds a node with a specified progress weightage to the tree. adds n nodes to the tree. gives equal weightage to all phases returns progress weightage of the given phase the phase number of the phase(child node) for which we need progress weightage the progress weightage of the specified phase called during execution to move to the next phase at this level in the tree. returns the current sub-node executing. completes this node, moving the parent node to its next child. called during execution on a leaf node to set its progress. returns the overall progress of the root. returns progress in this node. get() would give overall progress of the root node(not just given current node). computes progress in this node. www.apache.org/licenses/license-2.0 each phase can have different progress weightage. for example, in map task, map phase accounts for 66.7% and sort phase for 33.3%.  phases) in a progress object, if he wants to give weightage to any of the phases. so when nodes are added without specifying weightage, it means fixed weightage for all phases. set equal weightage for all phases ensure that the sum of weightages does not cross 1.0 set equal weightage for all phases all phases are of equal weightage we have to traverse up to our parent, so be careful about locking. this will synchronize on the parent, so we make sure we release our lock before getting the parent's, since we're traversing against the normal traversal direction used by get() or tostring(). we don't need transactional semantics, so we're ok doing this. this method probably does not need to be synchronized as getinternal() is synchronized and the node's parent never changes. still, it doesn't hurt. find the root same progress weightage for each phase progress weightages of phases could be different. add them"
org.apache.hadoop.util.Progressable "a facility for reporting progress. clients and/or applications can use the provided progressable to explicitly report progress to the hadoop framework. this is especially important for operations which take significant amount of time since, in-lieu of the reported progress, the framework has to assume that an error has occured and time-out the operation. report progress to the hadoop framework. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.ProtoUtil "read a variable length integer in the same format that protobufs encodes. the input stream to read from integer @throws ioexception if it is malformed or eof. this method creates the connection context using exactly the same logic as the old connection context as was done for writable where the effective and real in the connection context we send only additional www.apache.org/licenses/license-2.0 discard upper 32 bits. real send effective with token, the connection itself establishes both real and effective simple authentication no send both effective"
org.apache.hadoop.util.PureJavaCrc32 "a pure-java implementation of the crc32 checksum that uses the same polynomial as the built-in native crc32. this is to avoid the jni overhead for certain uses of checksumming where many small pieces of data are checksummed in succession. the current version is ~10x to 1.8x as fast as sun's native java.util.zip.crc32 in java 1.6 @see java.util.zip.crc32 the current crc value, bit-flipped create a new purejavacrc32 object. loop unroll - duff's device style nothing crc-32 lookup tables generated by the polynomial 0xedb88320. see also testpurejavacrc32.table. t8_0 t8_1 t8_2 t8_3 t8_4 t8_5 t8_6 t8_7 www.apache.org/licenses/license-2.0 publish crc out to object"
org.apache.hadoop.util.PureJavaCrc32C "a pure-java implementation of the crc32 checksum that uses the crc32-c polynomial, the same polynomial used by iscsi and implemented on many intel chipsets supporting sse4.2. the current crc value, bit-flipped create a new purejavacrc32 object. loop unroll - duff's device style nothing t8_0 t8_1 t8_2 t8_3 t8_4 t8_5 t8_6 t8_7 www.apache.org/licenses/license-2.0 publish crc out to object crc polynomial tables generated by: java -cp build/test/classes/:build/classes/ \ org.apache.hadoop.util.testpurejavacrc32\$table 82f63b78"
org.apache.hadoop.util.QuickSort "an implementation of the core algorithm of quicksort. deepest recursion before giving up and doing a heapsort. returns 2 ceil(log(n)). sort the given range of items using quick sort. {@inheritdoc} if the recursion depth falls below {@link #getmaxdepth}, then switch to {@link heapsort}. www.apache.org/licenses/license-2.0 give up select, move pivot into first position divide swap pivot- and all eq values- into position conquer recurse on smaller interval first to keep stack shallow"
org.apache.hadoop.util.ReflectionUtils "general reflection utils cache of constructors for each class. pins the classes so they can't be garbage collected until reflectionutils can be collected. check and set 'configuration' if necessary. object for which to set configuration configuration this code is to support backward compatibility and break the compile time dependency of core on mapred. this should be made deprecated along with the mapred package hadoop-1230. should be removed when mapred package is removed. create an object for the given class and initialize it from conf class of which an object is print all of the thread's information and stack traces. the stream to a string title for the stack trace log the current thread stacks at info level. the logger that logs the stack trace a descriptive title for the call stacks the minimum time from the last return the correctly-typed {@link class} of the given object. object whose correctly-typed class is to be obtained correctly typed class of the given object. a pair of input/output buffers that we use to clone writables. move the data from the output buffer to the input buffer. allocate a buffer for each thread that tries to clone objects. make a copy of the writable object using serialization to a buffer the object to copy from the object to copy into, which is destroyed @throws ioexception www.apache.org/licenses/license-2.0 if jobconf and jobconfigurable are in classpath, and theobject is of type jobconfigurable and conf is of type jobconf then invoke configure on theobject methods to support testing"
org.apache.hadoop.util.RunJar "run a hadoop job jar. pattern that matches any string priority of the runjar shutdown hook. unpack a jar file into a directory. this version unpacks all files inside the jar regardless of filename. unpack matching files from a jar. entries inside the jar that do not match the given pattern will be skipped. the .jar file to unpack the destination directory into which to unpack the jar the pattern to match jar entries against ensure the existence of a given directory. @throws ioexception if it cannot be run a hadoop job jar. if the main class is not in the jar's manifest, then it must be provided on the command line. www.apache.org/licenses/license-2.0 if "permission denied" message doesn't specify a filename."
org.apache.hadoop.util.ServicePlugin "service plug-in interface. service plug-ins may be used to expose functionality of datanodes or namenodes using arbitrary rpc protocols. plug-ins are instantiated by the service instance, and are notified of service life-cycle events using the methods defined by this class. service plug-ins are started after the service instance is started, and stopped before the service instance is stopped. this method is invoked when the service instance has been started. the service instance invoking this method this method is invoked when the service instance is about to be shut down. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.ServletUtil "initial html header get a parameter from a servletrequest. return null if the parameter contains only white spaces. long value as passed in the given parameter, throwing an exception if it is not present or if it is not a valid number. html footer to be added in the jsps. html footer. generate the percentage graph and returns html representation string of the same. the percentage value for which graph is to be generated the width of the display table string representation of the percentage graph @throws ioexception generate the percentage graph and returns html representation string of the same. the percentage value for which graph is to be generated the width of the display table string representation of the percentage graph @throws ioexception escape and encode a string regarded as within the query component of an uri. the value to encode query, null if the default charset is not supported escape and encode a string regarded as the path component of an uri. the path component to encode path, null if utf-8 is not supported parse and decode the path component from the given request. http request to parse the name of servlet that precedes the path path component, null if utf-8 is not supported parse the path component from the given request and return w/o decoding. http request to parse the name of servlet that precedes the path component, null if the default charset is not supported www.apache.org/licenses/license-2.0 hadoop.apache.org/core'>hadoop, " should never happen! should never happen! should never happen!"
org.apache.hadoop.util.Shell "a base class for running a unix command. shell can be used to run unix commands like du or df. it also offers facilities to gate commands by time-intervals. a unix command to get the current a unix command to get the current a unix command to get a given a unix command to get a given netgroup's a unix command to set permission a unix command to set owner a unix command to create a link a unix command to get a link target return a unix command to get permission information. time after which the executing script would be timedout if or not script timed out set to true on windows platforms borrowed from path.windows if or not script finished executing the minimum duration to wait before re-executing the command. set the environment for the command mapping of environment variables set the working directory the directory where the command would be executed check to see if a command needs to be executed and execute if needed run a command return an array containing the command name & its parameters parse the execution result get the current sub-process executing the given command executing the command get the exit code exit code of the process this is an ioexception with exit code added. a simple shell command executor. shellcommandexecutorshould be used in cases where the output of the command needs no explicit parsing and where the command, working directory and the environment remains unchanged. the output of the command is stored as-is and is expected to be small. create a new instance of the shellcommandexecutor to execute a command. the command to execute with arguments if not-null, specifies the directory which should be set as the current working directory for the command. if null, the current working directory is not modified. if not-null, environment of the command will include the key-value pairs specified in the map. if null, the current environment is not modified. specifies the time in milliseconds, after which the command will be killed and the status marked as timedout. if 0, the command will not be timed out. execute the shell command. get the output of the shell command. returns the commands of this instance. arguments with spaces in are presented with quotes round; other arguments are presented raw string representation of the object. to check if the passed script to shell command executor timed out or not. the script timed out. set if the command has timed out. static method to execute a shell command. covers most of the simple cases without requiring the static method to execute a shell command. covers most of the simple cases without requiring the static method to execute a shell command. covers most of the simple cases without requiring the timer which is used to timeout scripts spawned off by shell. www.apache.org/licenses/license-2.0 'groups 'groups force /bin/ls, except on windows. refresh interval in msec last time the command was performed env for the command execution sub process used to execute the command reset for next run one time scheduling. read error and input streams as this would free up the buffers free the error stream buffer parse the output clear the input stream buffer wait for the process to finish and check the exit code make sure that the error thread exits the timeout thread handling taken care in finally block close the input stream process has not terminated. so check if it has completed if not just destroy it."
org.apache.hadoop.util.ShutdownHookManager "the shutdownhookmanager enables running shutdownhook in a deterministic order, higher priority first.  the jvm runs shutdownhooks in a non-deterministic order or in parallel. this class registers a single jvm shutdownhook and run all the shutdownhooks registered to it (to this class) in order based on their priority. return shutdownhookmanager singleton. shutdownhookmanager singleton. private structure to store shutdownhook and its priority. returns the list of shutdownhooks in order of execution, highest priority first. list of shutdownhooks in order of execution. adds a shutdownhook with a priority, the higher the priority the earlier will run. shutdownhooks with same priority run in a non-deterministic order. shutdownhook runnable priority of the shutdownhook. removes a shutdownhook. shutdownhook to remove. if the shutdownhook was registered and removed, false otherwise. indicates if a shutdownhook is registered or not. shutdownhook to check if registered. /false depending if the shutdownhook is is registered. indicates if shutdown is in progress or not. if the shutdown is in progress, otherwise false. www.apache.org/licenses/license-2.0 private to constructor to ensure singularity reversing comparison so highest priority hooks are first"
org.apache.hadoop.util.StringInterner "provides equivalent behavior to string.intern() to optimize performance, whereby does not consume memory in the permanent generation. retains a strong reference to each string instance it has interned. retains a weak reference to each string instance it has interned. interns and returns a reference to the representative instance for any of a collection of string instances that are equal to each other. retains strong reference to the instance, thus preventing it from being garbage-collected. string instance to be interned reference to interned string instance interns and returns a reference to the representative instance for any of a collection of string instances that are equal to each other. retains weak reference to the instance, and so does not prevent it from being garbage-collected. string instance to be interned reference to interned string instance www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.StringUtils "general string utils priority of the stringutils shutdown hook. make a string representation of the exception. the exception to stringify string with exception name and call stack. given a full hostname, return the word upto the first dot. the full hostname hostname to the first dot given an integer, return a string that is in an approximate, but human readable format. the number to format human readable form of the integer @deprecated use {@link traditionalbinaryprefix#long2string(long, string, int)}. the same as string.format(locale.english, format, objects). format a percentage for presentation to the given an array of strings, return a comma-separated list of its elements. array of strings string if strs.length is 0, comma separated list of strings otherwise given an array of bytes it will convert the bytes to a hex string representation of the bytes start index, inclusively end index, exclusively string representation of the byte array same as bytetohexstring(bytes, 0, bytes.length). given a hexstring this will return the byte array corresponding to the string the hex string array byte array that is a hex string representation of the given string. the size of the byte array is therefore hex.length/2  the string array to be parsed into an uri array. null if str is null, else the uri array equivalent to str. @throws illegalargumentexception if any string in str violates rfc&nbsp;2396.  given a finish and start time in long milliseconds, returns a string in the format xhrs, ymins, z sec, for the time difference between two times. if finish time comes before start time then negative valeus of x, y and z wil return. finish time start time given the time in long milliseconds, returns a string in the format xhrs, ymins, z sec. the time difference to format formats time in ms and appends difference (finishtime - starttime) as returned by formattimediff(). if finish time is 0, empty string is returned, if start time is 0 then difference is not appended to return value. date format to use fnish time start time value. returns an arraylist of strings. the comma seperated string values arraylist of the comma seperated string values returns a collection of strings. comma seperated string values arraylist of string values splits a comma separated value string, trimming leading and trailing whitespace on each value. a comma separated  with values collection of string values splits a comma separated value string, trimming leading and trailing whitespace on each value. a comma separated  with values array of string values split a string using the default separator a string that may have escaped separator array of strings split a string using the given separator a string that may have escaped separator a char that be used to escape the separator a separator char array of strings split a string using the given separator, with no escaping performed. a string to be split. note that this may not be null. a separator char array of strings finds the first occurrence of the separator character ignoring the escaped separators starting from the index. note the substring between the index and the position of the separator is passed. the source string the character to find character used to escape from where to search used to pass back the extracted string escape commas in the string using the default escape char a string escaped string escape chartoescape in the string with the escape char escapechar string escape char the char to be escaped escaped string array of characters to be escaped unescape commas in the string using the default escape char a string unescaped string unescape chartoescape in the string with the escape char escapechar string escape char the escaped char unescaped string array of characters to unescape return a message for logging. prefix keyword for the message content of the message message for logging "); for(string s : msg) b.append("\n" + prefix + s); b.append("\n print a log message for starting up and shutting down the class of the server arguments the target log object the traditional binary prefixes, kilo, mega, ..., exa, which can be represented by a 64-bit integer. traditionalbinaryprefix symbol are case insensitive. traditionalbinaryprefix object corresponding to the symbol. convert a string to long. the input string is first be trimmed and then it is parsed with traditional binary prefix. for example, "-1230k" will be converted to -1230 1024 = -1259520; "891g" will be converted to 891 1024^3 = 956703965184; input string long value represented by the input string. convert a long integer to a string with traditional binary prefix. the value to be converted the unit, e.g. "b" for bytes. the number of decimal places. string with traditional binary prefix. escapes html special characters present in the string. escaped string representation byte description of the given long interger value. @deprecated use stringutils.format("%.2f", d). concatenates strings, using a separator. separator to join with. strings to join. convert some_stuff to somestuff input string string www.apache.org/licenses/license-2.0 return "0sec if no difference move over the separator for next search reset the buffer remove trailing empty split(s) last split string.split returns a single empty result for splitting the empty string. remove trailing empty split(s) last split separator check if the character array has the character special char no special char otherwise discard the escape char take care a special case take care negative numbers no prefix find traditional binary prefix exact division check a special rounding up case"
org.apache.hadoop.util.ThreadUtil "cause the current thread to sleep as close as possible to the provided number of milliseconds. this method will log and ignore any {@link interruptedexception} encountered. the number of milliseconds for the current thread to sleep www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.Time "utility methods for getting the time and computing intervals. current system time. do not use this to calculate a duration or interval to sleep, because it will be broken by settimeofday. instead, use monotonicnow. time in msec. current time from some arbitrary time base in the past, counting in milliseconds, and not affected by settimeofday or similar system clock changes. this is appropriate to use when computing how much longer to wait for an interval to expire. monotonic clock that counts in milliseconds. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.Tool "a tool interface that supports handling of generic command-line options. tool, is the standard for any map-reduce tool/application. the tool/application should delegate the handling of  standard command-line options to {@link toolrunner#run(tool, string[])} and only handle its custom arguments. here is how a typical tool is implemented:  public class myapp extends configured implements tool { public int run(string[] args) throws exception { // configuration processed by toolrunner configuration conf = getconf(); // create a jobconf using the processed conf jobconf job = new jobconf(conf, myapp.class); // process custom command-line options path in = new path(args[1]); path out = new path(args[2]); // specify various job-specific parameters job.setjobname("my-app"); job.setinputpath(in); job.setoutputpath(out); job.setmapperclass(mymapper.class); job.setreducerclass(myreducer.class); // submit the job, then poll for progress until the job is complete jobclient.runjob(job); return 0; } public static void main(string[] args) throws exception { // let toolrunner handle generic command-line options int res = toolrunner.run(new configuration(), new myapp(), args); system.exit(res); } }  @see genericoptionsparser @see toolrunner execute the command with the given arguments. command specific arguments. code. @throws exception www.apache.org/licenses/license-2.0 configuration processed by toolrunner create a jobconf using the processed conf process custom command-line options specify various job-specific parameters submit the job, then poll for progress until the job is complete let toolrunner handle generic command-line options"
org.apache.hadoop.util.ToolRunner "a utility to help run {@link tool}s. toolrunner can be used to run classes implementing tool interface. it works in conjunction with {@link genericoptionsparser} to parse the  generic hadoop command line arguments and modifies the configuration of the tool. the application-specific options are passed along without being modified.  @see tool @see genericoptionsparser runs the given tool by {@link tool#run(string[])}, after parsing with the given generic arguments. uses the given configuration, or builds one if null. sets the tool's configuration with the possibly modified version of the conf. configuration for the tool. tool to run. command-line arguments to the tool. code of the {@link tool#run(string[])} method. runs the tool with its configuration. equivalent to run(tool.getconf(), tool, args). tool to run. command-line arguments to the tool. code of the {@link tool#run(string[])} method. prints generic command-line argurments and usage information. stream to write usage information to. print out a prompt to the www.apache.org/licenses/license-2.0 set the configuration back, so that tool can configure itself get the args w/o generic hadoop args else ask them again"
org.apache.hadoop.util.UTF8ByteArrayUtils "find the first occurrence of the given byte b in a utf-8 encoded string a byte array containing a utf-8 encoded string starting offset ending position the byte to find that first byte occures otherwise -1 find the first occurrence of the given bytes b in a utf-8 encoded string a byte array containing a utf-8 encoded string starting offset ending position the bytes to find that first byte occures otherwise -1 find the nth occurrence of the given byte b in a utf-8 encoded string a byte array containing a utf-8 encoded string starting offset the length of byte array the byte to find the desired occurrence of the given byte that nth occurrence of the given byte if exists; otherwise -1 find the nth occurrence of the given byte b in a utf-8 encoded string a byte array containing a utf-8 encoded string the byte to find the desired occurrence of the given byte that nth occurrence of the given byte if exists; otherwise -1 www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.VersionInfo "this class finds the package info for hadoop and the hadoopversionannotation information. get the meta-data for the hadoop package. @return get the hadoop version. hadoop version string, eg. "0.6.3-dev" get the subversion revision number for the root directory revision number, eg. "451451" get the branch on which this originated. branch name, e.g. "trunk" or "branches/branch-0.20" the date that hadoop was compiled. compilation date in unix date format the get the subversion url for the root hadoop directory. get the checksum of the source files from which hadoop was built. returns the buildversion which includes version, revision, www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.VersionUtil "suffix added by maven for nightly builds and other snapshot releases. these releases are considered to precede the non-snapshot version with the same version number. this function splits the two versions on &quot;.&quot; and performs a naturally-ordered comparison of the resulting components. for example, the version string "0.3" is considered to precede "0.20", despite the fact that lexical comparison would consider "0.20" to precede "0.3". this method of comparison is similar to the method used by package versioning systems like deb and rpm. version components are compared numerically whenever possible, however a version component can contain non-numeric characters. when a non-numeric group of characters is found in a version component, this group is compared with the similarly-indexed group in the other version component. if the other group is numeric, then the numeric group is considered to precede the non-numeric group. if both groups are non-numeric, then a lexical comparison is performed. if two versions have a different number of components, then only the lower number of components are compared. if those components are identical between the two versions, then the version with fewer components is considered to precede the version with more components. in addition to the above rules, there is one special case: maven snapshot releases are considered to precede a non-snapshot release with an otherwise identical version number. for example, 2.0-snapshot precedes 2.0. this function returns a negative integer if version1 precedes version2, a positive integer if version2 precedes version1, and 0 if and only if the two versions' components are identical in value and cardinality. 1 the first version to compare 2 the second version to compare negative integer if version1 precedes version2, a positive integer if version2 precedes version1, and 0 if and only if the two versions are equal. www.apache.org/licenses/license-2.0"
org.apache.hadoop.util.XMLUtils "general xml utilities. transform input xml given a stylesheet. the style-sheet input xml data output @throws transformerconfigurationexception @throws transformerexception www.apache.org/licenses/license-2.0 instantiate a transformerfactory use the transformerfactory to process the stylesheet and generate a transformer use the transformer to transform an xml source and send the output to a result object."
