#CANONICAL_CLASS_NAME "COMMENTS"
com.aelitis.azureus.activities.VuzeActivitiesConstants ""
com.aelitis.azureus.activities.VuzeActivitiesEntry "comparable implementation sorts on timestamp. equals() implementation compares ids   the typeid to set typeid the iconid to set iconid the text to set text the id to set id the assethash to set assethash the dm to set dm the imagebytes to set imagebytes the showthumb to set showthumb independant for {@link #getdownloadmanger()}. this will be written to the map. returns totorrent set via {@link #settorrent(totorrent)} not needed if you {@link #setdownloadmanager(downloadmanager)}. this will be written the map. @see java.lang.object#equals(java.lang.object) @see java.lang.comparable#compareto(java.lang.object) we are bigger make a copy of the torrent remove any non-standard stuff (e.g. resume data) ignore contains no content our variable is an override use torrent so we don't recurse"
com.aelitis.azureus.activities.VuzeActivitiesListener ""
com.aelitis.azureus.activities.VuzeActivitiesLoadedListener ""
com.aelitis.azureus.activities.VuzeActivitiesManager "manage vuze news entries. loads, saves, and expires them key: networkid, value: last time we pulled news  final string id_str = cn.getserviceurl( contentnetwork.service_identify ); if ( id_str != null && id_str.length() > 0 ){ try{ simpletimer.addperiodicevent( "act:id", 2360601000, new timereventperformer() { public void perform( timerevent event ) { identify( cn, id_str ); } }); identify( cn, id_str ); }catch( throwable e ){ debug.out( e ); } } pull entries from webapp max time to wait before running request contentnetworkmanager cnm = contentnetworkmanagerfactory.getsingleton(); if (cnm == null) { return; } contentnetwork[] contentnetworks = cnm.getcontentnetworks(); for (contentnetwork cn : contentnetworks) { clear the removed entries list so that an entry that was once deleted will will be able to be added again   of entries actually added (no dups)  @return @see com.aelitis.azureus.core.cnetwork.contentnetworkpropertychangelistener#propertychanged(java.lang.string) short circuit.. only get vuzenews from default network continue; broken.. lastnewsat.put(id, new long(now)); clear all entries if we aren't on v2 "lastcheck" backward compat"
com.aelitis.azureus.core.AzureusCore "stop the core and inform lifecycle listeners of stopping @throws azureuscoreexception ask lifecycle listeners to perform a stop. they may veto this by throwing an exception, or do nothing if nothing is done then it will be stopped as per "stop" above @throws azureuscoreexception checks if restart operation is supported - if not an alert will be raised and an exception thrown @throws azureuscoreexception restart the system request a restart of the system - currently only available for swt based systems @throws azureuscoreexception @return  see ca_ constants above"
com.aelitis.azureus.core.AzureusCoreComponent ""
com.aelitis.azureus.core.AzureusCoreException ""
com.aelitis.azureus.core.AzureusCoreFactory "azureus core is a singleton that must be initially returns whether the core is available. all features of the core (such as globalmanager) may not be available yet. @return returns whether the core is running. all features of the core (globalmanager) should be available when the result is true. @return once adds a listener that is triggered once the core is running.  this is in azureuscorefactory instead of {@link azureuscorelifecyclelistener} so that listeners can be added before the core instance is even"
com.aelitis.azureus.core.AzureusCoreLifecycleAdapter ""
com.aelitis.azureus.core.AzureusCoreLifecycleListener "return true if the request has been accepted (and hence the listener will arrange for a stop to occur @return some listeners must be invoked on the same thread that initiates a core closedown. in particular the show-alerts-raised-during-closedown logic requires that it is invoked on the swt thread that initiated the closedown. @return hack - original semantics of the "started" event was that all plugins init complete however, some can take a long time (and even block if attempting to acquire, say, public ip address and version server is down...) so added this flag to allow listeners to indicate that they're happy to be "started" before plugin init complete @return"
com.aelitis.azureus.core.AzureusCoreListener ""
com.aelitis.azureus.core.AzureusCoreOperation ""
com.aelitis.azureus.core.AzureusCoreOperationListener "if the listener has taken responsibility for running an operation task"
com.aelitis.azureus.core.AzureusCoreOperationTask ""
com.aelitis.azureus.core.AzureusCoreRunningListener "see {@link azureuscorefactory#addcorerunninglistener(azureuscorerunninglistener)}"
com.aelitis.azureus.core.backup.BackupManager "-> abandon process"
com.aelitis.azureus.core.backup.BackupManagerFactory ""
com.aelitis.azureus.core.backup.impl.BackupManagerImpl "lazy but whatever, our config never gets this deep shouldn't really have file names as keys due to charset issues..."
com.aelitis.azureus.core.clientmessageservice.ClientMessageService "send the given message to the server service. note: blocking op (bencode-able) to send @throws ioexception on error receive the next message from the server. note: blocking op received @throws ioexception on error drop and closedown the connection with the server. override the default max message size _bytes"
com.aelitis.azureus.core.clientmessageservice.ClientMessageServiceClient "create a new message server service connection. _address of service _port of service type id to use for messages service connection"
com.aelitis.azureus.core.clientmessageservice.impl.AEClientService "ignore, since message type probably already registered register for incoming type decoding note: blocking op note: async operation! connect op finished connect failure register for read/write handling not yet connected queue message for sending block until send completes send op finished connect failure not yet connected block until receive completes there were still read messages left from the previous read call receive op finished no handler notification"
com.aelitis.azureus.core.clientmessageservice.impl.ClientConnection "create a new connection based on an incoming socket. create a new connection based on an already-established outgoing socket. parent get any messages read from the client. messages, or null of no new messages were read @throws ioexception on error write any queued messages back to the client. if more writing is required, false if all message data has been sent @throws ioexception on error marks the socket as complete and ready to undergo any close-delay prior to it being closed by the server get the last time this connection had read or write activity. time of last activity reset the last activity time to the current time. unfortunately we don't have an address at this point (see nattestservice) run as a lightweight save parent for close public boolean hasdatatosend() { return out_queue.gettotalsize() > 0; } have the parent do the close if possible assumption is write infrequently, read often -> copy-on-write"
com.aelitis.azureus.core.clientmessageservice.impl.ClientMessage ""
com.aelitis.azureus.core.clientmessageservice.impl.ClientMessageHandler "get the message type id that this handler handles. @return process the given message received from a client. from client to process notification of reply message send attempt completion. note: this method will always be called once for every preceeding clientmessageserver.sendreplymessage() call. sent true if reply send was successful, false if reply send failed"
com.aelitis.azureus.core.clientmessageservice.impl.NonBlockingReadWriteService "check for timeouts every 10sec check this at the end so we have one last run through the selectors to do cancels before exiting add to active list remove from active list reads success note no handler. we let the listener attach it system.out.println( "[" +new date()+ "] connection read error [" +sc.socket().getinetaddress()+ "] [" +client.getdebugstring()+ "]: " +t.getmessage() ); failure writes we need to resume since write selects are auto-paused after select op system.out.println( "[" +new date()+ "] connection write error [" +sc.socket().getinetaddress()+ "] [" +client.getdebugstring()+ "]: " +t.getmessage() ); start writing back to the connection wait until we've got something to send before selecting start reading from the connection check timeouts time went backwards! do actual removal outside the check loop don't change the exception text - it is used elsewhere system.out.println( "[" +new date()+ "] connection message send error [connection no longer connected]: " +vconn.getdebugstring()+ "]" ); listener.connectionerror( vconn ); //no need to call this, as there is no connection to remove start write selecting now that there's something to send"
com.aelitis.azureus.core.clientmessageservice.secure.impl.SecureMessageServiceClientHelper "fallback to the bc implementation for jdk1.4.2 as jce rsa not available"
com.aelitis.azureus.core.clientmessageservice.secure.impl.SecureMessageServiceClientImpl "these also occur in the server  if don't re-attempt this is when things have gone badly wrong server-side - we just dump the message"
com.aelitis.azureus.core.clientmessageservice.secure.impl.SecureMessageServiceClientMessageImpl ""
com.aelitis.azureus.core.clientmessageservice.secure.SecureMessageServiceClient "this shouldn't be required under normal circumstances as message addition causes dispatch and the server handles retries itself. however, sometimes it is necessary to force a dispatch to occur (e.g. to validate new authentication information immediately rather than wait for it to happen naturally)"
com.aelitis.azureus.core.clientmessageservice.secure.SecureMessageServiceClientAdapter ""
com.aelitis.azureus.core.clientmessageservice.secure.SecureMessageServiceClientFactory "used by the sms plugin."
com.aelitis.azureus.core.clientmessageservice.secure.SecureMessageServiceClientListener ""
com.aelitis.azureus.core.clientmessageservice.secure.SecureMessageServiceClientMessage ""
com.aelitis.azureus.core.cnetwork.ContentNetwork "you should normally not refer explicitly to this constant without consideration as we don't want network-specific code proliferating returns one of the above content_network constants @return test if the network supports a particular service _type @return returns the base url of the service. if not parameterised then this is sufficient to invoke the service _type @return generic parameterised service method _type @return search service helper method @return the topic number or a pre-defined topic constant found in faqtopics i'd rather this function we embedded into the contentnetwork service getting logic, but for the moment expose it for simplicity _in base url onto which the suffix should be appended _post whether this is for an http 'post' operation, in which case the parameter separator is always an '&' (for 'get' the first param uses a '?' sep) _azid whether or not we should include the azid in the suffix @return export to vuze file @return sets a locally persistent property. name should be from the pp_ names above so keep track of what attributes exist pp_ constant must be bencodable! set a non-persistent property of the content network string - query text string - query text; boolean - tosubscribe string - hash; string (can be null) - client ref string - hash string - login_id; string - client ref string - hash; string (can be null) - client ref string - topic entry string - message (can be null) string - relative url string - bg colour string - subscription id string - relative url content network properties string boolean string (tux prefers to integer) persistent (and local) properties boolean, default false boolean, default false boolean boolean string"
com.aelitis.azureus.core.cnetwork.ContentNetworkException ""
com.aelitis.azureus.core.cnetwork.ContentNetworkListener ""
com.aelitis.azureus.core.cnetwork.ContentNetworkManager "checks networks for those that have this url as their site network, null if none {@link contentnetworklistener#networkadded(contentnetwork)} will be called once the network is added"
com.aelitis.azureus.core.cnetwork.ContentNetworkManagerFactory ""
com.aelitis.azureus.core.cnetwork.ContentNetworkPropertyChangeListener ""
com.aelitis.azureus.core.cnetwork.impl.ContentNetworkImpl "boolean"
com.aelitis.azureus.core.cnetwork.impl.ContentNetworkManagerImpl "change here as we always add all networks upfront so we always set the customisation flag regardless of whether existing or not @see com.aelitis.azureus.core.cnetwork.contentnetworkmanager#addcontentnetwork(long) we never persist the vuze network"
com.aelitis.azureus.core.cnetwork.impl.ContentNetworkVuze "static{ if ( featureavailability.enable_plus()){ if ( system.getproperty( "platform_address", "" ).length() == 0 ){ system.setproperty( "platform_address", "www2.vuze.com" ); } } } import com.aelitis.azureus.core.util.featureavailability; do not touch !!!! use the -dplatform_address=ip override instead do not touch !!!! use the -drelay_address=ip override instead do not touch !!!! no icon"
com.aelitis.azureus.core.cnetwork.impl.ContentNetworkVuzeGeneric "don't change the order of the params as there's some code somewhere that depends on them (i think its code that removes the azid so we can fix this up when that code's migrated here i guess keeping this around for safetly, since it's a 4402 release no azid needed (+ makes url ugly) already present"
com.aelitis.azureus.core.content.AzureusContent ""
com.aelitis.azureus.core.content.AzureusContentDirectory "byte[] integer"
com.aelitis.azureus.core.content.AzureusContentDirectoryListener ""
com.aelitis.azureus.core.content.AzureusContentDirectoryManager ""
com.aelitis.azureus.core.content.AzureusContentDownload ""
com.aelitis.azureus.core.content.AzureusContentFile "string string long, millis long, millis long long string[] long, thousandths long, seconds"
com.aelitis.azureus.core.content.AzureusContentFilter ""
com.aelitis.azureus.core.content.AzureusPlatformContentDirectory "used in upnp for something secs -> millis"
com.aelitis.azureus.core.content.ContentException ""
com.aelitis.azureus.core.content.RelatedContent "legacy constructor as referenced from plugin - remove oneday!"
com.aelitis.azureus.core.content.RelatedContentLookupListener ""
com.aelitis.azureus.core.content.RelatedContentManager "don't change these, used in plugin remove one day debug.out( e ); debug.out( "inconsistent!" ); shouldn't happen but whatever system.out.println( from_hash + ": hits=" + hits + ", div=" + diversified ); system.out.println( from_hash + ": hits=" + hits + ", div=" + diversified ); system.out.println( "p=" + published + ", l=" + leechers + ", s=" + seeds ); we need something to use stuff in a couple primarys this will populate the cache target refers to downoad we already have target refers to downoad we already have we already know about this, see if new info delete oldest at highest level >= level with minimum rank we don't want high-ranked entries to get stuck there and prevent newer stuff from getting in and rising up give it a day at the top term is made up of space separated bits - all bits must match each bit can be prefixed by + or -, a leading - means 'bit doesn't match'. + doesn't mean anything each bit (with prefix removed) can be "(" regexp ")" if bit isn't regexp but has "|" in it it is turned into a regexp so a|b means 'a or b' pick up a direct match regardless of anything else dump(); this rank isn't that accurate, scale down test injection of local hinted_contacts.add( 0, ddb.getlocalcontact()); stick in map so non-hinted get removed below, but interleave later back fill with less reliable contacts if required interleave hinted ones so we get some variety hard limit of total results found and overall elapsed insert results from more predictable nodes after the less predictable ones give another 5 secs for results to come in rate limit a bit after the first 10 give another 5 secs for results to come in system.out.println( "search result=" + value ); system.out.println( "search result=" + value ); system.out.println( "search result=" + value ); fallback to default handling don't bother with tracker as no use to caller really todo bloom can get here is config's been deleted migration from when it was a map with non-ascii key issues debug.out( "inconsistent - no ids" ); debug.out( "inconsistent: id " + id + " missing" ); we don't currently remember all originators, just one that works debug.out( "inconsistent: info not referenced" ); debug.out( "total_unread - inconsistent (" + total_unread + "/" + new_total_unread + ")" ); debug.out( "rcm: cache inconsistent" ); debug.out( "inconsistent: info missing for " + di ); debug.out( "inconsistent" ); 6k ish ipv6 raw simple check for ipv4 raw note that it_dht is processed last don't want to stick with a stable one for too long otherwise the stabler nodes will end up in lots of other nodes' harvest set and receive undue attention logsearch( "not skipping local!!!!" ); ignore we need this reference here to manage garbage collection correctly persistence of this is piggy-backed on other saves to limit resource usage only therefore a vague measure"
com.aelitis.azureus.core.content.RelatedContentManagerListener ""
com.aelitis.azureus.core.crypto.VuzeCryptoException ""
com.aelitis.azureus.core.crypto.VuzeCryptoListener "indicates that the session password returned is incorrect. this is a bad state of affairs and the (probable) only solution is to generate a new public/private key pair"
com.aelitis.azureus.core.crypto.VuzeCryptoManager "crypt_man.addpasswordhandler( new cryptomanagerpasswordhandler() { private boolean error_logged = !constants.iscvsversion(); public int gethandlertype() { return( handler_type_system ); } public void passwordok( int handler_type, passworddetails details) { iterator it = listeners.iterator(); while( it.hasnext()){ ((vuzecryptolistener)it.next()).sessionpasswordcorrect(); } } public passworddetails getpassword( int handler_type, int action_type, boolean last_pw_incorrect, string reason ) { if ( last_pw_incorrect ){ iterator it = listeners.iterator(); while( it.hasnext()){ ((vuzecryptolistener)it.next()).sessionpasswordincorrect(); } return( null ); } if ( session_pw != null ){ return( session_pw ); } iterator it = listeners.iterator(); while( it.hasnext()){ try{ final char[] pw = ((vuzecryptolistener)it.next()).getsessionpassword( reason ); session_pw = new passworddetails() { public char[] getpassword() { return( pw ); } public int getpersistforseconds() { return( -1 ); // session } }; error_logged = false; return( session_pw ); }catch (vuzecryptoexception ve) { if ( !error_logged ){ error_logged = true; debug.out( "listener failed " + ve.tostring() + " on " + reason ); if (ve.getcause() != null) { debug.out(ve.getcause()); } } }catch( throwable e ){ debug.out( "listener failed", e ); } } if ( !error_logged ){ error_logged = true; debug.out( "vuzecryptomanager: no listeners returned session key" ); } return( null ); } }); // auto enable buddy plugin and system handler boolean init_done = coconfigurationmanager.getbooleanparameter( "vuze.crypto.manager.initial.login.done", false ); if ( !init_done ){ azureuscorefactory.addcorerunninglistener( new azureuscorerunninglistener() { public void azureuscorerunning( azureuscore core) { { initialise( core ); } } }); } protected void initialise( azureuscore core ) { synchronized( this ){ if ( init_tried ){ return; } init_tried = true; } plugininterface pi = core.getpluginmanager().getplugininterfacebyid( "azbuddy" ); if ( pi != null ){ plugin plugin = pi.getplugin(); if ( plugin instanceof buddyplugin ){ buddyplugin buddy_plugin = (buddyplugin)plugin; if ( !buddy_plugin.isenabled()){ cryptohandler handler = crypt_man.getecchandler(); // try and switch password handler if no keys yet defined if ( handler.peekpublickey() == null ){ try{ handler.setdefaultpasswordhandlertype( cryptomanagerpasswordhandler.handler_type_system ); }catch( throwable e ){ debug.out( "crypto: failed to set default password handler type: " + debug.getnestedexceptionmessage( e )); } } buddy_plugin.setenabled( true ); coconfigurationmanager.setparameter( "vuze.crypto.manager.initial.login.done", true ); coconfigurationmanager.save(); debug.out( "crypto: initialised buddy plugin and default handler type" ); } } } } public string getpublickey( string reason ) throws vuzecryptoexception { try{ return( base32.encode(crypt_man.getecchandler().getpublickey(reason))); }catch( throwable e ){ throw( new vuzecryptoexception( "failed to access public key", e )); } } public boolean haspublickey() { return crypt_man.getecchandler().peekpublickey() != null; } public void clearpassword() { session_pw = null; crypt_man.clearpasswords( cryptomanagerpasswordhandler.handler_type_system ); } public void setpassword( string pw ) { final char[] pw_chars = pw.tochararray(); session_pw = new cryptomanagerpasswordhandler.passworddetails() { public char[] getpassword() { return( pw_chars ); } public int getpersistforseconds() { return( -1 ); // session } }; new aethread2( "vuzecryptomanager:testunlock", true ) { public void run() { try{ crypt_man.getecchandler().unlock(); }catch( throwable e ){ debug.out( "password incorrect", e ); } } }.start(); } public void addlistener( vuzecryptolistener listener ) { listeners.add( listener ); } public void removelistener( vuzecryptolistener listener ) { listeners.remove( listener ); } private boolean init_tried; private copyonwritelist listeners = new copyonwritelist(); private volatile cryptomanagerpasswordhandler.passworddetails session_pw; session auto enable buddy plugin and system handler try and switch password handler if no keys yet defined session"
com.aelitis.azureus.core.custom.Customization "inputstream[] inputstream[] inputstream[]"
com.aelitis.azureus.core.custom.CustomizationException ""
com.aelitis.azureus.core.custom.CustomizationManager ""
com.aelitis.azureus.core.custom.CustomizationManagerFactory ""
com.aelitis.azureus.core.custom.impl.CustomizationImpl "todo:"
com.aelitis.azureus.core.custom.impl.CustomizationManagerImpl "try{ customizationmanagerimpl manager = (customizationmanagerimpl)getsingleton(); customizationimpl cust = new customizationimpl( manager, "blah", "1.2", new file( "c:\\temp\\cust\\details.zip" )); cust.exporttovuzefile( new file( "c:\\temp\\cust" )); }catch( throwable e ){ e.printstacktrace(); } hmm, its been deleted or not set yet. look for new ones"
com.aelitis.azureus.core.devices.Device "array of resource strings and their associated values @return"
com.aelitis.azureus.core.devices.DeviceContentDirectory ""
com.aelitis.azureus.core.devices.DeviceInternetGateway ""
com.aelitis.azureus.core.devices.DeviceListener ""
com.aelitis.azureus.core.devices.DeviceManager "not the best place for these, but it'll do for the moment"
com.aelitis.azureus.core.devices.DeviceManagerDiscoveryListener ""
com.aelitis.azureus.core.devices.DeviceManagerException ""
com.aelitis.azureus.core.devices.DeviceManagerFactory ""
com.aelitis.azureus.core.devices.DeviceManagerListener ""
com.aelitis.azureus.core.devices.DeviceMediaRenderer "this will change!!! @return rs_ copy to device copy to folder associate"
com.aelitis.azureus.core.devices.DeviceMediaRendererTemplate ""
com.aelitis.azureus.core.devices.DeviceOfflineDownload ""
com.aelitis.azureus.core.devices.DeviceOfflineDownloader ""
com.aelitis.azureus.core.devices.DeviceOfflineDownloaderListener ""
com.aelitis.azureus.core.devices.DeviceOfflineDownloaderManager ""
com.aelitis.azureus.core.devices.DeviceSearchListener ""
com.aelitis.azureus.core.devices.DeviceTemplate "auto devices can't be manually"
com.aelitis.azureus.core.devices.impl.DeviceContentDirectoryImpl ""
com.aelitis.azureus.core.devices.impl.DeviceDriveManager "this should synchronously first any discovered drives system.out.println("dd " + info.getlocation() + " via " + debug.getcompressedstacktrace()); historically, we gave ids to motorola, samsung, and htc phones based on their vendor and product id only. we need to maintain this id in order to not create duplicates. fortunately, both motorola and samsung include their model id in the sprodid. htc doesn't, however, their pid doesn't identify unique models anyway, so including that wouldn't have helped anyway samsungs that start with y are mp3 players samsungs that don't have a dash aren't smart phones (none that we know of anyway..) fake not writable so we remove the device instead of adding it fixup old bug where we were adding samsung hard drives as devices cheap hack to detect the psp when it has no psp or video dir damn, the above doesn't work until devices is turned on... this will cause a change event"
com.aelitis.azureus.core.devices.impl.DeviceImpl "don't overwite the name as transient note, overridden in itunes note, overridden in itunes bit of a mess here. first release used name as classification and mapped to species + device-classification here. second release moved to separate name and classification and used the correct device-classification as the classification so we deal with both for the moment... 'generic' means one we don't explicitly support, which are rendereres discovered by upnp apparently wmp isn't ready for the right chasm i would like to drill all the way up to just 'generic' but unfortunately this would break the current samsung/ms_wmp support that requires the detected profile set to be empty (we have two existing profiles at the 'generic' level) fire the listeners first as this gives listeners a chance to extract data from the file before it is deleted (otherwise operations fail with 'file has been deleted'"
com.aelitis.azureus.core.devices.impl.DeviceInternetGatewayImpl "no explicit one, try hitting location"
com.aelitis.azureus.core.devices.impl.DeviceiTunes "list> sources = (list>)properties.get( "sources" ); if ( sources != null ){ for ( map source: sources ){ system.out.println( source ); } } no default is true for itunes file has been deleted"
com.aelitis.azureus.core.devices.impl.DeviceiTunesManager ""
com.aelitis.azureus.core.devices.impl.DeviceManagerImpl "have to go async on this as there are situations where we end up firing listeners while holding monitors and this can result in deadlock if sync need to pick up auto-search early on init tivo before upnp as upnp init completion starts up tivo try to use the media server's name for xbox (currently) we automagically replace a manual entry with an auto one as we may have added the manual one when receiving a previous browse before getting the upnp renderer details don't trigger config save here, if anything has changed it will have been handled by the updatefrom call above if anything has changed then the updatefrom methods should have indicated this so there's no need to blindly fire a change event here devicechanged( existing, false ); transcoding is rolled into renderers offline downloader stuff sdsd to late to try writing i'd rather put this in a listener, but for now this will ensure it gets qos'd even before any listeners are added system.out.println(system.currenttimemillis() + "] change -> " + device.getid() + "/" + device.getname() + " via " + debug.getcompressedstacktrace()); migration from azureus downloads to vuze downloads default has changed, reset all device save locations so that they pick up the change"
com.aelitis.azureus.core.devices.impl.DeviceManagerRSSFeed "if ( thumb_url != null ){ pw.println( "" + escape( "" ) + description + escape( "" ) + "" ); }else{ absolute url is borked as it doesn't set the host properly. hack } media elements itunes elements"
com.aelitis.azureus.core.devices.impl.DeviceManagerUPnPImpl "system.out.println( "received browse: " + request.getclientaddress() + ", agent=" + startup can take a while as adding the upnp listener can sync call back device added and result in device details loading etc system.out.println( str ); system.out.println( str ); just test on ip, should be ok if the normally we can detect the xbox renderer and things work automagically. however, on occasion we receive the browse before detection and if the device's ip has changed we need to associate its new address here otherwise association of browse to device fails there's auto-hide code to hide devices when a concrete upnp-based version is discovered at the same ip we don't want this happening for these generic devices, especially the browser one as it is straight forward to get browse events from this on an ip that also happens to be exposing upnp devices (e.g. windows media player) ignore local offline downloader we end up with "::upnp:rootdevice" on the end of this - remove grab the actual device as the 'adddevice' call will update an existing one with same id"
com.aelitis.azureus.core.devices.impl.DeviceMediaRendererImpl "system.out.println("set address " + address.gethostaddress() + "; " + getname() + "/" + getclassification()); prefer upnp device over manual one added by a browse event prefer upnp device over manual one added by a browse event fix beta bug where we hid devices that had files. remove after 4605 device has upnp stuff, but did not register itself as renderer. nothing to do"
com.aelitis.azureus.core.devices.impl.DeviceMediaRendererManual "do this to pick up change in copy-to-device state caused by removal handle case where device is auto-detectable and copy-to is missing due to the two-phase mount of android devices and the fact that the copy-to location is %root$/videos, the root might exist but the sub-folder not. seterror( copy_error_key, messagetext.getstring( "device.error.copytomissing", new string[]{copy_to.getabsolutepath()})); sub_borked = true; all done, tidy up and exit file has been deleted @see com.aelitis.azureus.core.devices.impl.deviceimpl#getstatus()"
com.aelitis.azureus.core.devices.impl.DeviceMediaRendererTemplateImpl ""
com.aelitis.azureus.core.devices.impl.DeviceOfflineDownloaderImpl "not yet initialised or closing no usable service remove uninteresting ones state == downloadmanager.state_error ){ removed - might be out of disk space and fixable don't include 'stopping' here as we go through stopping on way to queued don't remove from downloader if simply paused if it is complete then of no interest manual, just use the tagged downloads assume not yet started and just use the non-skipped files store this so we have consistent record for downloads that queue/pause etc and therefore lose accessible piece details sort by download priority need to add the torrent for vuze content add in the azid todo: prevent continual attempts to add same torrent? if device isn't connectable then replace the error with something more  remove non-transferable entries add in new ones check current rotate through them in case something's stuck for whatever reason more restrictive test here to sync alive state with 'appears to be offline' error messages not totally accurate, in general will be > required as based purely on piece size as opposed to blocks. however, we need an initial estimate as the download may not yet be running and therefore we can't get accurate size now"
com.aelitis.azureus.core.devices.impl.DeviceTivo "guess we could implement this one day root folder /tivoconnect?command=querycontainer&container=%2f build list of applicable items see if we can set up a stream xcode for this but only if we know the duration and the transcode is in progress (done in setup) sort select items to return can be negative if x items from end either one before or one after item to be returned depending on count +ve/-ve default until we find out what it is - can't see any way to get it apart from wait for broadcast tivo has problems displaying a truncated title if it has no spaces in it /tivoconnect?command=queryformats&sourceformat=video%2fx-tivo-mpeg todo: we need access to max bitrate info... and then use duration and increase by, say, 5% upper limit of 3 mb a sec assumed"
com.aelitis.azureus.core.devices.impl.DeviceTivoManager "unfortunately we can't deduce the series from the browse request so start off with a series 3 this will be corrected later if we receive a beacon which does contain the series details possible race here so handle case where device already present see if time to auto-shutdown searching"
com.aelitis.azureus.core.devices.impl.DeviceUPnPImpl "imageid == null && triggers any address change logic in theory this is a plugin, so there should be a core.. however, check just in case todo: duration etc if the file completed transcoding then we leave the result around for the may have just completed, say things are ok as caller can continue todo: duration etc file deleted only overwrite categories with the downloads ones if none already set timing issues - we can get here during teh fail process so hang around a little if we're still running might have completed and then been removed commented out existing imageid check so upnp device image overrides"
com.aelitis.azureus.core.devices.impl.TranscodeFileImpl "don't store any local state here, store it in the map as this is just a wrapper for the underlying map and there can be multiple such wrappers concurrent options are either a download file or a link to an existing non-torrent based file if we're not transcoding then always return the source even if doesn't exist debug.out( "source file doesn't exist (hash=" + hash + ",link=" + link +"), returning cache file" ); options are either the cached file, if it exists, or failing that the source file if transcoding not required debug.out( "target file for " + cache_file + " doesn't exist" ); reset the file name as previous"
com.aelitis.azureus.core.devices.impl.TranscodeJobImpl "external file this is for an azureus restart with a paused job - we don't want to change the state as we want it to re-pause... process_time filled with negative pause time, so add to it process_time filled with negative pause time, so add to it manual start, scrub error details process_time filled with pause"
com.aelitis.azureus.core.devices.impl.TranscodeJobOutputLeecher "fall through and return 0 read data not yet available or file complete length can be -1 here meaning 'to the end'"
com.aelitis.azureus.core.devices.impl.TranscodeManagerImpl "we don't want things hanging around for init if we're closing new downloads don't get a category-change event fired when added we also want to delay things a bit to allow other components to set an initial category. there's no hurry anyways still uncategorised limit number of files we can add to avoid crazyness could be smarter here and check extension or whatever"
com.aelitis.azureus.core.devices.impl.TranscodePipe "average over 10s, update every 1000ms average over 10s, update every 1000ms system.out.println( "using cache entry: o_offset=" + pos + ",r_offset=" + rel_offset+",len=" + read_length ); system.out.println( "adding to cache: o_offset=" + pos + ", size=" + limit );"
com.aelitis.azureus.core.devices.impl.TranscodePipeFileSource "system.out.println( command + ": " + headers ); -500 = last 500 bytes of file prevent seeking too far"
com.aelitis.azureus.core.devices.impl.TranscodePipeStreamSource ""
com.aelitis.azureus.core.devices.impl.TranscodePipeStreamSource2 ""
com.aelitis.azureus.core.devices.impl.TranscodeProfileImpl ""
com.aelitis.azureus.core.devices.impl.TranscodeProviderVuze "race condition here on auto-transcodes due to downloadadded listeners - can add the xcode to queue and schedule before added to upnpms - simple hack is to hang about a bit running see if we can use the file directly todo"
com.aelitis.azureus.core.devices.impl.TranscodeQueueImpl "provider_job[0] = provider.transcode( adapter, job.getfile(), profile, new file( "c:\\temp\\arse").touri().tourl()); already advertised as a transcoded asset so no option not to transcode (as name/format would change if decided not to transcode and then this would confuse the clients) audio hack for psp audio problem on osx with some files thrashing the indirect piped input and eventually failing - try and spot this behaviour and revert to direct input if needed sanity check: for incomplete files at the start of the process ensure that they have completed actually this ain't so simple as we stream data prior to hash check completion (otherwise for large piece sizes we could be waiting for 4mb to complete downloading before playback) and getdownloaded() only returns the verified data size debug.out( e ); things might have improved, check again no transcode required... see if we can use the file directly see if we can use the file directly pick up any existing paused ones (remember, paused=running but with transcode paused remember paused here is paused after an azureus restart as 'process' blocks on pause only treat this as a failure if the job was initially running and has been explicitly stopped"
com.aelitis.azureus.core.devices.TranscodeActionVetoException ""
com.aelitis.azureus.core.devices.TranscodeAnalysisListener ""
com.aelitis.azureus.core.devices.TranscodeException ""
com.aelitis.azureus.core.devices.TranscodeFile "will return null unless there is a job in existance for this file @return don't change these, they are serialised"
com.aelitis.azureus.core.devices.TranscodeJob ""
com.aelitis.azureus.core.devices.TranscodeManager ""
com.aelitis.azureus.core.devices.TranscodeManagerListener ""
com.aelitis.azureus.core.devices.TranscodeProfile ""
com.aelitis.azureus.core.devices.TranscodeProvider ""
com.aelitis.azureus.core.devices.TranscodeProviderAdapter ""
com.aelitis.azureus.core.devices.TranscodeProviderAnalysis "boolean long long long long long boolean (set)"
com.aelitis.azureus.core.devices.TranscodeProviderJob ""
com.aelitis.azureus.core.devices.TranscodeQueue "from target.transcode_"
com.aelitis.azureus.core.devices.TranscodeQueueActionListener ""
com.aelitis.azureus.core.devices.TranscodeQueueListener ""
com.aelitis.azureus.core.devices.TranscodeTarget ""
com.aelitis.azureus.core.devices.TranscodeTargetListener "data = string property"
com.aelitis.azureus.core.dht.control.DHTControl "support methods for db debug method only"
com.aelitis.azureus.core.dht.control.DHTControlActivity ""
com.aelitis.azureus.core.dht.control.DHTControlAdapter ""
com.aelitis.azureus.core.dht.control.DHTControlContact ""
com.aelitis.azureus.core.dht.control.DHTControlFactory ""
com.aelitis.azureus.core.dht.control.DHTControlListener ""
com.aelitis.azureus.core.dht.control.DHTControlStats "uptime of the latest router instance @return number of routers instantiated - new one"
com.aelitis.azureus.core.dht.control.impl.DHTControlContactImpl ""
com.aelitis.azureus.core.dht.control.impl.DHTControlImpl "we need to be a bit smart about exporting state to deal with the situation where a dht is started (with good import state) and then stopped before the goodness of the state can be re-established. so we remember what we imported and take account of this on a re-export system.out.println( "div check starts for " + contact.getstring()); string keys_str = ""; for (int i=0;i " + div_str ); new aethread2( "sdsd", true ) { public void run() { dhttransportfullstats stats = contact.getstats(); system.out.println( contact.getstring() + "-> " +(stats==null?"":stats.getstring())); } }.start(); we don't use teh cache-at-closest kad feature if ( found_values.size() > 0 ){ dhttransportvalue[] values = new dhttransportvalue[found_values.size()]; found_values.toarray( values ); // cache the values at the 'n' closest seen locations for (int k=0;k failed: " + _error.getmessage()); router.contactdead( _contact.getid(), false ); } }, new byte[][]{ encoded_key }, new dhttransportvalue[][]{ values }); } } the lookup method returns up to k closest nodes to the target _id @return system.out.println( "r=" + byteformatter.encodestring( router_id ) + ",c=" + byteformatter.encodestring( contact_id ) + ",d=" + byteformatter.encodestring( distance ) + ",l=" + byteformatter.encodestring( nacul ) + ",r=" + res ); -ve -> n1  if you call n0 yourself, n1 the nearest peer, n2 the 2nd nearest peer ... np the pth nearest peer that you know (for example, n1 .. n20)  and if you call d1 the kad distance between you and n1, d2 between you and n2 ...  then you have to compute :  dc = sum(i di) / sum( i i)  and then :  nbpeers = 2^160 / dc list c = getcontacts(); for (int i=0;i 1 if diversification is replicating (for load balancing) system.out.println( "put: skipping key as already written" ); we don't consider diversification for direct puts (these are for republishing of cached mappings and we maintain these as normal - its up to the original publisher to diversify as required) we don't consider diversification for direct puts (these are for republishing of cached mappings and we maintain these as normal - its up to the original publisher to diversify as required) ensure plain key and obfuscated one differ at subsequent bytes to prevent potential clashes with code that uses 'n' byte prefix (e.g. db survey code) finally copy over last two bytes for code that uses challenge-response on this (survey code) if all failed then nothing to do only diversify on one hit as we're storing at closest 'n' so we only need to do it once for each key don't send to ourselves! if we've come back to an already hit contact due to a diversification loop then ignore it debug.out( "put: contact encountered for a second time, ignoring" ); each store is going to report its complete event can be null for old protocol versions remove this key for any subsequent publishes. quickest hack is to change it into a random key value - this will be rejected by the recipient as not being close enough anyway system.out.println( "net:" + transport.getnetwork() + " - max_div_depth=" + max ); get the initial starting point for the get - may have previously been diversified over-diversified we only want to follow one diversification should return a max of 1 (0 if diversification refused) however, could change one day to search > 1 cache the values at the 'n' closest seen locations don't consider diversification for cache stores as we're not that bothered not found locally, nothing to do we remove a key by pushing it back out again with zero length value not found locally, nothing to do keep querying successively closer nodes until we have got responses from the k closest nodes that we've seen. we might get a bunch of closer nodes that then fail to respond, which means we have reconsider further away nodes we keep a list of nodes that we have queried to avoid re-querying them we keep a list of nodes discovered that we have yet to query we have a parallel search limit of a. for each a we effectively loop grabbing the currently closest unqueried node, querying it and adding the results to the yet-to-query-set (unless already queried) we terminate when we have received responses from the k closest nodes we know about (excluding failed ones) note that we never widen the root of our search beyond the initial k closest that we know about - this could be relaxed contacts remaining to query closest at front record the set of contacts we've queried to avoid re-queries record the set of contacts that we've had a reply from furthest away at front this handles the search concurrency -1 terminated, 0 waiting, 1 running start the lookup record the set of contacts we've queried to avoid re-queries record the set of contacts that we've had a reply from furthest away at front this handles the search concurrency bail out and pretend everything worked with zero results system.out.println("timeout"); maybe unterminated searches still going on so protect ourselves against concurrent modification of result set we need to reverse the list as currently closest is at the end we can use the results of this to estimate the dht size system.out.println("reserve-exit"); system.out.println("release-start"); system.out.println( "task cancelled" ); individual lookup steps temporary stop, will be revived by release() or until a timeout occurs temporary stop, will be revived by release()/ for stats queries the values returned are unique to target so don't assume 2 replies sufficient all hits should have the same values anyway... if we've received a key block then easiest way to terminate the query is to dump any outstanding targets if nothing pending then we need to wait for the results of a previous search to arrive. of course, if there are no searches active then we've run out of things to do select the next contact to search if the next closest is further away than the furthest successful hit so far and we have k hits, we're done we optimise the first few entries based on their vivaldi distance. only a few however as we don't want to start too far away from the target. system.out.println( start + ": lookup for " + dhtlog.getstring2( lookup_id ) + ": vp override (dist = " + dist + ")"); override id closest with vp closes never search ourselves! ignore responses that are ourselves dunno if its alive or not, however record its existance delete the furthest away ignore responses that are ourselves dhtlog.log( " already queried: " + dhtlog.getstring( contact )); hack - this is set to 99 when recursing here during obsfuscated lookup diversification instruction ignore for stats queries as we're after the target key's stats, not the diversification thereof can't just use originator id as this value can be dosed (see db code) we have read the marker value, now issue a direct read with the real key if at least one reply has been received then we don't treat subsequent failure as indication of a contact failure (just packet loss) we don't want to kill the contact due to this so indicate that it is ok by setting the flag request methods system.out.println( "storerequest: received " + originating_contact.getrandomid() + " from " + originating_contact.getaddress()); system.out.println( "store request: keys=" + keys.length ); don't start accepting cache forwards until we have a good idea of our acceptable key space system.out.println( "not seeded" ); system.out.println( "verification fail" ); get the closest contacts to me system.out.println( "contact too far away - repeat" ); make sure the originator is in our group system.out.println( "contact too far away" ); make sure the key isn't too far away from us system.out.println( "key too far away" ); fortunately we can get away with this as diversifications are only taken note of by initial, single value stores and not by the multi-value cache forwards... parg: switched to live-only to reduce client lookup steps 2013/02/06 this helps both protect against idiot queries and also saved bytes when we use findnode to just get a random id prior to cache-forwards ignore ourselves when a new node is added we must check to see if we need to transfer any of our values to it. nothing to do, ping it if it isn't known to be alive see if we're one of the k closest to the new node optimise to avoid calculating for things obviously too far away time limit to pick up router changes caused by contacts being deleted system.out.println( "node added to router: id=" + byteformatter.encodestring( contact_id )); ok, we're close enough to worry about transferring values deleted in the meantime even if a result has been diversified we continue to maintain the base value set until the original publisher picks up the diversification (next publish period) and publishes to the correct place we don't consider any cached further away than the initial location, for transfer however, we do include ones we originate as, if we're the closest, we have to take responsibility for xfer (as others won't) if we're closest to the key, or the new node is closest and we're second closest, then we take responsibility for storing the value move to anti-spoof for cache forwards. we gotta do a findnode to update the contact's latest random id system.out.println( "nodeadded: pre-store findnode ok" ); system.out.println( "nodeadded: store ok" ); don't consider diversifications for node additions as they're not interested in getting values from us, they need to get them from nodes 'near' to the diversification targets or the originator system.out.println( "nodeadded: store failed" ); system.out.println( "nodeadded: pre-store findnode failed" ); finally transfer any key-blocks ensure that the key is close enough to us system.out.println( "nodeadded: pre-store findnode failed" ); profilers says l.size() is taking cpu (!) so put it into a variable this is safe since the list returned is system.out.println( "estimated dht size: " + size ); public method, trigger actual computation periodically with recent changes we pretty much have a router that only contains routeable contacts therefore the apparent size of the dht is less than real and we need to adjust by the routeable percentage to get an accurate figure current assumption is that around 50% are firewalled, so if less (at least during migration) assume unusable if called with contacts then this is in internal estimation based on lookup values 5 second limiter here algorithm works relative to a starting point in the id space so we grab the first here rather than using the initial lookup target can't estimate with less than 2 first entry should be us there's always us!!!! system.out.println( "getestimateddhtsize: " + sizes + "->" + dht_estimate + " (id=" + dhtlog.getstring2(id) + ",cont=" + (contacts==null?"null":(""+contacts.size())) + ",use=" + contacts_to_use ); ignore largest and smallest few values system.out.println( "estimatedhtsize: loc =" + local_dht_estimate + ", comb = " + combined_dht_estimate + " [" + remote_estimate_values.size() + "]"); during cache forwarding we get a lot of consecutive requests from the same contact so we can save cpu by caching the latest result and optimising for this system.out.println( "anti-spoof: cached " + existing + " for " + contact.getaddress() + " - total=" + spoof_gen_history.size()); system.out.println( "anti-spoof: generating " + res + " for " + contact.getaddress() + " - total=" + spoof_gen_history.size()); this comparator ensures that the closest to the key is first in the iterator traversal system.out.println( "activity added:" + activities.size()); system.out.println( "activity changed:" + activities.size()); system.out.println( "activity removed:" + activities.size());"
com.aelitis.azureus.core.dht.control.impl.DHTControlStatsImpl "averages db router"
com.aelitis.azureus.core.dht.db.DHTDB "local store @return remote store state internal lookup for locally originated values @return returns a value for the given key (local or remote) if found @return local remove - returns a value suitable for putting in the dht @return returns an iterator over hashwrapper values denoting the snapshot of keys thus by the time a key is used the entry may no longer exist @return"
com.aelitis.azureus.core.dht.db.DHTDBFactory ""
com.aelitis.azureus.core.dht.db.DHTDBLookupResult ""
com.aelitis.azureus.core.dht.db.DHTDBStats "returned values indexed by above vd_ constants for meaning @return total values locally stored data size directly stored to us by others indirectly (cache forwarded) stored diversifications caused by frequency diversifications caused by size"
com.aelitis.azureus.core.dht.db.DHTDBValue ""
com.aelitis.azureus.core.dht.db.impl.DHTDBImpl "private long store_ops; private long store_ops_bad1; private long store_ops_bad2; private void logstoreops() { system.out.println( "sops (" + control.gettransport().getnetwork() + ")=" + store_ops + ",bad1=" + store_ops_bad1 + ",bad2=" + store_ops_bad2 ); } iterator it2 = mapping.getindirectvalues(); system.out.println( "values=" + mapping.getvaluecount()); while( it2.hasnext()){ dhtdbvalueimpl val = it2.next(); system.out.println( new string( val.getvalue()) + " - " + val.getoriginator().getaddress()); } list contacts = control.getclosestkcontactslist( key, true ); for ( dhttransportcontact c: contacts ){ id_map.put( c.getid(), c ); } if ( mapping.getindirectsize() > 1000 ){ mapping.print(); } byte[] v = value.getvalue(); integer y = blah.get( v ); if ( y == null ){ blah.put( v, 1 ); }else{ blah.put( v, y+1 ); } long total_dup = 0; for ( byte[] k: blah.keys()){ int c = blah.get( k ); if ( c > 1 ){ total_dup += ( c k.length ); system.out.println( "dup: " + new string(k) + " -> " + c ); } } system.out.println( "total dup: " + total_dup ); its is non-trivial to do anything about nodes that get "close" to us and then spam us with crap. ultimately, of course, to take a key out you "just" create the 20 closest nodes to the key and then run nodes that swallow all registrations and return nothing. protecting against one or two such nodes that flood crap requires crap to be identified. tracing shows a large disparity between number of values registered per neighbour (factors of 100), so an approach based on number of registrations is non-trivial (assuming future scaling of the dht, what do we consider crap?) a further approach would be to query the claimed originators of values (obviously a low bandwith approach, e.g. query 3 values from the contact with highest number of forwarded values). this requires originators to support long term knowledge of what they've published (we don't want to blacklist a neighbour because an originator has deleted a value/been restarted). we also then have to consider how to deal with non-responses to queries (assuming an affirmative yes -> value has been forwarded correnctly, no -> probably crap). we can't treat non-replies as no. thus a bad neighbour only has to forward crap with originators that aren't az nodes (very easy to do!) to break this aproach. it2 = mapping.getindirectvalues(); while( it2.hasnext()){ dhtdbvalueimpl val = (dhtdbvalueimpl)it2.next(); dhttransportcontact sender = val.getsender(); hashwrapper hw = new hashwrapper( sender.getid()); integer sender_count = (integer)sender_map.get( hw ); if ( sender_count == null ){ sender_count = new integer(1); senders.add( sender ); }else{ sender_count = new integer( sender_count.intvalue() + 1 ); } sender_map.put( hw, sender_count ); } senders = control.sortcontactsbydistance( senders ); for (int i=0;i " + sender_map.get(new hashwrapper(sender.getid()))); } if ( !this_mon.isheld()){ debug.out( "monitor not held" ); } int actual_keys = stored_values.size(); int actual_values = 0; int actual_size = 0; iterator it = stored_values.values().iterator(); while( it.hasnext()){ dhtdbmapping mapping = (dhtdbmapping)it.next(); int reported_size = mapping.getlocalsize() + mapping.getdirectsize() + mapping.getindirectsize(); actual_values += mapping.getvaluecount(); iterator it2 = mapping.getvalues(); int sz = 0; while( it2.hasnext()){ dhtdbvalue val = (dhtdbvalue)it2.next(); sz += val.getvalue().length; } if ( sz != reported_size ){ debug.out( "reported mapping size != actual: " + reported_size + "/" + sz ); } actual_size += sz; } if ( actual_keys != total_keys ){ debug.out( "actual keys != total: " + actual_keys + "/" + total_keys ); } if ( adapter.getkeycount() != actual_keys ){ debug.out( "sm keys != total: " + actual_keys + "/" + adapter.getkeycount()); } if ( actual_values != total_values ){ debug.out( "actual values != total: " + actual_values + "/" + total_values ); } if ( actual_size != total_size ){ debug.out( "actual size != total: " + actual_size + "/" + total_size ); } if ( actual_values  all local get local remove for block requests sent to us (as opposed to being returned from other operations) make sure that the key is close enough to us we're republising the data, reset the creation time no point in worry about multi-value puts here as it is extremely unlikely that > 1 value will locally stored, or > 1 value will go to the same contact first refresh any leaves that have not performed at least one lookup in the last period assume that if we've diversified then the other k-1 locations are under similar stress and will have done likewise - no point in republishing cache values to them new nodes joining will have had stuff forwarded to them regardless of diversification status if this value was stored  list of keys to republish just use the closest contacts - if some have failed then they'll get flushed out by this operation. grabbing just the live ones is a bad idea as failures may rack up against the live ones due to network problems and kill them, leaving the dead ones! if we are no longer one of the k closest contacts then we shouldn't cache the value we carry on and do one last publish ignore ourselves move to anti-spoof on cache forwards - gotta do a find-node first to get the random id system.out.println( "cacheforward: pre-store findnode ok" ); we reduce the cache distance by 1 here as it is incremented by the recipients system.out.println( "cacheforward: pre-store findnode failed" ); ensure that the key is close enough to us ignore ourselves system.out.println( "nodeadded: pre-store findnode failed" ); distance 1 = initial store location. we use the initial creation date when deciding whether or not to remove this, plus a bit, as the original publisher is supposed to republish these scale the grace period for short lifetimes possible to have clashes, be consistent in which one we use to avoid confusing other nodes no point in worry about multi-value puts here as it is extremely unlikely that > 1 value will locally stored, or > 1 value will go to the same contact remove dead mappings obscure key so we don't leak any keys find the closest miss to us and recursively search if significant misses then re-query don't need to obscure here as its a node-id find closest nodes to this key in order to asses availability if we're not in the closest set to this key then ignore it remove ourselves from the equation here as we don't want to end up querying ourselves and we account for the replica we have later on build a list of requests to send to nodes to check their replicas prefix spread over multiple entries so ignore and just count suffix cost include new prefix, one byte prefix len, 2 bytes num-suffixes, then suffixes yes, this code should be elsewhere, but whatever header size remove entries that we know the contact already has and then add them back in in the query-reply. note that we still need to hit the contact if we end up with no values to query as need to ascertain liveness. we might want to wait until, say, 2 subsequent fails before treating contact as dead must match against our short-key mapping for consistency deleted one for local node + 1 for them us! ignore ipv6 for the moment... ignore common /16 s switch from age based to closest as per roxana's advice make it look like this target has the mapping as we don't want to store it there but we want to treat it as if it has it, effectively reducing availability but not skewing storage in favour of potentially malicious nodes bytearrayhashmap blah = new bytearrayhashmap(); ": " + data[1]); cvs dht can be significantly smaller than mainline (e.g. 1000) so will trigger un-necessary banning which then obviously affects the main dhts. so we disable banning for cvs delete their data on a separate thread so as not to interfere with the current action if we're not banning then rebuild bloom to avoid us continually going through this ban code assume a node stores 1000 values at 20 (k) locations -> 20,000 values assume a dht size of 100,000 nodes that is, on average, 1 value per 5 nodes assume nat of up to 30 ports per address this gives 6 values per address with a factor of 10 error this is still only 60 per address however, for cvs dhts we can have sizes of 1000 or less. allow up to 10% bloom filter utilisation obviously being spammed, drop all data originated by this ip and ban it map sender_map = new hashmap(); list senders = new arraylist(); logger.log( " adding " + val.getoriginator().getaddress()); survey our neighbourhood no persistent manager, just carry on incrementing system.out.println( "next chunk:" + next_value_version ); system.out.println( "next value version = " + res ); report before incrementing as this occurs before the key is locally added local lookup/put operations"
com.aelitis.azureus.core.dht.db.impl.DHTDBMapping "byte[] value_bytes = value.getvalue(); byte[] x = new byte[originator_id.length + value_bytes.length]; system.arraycopy( originator_id, 0, x, 0, originator_id.length ); system.arraycopy( value_bytes, 0, x, originator_id.length, value_bytes.length ); hashwrapper originator_value_id = new hashwrapper( new sha1hasher().calculatehash( x )); return( originator_value_id ); import org.gudy.azureus2.core3.util.sha1hasher; maps are access order, most recently used at tail, so we cycle values 4 bit filter - counts up to 15 pull out all the local values, reset the originator and then re-add them all values have 1) a key 2) a value 3) an originator (the contact who originally published it) 4) a sender (the contact who sent it, could be diff for caches) rethink time :p a) for a value where sender + originator are the same we store a single value b) where sender + originator differ we store an entry per originator/value pair as the send can legitimately forward multiple values but their originator should differ c) the code that adds values is responsible for not accepting values that are either to "far away" from our id, or that are cache-forwards from a contact "too far" away. for a given key c) we only allow up to 8 entries per sending ip address (excluding port) d) if multiple entries have the same value the value is only returned once e) only the originator can delete an entry a) prevents a single sender from filling up the mapping with garbage b) prevents the same key->value mapping being held multiple times when sent by different caches c) prevents multiple senders from same ip filling up, but supports multiple machines behind nat d) optimises responses. note that we can't trust the originator value in cache forwards, we therefore need to prevent someone from overwriting a valid originator->value1 mapping with an invalid originator->value2 mapping - that is we can't use uniqueness of originator a value can be "volatile" - this means that the cacher can ping the originator periodically and delete the value if it is dead the aim here is to 1) reduce ability for single contacts to spam the key while supporting up to 8 contacts on a given ip (assuming nat is being used) 2) stop one contact deleting or overwriting another contact's entry 3) support garbage collection for contacts that don't delete entries on exit todo: we should enforce a max-values-per-sender restriction to stop a sender from spamming lots of keys - however, for a small dht we need to be careful don't replace a closer cache value with a further away one. in particular we have to avoid the case where the original publisher of a key happens to be close to it and be asked by another node to cache it! direct contact from the originator is straight forward remove any indirect values we might already have for this not direct. if we have a value already for this originator then we drop the value as the originator originated one takes precedence rule (b) - one entry per originator/value pair system.out.println( " replacing existing" ); only add new values if not diversified relaxed this due to problems caused by multiple publishes by an originator with the same key but variant values (e.g. seed/peer counts). seeing as we only accept cache-forwards from contacts that are "close" enough to us to be performing such a forward, the dos possibilities here are limited (a nasty contact can only trash originator values for things it happens to be close to) our direct count includes local so remove that here zero length values imply deleted values so don't return them now update the access order so values get cycled local get slight chance of conc exception here, don't care local remove update store time as this means we don't need to republish as someone else has just done it its important to ignore old versions as a peer's increasing version sequence may have been reset and if this is the case we want the "future" values to timeout put the old value back! discard updates that are older than current value update store time as this means we don't need to republish as someone else has just done it put the old value back! vague backwards compatability - if the creation date of the "new" value is significantly less than the old then we ignore it (given that creation date is adjusted for time-skew you can see the problem with this approach...) put the old value back! we don't check for flooding on indirect stores as this could be used to force a direct store to be bounced (flood a node with indirect stores before the direct store occurs) system.out.println( "addtobloom: existing=" + ip_count_bloom_filter ); allow up to 10% bloom filter utilisation only do flood prevention on direct stores as we can't trust the originator details for indirect and this can be used to dos a direct store later logger.log( " adding " + val.getoriginator().getaddress()); remove before informing"
com.aelitis.azureus.core.dht.db.impl.DHTDBValueImpl "constructor for the originator of values only _creation_time _value _originator _sender _distance _flags constructor used to generate values for relaying to other contacts or receiving a value from another contact - adjusts the sender originator, creation time, flags and value are fixed. _sender _other we get quite a few zero length values - optimise mem usage make sure someone hasn't sent us a stupid creation time delete -> 0 length value"
com.aelitis.azureus.core.dht.DHT "default is high priority. if you change to low priority then do so consistently as operations can get out of order otherwise _priority returns value if originated from here for key @return _values externalises information that allows the dht to be re populate the dht with previously exported state @throws ioexception integrate the node into the dht can be invoked more than once if additional state is imported all property values are integer values local only local only diversification types, don't change as serialised!!!! 4 bits 1->14 republish hours; 0=vuze default | 4 bits 0->15 maintain replicas; [ff=no replication control-use default]"
com.aelitis.azureus.core.dht.DHTFactory ""
com.aelitis.azureus.core.dht.DHTListener ""
com.aelitis.azureus.core.dht.DHTLogger ""
com.aelitis.azureus.core.dht.DHTOperationAdapter ""
com.aelitis.azureus.core.dht.DHTOperationListener ""
com.aelitis.azureus.core.dht.DHTStorageAdapter "create a new storage key for a given key if the key shouldn't be allocated (e.g.out of space) local value operations local lookup/put operations"
com.aelitis.azureus.core.dht.DHTStorageBlock ""
com.aelitis.azureus.core.dht.DHTStorageKey ""
com.aelitis.azureus.core.dht.DHTStorageKeyStats ""
com.aelitis.azureus.core.dht.impl.DHTImpl ""
com.aelitis.azureus.core.dht.impl.DHTLog ""
com.aelitis.azureus.core.dht.nat.DHTNATPuncher "we're trying to run a rendezvous @return got a good running rendezvous @return input/output parameter for target of traversal _data @return"
com.aelitis.azureus.core.dht.nat.DHTNATPuncherAdapter ""
com.aelitis.azureus.core.dht.nat.DHTNATPuncherFactory ""
com.aelitis.azureus.core.dht.nat.DHTNATPuncherListener ""
com.aelitis.azureus.core.dht.nat.impl.DHTNATPuncherImpl "xxx: unused some routers only hold tunnel for 60s if you make this < 2 change code below! timeout see if the rendezvous has failed and therefore we are required to find a new one already running for the current local contact see if we've found a good one yet skip any known bad ones one's not null, worthwhile further investigation local has changed, remove existing publish only 2 attempts to start with here current_local == latest_local and neither is null! target changed, update publish only 2 attempts to start with denied access log(e); timeout most likely log(e); timeout most likely already present, no need to log again looks like it is failing, tell it to go away for a message payload (i.e. no_tunnel) we double the initiator timeout to give more chance for reasonable size messages to get through as they have to go through 2 xfer processes pick up port changes from the rendezvous ping the target a few times to try and establish a tunnel give the other end a few seconds to kick off some tunnel events to us routers often fiddle with the port when not mapped so we need to grab the right one to use for direct communication first priority goes to direct tunnel messages received ensure that we've received this from our current rendezvous node ping the origin a few times to try and establish a tunnel to do version"
com.aelitis.azureus.core.dht.netcoords.DHTNetworkPosition "number of bytes on wire @return .nan if no value available public static byte position_type_vivaldi_v2 = 3; // was 2 but serialisation format changed to include header and 5-dimensions public static byte position_type_vivaldi_v2 = 4; // staleness added another increment"
com.aelitis.azureus.core.dht.netcoords.DHTNetworkPositionListener ""
com.aelitis.azureus.core.dht.netcoords.DHTNetworkPositionManager "version version"
com.aelitis.azureus.core.dht.netcoords.DHTNetworkPositionProvider "returns a local, hopefully stable, network position for us or null if none available"
com.aelitis.azureus.core.dht.netcoords.DHTNetworkPositionProviderInstance ""
com.aelitis.azureus.core.dht.netcoords.DHTNetworkPositionProviderListener ""
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.Coordinates ""
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.impl.HeightCoordinatesImpl "the h. the x. the y. special vivaldi case, when u(0) = random unity vector"
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.impl.tests.VivaldiTest "init all main loop for each node : pick n random nodes system.out.println(position.getcoordinates());"
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.impl.tests.VivaldiVisualTest "init all main loop thread.sleep(100); todo: handle exception for each node : pick n random nodes rtt = (math.random() - 0.5)/20 + 1;"
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.impl.VivaldiPositionImpl "vivaldi papers : http://www.sigcomm.org/sigcomm2004/papers/p426-dabek111111.pdf not very interesting and occasionally happen... debug.out( "vivaldiposition: resetting as invalid: " + coordinates + "/" + error + " + " + rtt + "," + cj + "," + ej + "->" + new_coordinates + "/" + new_error ); system.out.println( "accepted vivaldi update:" + rtt + "/" + cj + "/" + ej ); ensure we have valid data in input (clock changes lead to crazy rtt values) sample weight balances local and remote error. (1) real error compute relative error of this sample. (2) update weighted moving average of local error. (3) update local coordinates. (4) system.out.println( "rejected vivaldi update:" + rtt + "/" + cj + "/" + ej ); 4 floats"
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.VivaldiPosition "controlling parameters serialisation stuff size of float-serialisation array size"
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.VivaldiPositionFactory ""
com.aelitis.azureus.core.dht.netcoords.vivaldi.ver1.VivaldiPositionProvider ""
com.aelitis.azureus.core.dht.router.DHTRouter "tells the router to perform its "start of day" functions required to integrate it into the dht (search for itself, refresh buckets) adds a contact to the router. the contact is not known to be alive (e.g. we've been returned the contact by someone but we've not either got a reply from it, nor has it invoked us. _id @return adds a contact to the router and marks it as "known to be alive" _id @return informs the router that an attempt to interact with the contact failed _id @return returns num_to_return or a few more closest contacts, unordered returns a list of best contacts in terms of uptime, best first @return returns a list of dhtroutercontact objects @return adds a routing table observer if it is not already observing. the observer to add true if now observing, false otherwise returns whether the given observer is already observing. the observer to query as observing true if observing, false otherwise removes the observer if it is already observing. the observer to remove true if no longer observing, false otherwise"
com.aelitis.azureus.core.dht.router.DHTRouterAdapter ""
com.aelitis.azureus.core.dht.router.DHTRouterContact "indicates whether or not a message has been received from, or an operation has successfully been made to, the contact. @return whether or not the contact has failed once or more since last alive (if ever) @return whether or not the contact's last interaction was successful @return time between first establishing the contact was alive and now, assuming that its not failing. 0 -> failing @return returns whether this router contact is in a bucket. @return true if in a bucket, false otherwise returns whether this router contact is a replacement. @return true if a replacement, false otherwise"
com.aelitis.azureus.core.dht.router.DHTRouterContactAttachment ""
com.aelitis.azureus.core.dht.router.DHTRouterFactory ""
com.aelitis.azureus.core.dht.router.DHTRouterFactoryObserver ""
com.aelitis.azureus.core.dht.router.DHTRouterObserver "observer interface to allow monitoring of contacts in the routing table. observer method invoked when a contact is added to the routing table. the added contact observer method invoked when a contact is removed from the routing table. the removed contact observer method invoked when a contact changes between a bucket entry and a replacement in the routing table. the contact that changed location observer method invoked when a contact is found to be alive. the contact now alive observer method invoked when a contact is found to be failing. the contact now failing router is not longer in use"
com.aelitis.azureus.core.dht.router.DHTRouterStats "returns number of nodes number of leaves number of contacts number of replacements number of live contacts number of unknown contacts number of dying contacts"
com.aelitis.azureus.core.dht.router.impl.DHTRouterContactImpl ""
com.aelitis.azureus.core.dht.router.impl.DHTRouterImpl "if ( consecutive_dead != 0 && consecutive_dead % 10 == 0 ){ system.out.println( "consecutive_dead: " + consecutive_dead ); } number of nodes number of leaves number of contacts number of replacements number of live contacts number of unknown contacts number of dying contacts only needed for in-process multi-router testing :p all incoming node actions come through either contactdead or addcontact a side effect of processing the node is that either a ping can be requested (if a replacement node is available and the router wants to check the liveness of an existing node) or a new node can be added (either directly to a node or indirectly via a replacement becoming "live" to avoid requesting these actions while synchronised these are recorded in lists and then kicked off separately here we should never become dead ourselves as this screws up things like checking that stored values are close enough to the k livest nodes (as if we are dead we don't return ourselves and it all goes doo daa ) some protection against network drop outs - start ignoring dead notifications if we're getting significant continous fails sleeping nodes are removed from the router as they're not generally available for doing stuff as we have reduced node id space the chance of us sharing a node id is higher. easiest way to handle this is just to bail out here keep non-important buckets less full when sleeping split if either 1) this list contains router_node_id or 2) depth % b is not 0 3) this is part of the smallest subtree note this will be true for 0 but other conditions will allow the split the smallest-subtree bit is to ensure that we remember all of our closest neighbours as ultimately they are the ones responsible for returning our identity to queries (due to binary choppery in general the query will home in on our neighbours before hitting us. it is therefore important that we keep ourselves live in their tree by refreshing. if we blindly chopped at k entries (down to b levels) then a highly unbalanced tree would result in us dropping some of them and therefore not refreshing them and therefore dropping out of their trees. there are also other benefits of maintaining this tree regarding stored value refresh note that it is rare for such an unbalanced tree. however, a possible dos here would be for a rogue node to deliberately try and create such a tree with a large number of entries. split!!!! we've todo: tidy up old smallest subtree - remember to factor in b... todo: tidy up old smallest subtree - remember to factor in b... not complete, retry addition split not appropriate, add as a replacemnet bucket space free, just add it complete - added to bucket find the num_to_return-ish closest nodes - consider all buckets, not just the closest add everything from the buckets - caller will sort and select the best ones as required use !failing at the moment to include unknown ones defer this a while to see how much refreshing is done by the normal dht traffic refresh all buckets apart from closest neighbour 0 -> don't check when seeding we don't do the smallest subtree and we also don't refresh the bucket containing the router id when seeding synchronous refresh may result in this bucket being split so we retest here to refresh sub-buckets as required pick a random id in the node's range. while we are synchronously refreshing the smallest subtree the tree can mutate underneath us as new contacts are discovered. we never merge things back together make sure we don't do the ping when synchronised make sure we don't do the addition when synchronised"
com.aelitis.azureus.core.dht.router.impl.DHTRouterNodeImpl "mgp: notify that node added to bucket we ping the oldest bucket entry only if we "improve" matters in the replacement if this replacement is known to be alive, replace any existing replacements that haven't been known to be alive mgp: kicking out a replacement that was never alive no unknown existing replacements but this is "newer" than the existing ones so replace the oldest one mgp: kicking out the oldest replacement replace old unknown ones with newer unknown ones mgp: kicking out an older replacement that was never alive no room, drop the contact mgp: notify observers contact added as a replacement don't ping ourselves or someone already being pinged mgp: will update observers of updated status in this method might be the same node but back after a restart. we need to treat this differently as we need to kick off the "store" events as required. if the new-id is zero this represents us hearing about a contact indirectly (imported or returned as a query). in this case we don't use this information as an indication of the target's instance identity because it isn't! if the instance id was 0, this means that it was unknown (e.g. contact imported). we still need to go ahead and treat as a new node check replacements as well mgp: will update observers of updated status in this method dhtlog.log( dhtlog.getstring( contact.getid()) + ": alive" ); record whether was alive mgp: notify observers that now alive mgp: simply reinserting, so do not notify observers that added to bucket mgp: notify observers that now alive this is a good time to probe the contacts as we know a replacement is alive and therefore in a position to replace a dead bucket entry. only do this if we haven't heard from this contact recently don't ping ourselves or someone already being pinged mgp: simply reinserting, so do not notify observers that added to replacments dhtlog.log( dhtlog.getstring( contact.getid()) + ": dead" ); record whether was failing check the contact is still present mgp: first notify observers that now failing mgp: notify that removed from bucket take most recent alive one and add to buckets mgp: notify that a replacement was promoted to the bucket non alive - just take most recently added mgp: notify that a replacement was promoted to the bucket add-node logic will ping the node if its not known to be alive mgp: first notify observers that now failing mgp: notify that removed from replacement list dos problem here - if a node deliberately flicked between instance ids we'll get into an update frenzy. only produce a warning if this is a definite change from one id to another (as opposed to a change from "unknown" to another) clock changed, don't know so make as large as possible"
com.aelitis.azureus.core.dht.router.impl.DHTRouterStatsImpl ""
com.aelitis.azureus.core.dht.speed.DHTSpeedTester ""
com.aelitis.azureus.core.dht.speed.DHTSpeedTesterContact "if you don't like the ping times from this contact you can always kill it, a new one will be"
com.aelitis.azureus.core.dht.speed.DHTSpeedTesterContactListener ""
com.aelitis.azureus.core.dht.speed.DHTSpeedTesterFactory ""
com.aelitis.azureus.core.dht.speed.DHTSpeedTesterListener ""
com.aelitis.azureus.core.dht.speed.impl.DHTSpeedTesterImpl "we try and keep three active pings running so we can spot overall trends in ping time each active ping is selected from the best rtt from the current 3 best three rtt estimates find best candidates system.out.println( " " + contact.getstring() + ": " + getelapsed() + ", " + contact.getvivaldiposition().estimatertt( dht.gettransport().getlocalcontact().getvivaldiposition().getcoordinates())); failing too often system.out.println( " " + contact.getstring() + ": failed" );"
com.aelitis.azureus.core.dht.transport.DHTTransport "gives access to the node id for this transport @return set the handler for incoming requests direct contact-contact communication"
com.aelitis.azureus.core.dht.transport.DHTTransportContact ""
com.aelitis.azureus.core.dht.transport.DHTTransportException ""
com.aelitis.azureus.core.dht.transport.DHTTransportFactory ""
com.aelitis.azureus.core.dht.transport.DHTTransportFindValueReply ""
com.aelitis.azureus.core.dht.transport.DHTTransportFullStats "db router transport totals averages"
com.aelitis.azureus.core.dht.transport.DHTTransportListener ""
com.aelitis.azureus.core.dht.transport.DHTTransportProgressListener ""
com.aelitis.azureus.core.dht.transport.DHTTransportQueryStoreReply ""
com.aelitis.azureus.core.dht.transport.DHTTransportReplyHandler ""
com.aelitis.azureus.core.dht.transport.DHTTransportReplyHandlerAdapter ""
com.aelitis.azureus.core.dht.transport.DHTTransportRequestHandler "mechanism for reporting that a contact has been imported"
com.aelitis.azureus.core.dht.transport.DHTTransportStats "returns pings sent, pings succeeded, pings failed, pings received @return -1 if stats not yet available @return aliens are indexed by these constants"
com.aelitis.azureus.core.dht.transport.DHTTransportStoreReply ""
com.aelitis.azureus.core.dht.transport.DHTTransportTransferHandler ""
com.aelitis.azureus.core.dht.transport.DHTTransportValue ""
com.aelitis.azureus.core.dht.transport.loopback.DHTTransportLoopbackContactImpl ""
com.aelitis.azureus.core.dht.transport.loopback.DHTTransportLoopbackImpl "transport ping stats store query store find node find value"
com.aelitis.azureus.core.dht.transport.loopback.DHTTransportLoopbackStatsImpl ""
com.aelitis.azureus.core.dht.transport.udp.DHTTransportUDP "min -> 17 somewhere min has gone to 22 cvs main min -> 50 refed from ddbase we can't fix the originator position until a previous fix regarding the incorrect use of a contact's version > sender's version is fixed. this will be done at 2.3.0.4 we can therefore only apply this fix after then introduced now (2403/v15) to support possible future change to id allocation if/when introduced the min dht version must be set to 15 at the same time nothing new here - added to we can track cvs another one to track fix to broken rep factor handling hopefully last one - needed to excluded nodes that don't support replication frequency ip and port based restrictions optional vivaldi flags field added to request and reply packets multiple networks reformats the requests and therefore needs the above fix to work current versions http://www.sharep2p.net/"
com.aelitis.azureus.core.dht.transport.udp.DHTTransportUDPContact ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTTransportUDPContactImpl "target supports a higher version than we thought, update gotta do anti-spoof"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTTransportUDPImpl "a node has reported that our external address and the one he's seen a message coming from differ. natural explanations are along the lines of 1) my address is dynamically allocated by my isp and it has changed 2) i have multiple network interfaces 3) there's some kind of proxy going on 4) this is a dos attempting to stuff me up we assume that our address won't change more frequently than once every 5 minutes we assume that if we can successfully obtain an external address by using the above explicit check then this is accurate only in the case where the above check fails do we believe the address that we've been told about int port = contact.getexternaladdress().getport(); try{ this_mon.enter(); int count; integer i = (integer)port_map.get(new integer(port)); if ( i != null ){ count = i.intvalue() + 1; }else{ count = 1; } port_map.put( new integer(port), new integer(count)); long now = systemtime.getcurrenttime(); if ( now - last_portmap_dump > 60000 ){ last_portmap_dump = now; iterator it = port_map.keyset().iterator(); map rev = new treemap(); while( it.hasnext()){ integer key = (integer)it.next(); integer val = (integer)port_map.get(key); rev.put( val, key ); } it = rev.keyset().iterator(); while( it.hasnext()){ integer val = (integer)it.next(); integer key = (integer)rev.get(val); system.out.println( "port:" + key + "->" + val ); } } }finally{ this_mon.exit(); } if ((int)(math.random() 4 )== 0 ){ system.out.println("dropping request packet:" + req.getstring()); return; } returns false if this isn't an error reply, true if it is and a retry can be performed, throws an exception otherwise @return private printwriter contact_log; private int contact_log_entries; private simpledateformat contact_log_format = new simpledateformat( "dd/mm/yyyy hh:mm:ss"); { contact_log_format.settimezone( timezone.gettimezone( "utc" )); } protected void log( dhttransportudpcontactimpl contact ) { if ( network == dht.nw_main ){ synchronized( this ){ try{ if ( contact_log == null ){ contact_log = new printwriter( new filewriter( new file( systemproperties.get 10 minute average bit more than 10 mins to allow average to establish dhtprudppacket relies on the request-handler being an instanceof this so watch out if you change it :) limit send and receive rates. receive rate is lower as we want a stricter limit on the max speed we generate packets than those we're willing to process. logger.log( "send delay = " + _dht_send_delay + ", recv = " + _dht_receive_delay ); only fiddle with the initial view of reachability when things have had time to stabilise system.out.println( "routables=" + other_routable_total + ", non=" + other_non_routable_total ); system.out.println( "net " + network + ": aliens = " + alien_average.getaverage() + ", alien fv = " + alien_fv_average.getaverage()); bootstrap node is special case and not generally routable externaladdresschange( c, new inetsocketaddress( "192.168.0.7", 6881 )); class level synchronisation is for testing purposes when running multiple udp instances in the same vm first attempt is via other contacts we know about. select three randomly select a number of entries to ping until we get three replies mismatch - give up dump addresses incompatible with our protocol reduce debug spam, just return throw( new dhttransportexception( "address " + new_address + " is incompatible with protocol family for " + external_address )); probably just be a second notification of an address change, return "ok to retry" as it should now work check for dodgy addresses that shouldn't appear as an external address! another situation to ignore is where the reported address is the same as the reporter (they must be seeing it via, say, socks connection on a local interface bump up min period for subsequent changes we need to perform this test on a separate thread otherwise we'll block in the udp handling code because we're already running on the "process" callback from the udp handler (the test attempts to ping contacts) address hasn't changed, notifier must be perceiving different address due to proxy or something only maintain stats on incoming requests so we get a fair sample. in general we'll only get replies from routable contacts so if we take this into account then everything gets skewed add current to average and reset if we have enough samples and no existing average then use current factor in current percantage -1 indicates we have no usable value instance id of 0 means "unknown" consider newly imported contacts as potential contacts for ip address queries if we've got space (in particular, at start of day we may be able to get an address off these if they're still alive ) logger.log( "imported contact " + contact.getstring()); protected hashmap port_map = new hashmap(); protected long last_portmap_dump = systemtime.getcurrenttime(); don't need to synchronize access to the bloom filter as it works fine without protection (especially as its add only) don't let an attacker deliberately fill up our filter so we start rejecting valid addresses stats request.setstatstype( dhtudppacketrequeststats.stats_type_np_ver2 ); currently no handler for new stats ping for deducing external ip address ping was ok so current address is ok store only report to caller the outcome of the first packet 1 for length marker all values from the current key have been processed no more keys allowed in this packet no more keys left, job done 1 for length marker no space left or we've used up our limit on the number of values permitted per key if last entry has no values then ignore it int packet_value_count = 0; packet_value_count++; system.out.println( " packet " + packet_count + ": keys = " + packet_entries + ", values = " + packet_value_count ); query store 1 byte prefix len, 2 byte num suffix copy out the random id in preparation for a possible subsequent store operation find node copy out the random id in preparation for a possible subsequent store operation scavenge any contacts here to help bootstrap process when ip wrong and no import history find value read request can get a lot of these on startup so we'll downgrade to just ignoring logger.log( "no transfer handler registered for key '" + byteformatter.encodestring(transfer_key) + "'" ); throw( new dhttransportexception( "no transfer handler registered for " + byteformatter.encodestring(transfer_key) )); special case 0 length data both requests and replies come through here. currently we only support read requests so we can safely use the data.length == 0 test to discriminate between a request and a reply to an existing transfer unmatched -> drop it unmatched -> drop it write request add the initial data for this write request set up the queue processor xfer complete, send ack if multi-packet xfer (ack already sent below if single packet) indicate that at least one packet has been received write transfer - data already on its way, no need to request it ignore overlaps see if we're done missing data, give up huzzah, we got the lot timeout, look for missing bits first packet only if we've has a reply bootstrap node returns details regardless of whether the originator id matches as the details will help the sender discover their correct id (hopefully) we need to patch the originator up otherwise we'll be populating our routing table with crap set originator address to transport as a bootstrap node we only accept find-node requests for the originator's id log( originating_contact ); let bad originators through to aid bootstrapping with bad ip if value too big, cram it in anyway won't fit, send what we've got continuation = true send the remaining (possible zero length) non-continuation values not interesting, send packet fail or something called when request received called before sending reply to request called before sending request called after receiving reply to request system.out.println( "request:" + contact.getaddress() + " = " + elapsed_time ); save current position of target update local positions unfortunately, to reuse the udp port with the tracker protocol we have to distinguish our connection ids by setting the msb. this allows the decode to work as there is no common header format for the request and reply packets note that tracker usage of udp via this handler is only for outbound messages, hence for that use a request will never be received by the handler just drop the packet see if this is the response to an outstanding call"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTTransportUDPStatsImpl ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacket ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketData "assume keys are 20 bytes + 1 len, data len is 2 bytes"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketHelper "these actions have to co-exist with the tracker ones when the connection is shared, hence 1024 most likely cause is dht packet ending up on the udp tracker as it'll get router here but with a null-handler we an get this after a port change and the old port listener is still running (e.g. its still doing udp tracker) most likely cause is dht packet ending up on the udp tracker as it'll get router here but with a null-handler we an get this after a port change and the old port listener is still running (e.g. its still doing udp tracker)"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReply "con id ver net instance flags system.out.println( "reply to " + _remote_contact.getaddress() + ", proto=" + protocol_version ); the target might be at a higher protocol version that us, so trim back if necessary as we obviously can't talk a higher version than what we are! system.out.println( "reply prot=" + protocol_version ); we can only get the correct transport after decoding the network... add to this and you need to adjust header_size above"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyError "file : prudppacketreplyconnect.java"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyFindNode "file : prudppacketreplyconnect.java"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyFindValue "file : prudppacketreplyconnect.java  values returned to a caller are adjusted by - skew"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyKeyBlock "file : prudppacketreplyconnect.java"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyPing "file : prudppacketreplyconnect.java"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyQueryStorage "file : prudppacketreplyconnect.java"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyStats "file : prudppacketreplyconnect.java"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketReplyStore "file : prudppacketreplyconnect.java"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequest "protocol version originator version network instance id time serialisation constructor system.out.println( "request to " + _remote_contact.getaddress() + ", proto=" + protocol_version ); the target might be at a higher protocol version that us, so trim back if necessary as we obviously can't talk a higher version than what we are! deserialisation constructor system.out.println( "request received prot=" + protocol_version ); we can only get the correct transport after decoding the network... this should be set correctly in the post-deserialise code, however default it for now we maintain a rough view of the clock diff between them and us, times are then normalised appropriately. if the skew is positive then this means our clock is ahead of their clock. thus any times they send us will need to have the skew added in so that they're correct relative to us. for example: x has clock = 01:00, they create a value that expires at x+8 hours 09:00. they send x to us. our clock is an hour ahead (skew=+1hr) we receive it at 02:00 (our time) and therefore time it out an hour early. we therefore need to adjust the creation time to be 02:00. likewise, when we return a time to a caller we need to adjust by - skew to put the time into their frame of reference. if the originator is a higher version than us then we can't do anything sensible working at their version (e.g. we can't reply to them using that version). therefore trim their perceived version back to something we can deal with add to this and you need to amend header_size above originator version originator version is at tail so it works with older versions"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequestFindNode ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequestFindValue ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequestKeyBlock ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequestPing ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequestQueryStorage "add anything here be sure to adjust the space above"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequestStats ""
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPPacketRequestStore "1 byte dhtudppacket.packet_max_bytes / 20; 1 byte dhtudppacket.packet_max_bytes / dhtudputils.dhttransportvalue_size_without_value; times receieved are adjusted by + skew"
com.aelitis.azureus.core.dht.transport.udp.impl.DHTUDPUtils "debug.out( "address '" + address + "' is unresolved" ); final long k0 = long.max_value; final long k1 = long.max_value; restrictions suggested by uow researchers as effective at reducing sybil opportunity but not so restrictive as to impact dht performance % k1; % k0; stick with existing approach for ipv6 at the moment more draconian limit, analysis shows that of 500,000 node addresses only 0.01% had >= 8 ports active. ( 1% had 2 ports, 0.1% 3) having > 1 node with the same id doesn't actually cause too much grief limit range to around 2000 (1999 is prime) system.out.println( "nodeid: " + address + " -> " + dhtlog.getstring( res )); 16 (pv6) + 1 length 19 1 2 2 + address we don't transport instance ids around via this route as they are just cached versions and not useful system.out.println( "read: version = " + version ); int distance = is.readint(); system.out.println( "read:" + distance ); system.out.println( " adjusted creation time by " + skew ); system.out.println( "write: version = " + version ); system.out.println( "write: 0" ); don't forget to change the constant above if you change the size of this! 12 12+2+x 12 + 2+x + contact 13 + 2+ x + contact 14 + 2+ x + contact 15 + 2+ x + contact not much we can do here to recover - shouldn't fail anyways need to add one in for backward compatability dead code these days dead code these days router averages"
com.aelitis.azureus.core.dht.transport.udp.impl.packethandler.DHTUDPPacketHandler "system.out.println( "bloom rotate: entries = " + bloom1.getentrycount() + "/" + bloom2.getentrycount()); send and receive pair one way send (no matching reply expected ) send reply to a request outgoing request incoming request an alien request is one that originates from a peer that we haven't recently talked to avoid counting consecutive requests from same contact more than once todo: hmm"
com.aelitis.azureus.core.dht.transport.udp.impl.packethandler.DHTUDPPacketHandlerException ""
com.aelitis.azureus.core.dht.transport.udp.impl.packethandler.DHTUDPPacketHandlerFactory "if ( network != 0 ){ system.out.println( "process:" + network + ":" + request.getstring()); }"
com.aelitis.azureus.core.dht.transport.udp.impl.packethandler.DHTUDPPacketHandlerStats "update access"
com.aelitis.azureus.core.dht.transport.udp.impl.packethandler.DHTUDPPacketNetworkHandler ""
com.aelitis.azureus.core.dht.transport.udp.impl.packethandler.DHTUDPPacketReceiver ""
com.aelitis.azureus.core.dht.transport.udp.impl.packethandler.DHTUDPRequestHandler ""
com.aelitis.azureus.core.dht.transport.util.DHTTransportRequestCounter ""
com.aelitis.azureus.core.dht.transport.util.DHTTransportStatsImpl "ping key blocks store queries find node find value store stats data system.out.println( "alien on net " + request.getnetwork() + " - sender=" + request.getaddress()); system.out.println( "skipping skew: " + originator_address ); system.out.println( "adding skew: " + originator_address + "/" + skew ); no sync here as not important so ensure things work ok remove outliers"
com.aelitis.azureus.core.diskmanager.access.DiskAccessController ""
com.aelitis.azureus.core.diskmanager.access.DiskAccessControllerFactory ""
com.aelitis.azureus.core.diskmanager.access.DiskAccessControllerStats ""
com.aelitis.azureus.core.diskmanager.access.DiskAccessRequest ""
com.aelitis.azureus.core.diskmanager.access.DiskAccessRequestListener "called to indicate that an actual request operation occurred. if this request has been aggregated with others then the byted reported will be for the contiguous region and subsequent aggregated requests will be reported with 0 bytes"
com.aelitis.azureus.core.diskmanager.access.impl.DiskAccessControllerImpl "read write system.out.println( "write request: " + offset );"
com.aelitis.azureus.core.diskmanager.access.impl.DiskAccessControllerInstance "system.out.println( "aggregated read: requests=" + aggregated.size() + ", size=" + aggregated_bytes + ", a_reqs=" + requests.size() + ", f_reqs=" + file_map.size()); if this request is bigger than the max allowed queueable then easiest approach is to bump up the limit system.out.println( "dac:" + name + ": requests = " + requests_queued ); system.out.println( "dac:" + name + ": bytes = " + request_bytes_queued ); let recursive calls straight through stats not synced on the right object, but they're only stats... long io_start = systemtime.gethighprecisioncounter(); actually, for recursive calls the time of this request will be included in the timing of the call resulting in the recursion long io_end = systemtime.gethighprecisioncounter(); io_time += ( io_end - io_start ); check for and discard manky old files from stopped/removed downloads system.out.println( "request queue: req = " + requests.size() + ", bytes = " + request_bytes_queued ); requests monitor held it is possible for the file_map to be null here due to the fact that the entries can be zero sized even though requests for the file are outstanding (as we key on non-unique request.offset) doesn't matter if we remove from this and don't end up using it semaphore should already be > 0 as we've removed an element... for fairness we only return immediately if we can and there are no waiters no waiters we just increment the value otherwise we share num out amongst the waiters in order we've got enough now to release this waiter if we have any left over then save it"
com.aelitis.azureus.core.diskmanager.access.impl.DiskAccessRequestImpl "system.out.println( "diskreq:" + thread.currentthread().getname() + ": " + op + " - " + offset ); assumption - they are all for the same file, sequential offsets and aggregatable, not cancelled"
com.aelitis.azureus.core.diskmanager.cache.CacheFile "writes the block to the cache and gives control of the buffer to the cache. @throws cachefilemanagerexception write failed and buffer not taken - i.e. caller must de-allocate flushes the cache to disk but retains entries @throws cachefilemanagerexception flushes the cache and discards entries @throws cachefilemanagerexception"
com.aelitis.azureus.core.diskmanager.cache.CacheFileManager ""
com.aelitis.azureus.core.diskmanager.cache.CacheFileManagerException ""
com.aelitis.azureus.core.diskmanager.cache.CacheFileManagerFactory ""
com.aelitis.azureus.core.diskmanager.cache.CacheFileManagerStats "returns the number of bytes in the requested range that are in cache"
com.aelitis.azureus.core.diskmanager.cache.CacheFileOwner ""
com.aelitis.azureus.core.diskmanager.cache.impl.CacheEntry "constructs a dummy cache entry used to search in a set"
com.aelitis.azureus.core.diskmanager.cache.impl.CacheFileManagerImpl "allocates space but does not add it to the cache list due to synchronization issues. basically the caller mustn't hold their monitor when calling allocate, as a flush may result in one or more other files being flushed which results in their monitor being taken, and we've got an a->b and b->a classic deadlock situation. however, we must keep the file's cache and our cache in step. it is not acceptable to have an entry inserted into our records but not in the file's as this then screws up the flush algorithm (which assumes that if it finds an entry in our list, a flush of that file is guaranteed to release space). therefore we add the cache entry in addcachespace so that the caller can safely do this while synchronised firstly on its monitor and then we can sync on our. hence we only ever get a->b monitor grabs which won't deadlock _position @return @throws cachefilemanagerexception if ( entry.gettype() == cacheentry.ct_read_ahead ){ if ( entry.getusagecount()  no cache entry never found a matching torrentfile grab a copy to avoid potential deadlock as we never take the manager monitor and then the file's own monitor, always the other way around"
com.aelitis.azureus.core.diskmanager.cache.impl.CacheFileManagerStatsImpl "average over 10 seconds file writes are bursty so use a lower average time cache read cache write file read file write"
com.aelitis.azureus.core.diskmanager.cache.impl.CacheFileWithCache "make code prettier by bringing over ss_cache from directbytebuffer entries in the cache should never overlap lazy allocation give changes made to read ahead size a chance to work through the stats before recalculating see if we need to adjust the read-ahead size if used average > 75% of made average then increase no bigger than a piece no bigger than the fixed max size no bigger than a 16th of the cache, in case its really small (e.g. 1m) no smaller than the min system.out.println( "read-ahead: done = " + read_ahead_bytes_made + ", used = " + read_ahead_bytes_used + ", done_av = " + read_ahead_made_average.getaverage() + ", used_av = " + read_ahead_used_average.getaverage()+ ", size = " + current_read_ahead_size ); nothing to do if we can totally satisfy the read from the cache, then use it otherwise flush the cache (not so smart here to only read missing) record the position of the byte following the end of this read data missing at the start of the read section not got there yet copy required amount into read buffer only record this as a cache read hit if we haven't just read the data from the file system reset in case we've done some partial reads if read-ahead fails then we resort to a straight read read-ahead can fail if a cache-flush fails (e.g. out of disk space on a file belonging to a different torrent than this. we don't want such a failure to break this read operation first time round only read ahead if this is a continuation of a prior read within history don't read ahead over the end of a piece system.out.println( "request offset = " + request_piece_offset ); no point in using read-ahead logic if actual read ahead smaller or same as request size! system.out.println( " trimmed to " + data_left ); must allocate space outside sync block (see manager for details) flush before read so that any bits in cache get re-read correctly on read if the read operation failed, and hence the buffer wasn't added to the cache, then release it here recursively read from the cache, should hit the data we just read although there is the possibility that it could be flushed before then - hence the recursion flag that will avoid this happening next time around nothing to do if the data is smaller than a piece and not handed over then it is most likely apart of a piece at the start or end of a file. if so, copy it and insert the copy into cache make it look like this buffer has been handed over cache this write, allocate outside sync block (see manager for details) if we are overwriting stuff already in the cache then force-write overlapped data (easiest solution as this should only occur on hash-fails) do the flush and add sychronized to avoid possibility of another thread getting in-between and adding same block thus causing mutiple entries for same space not handed over, invalidate any cache that exists for the area as it is now out of date -1 -> do all from position onwards -1 -> all dirty entries newer than this won't be flushed 0 -> now minimum contiguous size for flushing, -1 -> no limit make sure we release the offending buffer entries otherwise they'll hang around in memory causing grief when the next attempt it made to flush them... -1 -> do all from position onwards -1 -> all dirty entries newer than this won't be flushed 0 -> now minimum contiguous size for flushing, -1 -> no limit to the left to the right, give up overlap!!!! we're going to deal with this entry one way or another. in particular if we are releasing entries then this is guaranteed to be released, either directly or via a flush if dirty start of day continuation, add in we've got a gap - flush current and start another series set up ready for next block in case the flush fails - we try and flush as much as possible in the face of failure if it is dirty it will be released when the flush is done if this entry needs flushing this is done outside the loop sanitity check - we should always be flushing entire entries this is the flush method used by the public methods directly (as opposed to those use when reading, writing etc) and it is the place that pending exceptions are checked for. we don't want to check for this in the internal logic for flushing as we need to be able to flush from files that have a pending error to clear the cache state chunk might span file boundaries iterator it = cache.iterator(); the following check ensures that we are within the interesting region skip forward until we reach a chunk perform skipping from the previous round chunk completely falls into cache entry -> don't invalidate chunk spans multiple cache entries and we skipped -> invalidate end of a spanning chunk -> don't invalidate we fell through the loop but there's still cleanup to do support methods public methods not sure of the difference between "size" and "length" here. old code used to use "size" so i'm going to carry on for the moment in case there is some weirdness here bug found here with "incremental creation" failing with lots of hash fails. caused by the reported length not taking into account the cache entries that have yet to be flushed. last entry is furthest down the file we can optimise this if the file's already big enough as cache entries can only make it bigger flush in case length change will invalidate cache data (unlikely but possible) we've got to always close the file here, even if the flush fails we're already on our way out via exception, no need to throw a new one"
com.aelitis.azureus.core.diskmanager.cache.impl.CacheFileWithoutCache "system.out.println( "without cache = " + file.getfile().tostring());"
com.aelitis.azureus.core.diskmanager.cache.impl.CacheFileWithoutCacheMT "system.out.println( "destroyed clone " + file.getname()); system.out.println( "destroyed clone " + file.getname()); all files already in use system.out.println( " system.out.println( "file clones=" + num_clones ); system.out.println( "clone depth of " + new_num + " for " + clone.getname()); system.out.println( "destroyed clone " + file.getname());"
com.aelitis.azureus.core.diskmanager.file.FMFile "file : fmfile.java"
com.aelitis.azureus.core.diskmanager.file.FMFileManager "file : fmfilemanager.java"
com.aelitis.azureus.core.diskmanager.file.FMFileManagerException "file : fmfilemanagerexception.java"
com.aelitis.azureus.core.diskmanager.file.FMFileManagerFactory "file : filemanagerfactory.java"
com.aelitis.azureus.core.diskmanager.file.FMFileOwner "file : fmfileowner.java"
com.aelitis.azureus.core.diskmanager.file.impl.FMFileAccess ""
com.aelitis.azureus.core.diskmanager.file.impl.FMFileAccessCompact "system.out.println( "file " + new string(torrent_file.getpathcomponents()[0]) + ": " + "off = " + file_offset_in_torrent + ", len = " + file_length + ", fp = " + first_piece_start + "/" + first_piece_length + ", lp = " + last_piece_start + "/" + last_piece_length ); first piece takes up all the file system.out.println( "compact: read - " + position + "/" + len ); deal with any read access to the first piece all they require is in the first piece system.out.println( " all in first piece" ); read goes past end of first piece system.out.println( " part in first piece" ); position is at start of gap between start and end - work out how much, if any, space has been requested all they require is space system.out.println( " all in space" ); read goes past end of space system.out.println( " part in space" ); lastly read from last piece system.out.println( " some in last piece" ); system.out.println( "compact: write - " + position + "/" + len ); deal with any write access to the first piece all they require is in the first piece system.out.println( " all in first piece" ); write goes past end of first piece system.out.println( " part of first piece" ); position is at start of gap between start and end - work out how much, if any, space has been requested system.out.println( " all in space" ); all they require is space write goes past end of space system.out.println( " part in space" ); lastly write to last piece system.out.println( " some in last piece" );"
com.aelitis.azureus.core.diskmanager.file.impl.FMFileAccessController "actual file shouldn't exist for change to occur - it is the responsibility of the caller to delete the file first and take consequent actions (in particular force recheck the file to ensure that the loss in save state is represented in the resume view of the world ) in the future, if we support format conversion, this obviously changes debug.out( "no control file" ); in optimised environments we don't support compact and return null here these two access modes are in fact identical at the moment due to the simplistic implementation of compact we only actually need to deal with the last piece of the file (first piece is in the right place already) see if we have any potential data for the last piece see if we need to truncate no last piece, truncate after the first piece override original exception if there isn't one conversion failed - replace with linear access, caller is responsible for handling this (marking file requiring recheck) fileaccess"
com.aelitis.azureus.core.diskmanager.file.impl.FMFileAccessLinear "we sometimes read off the end of the file (when rechecking) so bail out if we've completed the read or got to file end a "better" fix would be to prevent the over-read in the first place, but hey, we're just about to release and there may be other instances of this... no state to flush"
com.aelitis.azureus.core.diskmanager.file.impl.FMFileAccessPieceReorderer "idea is to grow the file as needed on a piece-write basis each file in general starts with a part of a piece and then is optionally followed by zero or more complete pieces and ends with an option part of a piece. the first part-piece of the file is always stored in position. whenever we receive a write request we calculate which piece number(s) it affects if we have already allocated piece sized chunks for the pieces then we simply write to the relevant part of the file if we haven't then we allocate new piece size chunks at file end and record their position in the control file. if it now turns out that we have allocated the space required for a piece previously completed then we copy that piece data into the new block and reuse the space it has been copied from for the new chunk when allocating space for the last part-piece we allocate an entire piece sized chunk and trim later whenever a piece is marked as complete we look up its location. if the required piece of the file has already been allocated (and its not alread in the right place) then we swap the piece data at that location with the current piece's. if the file chunk hasn't been allocated yet then we leave the piece where it is - it'll be moved later. if the control file is lost then there is an opportunity to recover completed pieces by hashing all of the allocated chunks and checking the sha1 results with the file's piece hashes. however, this would require the addition of further interfaces etc to integrate somehow with the existing force-recheck functionality... obviously the setlength/getlength calls just have to be consistent, they don't actually modify the length of the physical file conversion between storage formats is another possibility to consider - conversion from this to linear can fairly easily be done here as it just needs pieces to be written to their correct locations. conversion to this format can't be done here as we don't know which pieces and blocks contain valid data. i guess such details could be added to the setstoragetype call as a further parameter first piece fixed at file start so need 3 to do anything worthwhile ensure control file exists as this marks the file as piece-reordered always do this, even for " + store_index ); most likely add-for-seeding which means a recheck will occur. just map all existing pieces to their correct positions and let the recheck sort things out"
com.aelitis.azureus.core.diskmanager.file.impl.FMFileImpl "file : fmfileimpl.java  for (int i= "rwd"; - removing this to improve performance if there is an exception that occurs, which causes us to try and perform a reopen, setting this flag to true will print it to debug. "new_canonical_path temporarily set to [" +abs_path+ "]"; full close, this will release any slots in the limited file case ensure open will regain slots in limited file case "new_canonical_path temporarily set to [" +abs_path+ "]"; full close, this will release any slots in the limited file case ensure open will regain slots in limited file case ignore any close failure as can't do much don't clear down raf here as we want to leave things looking as they were if the subsequent open fails may have previously been implicitly closed, tidy up if required file reservation is used to manage the possibility of multiple torrents refering to the same file. initially introduced to stop a common problem whereby different torrents contain the same files - without this code the torrents could interfere resulting in all sorts of problems the original behavior was to completely prevent the sharing of files. however, better behaviour is to allow sharing of a file as long as only read access is required. we store a list of owners against each canonical file with a boolean "write" marker system.out.println( "fmfile::reservefile:" + canonical_path + "("+ owner.getname() + ")" + " - " + debug.getcompressedstacktrace() ); system.out.println( " creating new owners entr" ); system.out.println( " existing entry: " + entry_name ); already present, start off read-access system.out.println( "fmfile::reserveaccess:" + canonical_path + "("+ owner.getname() + ")" + " [" + (access_mode==fm_write?"write":"read") + "]" + " - " + debug.getcompressedstacktrace()); system.out.println( " existing entry: " + entry_name ); relax locking if strict is disabled and torrent file is same size system.out.println( "fmfile::releasefile:" + canonical_path + "("+ owner.getname() + ")" + " - " + debug.getcompressedstacktrace()); delete any dirs we system.out.println( "deleted " + dir );"
com.aelitis.azureus.core.diskmanager.file.impl.FMFileLimited "file : fmfilemanagerlimited.java  import java.nio.bytebuffer; switching mode closes the file..."
com.aelitis.azureus.core.diskmanager.file.impl.FMFileManagerImpl "file : fmfilemanagerimpl.java  access order selected - this means oldest system.out.println( "setlink:" + source + " -> " + target ); system.out.println( "getlink:" + file + " -> " + res ); must close the oldest file outside sync block else we'll get possible deadlock only update if already present - might not be due to delay in closing files update mru"
com.aelitis.azureus.core.diskmanager.file.impl.FMFileTestImpl ""
com.aelitis.azureus.core.diskmanager.file.impl.FMFileUnlimited "file : fmfileunlimited.java  switching mode closes the file..."
com.aelitis.azureus.core.diskmanager.MemoryMappedFile "this class implements a virtual disk file backed by a pool of direct memory mappedbytebuffers, designed for high-speed random reads/writes. note: abandoning this code for now, as jre 1.4 series does not provide an unmap() method for mappedbytebuffers, so we have to wait for the vm to lazily free the underlying direct memory regions, which means we eventually run out of direct memory when accessing files larger than the allowed direct memory space, since the unused buffers are not freed quickly enough. forcing the memory unmap via sun.misc.cleaner does not work properly under windows, throwing the error described in the clean(x) method below. 10mb try grab the buffer from the pool check if the current buff is too small need to extend the key array create new buffer, fully-sized, unless it's extending the file do the write add the buffer (back) to the pool 100mb this method is called just after a new entry has been added may return null despite a previously-valid key, because the buffer was gc'd note: the buffer is removed from the pool manually cleaning the buffer doesn't work properly under windows: java.lang.error: cleaner terminated abnormally: caused by: java.io.ioexception: the process cannot access the file because another process has locked a portion of the file see sun bug id #4938372. method getcleanermethod = buffer.getclass().getmethod( "cleaner", new class[0] ); getcleanermethod.setaccessible( true ); sun.misc.cleaner cleaner = (sun.misc.cleaner)getcleanermethod.invoke( buffer, new object[0] ); cleaner.clean();"
com.aelitis.azureus.core.diskmanager.test.MemoryMappedFileTester "static long max_size = 23710241024; ///////////////////////////////////////////////////// raf.seek( start_pos ); raf.write( raw ); ///////////////////////////////////////////////////// fc.write( dbb.buff, start_pos ); ///////////////////////////////////////////////////// /////////////////////////////////////////////////////"
com.aelitis.azureus.core.download.DiskManagerFileInfoDelegate ""
com.aelitis.azureus.core.download.DiskManagerFileInfoFile ""
com.aelitis.azureus.core.download.DiskManagerFileInfoStream "length can be -1 here meaning 'to the end'"
com.aelitis.azureus.core.download.DiskManagerFileInfoURL "length can be -1 here meaning 'to the end' system.out.println( "connecting to " + url + ": " + thread.currentthread().getid()); allow for certs that contain ip addresses rather than dns names auto redirect doesn't work from http to https or vice-versa try again with original url certificate has been installed retry with new certificate don't need another ssl loop system.out.println( "done to " + url + ": " + thread.currentthread().getid() + ", outcome=" + outcome );"
com.aelitis.azureus.core.download.DownloadManagerEnhancer "don't auto-add to download_map. getenhanceddownload will take care of it later if we ever need the download resume any downloads we paused ensure we have an enhanced download object for it if its complete then obviously 0 listener to pick up on streams kicked off externally"
com.aelitis.azureus.core.download.EnhancedDownloadManager "number of seconds of buffer required before we fall back to normal bt mode check existing downloading torrents and turn off any existing progressive/downloading make sure download can start by moving out of stop state and putting at top might need to re-enable the buffer provider if speeds change not bothered about times here but need to use large increments to ensure that pieces are picked in order even for slower peers gives less weight to incomplete pieces hack in some test values for torrents that don't have a bps in them yet bump it up by a bit to be conservative to deal with fluctuations, discards etc. the file channel provider will try best-effort-rta based which will result in high discard - back it off based on how much slack we have don't be too aggresive with small buffers ok as initial dl is forced in order byte buffer-rta"
com.aelitis.azureus.core.download.EnhancedDownloadManagerFile ""
com.aelitis.azureus.core.download.StreamManager "need win or osx 10.5+ can't use if xcode borked otherwise xcode will be installed on demand will be installed on demand can't use if emp borked emp installed but need version with preparewindow, wait for update"
com.aelitis.azureus.core.download.StreamManagerDownload ""
com.aelitis.azureus.core.download.StreamManagerDownloadListener ""
com.aelitis.azureus.core.drivedetector.DriveDetectedInfo ""
com.aelitis.azureus.core.drivedetector.DriveDetectedListener ""
com.aelitis.azureus.core.drivedetector.DriveDetector ""
com.aelitis.azureus.core.drivedetector.DriveDetectorFactory ""
com.aelitis.azureus.core.drivedetector.impl.DriveDetectedInfoImpl ""
com.aelitis.azureus.core.drivedetector.impl.DriveDetectorImpl "already added, skip trigger already there, no trigger not there, no trigger @see org.gudy.azureus2.core3.util.aediagnosticsevidencegenerator#generate(org.gudy.azureus2.core3.util.indentwriter)"
com.aelitis.azureus.core.helpers.TorrentFolderWatcher "watches a folder for new torrents and imports them. note: folder-to-watch and other watching params are taken from a global config option right now, so starting multiple instances of torrentfolderwatcher is useless as currently coded. start a folder watcher, which will auto-import torrents via the given manager. _manager stop and terminate this folder importer watcher. path is not an existing directory. if path is still not a directory, abort. if we get here, and this is true, then data_save_path isn't valid. if we are saving torrents to the same location as we import them from then we can't assume that its safe to delete the torrent after import! delete torrents from the previous import run make sure we've got a valid torrent file before proceeding we can't touch the torrent file as it is (probably) being used for the download add torrent for deletion, since there will be a saved copy elsewhere"
com.aelitis.azureus.core.impl.AzureusCoreImpl "listeners that will be fired after core has completed initialization test to see if ui plays nicely with a really slow initialization set up a backwards pointer from config -> app dir so we can derive one from the other. it'll get saved on closedown, no need to do so now ensure early initialization used to be a plugin, but not any more... one off explicit gc to clear up initialisation mem if a not using localised text - not sure it's safe to this early. run plugin loading in parallel to the global manager loading disable async loading of existing torrents, because there are many things (like hosting) that require all the torrents to be loaded. while we can write code for each of these cases to wait until the torrents are loaded, it's a pretty big job to find them all and fix all their quirks. too big of a job for this late in the release stage. other example is the tracker plugin that is coded in a way where it must always publish a complete rss feed wait until plugin loading is done ignore event if nothing usable trigger listeners now that core is started typicially there are many runninglisteners, most with quick execution, and a few longer ones. let 3 run at a time, queue the rest. without a threadpool, the slow ones would delay the startup processes that run after this start() method debug.out("core start complete"); ensure config is saved as there may be pending changes to persist and we've got here via a shutdown hook might have been marked dirty due to core being in case something hangs during listener notification (e.g. version check server is down and the instance manager tries to obtain external address) we limit overall dispatch time to 10 seconds shut down diags - this marks the shutdown as tidy and saves the config if any installers exist then we need to closedown via the updater best we can do here is wait a while for updates to be applied don't test for running here, the restart process calls this after terminating the core... its effectively seeding, change so logic about recheck obeyed below wait until recheck is complete before we mark as downloading-complete if anything's paused we don't want to trigger any actions as something transient (e.g. speed test) is going on its effectively seeding, change so logic about recheck obeyed below wait until recheck is complete before we mark as downloading-complete prevent retriggering on resume from standby quit vuze -> quit shutdown computer -> quit vuze + shutdown sleep/hibernate = announceall and then sleep/hibernate with vuze still running don't catch exceptions here as we want errors from task execution to propagate back to the invoker nobody volunteeered to run it for us, we'd better do it"
com.aelitis.azureus.core.impl.AzureusCoreSingleInstanceClient "single instance management is a bit of a mess. for some reason the uis have their own implementations of clients and servers. we also have a more generic plugin-accessible single instance service that can be used by launchable plugins but don't give a generic mechanism for dealing with the basic mechanism used by the uis (that run on 6880). i have introduced this class to give a programmatic way of passing arguments using the existing 6880 port. perhaps one day the various ui implementations will be rewritten to use this... limit the subset here as we're looping waiting for something to be alive and we can't afford to take ages getting back to the start"
com.aelitis.azureus.core.instancemanager.AZInstance ""
com.aelitis.azureus.core.instancemanager.AZInstanceManager ""
com.aelitis.azureus.core.instancemanager.AZInstanceManagerAdapter ""
com.aelitis.azureus.core.instancemanager.AZInstanceManagerFactory ""
com.aelitis.azureus.core.instancemanager.AZInstanceManagerListener ""
com.aelitis.azureus.core.instancemanager.AZInstanceTracked ""
com.aelitis.azureus.core.instancemanager.impl.AZInstanceImpl ""
com.aelitis.azureus.core.instancemanager.impl.AZInstanceManagerImpl "239.255.000.000-239.255.255.255  pick up our own details as soon as we can system.out.println( "received result: " + st + "/" + al ); not the most efficient code in the world this... will need rev system.out.println( "modaddress: " + key + " -> " + value + " - " + (add?"add":"remove")); if not yet initialised then we'll send out our details in a mo anyway. plus we need to wait for init to occur to ensure dht plugin initialised before trying to get external address take this off the current thread as there are potential deadlock issues regarding this during initialisation as sending the event attempts to get the external address, this may hit dht and the current thread maybe initialising the dht..."
com.aelitis.azureus.core.instancemanager.impl.AZMyInstanceImpl "no point in kicking off any queries if we're closing if dht has informed us of an address then we use this - most reliable up to date one unless the version server cache time is more recent use cached version if available and the dht isn't no cache, use dht (this will hang during initialisation, hence the use of cached version above ignore any v6 addresses from dht try upnp - limit frequency unless external read is possible in which case we try upnp first currently we only use upnp to validate our current external address, not to deduce new ones (as for example there may be multiple upnp devices and we don't know which one to believe force read it no good address available"
com.aelitis.azureus.core.instancemanager.impl.AZOtherInstanceImpl "we dont know, but this is most likely ignore incompatible address mappings"
com.aelitis.azureus.core.instancemanager.impl.AZPortClashHandler ""
com.aelitis.azureus.core.lws.LightWeightSeed "use a facade here to delay loading the actual torrent until the download is activated tracker shouldn't return seeds to seeds to we can assume that if peers returned this means we have someone to talk to got to ask for at least one to trigger activation!"
com.aelitis.azureus.core.lws.LightWeightSeedAdapter ""
com.aelitis.azureus.core.lws.LightWeightSeedManager ""
com.aelitis.azureus.core.lws.LWSDiskManager "get here when doing delayed rechecks throw( new runtimeexception( "setdone not implemented" ));"
com.aelitis.azureus.core.lws.LWSDiskManagerState ""
com.aelitis.azureus.core.lws.LWSDownload ""
com.aelitis.azureus.core.lws.LWSPeerManagerAdapter ""
com.aelitis.azureus.core.lws.LWSTorrent ""
com.aelitis.azureus.core.messenger.browser.BrowserMessage "holds a message being dispatched to a {@link browsermessagelistener}. all messages must start with this prefix. separates prefix and listener id from rest of message. there were no parameters passed with the message. parameters were an encoded jsonobject. parameters were an encoded jsonarray. sets the message complete and fires of the listeners who are waiting for a response. only mark complete if this message does not have a delayed reponse success level of the message any data the message results wants to send system.out.println("complete called with " + bonlynondelayed); system.out.println("exit early" + completed);"
com.aelitis.azureus.core.messenger.browser.BrowserMessageDispatcher "registers the given listener for the given id. unique identifier used when dispatching messages receives messages targetted at the given id @throws illegalstateexception if another listener is already registered under the same id dispatches the given message to the appropriate listener. holds the listener id, operation id and parameters @throws illegalargumentexception if no listener is registered with the given id returns the listener with the given id. unique identifier of the listener to be returned located listener deregisters the listener with the given id. unique identifier of the listener to be removed deregisters the listener with the given id. unique identifier of the listener to be removed"
com.aelitis.azureus.core.messenger.browser.listeners.AbstractBrowserMessageListener "accepts and handles messages dispatched from {@link browsermessagedispatcher}. subclasses should use the message's operation id and parameters to perform the requested operation. stores the given context for accessing the browser and its services. used to access the browser displays a debug message tagged with the listener id. sent to the debug log displays a debug message and exception tagged with the listener id. sent to the debug log exception to log with message returns the context for this listener. 's context returns the unique id for this listener. 's unique id handles the given message, usually by parsing the parameters and calling the appropriate operation. holds all message information sets the context for this listener. called by its dispatcher when attached. the new context for this listener"
com.aelitis.azureus.core.messenger.browser.listeners.BrowserMessageListener "accepts and handles messages dispatched from {@link browsermessagedispatcher}. subclasses should use the message's operation id and parameters to perform the requested operation. returns the context for this listener. 's context returns the unique id for this listener. 's unique id handles the given message, usually by parsing the parameters and calling the appropriate operation. holds all message information sets the context for this listener. called by its dispatcher when attached. the new context for this listener"
com.aelitis.azureus.core.messenger.browser.listeners.MessageCompletionListener ""
com.aelitis.azureus.core.messenger.ClientMessageContext "sends a message to the javascript message dispatcher in the page. identifies the listener to receive the message identifies the operation to perform sends a message to the javascript message dispatcher in the page. identifies the listener to receive the message identifies the operation to perform optional message parameters displays a debug message tagged with the context id. sent to the debug log displays a debug message and exception tagged with the context id. sent to the debug log exception to log with message @return   @return"
com.aelitis.azureus.core.messenger.ClientMessageContextImpl ""
com.aelitis.azureus.core.messenger.config.PlatformConfigMessenger "irpcversion @return todo if we ever re-enable this rpc, need to fix it being called 2x back-to-back bug...."
com.aelitis.azureus.core.messenger.config.PlatformDevicesMessenger "string name = device.getname(); string classification = device.getclassification(); stringbuffer devicename = new stringbuffer(); if (device.isgenericusb()) { devicename.append("{g}"); } if (device.ishidden()) { devicename.append("{h}"); } devicename.append(name); if (!name.equals(classification)) { devicename.append('/'); devicename.append(classification); } return devicename.tostring();"
com.aelitis.azureus.core.messenger.config.PlatformMessengerConfig ""
com.aelitis.azureus.core.messenger.config.PlatformMetaDataMessenger "cached but not stored"
com.aelitis.azureus.core.messenger.config.PlatformMetaSearchMessenger "simpledateformat format = new simpledateformat( "yyyy-mm-dd hh:mm:ss.s"); format.settimezone( timezone.gettimezone( "utc" )); try{ long millis = format.parse( date ).gettime(); return( new templateinfo( id.longvalue(), millis, show.booleanvalue())); }catch( throwable e ){ platformmessenger.debug( "invalid date received in template: " + m ); }"
com.aelitis.azureus.core.messenger.config.PlatformSubscriptionsMessenger "signature verify = cryptoeccutils.getsignature( cryptoeccutils.rawdatatopubkey( public_key )); verify.update( ( name + pk_str + sid_str + version + content ).getbytes( "utf-8" )); boolean ok = verify.verify( sig_bytes ); test migrate"
com.aelitis.azureus.core.messenger.config.PlatformTorrentMessenger ""
com.aelitis.azureus.core.messenger.config.PlatformVuzeActivitiesMessenger ""
com.aelitis.azureus.core.messenger.PlatformMessage "@return"
com.aelitis.azureus.core.messenger.PlatformMessenger "key: id of queue; value: map of queued messages & listeners object[] keys = mapqueues.keyset().toarray(); for (int i = 0; i  0) { processqueue(mapqueue); } }  sends the message almost immediately, skipping delay  @throws exception 1.5m static private timerevent timerevent = null; the ui will initialize this create urlstem (or post data) add one at a time, ensure relay server messages are seperate build urlstem we used to check on max_post_length, but with the changes that would require converting the map to json on every iteration to get the length. for now, just limit to 10 adjust lists debug("about to process " + mapprocessing.size()); build base rpc url based on listener and server one day all this url hacking should be moved into the contentnetwork... build full url and data to send one at a time to take advantage of keep-alive connections we could report percentage to listeners, but there's no need to atm rd.addlistener(new resourcedownloaderlistener() {  public void reportpercentcomplete(resourcedownloader downloader, int percentage) { }  public void reportactivity(resourcedownloader downloader, string activity) { }  public void failed(resourcedownloader downloader, resourcedownloaderexception e) { }  public boolean completed(resourcedownloader downloader, inputstream data) { return true; } });"
com.aelitis.azureus.core.messenger.PlatformMessengerException ""
com.aelitis.azureus.core.messenger.PlatformMessengerListener ""
com.aelitis.azureus.core.metasearch.CookieParameter ""
com.aelitis.azureus.core.metasearch.Engine "az_version: 1: original 2: field value substitution in json engine type using ${field_id} 3: field value substitution in regex engine type using ${field_no} don't change these as they are externalised of auto_dl constants above tests for sameness in terms of function (ignores id, selection state etc) @return resets to initial state (e.g. if the engine has state pertaining to what has/hasn't been downloaded such as etags then this will be cleared) don't change these values as they get persisted ignore if-modified stuff and force a full search"
com.aelitis.azureus.core.metasearch.impl.DateParser ""
com.aelitis.azureus.core.metasearch.impl.DateParserClassic "todo : in debug mode throw an exception telling the todo : in debug mode, throw an exception to tell the "today hh:mm" and "y-day hh:mm" cases "07-25 2006", "02-01 02:53" and "03 mar 2006" cases "07-25 2006" and "02-01 02:53" cases "02-01 02:53" case "07-25 2006" "03-25" case "21 mar 08" case "03 mar 2006" case age based stuff calendar.set(calendar.hour_of_day,12); calendar.set(calendar.minute,0); calendar.set(calendar.hour_of_day,12); calendar.set(calendar.minute,0);"
com.aelitis.azureus.core.metasearch.impl.DateParserRegex "todo : in debug mode throw an exception telling the todo : in debug mode, throw an exception to tell the find if there is any time information in the date remove the time information in order to not confuse the date parsing handle date with format "2009-01-12 at 03:36:38" by removing trailing " at"; find if the date contains letters we have a date with letters, could be age-based or time based (with a literal month) try to determine if it is age-based or time-based age based date nothing to do for today as we base our initial date on the current one we're in the real "ago" case, let's remove " ago" if it's there 604800 seconds in a week the month case when m is not a minute about 720 hours per month about 8760 hours per year system.out.println("unit not matched : " + unit); system.out.println(input + " > " + calendar.gettime()); time based date system.out.println("dl : " + s); system.out.println(input + " > " + calendar.gettime() + "( " + calendar.gettimezone() + " )"); system.out.println(input + " > " + calendar.gettime() + "( " + calendar.gettimezone() + " )"); we have a date with only numbers system.out.println("dn : " + s );//+ "(" + input + ")"); let's assume a default order of m/d and switch if it doesn't make sense 2 numbers only, we assume it's day and month todo : fire an exception ? system.out.println(input + " > " + calendar.gettime()); extract the time information if(calendar.after(calendarcompare)) { system.err.println("maintenant ici: "+calendarcompare.gettimeinmillis()+" -- "+calendarcompare.gettime()+" -- "+calendarcompare); system.err.println("maintenant la bas: "+calendar.gettimeinmillis()+" -- "+calendar.gettime()+" -- "+calendar); system.err.println("calendars: after? " + calendar.after(calendarcompare) + " // before? " + calendar.before(calendarcompare)); } calendar.settimezone(timezone.getdefault()); 22 nov           18 jun at 2:10am today      1.4 hours ago 3 and a half days ago 392 weeks ago"
com.aelitis.azureus.core.metasearch.impl.EngineImpl "if you extend this with state pertaining to what has been downloaded then make sure you extend the code in reset(); first mappings used to canonicalise names and map field to same field typically used for categories (musak->music) second mappings used to generate derived field values typically used to derive content_type from category (music->audio) applicable to non-vuze hosted templates manual constructor bencoded constructor bencoded export json constructor json export limited support for the moment: level one always maps to same field level two always maps to content type int to_field = remapper.getoutfield(); id is a vuze one, derive uid from it as already unique default values only record transitions to or from manual selection for non-local templates system.out.println( getname() + " (" + rank_bias+"/"+preferred_count + "): " + _rank + " -> " + rank ); default impl based on availability of fields that permit auto-dl overridden in rss engine ( for example ) to be based on explicit channel tag"
com.aelitis.azureus.core.metasearch.impl.FieldRemapper ""
com.aelitis.azureus.core.metasearch.impl.FieldRemapping ""
com.aelitis.azureus.core.metasearch.impl.MetaSearchImpl "unfortunately pid can be internationalised and thus musn't be used as a key to a bencoded-map as it can lead of nastyness. delete any existing entries that have got out of control skip to next scheduled update time reply is either "response" meaning "no update" and giving possibly changed update secs or vuze file with updated template single thread listener calls"
com.aelitis.azureus.core.metasearch.impl.MetaSearchManagerImpl "engine[] engines = meta_search.getengines( false, false ); for (int i=0;i<engines.length;i++){ engine engine = engines[i]; if ( engine.getselectionstate()) == engine.sel_state_manual_selected ){ } } reset engines featured templates are always shown - can't be deselected popular ones are selected if in 'auto' mode manually selected ones are, well, manually selected pick up explicitly selected vuze ones we've compiled a list of the engines we should have and their latest dates update any that are out of date do any pre-loads deselect any not in use finally pick up any unreported selection changes and re-affirm positive selections first update state of auto and existing engines now refresh - this will pick up latest state of things next add in any missing engines deselect any existing manually selected ones that are no longer selected if local template then we try to use the id as is otherwise we emsure that it is a local one boring boring"
com.aelitis.azureus.core.metasearch.impl.plugin.PluginEngine "recovery from when incorrectly defaulted to 0.0"
com.aelitis.azureus.core.metasearch.impl.plugin.PluginResult "if we have seeds/peers just use the usual mechanism"
com.aelitis.azureus.core.metasearch.impl.SearchExecuter ""
com.aelitis.azureus.core.metasearch.impl.web.CookieParser ""
com.aelitis.azureus.core.metasearch.impl.web.FieldMapping ""
com.aelitis.azureus.core.metasearch.impl.web.json.JSONEngine "explicit test constructor bencoded constructor json constructor fix a vaguely common error: trailing \ before end-of-string: - \", sort for consistent order system.out.println( page );"
com.aelitis.azureus.core.metasearch.impl.web.regex.RegexEngine "if ( getid() == 3 ){ writetofile( "c:\\temp\\template.txt", page ); writetofile( "c:\\temp\\pattern.txt", pattern.pattern()); string page2 = readfile( "c:\\temp\\template.txt" ); set s1 = new hashset(); set s2 = new hashset(); for (int i=0;i<page.length();i++){ s1.add( new character( page.charat(i))); } for (int i=0;i<page2.length();i++){ s2.add( new character( page2.charat(i))); } s1.removeall(s2); iterator it = s1.iterator(); while( it.hasnext()){ character c = (character)it.next(); system.out.println( "diff: " + c + "/" + (int)c.charvalue()); } } try{ regexptest(); }catch( throwable e ){ } protected void writetofile( string file, string str ) { try{ printwriter pw = new printwriter( new filewriter( new file( file ))); pw.println( str ); pw.close(); }catch( throwable e ){ e.printstacktrace(); } } private static string readfile( string file ) { try{ stringbuffer sb = new stringbuffer(); linenumberreader lnr = new linenumberreader( new filereader( new file( file ))); while( true ){ string line = lnr.readline(); if ( line == null ){ break; } sb.append( line ); } return( sb.tostring()); }catch( throwable e ){ e.printstacktrace(); return( null ); } } private static void regexptest() throws exception { pattern pattern = pattern.compile( readfile( "c:\\temp\\pattern.txt" )); string page = readfile( "c:\\temp\\template.txt" ); matcher m = pattern.matcher( page); while(m.find()) { int groups = m.groupcount(); system.out.println( "found match: groups = " + groups ); } } explicit test constructor bencoded json only try subsequent patterns if all previous have failed to find results in "debug/test" mode, we should fire an exception / notification ignore "matches" that don't actually populate any fields hack - if no results and redirected to https and auth required then assume we need to log in..."
com.aelitis.azureus.core.metasearch.impl.web.rss.RSSEngine "explicit constructor json don't know about optional fields (such as direct download - be optimistic) unknown until a successful feed download has occurred so that we know the status of the feed tag see if this is an atom feed  entry if present if we still have no download link see if the magnet is in the title"
com.aelitis.azureus.core.metasearch.impl.web.WebEngine "manual test constructor bencoded constructor json encoded constructor wha? getting some nulls here :( from json like "column_map\":[null,null,{\"group_nb\":\"3 backwards compact from when there was a mapping entry regexp case json case didn't find the root url within the url didn't find the root url within the url normalise to permit == to be safely used when testing method see if we have explicit cookie information in the url: allow local addresses for testing purposes system.out.println(searchurl); hack to support post by encoding into url http://xxxx/index.php?main=search&azmethod=post_basic:searchstring1=%s&searchstring=&search=search already url encoded handle file://c:/ - map to file:/c:/ not modified well, not much we can do with the cookies anyway as in general the ones set are the ones missing/expired, not the existing ones. that is, we can't deduce anything from the fact that a required cookie is not 'set' here the most we could do is catch a server that explicitly deleted invalid cookies by expiring it, but i doubt this is a common practice. also note the complexity of cookie syntax set-cookie: old standard using expires=, new using maxage set-cookie2: maybe use http://jcookie.sourceforge.net/ if needed handle lowercase 'utf-8' for example first look for xml charset e.g.  some feeds have crap at the start which makes pos2 mismatch for the above '?' - adjust if necessary next look for http-equiv charset e.g.  some feeds have crap at the start which makes pos2 mismatch for the above '?' - adjust if necessary list cookie = (list)url_rd.getproperty( "url_set-cookie" ); no base tag in the page e.printstacktrace(); let's try with no login page url return loginpageurl;"
com.aelitis.azureus.core.metasearch.impl.web.WebResult "int separator = this.category.indexof(">"); if(separator != -1) { this.category = this.category.substring(separator+1).trim(); } e.printstacktrace(); this.nbpeers = 0; e.printstacktrace(); this.nbseeds = 0; e.printstacktrace(); this.nbseeds = 0; e.printstacktrace(); either a float 0->1 or integer 0->100 add a space between the digits and unit if there is none no unit e.printstacktrace(); e.printstacktrace(); e.printstacktrace(); base 32 hash base 16 if we don't have a download button link, but we do have a direct download link, then we should use the direct download link..."
com.aelitis.azureus.core.metasearch.MetaSearch ""
com.aelitis.azureus.core.metasearch.MetaSearchException ""
com.aelitis.azureus.core.metasearch.MetaSearchListener ""
com.aelitis.azureus.core.metasearch.MetaSearchManager ""
com.aelitis.azureus.core.metasearch.MetaSearchManagerFactory ""
com.aelitis.azureus.core.metasearch.MetaSearchManagerListener ""
com.aelitis.azureus.core.metasearch.Result "public string getnamehtml() { if(getname() != null) { return( getname()); //return( xuxmlwriter.escapexml( getname())); //return entities.xml.escape(getname()); } return null; } public string getcategoryhtml() { if(getcategory() != null) { return( getcategory()); //return entities.xml.escape(getcategory()); } return null; } value between 0 and 1 representing the rank of the result if ( rank != _rank ){ system.out.println( "bias applied for " + engine.getname() + ": " + _rank + "-> " + rank ); } links 0.0 -> 1.0 and -1 if not supported return( xuxmlwriter.escapexml( getname())); return entities.xml.escape(getname()); return entities.xml.escape(getcategory()); max three digits for display purposes this is also used by subscription code to extract download link so if you change this you'll need to change that too... used by subscriptions..."
com.aelitis.azureus.core.metasearch.ResultListener ""
com.aelitis.azureus.core.metasearch.SearchException ""
com.aelitis.azureus.core.metasearch.SearchLoginException ""
com.aelitis.azureus.core.metasearch.SearchParameter ""
com.aelitis.azureus.core.metasearch.utils.MomentsAgoDateFormatter "improvement based on azweb-318. @version 3.2.1.0 6/19/2007 6/19/2007 returns "x  ago on " by comparing the given pastdate with the current time. all formats are converted to gmt time. in the future the returns "x  ago on " by comparing the given pastdate with the current time. a default locale date in the past "x  ago" checks to see if the unit we're comparing is less than the difference of the given "then" and "now" dates in milliseconds. the date we're evaluating the current time the field which we're evaluating ("units") if then is 0 "units" from now, otherwise a displayable string that will notify the a list of id's that we use in the two maps to ensure we have valid refs a list of units we're comparing. a few externalized strings to display to the build the map at system start build the map at system start if (timeago > 1) result = result.concat(plural); result = result.concat(ago);"
com.aelitis.azureus.core.nat.NATTraversal ""
com.aelitis.azureus.core.nat.NATTraversalException ""
com.aelitis.azureus.core.nat.NATTraversalHandler ""
com.aelitis.azureus.core.nat.NATTraversalObserver ""
com.aelitis.azureus.core.nat.NATTraverser ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminASNImpl ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminASNLookupImpl "inetaddress test = inetaddress.getbyname( "255.71.15.1" ); system.out.println( test + " -> " + matchescidr( "255.71.0.0/20", test )); debug.outdiagloggeronly( "asn lookup for '" + address + "'" ); first query for the as "33544 | 64.71.0.0/20 | us | arin | 2006-05-04" now query for asn 33544 | us | arin | 2005-01-19 | wiline - wiline networks inc."
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminHTTPProxyImpl ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminHTTPTester "try to use our service first fallback to something else"
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminImpl "populate initial values networkinterface's "equals" method is based on ni name + addresses literal ipv4 or ipv6 address ignore, could be an interface name containing a ':' allow wildcard address as 1st address, otherwise only interface addresses interface name should not happen ignore, interface name if the logic as they may be switching vpn see if we have a choice if we have a socks server then let's use a compatible address for it next, same for nat devices take a chance on any non local addresses we have lastly, select local one at random ho hum prioritise 192.168.0. and 192.168.1. as common then ipv4 over ipv6 check bindings first found a specific bind address, use that one found v6 any-local address, check interfaces for a best match migration from when we only persisted a single as ignore ignore for the icon system.out.println( "checking connection routes" ); system.out.println( " bind ip found" ); check if all outgoing tcp connections are being routed through the same interface unfortunately we don't always get access to the locally bound interfaces, so for the case where we have nothing useful look to see if there are more than one interface and if one looks like a vpn. i did try an explicit 'connect with bind' to see if i could work out if just one was routing but unfortunately this didn't work (with open-vpn at least) as it still permits explicit connections through non-vpn interface - grrr! system.out.println( "everything wildcard" ); system.out.println( "bindable=" + bindable_addresses.length ); system.out.println( "all outgoing tcp bound to same: " + bound_intf ); maybevpn( bound_intf ); might have done in the meantime bah networkinterface has lots of goodies but is 1.6 admin.lognatstatus( iw );"
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminNATDeviceImpl ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminNATUDPCodecs ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminNATUDPReply ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminNATUDPRequest ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminProtocolImpl "give upnp a chance to work"
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminProtocolTester ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminSocksProxyImpl ""
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminSpeedTesterBTImpl "- plugininterface is used to get manager classes. the downloads have been stopped just need to do the testing. - torrent recieved from testing service. get the result for object of speed test. depending on the mode we want to upload all the set all, none or only half the pieces to done. - int that maps to networkadminspeedtestscheduler.test_type... - total pieces in this test torrent. - int - the starting piece number to setdone to true. -------------------- helper class to monitor test. ------------------- calculate the test progression as a value between 0-100. - current time as long. - download stats calculate the avererage and standard deviation for a history. - list of long values but that contains the sum downloaded at that time.  with values "ave" and "stddev" set convert a list of sums into a list of download rates per second. - list with download sum for each second. - list with the download rate for each second. based on the previous data cancluate an average and a standard deviation. return this data in a map object.  as a contain for stats. map keys are "ave" and "dev". in this version the test is limited to max_test_time since the start of the test of max_peak_time (i.e. time since the peak download rate has been reached). which ever condition is first will finish the download. if the test done condition has been reached. we set a new "peak" value if it has exceeded the previous peak value by 10%. - - - downloaded so far. build a result for a successful test. - map of upload results. - map of download results. build a result if the test failed with an error. - why the test failed. ok lets start the test. create a blank file of specified size. (using the temporary name.) make sure we've got a bunch of upload slots the test has now started!! we need to defer the reporting of a failure until the test is complete as this prevents us from starting another test while the current one is terminating ------------------ private methods --------------- if(mode==test_type_upload_and_download){ //upload half the pieces return totalpieces/2; }else upload all the pieces download all the pieces    limit test to 2 minutes. limit to 30 seconds at peak. can flick out of force-mode when transitioning from downloading to seeding - easiest fix is: use the ip as the key so we don't count reconnects multiple times someone interrupted this thread for a reason. "test is now over" it is time to stop the test. check the stats for peers we connected to during the test calculate the measured download rate. todo: persist it log the result. do two calculations. frist based on the total time allowed for the test second for the time since the peak value has been reached. the larger of the two wins. include the upload and download values. map calculate(list history) convert the list of long values that sum the value into a list of deltas. sort remove the top and bottom 10% of the sample. this removes outliers from the mean. removing high values. remove low values. sum values calculate average. debug.out("ave rate:"+averate); calculate standard deviation. debug.out( j+","+deltas.get(j) ); map retval = new hashmap(); find the first element to inlcude in the stat. calculate the bt download rate. map resdown = calculate(historydownloadspeed); calculate the bt upload rate. map resup = calculate(historyuploadspeed); have we reached the max time for this test? have we been near the peak download value for max time? upload only used the "uploaded" data. the "download only" and "both" uses download. if the current rate is 10% greater then the previous max, reset the max, and test timer. time get test info. get crypto error result"
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminSpeedTesterImpl "send a result to all of the networkadminspeedtestlisteners. - result of the test. just report the first result in case an implementation hits this more than once"
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminSpeedTestScheduledTestImpl "request a test from the speed testing service, handle the "challenge" if request and then get the id for the test. per spec all request are bencoded maps. - true if the test has been reserved with the service. _file - file azureus jar used to load classes. - map from the previous response - from the current response. read from url and return byte array. - [] of the results. max size currently 100k. @throws java.io.ioexception - restore all the downloads the state before the speed test started. preserve all the data about the downloads while the test is running. preservers the state of all the downloads before the speed test started. get the global limits from the transferspeedvalidator class. call before starting a speed test. call this method after a speed test completes to restore the global limits. call this method after the speed test is completed to restore the individual download limits before the test started. save the upload/download limits of this download object before the test started. - download - int - int get the upload or download limit for this download object before the test started. - download - string - limit as int. get all the download keys in this map. - download[] types of requests sent to speedtest scheduler. detect the router. lookup upnp devices found. one might be a router. send "schedule test" request. get jar file its version for the test over-ride the jar version, and location for debugging. todo: make this a -d option with this default. system.out.println( "speedtest: using class-based challenge jar " + jar_file.getabsolutepath() + ", version " + jar_version ); system.out.println( "speedtest: using config-based challenge jar " + jar_file.getabsolutepath() + ", version " + jar_version ); todo: remove once challenge testing is done. where to i get the az-id and client version from the configuration? a challenge has occured. a test has been scheduled. set the map properly. this is test-specific data force the urlclassloader to load from the url and not delegate and find the currently jar's constants verify the following items are in the response. size (in bytes) offset (in bytes) challenge_id read the bytes build the url. close all replys of this type contains a "result" detail is of no interest to the msg.append(" ,error detail: ").append(errdetail); rethrow this type of exception. in case we've already saved limits --------------- helper classes below here ---------------- // map  > global limits. add some trace so we have some clue as to what has made the change! a bunch of plugins mess with limits (autospeed, shaper, speedscheduler...) - disable their ability to mess with config during the test preserve the limits for all the downloads and set each to zero. preserve the global limits int settings. boolean setting."
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminSpeedTestSchedulerImpl "get the most recent result for the test. - result"
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminTCPTester "try to use our service first fallback to something else"
com.aelitis.azureus.core.networkmanager.admin.impl.NetworkAdminUDPTester "upnpmapping mapping = upnp.getmapping( true, port ); if ( mapping == null ) { new_mapping = mapping = upnp.addmapping( "nat tester", true, port, true ); // give upnp a chance to work try { thread.sleep( 500 ); } catch (throwable e) { debug.printstacktrace( e ); } } 2084; give upnp a chance to work connection ids for requests must always have their msb set... apart from the original darn udp tracker spec.... fire off one last packet in attempt to inform server of completion"
com.aelitis.azureus.core.networkmanager.admin.impl.swt.NetworkAdminSWTImpl ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdmin "@throws unsupportedaddresstypeexception when no address matching the v4/v6 requirements is found, always returns an address when auto is selected returns the list of current addresses that can successfully be bound to with an ephemeral port @return only call if the supplied address is believed to be the current public address @return @throws networkadminexception ad-hoc query @return @throws networkadminexception"
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminASN ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminASNListener ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminException ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminHTTPProxy ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminNATDevice ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminNetworkInterface ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminNetworkInterfaceAddress ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminNode ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminProgressListener ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminPropertyChangeListener ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminProtocol ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminRouteListener ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminRoutesListener ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminSocksProxy ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminSpeedTester ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminSpeedTesterListener "when a test completes. - test ran - string with the result informs listener when the test is at a new stage. - test running. - string with stage."
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminSpeedTesterResult ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminSpeedTestScheduledTest "these are the limits up to which the test can run, not the result of the test @return"
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminSpeedTestScheduledTestListener ""
com.aelitis.azureus.core.networkmanager.admin.NetworkAdminSpeedTestScheduler "if system crashes on start-up, then speed tests torrents need to be cleaned on start-up etc - call this method on start to allow this returns the currently scheduled test, null if none - networkadminspeedtestscheduledtest request a test using the testing service. - id for the type of test - use abouve constants - true if a success, otherwise false. @throws networkadminexception - get the most recent result for a given test type, null if none - id for the type of test - use abouve constants - result"
com.aelitis.azureus.core.networkmanager.ConnectionAttempt ""
com.aelitis.azureus.core.networkmanager.ConnectionEndpoint "system.out.println( "connect took " + (systemtime.getcurrenttime() - start_time ) + " for " + transport.getdescription()); p2 is utp, p1 is tcp"
com.aelitis.azureus.core.networkmanager.EventWaiter ""
com.aelitis.azureus.core.networkmanager.impl.ByteBucket "byte-bucket implementation based on the token bucket algorithm. buckets can be configured with a guaranteed normal rate, along with a burst rate."
com.aelitis.azureus.core.networkmanager.impl.ByteBucketMT "create a new byte-bucket with the given byte fill (guaranteed) rate. burst rate is set to default 1.2x of given fill rate. _bytes_per_sec fill rate create a new byte-bucket with the given byte fill (guaranteed) rate and the given burst rate. _bytes_per_sec fill rate _rate max rate get the number of bytes currently available for use. of free bytes update the bucket with the number of bytes just used. _used get the configured fill rate. rate in bytes per sec get the configured burst rate. rate in bytes per sec set the current fill/guaranteed rate, with a burst rate of 1.2x the given rate. _bytes_per_sec set the current fill/guaranteed rate, along with the burst rate. _bytes_per_sec _rate make sure the bucket's burst rate is at least mss-sized, otherwise it will never allow a full packet's worth of data. start bucket empty if( avail_bytes < 0 ) debug.out( "avail_bytes < 0: " + avail_bytes); debug.out("error: avail_bytes < 0: " + avail_bytes); oops, this won't ever allow a full packet so increase the max byte size"
com.aelitis.azureus.core.networkmanager.impl.ByteBucketST "create a new byte-bucket with the given byte fill (guaranteed) rate. burst rate is set to default 1.2x of given fill rate. _bytes_per_sec fill rate create a new byte-bucket with the given byte fill (guaranteed) rate and the given burst rate. _bytes_per_sec fill rate _rate max rate get the number of bytes currently available for use. of free bytes update the bucket with the number of bytes just used. _used get the configured fill rate. rate in bytes per sec get the configured burst rate. rate in bytes per sec set the current fill/guaranteed rate, with a burst rate of 1.2x the given rate. _bytes_per_sec set the current fill/guaranteed rate, along with the burst rate. _bytes_per_sec _rate make sure the bucket's burst rate is at least mss-sized, otherwise it will never allow a full packet's worth of data. start bucket empty oops, this won't ever allow a full packet so increase the max byte size"
com.aelitis.azureus.core.networkmanager.impl.EntityHandler "manages transfer entities on behalf of peer connections. each entity handler has a global pool which manages all connections by default. connections can also be "upgraded" to a higher connection control level, i.e. each connection has its own specialized entity for performance purposes. create a new entity handler using the given rate handler. read or write type handler _handler global max rate handler register a peer connection for management by the handler. to add to the global pool remove a peer connection from the entity handler. to cancel upgrade a peer connection from the general pool to its own high-speed entity. to upgrade from global management individual connection rate handler downgrade (return) a peer connection back into the general pool. to downgrade back into the global entity is the general pool entity in need of a transfer op. note: because the general pool is backed by a multipeer entity, it requires at least mss available bytes before it will/can perform a successful transfer. this method allows higher-level bandwidth allocation to determine if it should reserve the necessary mss bytes for the general pool's needs. of it has data to transfer, false if not public boolean isgeneralpoolreserveneeded() { if( handler_type == transferprocessor.type_upload ) { return global_uploader.haswritedataavailable(); } return global_downloader.hasreaddataavailable(); } download type register global upload entity register global download entity if not found in the pool entity check for it in the upgraded list cancel from write processing if not found in the pool entity check for it in the upgraded list cancel from read processing remove it from the general upload pool register it for write processing add it to the upgraded list remove it from the general upload pool register it for read processing add it to the upgraded list remove from the upgraded list cancel from write processing move back to the general pool remove from the upgraded list cancel from read processing move back to the general pool"
com.aelitis.azureus.core.networkmanager.impl.http.HTTPMessage ""
com.aelitis.azureus.core.networkmanager.impl.http.HTTPMessageDecoder "before we start message processing we should have had the connection bound system.out.println( "performstreamdecode" ); as access to messages_last_read isn't synchronized we can get this error if we destroy the decoder in parallel with messages being removed. we don't really want to synchornized access to this so we'll take the hit here"
com.aelitis.azureus.core.networkmanager.impl.http.HTTPMessageEncoder "system.out.println( "encodemessage: " + message.getid());"
com.aelitis.azureus.core.networkmanager.impl.http.HTTPNetworkConnection "string times = ""; for (int i=0;i " + connections.size() + " - " + times ); string str =""; for (int i=0;i<lengths.length;i++){ str += (i==0?"":",") +"[" + offsets[i] + "/" + lengths[i] + "]"; } system.out.println( network_connection_key.getname() + ": requested " + str + ",part=" + partial_content +",ka=" + keep_alive ); if ( peer.getip().equals( "64.71.5.2")){ timeformatter.millitrace( "http_req_create: " + piece + "/" + start + " [hr=" + http_requests.size() + ",cr=" + choked_requests.size() + ",or=" + outstanding_requests.size() + ",d=" + decoder.getqueuesize() + "]" ); } if ( peer.getip().equals( "64.71.5.2")){ timeformatter.millitrace( "http_req_data: " + piece + "/" + start + " [hr=" + http_requests.size() + ",cr=" + choked_requests.size() + ",or=" + outstanding_requests.size() + ",d=" + decoder.getqueuesize() + "]" ); } if ( peer.getip().equals( "64.71.5.2")){ timeformatter.millitrace( "http_req_out: " + piece + "/" + start + " [hr=" + http_requests.size() + ",cr=" + choked_requests.size() + ",or=" + outstanding_requests.size() + ",d=" + decoder.getqueuesize() + "]" ); } might have a concurrent mod to the iterator note that the decoder can synchronously call-back if is preloaded with a header here... not sure about etag. i was going to use the torrent hash but i don't understand the link between url, range requests and etags. do we need to generate different etags for each webseed piece request url or can we use the torrent hash and rely on the fact that the url changes? are range-requests irrelevant as far as etags go - i'd like to think so... we have to do this as core code assumes buffer entry 0 is protocol kinda crappy as it triggers on first block of piece, however better than nothing if after adding the message there's no bytes on the queue then we need to trigger an immediate flushed event as the queue won't get processed (0 bytes on it...)"
com.aelitis.azureus.core.networkmanager.impl.http.HTTPNetworkConnectionFile "system.out.println( "line " + line_num + " -> " + line ); optimise for simple torrents. also support the case where client has the hash but doesn't know the file name latest spec has torrent file name encoded first for non-simple torrents remove it if we find it so we have some backward compat -222 is last 222 bytes of file"
com.aelitis.azureus.core.networkmanager.impl.http.HTTPNetworkConnectionWebSeed ""
com.aelitis.azureus.core.networkmanager.impl.http.HTTPNetworkManager "try{ system.out.println( "/webseed?info_hash=" + urlencoder.encode( new string( byteformatter.decodestring("c9c04d96f11fb5c5ecc99d418d3575fbfc2208b0"), "iso-8859-1"), "iso-8859-1" )); }catch( throwable e ){ e.printstacktrace(); } system.out.println( "re-routed " + new_connection.getendpoint().getnotionaladdress() + " from " + byteformatter.encodestring( old_hash ) + " to " + byteformatter.encodestring( new_hash ) ); get ' '  ' http/1.1' max get  size - boiler plate plus small url plus hash enough to match get note duplication of this in min-matches below format is get url http/1.1 note that we don't insist on a full url here, just the start of one ping is used for inbound http port checking not read the end yet not read the end of the hash yet trim back url as it currently has header in it too restore buffer structure register for incoming connection routing routed on failure fake a wakeup so pre-read header is processed"
com.aelitis.azureus.core.networkmanager.impl.IncomingConnectionManager "register the given byte sequence matcher to handle matching against new incoming connection initial data; i.e. the first bytes read from a connection must match in order for the given listener to be invoked. byte filter sequence to call upon match remove the given byte sequence match from the registration list. _remove byte sequence originally used to register listener for byte matches. currently if message crypto is on and default fallback for incoming not enabled then we would bounce incoming messages from non-crypto transports for example, nat check this method allows auto-fallback for such transports @return the given socket has been accepted as matching the byte filter. matching accepted connection _so_far bytes already read copy-on-write returns matchlistener,routingdata if matched remember original values for later restore rewind not enough bytes yet to compare not enough bytes yet to compare restore original values in case the checks changed them recalc longest buffer if necessary no match registrations, just close note that the filter may have some data internally queued in it after the crypto handshake decode (in particular the bt header). however, there should be some data right behind it that will trigger a read-select below, thus giving prompt access to the queued data might be stuff queued up in the filter - force one process cycle (nat check in particular ) remove from connection list at least one read op has occured time went backwards! no bytes have been read yet time went backwards! no match found we've already read in enough bytes to have compared against all potential match buffers match found! failure"
com.aelitis.azureus.core.networkmanager.impl.IncomingMessageQueueImpl "inbound peer message queue. create a new incoming message queue. _decoder default message stream decoder owner to read from set the message stream decoder that will be used to decode incoming messages. _stream_decoder to use get the percentage of the current message that has already been received. complete (0-99), or -1 if no message is currently being received receive (read) message(s) data from the underlying transport. _bytes to read of bytes received @throws ioexception on receive error notifty the queue (and its listeners) of a message received externally on the queue's behalf. received externally manually resume processing (reading) incoming messages. note: allows us to resume docoding externally, in case it was auto-paused internally. add a listener to be notified of queue events. cancel queue event notification listener. destroy this queue. copy-on-write perform decode op check if anything was decoded and notify listeners if so copy-on-write copy-on-write copy-on-write copy-on-write copy-on-write copy-on-write"
com.aelitis.azureus.core.networkmanager.impl.MultiPeerDownloader "create new downloader using the given "global" rate handler to limit all peers managed by this downloader. _handler add the given connection to the downloader. to add remove the given connection from the downloader. to remove if the connection was found and removed, false if not removed networkmanager.gettcpmsssize() copied-on-write copy-on-write copy-on-write make circular"
com.aelitis.azureus.core.networkmanager.impl.MultiPeerDownloader2 "create new downloader using the given "global" rate handler to limit all peers managed by this downloader. _handler add the given connection to the downloader. to add remove the given connection from the downloader. to remove if the connection was found and removed, false if not removed networkmanager.gettcpmsssize() copied-on-write copy-on-write copy-on-write note - this is single threaded system.out.println( "mpd: do process - " + connections_cow.size() + "/" + active_connections.size() + "/" + idle_connections.size()); move any active ones off of the idle queue system.out.println( " moving to active " + connection.getstring()); process the active set system.out.println( " " + connection.getstring() + " - " + ready ); system.out.println( " moving to end " + connection.getstring()); system.out.println( " moving to idle " + connection.getstring()); zero bytes read"
com.aelitis.azureus.core.networkmanager.impl.MultiPeerUploader "a rate-controlled write entity backed by multiple peer connections, with an emphasis on transmitting packets with full payloads, i.e. it writes to the transport in mss-sized chunks if at all possible. it also employs fair, round-robin write scheduling, where connections each take turns writing a single full packet per round. create a new packet-filling multi-peer upload entity, rate-controlled by the given handler. _handler listener to handle upload rate limits checks the connections in the waiting list to see if it's time to be force-flushed. destroy this upload entity. note: removes all peer connections in the process. add the given connection to be managed by this upload entity. _connection to be write managed remove the given connection from this upload entity. _connection to be removed if the connection was found and removed, false if not removed ignore ignore ignore ignore does this entity have data ready for writing. if it has data to send, false if empty public boolean haswritedataavailable() { if( ready_connections.isempty() ) return false; return true; } networkmanager.gettcpmsssize() 500ms 3sec no-new-data wait before forcing write flush time to force flush has data to flush cancel the listener remove from the waiting list no data, so reset flush wait time remove and cancel all connections in waiting list remove from ready list has a full packet's worth, or has urgent data has data to send, but not enough for a full packet look for the connection in the waiting list and cancel listener if found look for the connection in the ready list connections with less than a packet's worth of data connection now has more data to send connection has already been removed from the waiting list stop further processing has a full packet's worth, or has urgent data remove from waiting list cancel this listener still not enough data for a full packet only do this once to avoid the possibility that, for example, sending a have every 2.9 seconds can result in them being delayed for a significant amount of time (e.g. 60 seconds +...) update last message added time attach listener start flush wait time add to waiting list listen for added data connections ready to write add to ready list todo: model this class after the simplicity of multipeerdownloader not allowed to write not yet ready for writing re-add to end as currently unusable move on to the next connection oops, all messages have been removed move on to the next connection allow a single full packet at most allow a single full packet at most we're allowed enough (for either a full packet or to drain any remaining data) register it for manual listener notification still has a full packet's worth, or has urgent data re-add to end for further writing reset the unusable count so that it has a chance to try this connection again in the loop connection does not have enough for a full packet, so remove and place into waiting list write exception, so move to waiting list while it waits for removal do exception notification outside of sync'd block we're not allowed enough to maximize the packet payload re-add to end as currently unusable move on to the next connection manual queue listener notifications exception notifications ////////////// ratecontrolledwriteentity implementation //////////////////// since this method is called repeatedly from a loop, we can use it to check flushes no data to send"
com.aelitis.azureus.core.networkmanager.impl.NetworkConnectionImpl "constructor for new outbound connection. the connection is not yet established upon instantiation; use connect() to do so. _remote_address to connect to default message stream encoder to use for the outgoing queue default message stream decoder to use for the incoming queue constructor for new inbound connection. the connection is assumed to be already established, by the given already-connected channel. _remote_channel connected by _already_read bytestream already read during routing default message stream encoder to use for the outgoing queue default message stream decoder to use for the incoming queue we get here after detaching a transport and then closing the peer connection"
com.aelitis.azureus.core.networkmanager.impl.NetworkManagerUtilities "translate the group speed limit to a proper real rate. to use real limit in bytes per second unlimited disabled"
com.aelitis.azureus.core.networkmanager.impl.OutgoingMessageQueueImpl "priority-based outbound peer message queue. create a new outgoing message queue. _encoder default message encoder set the message stream encoder that will be used to encode outgoing messages. _encoder to use get the percentage of the current message that has already been sent out. complete (0-99), or -1 if no message is currently being sent destroy this queue; i.e. perform cleanup actions. get the total number of bytes ready to be transported. bytes remaining whether or not an urgent message (one that needs an immediate send, i.e. a no-delay message) is queued. if there's a message tagged for immediate write add a message to the message queue. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. message to add _listener_notify true for manual notification, false for automatic remove all messages of the given types from the queue. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. _types type to remove _listener_notify true for manual notification, false for automatic remove a particular message from the queue. note: only the original message found in the queue will be destroyed upon removal, which may not necessarily be the one passed as the method parameter, as some messages override equals() (i.e. btrequest messages) instead of using reference equality, and could be a completely different object, and would need to be destroyed manually. if the message does not override equals, then any such method will likely not be found and removed, as internal queued object was a new allocation on insertion. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. to remove _listener_notify true for manual notification, false for automatic if the message was removed, false otherwise deliver (write) message(s) data to the underlying transport. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. _bytes maximum number of bytes to deliver _listener_notify true for manual notification, false for automatic of bytes delivered @throws ioexception on delivery error manually send any unsent listener notifications. add a listener to be notified of queue events. cancel queue event notification listener. notifty the queue (and its listeners) of a message sent externally on the queue's behalf. sent externally copied-on-write todo do message add notifications message addition not allowed lglogger.log( "message [" +message.getdescription()+ "] not allowed for queueing, message addition skipped." ); message.destroy(); //todo destroy???? queue is shutdown, drop any added messages but don't insert in front of a half-sent message register listener event for later, manual notification do listener notification now dont remove a half-sent message do listener notifications now dont remove a half-sent message delayed manual notification do listener notification now system.out.println( "deliver: %=" + percent_complete + ", queue=" + queue.size()); assumes the first buffer is message header still data left to send in this message so don't bother checking later messages for completion compute send percentage if in front of non-empty buffer is non-empty buffer last payload buffer of message is empty reset send percentage we can have messages that end up getting serialised as 0 bytes (for http connections for example) - we still need to notify them of being sent... data bytes notify protocol bytes notify do listener notification now the last listener notification, so destroy for each notification for each listener for each listener for each listener for each listener for each listener copy-on-write copy-on-write system.out.println( "notifiedofexternallysentmessage:: [" +message.getid()+ "] size=" +size );"
com.aelitis.azureus.core.networkmanager.impl.ProtocolDecoder ""
com.aelitis.azureus.core.networkmanager.impl.ProtocolDecoderAdapter ""
com.aelitis.azureus.core.networkmanager.impl.ProtocolDecoderInitial "we assume that for outgoing connections, if we are here, we want to use crypto can happen during initialisation"
com.aelitis.azureus.core.networkmanager.impl.ProtocolDecoderPHE "private static final string aes_stream_alg = "aes"; private static final string aes_stream_cipher = "aes/cfb8/nopadding"; private static final int aes_stream_key_size = 128; private static final int aes_stream_key_size_bytes = aes_stream_key_size/8; try{ byte[] aes_test_secret = new byte[aes_stream_key_size_bytes]; secretkeyspec aes_test_secret_key_spec = new secretkeyspec(aes_test_secret, 0, aes_stream_key_size_bytes, aes_stream_alg ); algorithmparameterspec spec = new ivparameterspec( aes_test_secret ); tcptransportcipher aes_cipher = new tcptransportcipher( aes_stream_cipher, cipher.encrypt_mode, aes_test_secret_key_spec, spec ); aes_cipher = new tcptransportcipher( aes_stream_cipher, cipher.decrypt_mode, aes_test_secret_key_spec, spec ); aes_ok = true; }catch( throwable e ){ logger.log( new logevent(logid, "aes unavailable", e )); } protected void completedh( byte[] buffer ) throws ioexception { try{ biginteger other_dh_y = bytestobiginteger( buffer, 0, dh_size_bytes ); keyfactory dh_key_factory = keyfactory.getinstance("dh"); publickey other_public_key = dh_key_factory.generatepublic( new dhpublickeyspec( other_dh_y, dh_p_bi, dh_g_bi )); key_agreement.dophase( other_public_key, true ); byte[] secret_bytes_64 = key_agreement.generatesecret(); // we only want the first 32 bytes of the secret secret_bytes = new byte[32]; system.arraycopy( secret_bytes_64, 0, secret_bytes, 0, 32 ); sha1_secret_bytes = new sha1simple().calculatehash( secret_bytes ); secretkeyspec secret_key_spec_a = new secretkeyspec( secret_bytes, 0, rc4_stream_key_size_bytes, rc4_stream_alg ); secretkeyspec secret_key_spec_b = new secretkeyspec( secret_bytes, 16, rc4_stream_key_size_bytes, rc4_stream_alg ); write_cipher = new tcptransportcipher( rc4_stream_cipher, cipher.encrypt_mode, outbound?secret_key_spec_a:secret_key_spec_b ); read_cipher = new tcptransportcipher( rc4_stream_cipher, cipher.decrypt_mode, outbound?secret_key_spec_b:secret_key_spec_a ); }catch( throwable e ){ throw( new ioexception( debug.getnestedexceptionmessage(e))); } } }else if ( selected_protocol == crypto_aes ){ try{ secretkeyspec secret_key_spec = new secretkeyspec( secret_bytes, 32, aes_stream_key_size_bytes, aes_stream_alg ); algorithmparameterspec spec = new ivparameterspec( secret_bytes, 48, aes_stream_key_size_bytes ); write_cipher = new tcptransportcipher( aes_stream_cipher, cipher.encrypt_mode, secret_key_spec, spec ); read_cipher = new tcptransportcipher( aes_stream_cipher, cipher.decrypt_mode, secret_key_spec, spec ); filter = new tcptransporthelperfilterstreamcipher( helper, read_cipher, write_cipher ); }catch( throwable e ){ throw( new ioexception( "aes crypto init failed: " + debug.getnestedexceptionmessage(e))); } x_1 a->b: diffie hellman ya, pada x_2 b->a: diffie hellman yb, padb x_3 a->b: hash('req1', s), hash('req2', skey)^hash('req3', s), encrypt(vc, crypto_provide, len(padc), padc, len(ia)), encrypt(ia) x_4 b->a: encrypt(vc, crypto_select, len(padd), padd ) // , len(ib)), encrypt(ib) int ib_len = 0xffff & ((( data[data.length-2] & 0xff )  65535 ){ throw( new ioexception( "invalid ib length '" + ib_len + "'" )); } read_buffer = bytebuffer.allocate( ib_len ); protocol_substate = 4; }else{ read_buffer.flip(); byte[] data = new byte[read_buffer.remaining()]; read_buffer.get( data ); initial_data_in = read_cipher.update( data ); handshakecomplete(); read_buffer = null; break; outbound_1 a sends b odd/even byte + ya + pa inbound_1 b receives ya b computes yb b computes s and hs outbound_2 b sends a yb + hs( "supported methods" + len(pb)) + pb inbound_2 a receives yb a computes s and hs a receives hs( "supported methods" + len(pb)) and decrypts using hs a skips len(pb) random bytes outbound_3 a sends sha1(s) + hs( "selected method" + len(pc)) + pc + selectedcrypt( payload ) inbound_3 b skips pa bytes until receives sha1(s) b decrypts "selected method" + len(pc) and skips len(pc) bytes to get to selectedcrypt( payload... ) b sends a selectedcrypt( payload... ) protected void process() throws ioexception { try{ process_mon.enter(); if ( handshake_complete ){ debug.out( "handshake process already completed" ); return; } boolean loop = true; while( loop ){ if ( protocol_state == ps_outbound_1 ){ if ( write_buffer == null ){ // a sends b odd/even ya + pa byte[] padding = getpadding(); write_buffer = bytebuffer.allocate( dh_public_key_bytes.length + padding.length ); write_buffer.put( dh_public_key_bytes ); write_buffer.put( padding ); write_buffer.flip(); } write( write_buffer ); if ( !write_buffer.hasremaining()){ write_buffer = null; protocol_state = ps_inbound_2; } }else if ( protocol_state == ps_outbound_2 ){ // b sends a yb + hs( "supported methods" + len(pb)) + pb if ( write_buffer == null ){ byte[] padding = getpadding(); write_buffer = bytebuffer.allocate( dh_public_key_bytes.length + 4 + 2 + padding.length ); write_buffer.put( dh_public_key_bytes ); // 4 bytes for my supported protocols write_buffer.put( write_cipher.update( new byte[]{ 0, 0, 0, my_supported_protocols })); write_buffer.put( write_cipher.update( new byte[]{ (byte)(padding.length>>8),(byte)padding.length })); write_buffer.put( padding ); write_buffer.flip(); } write( write_buffer ); if ( !write_buffer.hasremaining()){ write_buffer = null; protocol_state = ps_inbound_3; } }else if ( protocol_state == ps_outbound_3 ){ // a sends sha1(s) + hs( "selected method" + len(pc)) + pc + selectedcrypt( payload ) if ( write_buffer == null ){ byte[] padding = getpadding(); write_buffer = bytebuffer.allocate( 20 + 4 + 2 + padding.length ); write_buffer.put( sha1_secret_bytes ); write_buffer.put( write_cipher.update( new byte[]{ 0, 0, 0, selected_protocol })); write_buffer.put( write_cipher.update( new byte[]{ (byte)(padding.length>>8),(byte)padding.length })); write_buffer.put( padding ); write_buffer.flip(); } write( write_buffer ); if ( !write_buffer.hasremaining()){ write_buffer = null; handshakecomplete(); } }else if ( protocol_state == ps_inbound_1 ){ // b receives marker + ya read( read_buffer ); if ( !read_buffer.hasremaining()){ read_buffer.flip(); byte[] other_dh_public_key_bytes = new byte[read_buffer.remaining()]; read_buffer.get( other_dh_public_key_bytes ); completedh( other_dh_public_key_bytes ); read_buffer = null; protocol_state = ps_outbound_2; } }else if ( protocol_state == ps_inbound_2 ){ //a receives yb //a computes s and hs //a receives hs( "supported methods" + len(pb)) and decrypts using hs //a skips len(pb) random bytes if ( read_buffer == null ){ read_buffer = bytebuffer.allocate( dh_public_key_bytes.length + 6 ); protocol_substate = 1; } while( true ){ read( read_buffer ); if ( read_buffer.hasremaining()){ break; } if ( protocol_substate == 1 ){ read_buffer.flip(); byte[] other_dh_public_key_bytes_etc = read_buffer.array(); completedh( other_dh_public_key_bytes_etc ); byte[] etc = read_cipher.update( other_dh_public_key_bytes_etc, dh_size_bytes, 6 ); byte other_supported_protocols = etc[3]; int common_protocols = my_supported_protocols & other_supported_protocols; if (( common_protocols & crypto_plain )!= 0 ){ selected_protocol = crypto_plain; }else if (( common_protocols & crypto_xor )!= 0 ){ selected_protocol = crypto_xor; }else if (( common_protocols & crypto_rc4 )!= 0 ){ selected_protocol = crypto_rc4; }else if (( common_protocols & crypto_aes )!= 0 ){ selected_protocol = crypto_aes; }else{ throw( new ioexception( "no crypto protocol in common: mine = " + integer.tohexstring((byte)my_supported_protocols) + ", theirs = " + integer.tohexstring((byte)other_supported_protocols))); } int padding = (( etc[4] & 0xff )  padding_max ){ throw( new ioexception( "invalid padding '" + padding + "'" )); } read_buffer = bytebuffer.allocate( padding ); protocol_substate = 2; }else{ read_buffer = null; protocol_state = ps_outbound_3; break; } } }else if ( protocol_state == ps_inbound_3 ){ // b skips pa bytes until receives sha1(s) // b decrypts "selected method" + len(pc) and skips len(pc) bytes if ( read_buffer == null ){ read_buffer = bytebuffer.allocate( 20 + padding_max ); read_buffer.limit( 20 ); protocol_substate = 1; } while( true ){ read( read_buffer ); if ( read_buffer.hasremaining()){ break; } if ( protocol_substate == 1 ){ int limit = read_buffer.limit(); read_buffer.position( limit - 20 ); boolean match = true; for (int i=0;i padding_max ){ throw( new ioexception( "invalid padding '" + padding + "'" )); } read_buffer = bytebuffer.allocate( padding ); protocol_substate = 3; }else{ read_buffer = null; handshakecomplete(); break; } } } if ( handshake_complete ){ read_selector.cancel( channel ); write_selector.cancel( channel ); loop = false; complete(); }else{ if ( read_buffer == null ){ read_selector.pauseselects( channel ); }else{ read_selector.resumeselects ( channel ); loop = false; } if ( write_buffer == null ){ write_selector.pauseselects( channel ); }else{ write_selector.resumeselects ( channel ); loop = false; } } } }catch( throwable e ){ failed( e ); if ( e instanceof ioexception ){ throw((ioexception)e); }else{ throw( new ioexception( debug.getnestedexceptionmessage(e))); } }finally{ process_mon.exit(); } } private static final string dh_p = "92d862b3a95bff4e6cbdce3a266ff4b46e6e1ecad76c0a877d92a3dae4999e6414efde56fc14d1cca6d5408a8ef9ea248389168876b6e8f4503845dfe373549f"; private static final string dh_g = "4383b53ee650fd73e41e8c9e8527997ab8cb41e1cbd73ac7685493e1e5d091e3e3789dea03ab9d5b2c368faa617bb30e427cbaeb23c268edb38eb8c747756080"; private static final string dh_p = "f3f90c790c63b119f9c1be43fdb12dc6ed6f26325999c01ba6ed373e75d6b2dee8d1c0475652a987c8df57b23d395bdb142be316d780b9361f85629535030873"; private static boolean aes_ok; private static final byte[] aes_stream_iv = { (byte)0x15, (byte)0xe0, (byte)0x6b, (byte)0x7e, (byte)0x98, (byte)0x59, (byte)0xe4, (byte)0xa7, (byte)0x34, (byte)0x66, (byte)0xad, (byte)0x48, (byte)0x35, (byte)0xe2, (byte)0xd0, (byte)0x24 }; running without phe classes, not such a severe error private static final byte supported_protocols = (byte)((aes_ok?crypto_aes:0) | crypto_rc4 | crypto_xor | crypto_plain ); scattering mode for the first kb, that should include the crypto handshake and part of the bittorrent handshake system.out.println( "outbound - using crypto secret " + byteformatter.encodestring( shared_secret )); if ( !networkmanager.require_crypto_handshake ){ throw( new ioexception( "crypto encoder selected for outbound but crypto not required" )); } outbound connection, we require a certain minimal level of support incoming. if we require crypto then we use minimum otherwise available system.out.println( "secret = " + byteformatter.encodestring( secret_bytes )); "hash('keya', s, skey)" if you're a "hash('keyb', s, skey)" if you're b we only want the first 32 bytes of the secret , len(ib)), encrypt(ib) system.out.println( this + ":" + (outbound?"out: ":"in : ") + protocol_state + "/" + protocol_substate + ": r " + bytes_read + " - " + read_buffer + ", w " + bytes_written + " - " + write_buffer ); a sends b ya + pa note that /2 also used in calculating max initial packet size above b receives ya b->a: yb padb a receives: yb a initiates skey so we can now set up crypto a->b: hash('req1', s), hash('req2', skey)^hash('req3', s), encrypt(vc, crypto_provide, len(padc), padc, len(ia)), encrypt(ia) padding_a here is half of the padding from before hash('req1', s) hash('req2', skey)^hash('req3', s) encrypt(vc, crypto_provide, len(padc), padc, len(ia) reset in case buffer needs to be used again by caller b receives: hash('req1', s), hash('req2', skey)^hash('req3', s), encrypt(vc, crypto_provide, len(padc), padc, len(ia)), encrypt(ia) skip up to hash('req1', s) find skey using hash('req2', skey)^hash('req3', s) , encrypt(vc, crypto_provide, len(padc), system.out.println( "inbound - using crypto secret " + byteformatter.encodestring( shared_secret )); skip the padding encrypt( len(ia)), { encrypt(ia) } skip the padding encrypt(ia) hack alert - we can delay the writing of the outbound_4 packet if this is an incoming packet with a piggybacked bt handshake as we know that we'll be sending our own handshake back out pretty soon and it'll take the delayed data with it. to be more generic we'd need to add a callback to the pattern matcher to allow it to decide whether delaying was sensible / or stick a timer on the delayed data system.out.println( "initial data in: " + new string( data ) + "->delay=" +delay_outbound_4 ); b->a: encrypt(vc, crypto_select, len(padd), padd, // len(ib)), encrypt(ib) half padding b sent here + 2 + initial_data_out.length ); write_buffer.put( write_cipher.update( new byte[]{ (byte)(initial_data_out.length>>8),(byte)initial_data_out.length })); write_buffer.put( write_cipher.update( initial_data_out )); b->a: encrypt(vc, crypto_select, len(padd), padd // , len(ib)), encrypt(ib) skip up to marker encrypt( crypto_select, len(padd)) + 2 ); a sends b odd/even ya + pa b sends a yb + hs( "supported methods" + len(pb)) + pb 4 bytes for my supported protocols a sends sha1(s) + hs( "selected method" + len(pc)) + pc + selectedcrypt( payload ) b receives marker + ya a receives yb a computes s and hs a receives hs( "supported methods" + len(pb)) and decrypts using hs a skips len(pb) random bytes b skips pa bytes until receives sha1(s) b decrypts "selected method" + len(pc) and skips len(pc) bytes system.out.println( "read:" + this + "/" + protocol_state + "/" + protocol_substate + " -> " + len +"[" + buffer +"]"); system.out.println( "write pre:" + this + "/" + protocol_state + "/" + protocol_substate + " - " + buffer ); system.out.println( "write:" + this + "/" + protocol_state + "/" + protocol_substate + " -> " + len +"[" + buffer +"]"); allow up to 10% bloom filter utilisation limit key gen operations to 10 a second system.out.println( (outbound?"out: ":"in :") + " complete, r " + bytes_read + ", w " + bytes_written + ", initial data = " + initial_data_in.length + "/" + initial_data_out.length ); system.out.println( (outbound?"out: ":"in :") + " failed, " + cause.getmessage());"
com.aelitis.azureus.core.networkmanager.impl.RateControlledEntity "interface designation for rate-limited entities controlled by a handler. uses fair round-robin scheduling of processing ops. guaranteed scheduling of processing ops, with preference over normal-priority entities. is ready for a processing op. if it can process >0 bytes, false if not ready attempt to do a processing operation. if >0 bytes were processed (success), false if 0 bytes were processed (failure) get this entity's priority level. stats functions @return"
com.aelitis.azureus.core.networkmanager.impl.RawMessageImpl "basic raw message implementation used internally for message-->rawmessage conversions. create a new raw message using the given parameters. original message _payload headers + original message data in queue _no_delay is an urgent message _remove message types to auto-remove upon queue public boolean equals( object obj ) { //ensure we are comparing the underlying message (and its equals() override if exists) if( obj instanceof rawmessage ) { obj = ((rawmessage)obj).getbasemessage(); } return message.equals( obj ); } public int hashcode() { return message.hashcode(); } message impl rawmessage impl note: assumes that the raw payload is made up of the original message data buffers plus some header data, so returning the raw buffers will therefore also take care of the data buffers return. ensure we are comparing the underlying message (and its equals() override if exists)"
com.aelitis.azureus.core.networkmanager.impl.ReadController "processes reads of read-entities and handles the read selector. add the given entity to the controller for read processing. to process reads for remove the given entity from the controller. to remove from read processing copied-on-write copied-on-write start read handler processing skip over failed readers to find a good one force a wait make circular is ready none found ready make circular is ready none found ready copy-on-write copy-on-write copy-on-write copy-on-write"
com.aelitis.azureus.core.networkmanager.impl.SinglePeerDownloader "a fast read entity backed by a single peer connection. underlying transport not ready not allowed to receive any bytes int mss = networkmanager.gettcpmsssize(); if( num_bytes_allowed > mss ) num_bytes_allowed = mss;"
com.aelitis.azureus.core.networkmanager.impl.SinglePeerUploader "a fast write entity backed by a single peer connection. //////////////ratecontrolledwriteentity implementation //////////////////// underlying transport not ready no data to send not allowed to send any bytes debug.out("dw:not ready"); happens sometimes, just live with it as non-fatal debug.out("dw:not avail"); happens sometimes, just live with it as non-fatal int mss = networkmanager.gettcpmsssize(); if( num_bytes_to_write > mss ) num_bytes_to_write = mss;"
com.aelitis.azureus.core.networkmanager.impl.tcp.IncomingSocketChannelManager "accepts new incoming socket connections and manages routing of them to registered handlers. create manager and begin accepting and routing new connections. get port that the tcp server socket is listening for incoming connections on. number // we can have problems with sockets stuck in a time_wait state if we just // close an incoming channel - to clear things down properly the client needs // to initiate the close. so what we do is send some random bytes to the client // under the assumption this will cause them to close, and we delay our socket close // for 10 seconds to give them a chance to do so. try{ random random = new random(); byte[] random_bytes = new byte[68+random.nextint(128-68)]; random.nextbytes( random_bytes ); channel.write( bytebuffer.wrap( random_bytes )); }catch( throwable e ){ // ignore anything here } networkmanager.getsingleton().closesocketchannel( channel, 101000 ); allow dynamic port number changes allow dynamic receive buffer size changes allow dynamic bind address changes start processing run a daemon thread to poll listen port for connectivity it seems that sometimes under osx that listen server sockets sometimes stop accepting incoming connections for some unknown reason this checker tests to make sure the listen socket is still accepting connections, and if not, recreates the socket ensure it's actually running the socket server hasn't accepted any new connections in the last 10min so manually test the listen port for connectivity failback ok, let's try again without the explicit local bind it's recently accepted an inbound connection check for encrypted transport we can have problems with sockets stuck in a time_wait state if we just close an incoming channel - to clear things down properly the client needs to initiate the close. so what we do is send some random bytes to the client under the assumption this will cause them to close, and we delay our socket close for 10 seconds to give them a chance to do so. ignore anything here set advanced socket options"
com.aelitis.azureus.core.networkmanager.impl.tcp.LightweightTCPTransport "this class is essentially a socket channel wrapper to support working with az message encoders/decoders. close() can block todo auto-generated method stub"
com.aelitis.azureus.core.networkmanager.impl.tcp.ProtocolEndpointTCP ""
com.aelitis.azureus.core.networkmanager.impl.tcp.ProxyLoginHandler "handles the process of proxy login/authentication/setup. do proxy login. _connection transport connected to proxy server _address address to proxy to for proxy login success or faulure the proxied connection attempt succeeded. the proxied connection attempt failed. _msg failure reason deal with long "hostnames" that we get for, e.g., i2p destinations debug.out( t ); debug.out( t ); "v5" always use a proxy here as the calling code should know what it is doing... send initial handshake to get things started register for read ops will throw exception on error resume read ops debug.out( t ); debug.out( msg ); debug.out( t ); message reply buff send initial handshake to get things started register for read ops will throw exception on error resume read ops resume read ops debug.out( t ); debug.out( msg ); debug.out( t ); debug.out( error ); reset for next round debug.out( error ); todo convert to direct? socks 4(a) command = connect todo convert to direct? todo convert to direct? socks 4(a) command = connect port indicates socks 4a todo convert to direct? todo convert to direct? say hello system.out.println( "socks5 write phase 0" ); socks 5 2 methods no auth  todo convert to direct?  system.out.println( "socks5 write phase 1" );   password length todo convert to direct? request system.out.println( "socks5 write phase 2" ); version connect reserved use the maped ip for dns resolution so we don't leak the actual address if this is a secure one (e.g. i2p one) ip4 address type = domain name address type = domain name port todo convert to direct? system.out.println( "socks5 write phase 3..." ); reply has to be processed in two parts as it has variable length component at the end socks5_handshake_phase == 3, part two todo convert to direct? reply from hello system.out.println( "socks5 read phase 1" ); version byte no auth -> go to request phase reply from auth system.out.println( "socks5 read phase 2" ); version byte reply from request, first part system.out.println( "socks5 read phase 3" ); version byte reserved byte already read one domain name, first byte gives length of remainder already read one 2 bytes for port system.out.println( "socks5 read phase 4..." ); socks5_handshake_phase 4 reply from request, last part only done after last part of request reply has been read from stream"
com.aelitis.azureus.core.networkmanager.impl.tcp.SelectorGuard "temp class designed to help detect selector anomalies and cleanly re-open if necessary. note: as of jvm 1.4.2_03, after network connection disconnect/reconnect, usually-blocking select() and select(long) calls no longer block, and will instead return immediately. this can cause selector spinning and 100% cpu usage. see: http://forum.java.sun.com/thread.jsp?forum=4&thread=293213 http://developer.java.sun.com/developer/bugparade/bugs/4850373.html http://developer.java.sun.com/developer/bugparade/bugs/4881228.html fixed in jvm 1.4.2_05+ and 1.5b2+ create a new selectorguard with the given failed count threshold. run this method right before the select() operation to mark the start time. checks whether selector is still ok, and not spinning. non-zero select, so ok allow max_ignores / selector_spin_threshold to be successful select ops and still trigger a spin alert zero-select, but over the time threshold, so ok if we've gotten here, then we have a potential selector anomalie getting triggered with 20 +_ sometimes 40 due to general high cpu usage under windows, it seems that selector spin can sometimes appear when >63 socket channels are registered with a selector should be fixed in later 1.5, but play it safe and assume 1.6 or newer only. under linux, it seems that selector spin is somewhat common, but normal??? behavior, so just sleep a bit should only happen under windows + jre 1.4 not yet over the count threshold"
com.aelitis.azureus.core.networkmanager.impl.tcp.TCPConnectionManager "manages new connection establishment and ended connection termination. inetsocketaddress a1 = r1.address; inetsocketaddress a2 = r2.address; if ( a1.getport() == a2.getport()){ if ( arrays.equals( a1.getaddress().getaddress(), a2.getaddress().getaddress())){ return( 0 ); } } debug.printstacktrace(t); request that a new connection be made out to the given address. remote ip+port to connect to to receive notification of connect attempt success/failure close the given connection. to close cancel a pending new connection request. _key used in the initial connect request listener for notification of connection establishment. the connection establishment process has started, i.e. the connection is actively being attempted. connect timeout the connection attempt succeeded. connected socket channel the connection attempt failed. _msg failure reason should never happen, but hey parg: reduced from 30 sec as almost never see worthwhile connections take longer that this 3sec check for duplicates erm, no, think of the socks data proxy connections luke advanced socket options dont pass the exception outwards, so we will continue processing connection without advanced options set can't support nio + ipv6 on this system, pass on and don't raise an alert already connected not yet connected, so register for connect selection ensure the request hasn't been canceled during the select op should never happen do cancellations run select do connect attempt timeout checks time went backwards check if our connect queue is stalled, and expand if so insert at a random position because new connections are usually added in 50-peer chunks, i.e. from a tracker announce reply, and we want to evenly distribute the connect attempts if there are multiple torrents running this comparison is via comparator and will weed out same address being added > once check if we can cancel it right away else add for later removal during select ///////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////"
com.aelitis.azureus.core.networkmanager.impl.tcp.TCPNetworkManager "get the configured tcp mss (maximum segment size) unit, i.e. the max (preferred) packet payload size. note: mss is mtu-40bytes for tcpip headers, usually 1460 (1500-40) for standard ethernet connections, or 1452 (1492-40) for pppoe connections. size in bytes get the socket channel connect / disconnect manager. manager get the virtual selector used for socket channel read readiness. readiness selector get the virtual selector used for socket channel write readiness. readiness selector get port that the tcp server socket is listening for incoming connections on. number start read selector processing start write selector processing"
com.aelitis.azureus.core.networkmanager.impl.tcp.TCPTransportHelper "protected void log( bytebuffer[] buffers, int array_offset, int length ) { system.out.println( "writing group of " + length ); for( int i=array_offset; i 64?64:rem]; buffer.get( temp ); buffer.position( position ); system.out.println( " writing " + rem + ": " + new string(temp)); } aim here is to catch headers default is tcp so don't clutter up views with this info partial-write means we are guaranteed to get hit with another write straight away system.out.println( "delayed write: single" ); note that we can't report delayed bytes actually written as these have already been accounted for and confuse the layers above if we report them now system.out.println( "delayed write: single incomp" ); log( buffer ); system.out.println( "delayed write: mult (" + buffers2.length + ")" ); system.out.println( "delayed write: mult incomp" ); log( buffers, array_offset, length ); a bug only fixed in tiger (1.5 series): http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4854354 single-buffer mode a bug only fixed in tiger (1.5 series): http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4854354 single-buffer mode"
com.aelitis.azureus.core.networkmanager.impl.tcp.TCPTransportHelperFilterFactory ""
com.aelitis.azureus.core.networkmanager.impl.tcp.TCPTransportImpl "represents a peer tcp transport connection (eg. a network socket). constructor for disconnected (outbound) transport. constructor for connected (inbound) transport. connection _read bytes from the channel get the socket channel used by the transport. socket channel get a textual description for this transport. request the transport connection be established. note: will automatically connect via configured proxy if necessary. remote peer address to connect to establishment failure/success listener set the transport to the given speed mode. to change to get the transport's speed mode. mode close the transport connection. inbound connections will automatically be using crypto if necessary already connected closed between select ops just close it proxy server connection established, login set up a transparent filter for socks negotiation direct connection established, notify attempt encrypted transport system.out.println( description+ " | crypto handshake success [" +_filter.getname()+ "]" ); this is outgoing this is outgoing no crypto if( fallback_count > 0 ) { system.out.println( channel.socket()+ " | non-crypto fallback successful!" ); } already in mode we need to set it ready for reading so that the other scheduling thread wakes up and discovers that this connection has been closed"
com.aelitis.azureus.core.networkmanager.impl.tcp.TransportEndpointTCP ""
com.aelitis.azureus.core.networkmanager.impl.tcp.TransportStartpointTCP ""
com.aelitis.azureus.core.networkmanager.impl.tcp.VirtualAcceptSelector "virtual server socket channel for listening and accepting incoming connections. listener notified when a new incoming connection is accepted. the given connection has just been accepted. new connection"
com.aelitis.azureus.core.networkmanager.impl.tcp.VirtualBlockingServerChannelSelector "virtual server socket channel for listening and accepting incoming connections. create a new server listening on the given address and reporting to the given listener. _address ip+port to listen on _rcvbuf_size new socket receive buffer size to notify of incoming connections start the server and begin accepting incoming connections. stop the server. is thrown when stop() is called is this selector actively running if enabled, false if not running init to now"
com.aelitis.azureus.core.networkmanager.impl.tcp.VirtualChannelSelectorImpl "provides a simplified and safe (selectable-channel) socket single-op selector. static boolean rm_trace = false; static boolean rm_test_fix = false; static{ coconfigurationmanager.addandfireparameterlisteners( new string[]{ " if( interest_op == virtualchannelselector.op_read ) { //todo select_counts[ round ] = count; round++; if( round == select_counts.length ) { stringbuffer buf = new stringbuffer( select_counts.length 3 ); buf.append( "select_counts=" ); for( int i=0; i = 0 ) { try { thread.sleep( 10 - time_diff ); }catch(throwable e) { e.printstacktrace(); } } } if ( rm_trace ){ if ( select_start_time - rm_flag_last_log > 10000 ){ rm_flag_last_log = select_start_time; iterator it = rm_listener_map.entryset().iterator(); string str = ""; while( it.hasnext()){ map.entry entry = (map.entry)it.next(); class cla = (class)entry.getkey(); string name = cla.getname(); int pos = name.lastindexof('.'); name = name.substring( pos+1 ); int[] counts = (int[])entry.getvalue(); str += (str.length()==0?"":",")+ name + ":" + counts[0]+"/"+counts[1]+"/"+counts[2]+"/"+counts[3]; } debug.outnostack( "rm trace: " + hashcode() + ": op=" + interest_op + "-" + str ); } } note that you have to ensure that a select operation is performed on the normal select loop after destroying the selector to actually cause the destroy to occur freebsd 7.x and diablo 1.6 no works as selector returns none ready even though there's a bunch readable seems to not just be diablo java, but general 7.1 problem hack for 10.6 - will switch to not doing system.setproperty( "java.nio.preferselect", "true" ); later unfortunately the package maintainer has set os.name to linux for freebsd... private int[] select_counts = new int[ 50 ]; private int round = 0; success ! failure system.out.println( "pauseselects: " + channel + " - " + debug.getcompressedstacktrace() ); channel not (yet?) registered only bother if channel has not already been closed ensure the op is paused upon reg select-time reg system.out.println( "resumeselects: " + channel + " - " + debug.getcompressedstacktrace() ); if we're resuming a non-interested key then reset the metrics channel not (yet?) registered check if the channel's op has been already paused before select-time reg try{ selector.wakeup(); } catch( throwable t ) { debug.out( "selector.wakeup():: caught exception: ", t ); } system.out.println( "cancel: " + channel + " - " + debug.getcompressedstacktrace() ); don't worry too much about cancels ensure that there's only one operation outstanding for a given channel at any one time (the latest operation requested ) remove existing cancel or register ensure that there's only one operation outstanding for a given channel at any one time (the latest operation requested ) store these when they occur so they can be raised outside of the monitor to avoid potential deadlocks process cancellations don't use an iterator here as it is possible that error notifications to listeners can result in the addition of a cancel request. note that this can only happen for registrations, and this should only result in possibly a cancel being added (i.e. not a further registration), hence this can't loop. also note the approach of removing the entry before processing. this is so that the logic used when adding a cancel (the removal of any matching entries) does not cause the entry we're processing to be removed process cancellation cancel the key, since already registered process new registrations see if already registered already registered ensure op is enabled check if op has been paused before registration moment pause it reset after every registration round do the actual select do this after the select so that any pending cancels (prior to destroy) are processed by the selector before we kill it todo system.out.println( buf.tostring() ); notification of ready keys via listener callback debug handling for channels stuck pending write select for long periods int rm_type; it must have been paused between select and notification rm_type = 2; rm_type = 0; rm_type = 1; we get no progress triggers when looping back for transcoding due to high cpu usage of xcode process - remove debug spew and be more tolerant rm_type = 3; can get this if socket has been closed between select and here get quite a few of these exceptions, ignore the key hack - trigger a dummy write select to see if things are still ok if any of the ready keys hasn't made any progress then enforce minimum sleep period to avoid spinning ensure that it always takes at least 'timeout' time to complete the select op"
com.aelitis.azureus.core.networkmanager.impl.tcp.VirtualNonBlockingServerChannelSelector "virtual server socket channel for listening and accepting incoming connections. create a new server listening on the given address and reporting to the given listener. _address ip+port to listen on _rcvbuf_size new socket receive buffer size to notify of incoming connections start the server and begin accepting incoming connections. stop the server. is this selector actively running if enabled, false if not running init to now"
com.aelitis.azureus.core.networkmanager.impl.test.PHETester "outgoing_plain = true;"
com.aelitis.azureus.core.networkmanager.impl.TransferProcessor "create new transfer processor for the given read/write type, limited to the given max rate. _type read or write processor _rate_limit to use register peer connection for upload handling. note: the given max rate limit is ignored until the connection is upgraded. to register rate limit group if ( log ){ system.out.println( "creating rl1: " + group.getname() + " -> " + group_data ); } if ( log ){ system.out.println( "applying rl1: " + group.getname() + " -> " + connection ); } cancel upload handling for the given peer connection. to cancel if ( log ){ system.out.println( "creating rl2: " + group.getname() + " -> " + group_data ); } if ( log ){ system.out.println( "applying rl2: " + group.getname() + " -> " + connection ); } upgrade the given connection to a high-speed transfer handler. to upgrade if ( log ){ long now = systemtime.getcurrenttime(); if ( now - last_log > 500 ){ last_log = now; system.out.println( " " + group.getname() + " -> " + group_rate + "/" + group_bucket.getavailablebytecount()); } } downgrade the given connection back to a normal-speed transfer handler. to downgrade sync rate do group registration boolean log = group.getname().contains("parg"); do groups de-registration last of the group so remove boolean log = group.getname().contains("parg"); last of the group so remove private static long last_log = 0; sync global rate reserve bandwidth for the general pool only apply group rates to non-lan local connections sync group rates boolean log = group.getname().contains("parg"); conn_data.group stuff is not synchronized for speed but can cause borkage if new limiters added so trap here"
com.aelitis.azureus.core.networkmanager.impl.TransportCipher "force internal as we want 160 bit and jce no supports it system.out.println( "rc4 key: " + byteformatter.encodestring( key_spec.getencoded())); skip first 1024 bytes of stream to protected against a fluhrer, mantin and shamir attack system.out.println( "rc4: first discard = " + byteformatter.encodestring( temp, 0, 4 )); watch out, cipher.update returns null with 0 length input todo: 1.5 supports update( bytebuffer, bytebuffer )"
com.aelitis.azureus.core.networkmanager.impl.TransportCryptoManager ""
com.aelitis.azureus.core.networkmanager.impl.TransportHelper ""
com.aelitis.azureus.core.networkmanager.impl.TransportHelperFilter ""
com.aelitis.azureus.core.networkmanager.impl.TransportHelperFilterInserter ""
com.aelitis.azureus.core.networkmanager.impl.TransportHelperFilterStream "deal with any outstanding cached crypted data first skip "written" bytes in the source if write came up short or we've filled the source buffer then we can't do any more problem - we must only crypt stuff once and when crypted it has to be sent (else the stream will get out of sync). so we have to turn this into single buffer operations we gotta pretend at least 1 byte was written to guarantee that the caller writes the rest system.out.println( "...write " + total_written ); system.out.println( "...read " + total_read );"
com.aelitis.azureus.core.networkmanager.impl.TransportHelperFilterStreamCipher ""
com.aelitis.azureus.core.networkmanager.impl.TransportHelperFilterStreamXOR ""
com.aelitis.azureus.core.networkmanager.impl.TransportHelperFilterSwitcher "writer may have data buffered up pending next write call - if so then we need to get out now"
com.aelitis.azureus.core.networkmanager.impl.TransportHelperFilterTransparent ""
com.aelitis.azureus.core.networkmanager.impl.TransportImpl "is the transport ready to write, i.e. will a write request result in >0 bytes written. if the transport is write ready, false if not yet ready is the transport ready to read, i.e. will a read request result in >0 bytes read. 0 if the transport is read ready, millis since last ready or -1 if never ready write data to the transport from the given buffers. note: works like gatheringbytechannel. from which bytes are to be retrieved _offset offset within the buffer array of the first buffer from which bytes are to be retrieved maximum number of buffers to be accessed of bytes written @throws ioexception on write error read data from the transport into the given buffers. note: works like scatteringbytechannel. into which bytes are to be placed _offset offset within the buffer array of the first buffer into which bytes are to be placed maximum number of buffers to be accessed of bytes read @throws ioexception on read error default impl = extract from encryption set to true so that the next write attempt will throw an exception make sure > 0 msg picked up on another thread - make sure trace is available set to true so that the next read attempt will throw an exception todo insert already-read data into the front of the stream the last buffer has nothing left to read into normally so return right away, skipping socket read todo read selection"
com.aelitis.azureus.core.networkmanager.impl.TransportStats "bytes"
com.aelitis.azureus.core.networkmanager.impl.udp.NetworkGlue ""
com.aelitis.azureus.core.networkmanager.impl.udp.NetworkGlueListener ""
com.aelitis.azureus.core.networkmanager.impl.udp.NetworkGlueLoopBack ""
com.aelitis.azureus.core.networkmanager.impl.udp.NetworkGlueUDP "first and third word must have something set in mask: 0xfffff800 consume this packet don't consume it, allow it to be passed on for further processing"
com.aelitis.azureus.core.networkmanager.impl.udp.ProtocolEndpointUDP ""
com.aelitis.azureus.core.networkmanager.impl.udp.TransportEndpointUDP ""
com.aelitis.azureus.core.networkmanager.impl.udp.UDPConnection "packets reach us using 8k space regardless of content - trim this back for small protocol messages to save memory notification that a packet has been sent system.out.println( "connection(" + getid() + ") - write -> " + written ); system.out.println( "connection(" + getid() + ") - read -> " +total );"
com.aelitis.azureus.core.networkmanager.impl.udp.UDPConnectionManager "this is outgoing this is outgoing called while holding the "connections" monitor called while holding the "connections" monitor check that this at least looks like an initial crypto packet we can get quite a lot of these if things get out of sync debug.out( "incoming udp packet mismatch for connection establishment: " + key ); system.out.println( "recv:" + byteformatter.encodestring( data, 0, data_length>64?64:data_length ) + (data_length>64?"...":"")); allow up to 10% bloom filter utilisation limit to 10 a second no fallback for udp"
com.aelitis.azureus.core.networkmanager.impl.udp.UDPConnectionSet "string done = ""; for (int i=0;i7 bytes int checksum for data packets: payload change this for data and you must change protocol_data_header_size insert space for length added later hash includes real sequence + header content but obviously not including the hash don't encrypt the sequence numbers final poll incase there are ignorant listeners run a final poll operation to inform any selector listeners of the failure this is here to draw attention to the fact that there's a dependency between packet formats... damn tracker udp protocol has: request: long (random connection id) int (action) reply: int (action) int (random txn id) now action is always < 2048 so all other uses of udp packets will have either 0x000007ff in either bytes 9 or 0 onwards. so we're forced to use 12 byte sequence numbers internally we use the middle integer as the packet sequence a secondary identifier for the sequence is also generated to be used in header position 0-2 and 8-10 when reporting last sequences in the clear"
com.aelitis.azureus.core.networkmanager.impl.udp.UDPNetworkManager ""
com.aelitis.azureus.core.networkmanager.impl.udp.UDPPacket ""
com.aelitis.azureus.core.networkmanager.impl.udp.UDPSelector "one last dispatch cycle"
com.aelitis.azureus.core.networkmanager.impl.udp.UDPTransport ""
com.aelitis.azureus.core.networkmanager.impl.udp.UDPTransportHelper "default is paused outgoing incoming system.out.println( "total = " + total ); most likely selector has been destroyed so don't fire writeselect else we'll get into a loop most likely selector has been destroyed so don't fire readselect else we'll get into a loop currently not implemented for udp"
com.aelitis.azureus.core.networkmanager.impl.WriteController "processes writes of write-entities and handles the write selector. create a new write controller. add the given entity to the controller for write processing. to process writes for remove the given entity from the controller. to remove from write processing copied-on-write copied-on-write copied-on-write start write handler processing make circular is ready give remaining normal peers a chance to use the bandwidth boosted peers couldn't, but prevent more from being allocated while doing so to prevent them from grabbing more than they should make circular is ready make circular is ready none found ready copy-on-write copy-on-write copy-on-write"
com.aelitis.azureus.core.networkmanager.IncomingMessageQueue "inbound peer message queue. set the message stream decoder that will be used to decode incoming messages. _stream_decoder to use get the percentage of the current message that has already been received. complete (0-99), or -1 if no message is currently being received receive (read) message(s) data from the underlying transport. _bytes to read of bytes received @throws ioexception on receive error notifty the queue (and its listeners) of a message received externally on the queue's behalf. received externally manually resume processing (reading) incoming messages. note: allows us to resume docoding externally, in case it was auto-paused internally. add a listener to be notified of queue events. cancel queue event notification listener. destroy this queue. for notification of queue events. a message has been read from the connection. recevied if this message was accepted, false if not handled the given number of protocol (overhead) bytes read from the connection. _count number of protocol bytes the given number of (piece) data bytes read from the connection. _count number of data bytes"
com.aelitis.azureus.core.networkmanager.LimitedRateGroup "allows for grouping of connections under a singular limit. get the current rate limit. in bytes per second, 0 for unlimited, -1 for disabled"
com.aelitis.azureus.core.networkmanager.NetworkConnection "represents a managed network connection, over which messages can be sent and received. connect this connection's transport, i.e. establish the network connection. if this connection is already established (from an incoming connection for example), then this provides a mechanism to register the connection listener, in which case connectsuccess() will be called immediately. notified on connect success or failure close and shutdown this connection. begin processing incoming and outgoing message queues. _group upload rate limit group to use _group download rate limit group to use upgrade the connection to high-speed transfer processing. true for high-speed processing, false for normal processing decouples the transport from this network connection so it can be reused if detach failed get the connection's data transport interface. transport - may be null if not yet fully connected listener for notification of connection events. the connection establishment process has started, i.e. the connection is actively being attempted. connect timeout the connection attempt succeeded. the connection is now established. note: called only during initial connect attempt. the connection attempt failed. note: called only during initial connect attempt. _msg failure reason handle exception thrown by this connection. note: can be called at any time during connection lifetime. exception"
com.aelitis.azureus.core.networkmanager.NetworkConnectionBase "inform connection of a thrown exception. exception get the connection's outgoing message queue. message queue get the connection's incoming message queue. message queue get the connection's data transport interface. transport - may be null if not yet fully connected is the connection within the local lan network. if within lan, false of outside the lan segment"
com.aelitis.azureus.core.networkmanager.NetworkConnectionFactory "create an outgoing connection. _address connection create an incoming connection. _channel _already_read connection"
com.aelitis.azureus.core.networkmanager.NetworkConnectionHelper ""
com.aelitis.azureus.core.networkmanager.NetworkManager "this method is for display purposes only, the internal rate limiting is 10% higher than returned by this method! get the singleton instance of the network manager. network manager create a new unconnected remote network connection (for outbound-initiated connections). _address to connect to default message stream encoder to use for the outgoing queue default message stream decoder to use for the incoming queue new connection request the acceptance and routing of new incoming connections that match the given initial byte sequence. initial byte sequence used for routing for handling new inbound connections to use for creating default stream encoder/decoders cancel a request for inbound connection routing. byte sequence originally used to register add an upload entity for write processing. to add remove an upload entity from write processing. to remove add a download entity for read processing. to add remove a download entity from read processing. to remove register peer connection for network upload and download handling. note: the given max rate limits are ignored until the connection is upgraded. note: the given max rate limits are ignored for lanlocal connections. _connection to register for network transfer processing _group upload rate limit group _group download rate limit group cancel network upload and download handling for the given connection. _connection to cancel upgrade the given connection to high-speed network transfer handling. _connection to upgrade downgrade the given connection back to a normal-speed network transfer handling. _connection to downgrade byte stream match filter for routing. number of bytes of buffer at or beyond which the "match" method will be called to test for a match @return get the max number of bytes this matcher requires. if it fails with this (or more) bytes then the connection will be dropped in bytes get the minimum number of bytes required to determine if this matcher applies @return check byte stream for match. the originator of the connection _compare return "routing data" in case of a match, null otherwise check for a minimum match _compare "routing data" in case of a match, null otherwise listener for routing events. currently if message crypto is on and default fallback for incoming not enabled then we would bounce incoming messages from non-crypto transports for example, nat check this method allows auto-fallback for such transports @return the given incoming connection has been accepted. accepted 100 mbyte/s leave 5kib/s room for the request limiting ensure that mss isn't greater than up/down rate limits"
com.aelitis.azureus.core.networkmanager.OutgoingMessageQueue "priority-based outbound peer message queue. set the message stream encoder that will be used to encode outgoing messages. _encoder to use get the percentage of the current message that has already been sent out. complete (0-99), or -1 if no message is currently being sent destroy this queue; i.e. perform cleanup actions. get the total number of bytes ready to be transported. bytes remaining whether or not an urgent message (one that needs an immediate send, i.e. a no-delay message) is queued. if there's a message tagged for immediate write add a message to the message queue. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. message to add _listener_notify true for manual notification, false for automatic remove all messages of the given types from the queue. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. _types type to remove _listener_notify true for manual notification, false for automatic remove a particular message from the queue. note: only the original message found in the queue will be destroyed upon removal, which may not necessarily be the one passed as the method parameter, as some messages override equals() (i.e. btrequest messages) instead of using reference equality, and could be a completely different object, and would need to be destroyed manually. if the message does not override equals, then any such method will likely not be found and removed, as internal queued object was a new allocation on insertion. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. to remove _listener_notify true for manual notification, false for automatic if the message was removed, false otherwise deliver (write) message(s) data to the underlying transport. note: allows for manual listener notification at some later time, using dolistenernotifications(), instead of notifying immediately from within this method. this is useful if you want to invoke listeners outside of some greater synchronised block to avoid deadlock. _bytes maximum number of bytes to deliver _listener_notify true for manual notification, false for automatic of bytes delivered @throws ioexception on delivery error manually send any unsent listener notifications. force all pending messages to be delivered add a listener to be notified of queue events. cancel queue event notification listener. notifty the queue (and its listeners) of a message sent externally on the queue's behalf. sent externally receive notification of queue events. the given message has just been added to the queue. added for queuing if this message addition is allowed, false if it should be immediately removed without being queued or sent the given message has just been queued for sending out the transport. queued the given message has just been forcibly removed from the queue, i.e. it was not sent out the transport. removed the given message has been completely sent out through the transport. sent the given number of protocol (overhead) bytes has been written to the transport. _count number of protocol bytes the given number of (piece) data bytes has been written to the transport. _count number of data bytes ///////////////////////////////////////////////////////////////"
com.aelitis.azureus.core.networkmanager.ProtocolEndpoint ""
com.aelitis.azureus.core.networkmanager.ProtocolEndpointFactory ""
com.aelitis.azureus.core.networkmanager.ProtocolEndpointHandler ""
com.aelitis.azureus.core.networkmanager.ProtocolStartpoint ""
com.aelitis.azureus.core.networkmanager.RateHandler "handler to allow external control of an entity's byte processing rate. get the current number of bytes allowed to be processed by the entity. of bytes allowed notification of any bytes processed by the entity. _bytes_processed"
com.aelitis.azureus.core.networkmanager.RawMessage "a raw data message designed for advanced queueing. get the message's raw data payload. payload buffers get the message's queue priority. is this a no-delay message. no-delay messages are transmitted immediately, i.e. force-flushed out the transport. if a no-delay message set no-delay for this message _delay get the yet-unsent message types that should be removed before queueing this message for sending. types; null if no types get the message this raw message is based upon. message"
com.aelitis.azureus.core.networkmanager.Transport "represents a peer transport connection (eg. a network socket). inject the given already-read data back into the read stream. _already_read data get the socket channel used by the transport. socket channel return a textual description of the encryption for this transport @return fake a wakeup so that a read cycle is attempted write data to the transport from the given buffers. note: works like gatheringbytechannel. from which bytes are to be retrieved _offset offset within the buffer array of the first buffer from which bytes are to be retrieved maximum number of buffers to be accessed of bytes written @throws ioexception on write error read data from the transport into the given buffers. note: works like scatteringbytechannel. into which bytes are to be placed _offset offset within the buffer array of the first buffer into which bytes are to be placed maximum number of buffers to be accessed of bytes read @throws ioexception on read error set the transport to the given speed mode. to change to get the transport's speed mode. mode kick off an outbound connection indicate that inbound connection is complete close the transport connection. listener for notification of connection establishment. the connection establishment process has started, i.e. the connection is actively being attempted. timeout the connection attempt succeeded. the connection is now established. the connection attempt failed. _msg failure reason"
com.aelitis.azureus.core.networkmanager.TransportBase "is the transport ready to write, i.e. will a write request result in >0 bytes written. if the transport is write ready, false if not yet ready is the transport ready to read, i.e. will a read request result in >0 bytes read. 0 if the transport is read ready, ms since last ready or get a textual description for this transport."
com.aelitis.azureus.core.networkmanager.TransportEndpoint ""
com.aelitis.azureus.core.networkmanager.TransportStartpoint ""
com.aelitis.azureus.core.networkmanager.VirtualChannelSelector "create a new virtual selectable-channel selector, selecting over the given interest-op. _op operation set of op_connect, op_accept, op_read, or op_write _after_select whether or not to auto-disable interest op after select register the given selectable channel, using the given listener for notification of completed select operations. note: for op_connect and op_write -type selectors, once a selection request op completes, the channel's op registration is automatically disabled (paused); any future wanted selection notification requires re-enabling via resume. for op_read selectors, it stays enabled until actively paused, no matter how many times it is selected. socket to listen for op-complete listener object to be passed back with listener notification pause selection operations for the given channel to pause resume selection operations for the given channel to resume cancel the selection operations for the given channel. channel originally registered run a virtual select() operation, with the given selection timeout value; (1) cancellations are processed (2) the select operation is performed; (3) listener notification of completed selects (4) new registrations are processed in ms; if zero, block indefinitely of sockets selected listener for notification upon socket channel selection. called when a channel is successfully selected for readyness. originally given with the channel's registration of whether or not any 'progress' was made due to this select null -> progress made, string -> location of non progress e.g. read-select -> read >0 bytes, write-select -> wrote > 0 bytes called when a channel selection fails. failure message called when a channel is successfully selected for readyness. originally given with the channel's registration of whether or not any 'progress' was made due to this select e.g. read-select -> read >0 bytes, write-select -> wrote > 0 bytes called when a channel selection fails. failure message only used in faulty mode system.out.println( " safe socket selector mode enabled " ); system.out.println( "register - " + channel.hashcode() + " - " + debug.getcompressedstacktrace()); it seems that we have a bug somewhere where a selector is being registered but not cancelled on close. as an interim fix scan channels and remove any closed ones there's room in the current selector we couldnt find room in any of the existing selectors, so start up a new one if allowed max limit to the number of selectors we are allowed to create reject registration reject registration system.out.println( "pause - " + channel.hashcode() + " - " + debug.getcompressedstacktrace()); system.out.println( "resume - " + channel.hashcode() + " - " + debug.getcompressedstacktrace()); system.out.println( "cancel - " + channel.hashcode() + " - " + debug.getcompressedstacktrace()); destruction process requires select op after destroy..."
com.aelitis.azureus.core.networkmanager.VirtualServerChannelSelector "virtual server socket channel for listening and accepting incoming connections. listener notified when a new incoming connection is accepted. the given connection has just been accepted. new connection"
com.aelitis.azureus.core.networkmanager.VirtualServerChannelSelectorFactory "test param to allow multiple ports to be"
com.aelitis.azureus.core.neuronal.ActivationFunction ""
com.aelitis.azureus.core.neuronal.LogisticActivationFunction ""
com.aelitis.azureus.core.neuronal.NeuralNetwork ""
com.aelitis.azureus.core.neuronal.NeuralNetworkLayer "output layer input layer hidden layer"
com.aelitis.azureus.core.neuronal.NeuralSpeedLimiter "system.out.println("input : " + (double)dlspeed/maxdlspeed + ", " + (double)ulspeed/maxulspeed + ", " + ((double)latency-(double)minlatency)/maxlatency); system.out.println("output : " + neuralnetwork.getoutput(0) + ", " + neuralnetwork.getoutput(1) + ", " + neuralnetwork.getoutput(2) + ", " + neuralnetwork.getoutput(3)); in bytes / sec in ms dl speed, ul speed, latency, no_down_limit, down_limit, no_up_limit, up_limit no latency => no limits no download, high upload, mid-high latency => limit upload no download, high upload, low-mid latency => some limit upload no upload, high download, mid-high latency => limit download no upload, high download, low-mid latency => some limit download system.out.println(neuralnetwork); ignore only 100 loops at a time so, we have a high latency, let's re-train the neural network to lower upload speed in this case so, we have a low latency, let's re-train the neural network to increase upload speed in this case"
com.aelitis.azureus.core.pairing.impl.PairingManagerImpl "srp explicit ignore swt errors console ui try not to loose this! when we get to zero services we want to push through the last update to remove cd we need a valid access code here! grab some upnp info for diagnostics some don't record any incoming stuff during a test as we don't want to show this as a significant event ignore swt errors console ui"
com.aelitis.azureus.core.pairing.impl.PairingManagerTunnelHandler "no point in this atm as we don't support v6 tunnels dhtnatpuncher puncher = dht.getnatpuncher(); if ( puncher != null ){ punchers.add( puncher ); nat_punchers_ipv6.add( puncher ); puncher = puncher.getsecondarypuncher(); punchers.add( puncher ); nat_punchers_ipv6.add( puncher ); } remove /pairing/tunnel/"
com.aelitis.azureus.core.pairing.impl.PairManagerTunnel "'expected' failure if we're getting dumped on by proxies etc."
com.aelitis.azureus.core.pairing.impl.swt.PMSWTImpl ""
com.aelitis.azureus.core.pairing.PairedNode ""
com.aelitis.azureus.core.pairing.PairedService ""
com.aelitis.azureus.core.pairing.PairedServiceRequestHandler ""
com.aelitis.azureus.core.pairing.PairingConnectionData ""
com.aelitis.azureus.core.pairing.PairingException ""
com.aelitis.azureus.core.pairing.PairingManager ""
com.aelitis.azureus.core.pairing.PairingManagerFactory ""
com.aelitis.azureus.core.pairing.PairingManagerListener ""
com.aelitis.azureus.core.pairing.PairingTest "waiting to start yay server did its stuff, couldn't connect server not running server too busy server failed (e.g. db down) you cancelled the test"
com.aelitis.azureus.core.pairing.PairingTestListener ""
com.aelitis.azureus.core.peer.cache.CacheDiscoverer ""
com.aelitis.azureus.core.peer.cache.CacheDiscovery "import com.aelitis.azureus.core.peer.cache.cachelogic.clcachediscovery; no longer supported: new clcachediscovery(), reverse lookups can be very slow"
com.aelitis.azureus.core.peer.cache.cachelogic.CLCacheDiscovery "this class handles communication with the cdp server. original code from convert an array of bytes into a its hexadecimal representation. find out the farm name via cdp. get the ip address of the cdp servers. create a udp socket for the cdp query. build the cdp query. send the query to the cdp server. receive the cdp response. parse the cdp response. return the farmid from the cdp response. calculate publisher string for dns query from announce url. calculate sha1 hash of the hostname. return first 32 characters of the hexadecimal representation of the sha1 hash. find a cache for a given announce url and bittorrent hash. build the hostname for the dns query: bt-.bt--.find-cache.com short hash: first four hexadecimal digits of the bittorrent hash announce hash: see hashannounceurl() farm: farm name returned by cdp query. find a cache for a given announce url and bittorrent hash. construct a query for a given client identifier. return the binary representation. this class parses a cdp response returned by the cdp server. the cdp farm id. this cdp response is valid until this point of time. create a response object from a given binary encoded cdp message. return the farm id. is the information in this cdp response still valid? system.out.println("findcache(): " + announce_url + " " + hex_hash + " --> " + hostname); version flags length of the client identifier"
com.aelitis.azureus.core.peer.cache.CachePeer ""
com.aelitis.azureus.core.peermanager.control.impl.PeerControlSchedulerBasic "system.out.println( "stats: time = " + stats_diff + ", ticks = " + tick_count + ", inst = " + instances.size());"
com.aelitis.azureus.core.peermanager.control.impl.PeerControlSchedulerImpl ""
com.aelitis.azureus.core.peermanager.control.impl.PeerControlSchedulerPrioritised "for (iterator it=instances.iterator();it.hasnext();){ instancewrapper inst = (instancewrapper)it.next(); long target = inst.getnexttick(); long diff = target - latest_time_used; if ( diff  schedule_period_millis ){ inst.schedule(); long new_target = target + schedule_period_millis; diff = new_target - latest_time_used; if ( diff  schedule_period_millis ) new_target = latest_time_used + schedule_period_millis; inst.setnexttick( new_target ); } } order instances by their priority (lowest number first) too early for next task, continue waiting system.out.println("scheduling "+i+" time:"+latest_time); try to run every task every schedule_period_millis on average if tasks hog too much time then delay to prevent massive catch-up-hammering system.out.println( "stats: time = " + stats_diff + ", ticks = " + tick_count + ", inst = " + instances.size());"
com.aelitis.azureus.core.peermanager.control.impl.SpeedTokenDispenserBasic ""
com.aelitis.azureus.core.peermanager.control.impl.SpeedTokenDispenserPrioritised "crude tbf implementation sanity check shortest possible delta cap buffer to threshold in case something accumulated allow at least 2 outstanding requests time (in seconds) at max speed until the buffer is empty: too low = latency issues; too high = overshooting for too long n kib buffer per 1kib/s speed, that should be roughly n seconds max response time upcast to long since we might exceed int-max when rate and delta are large enough; then downcast again... system.out.println("threshold:" + threshold + " update: " + bucket + " time delta:" + delta);"
com.aelitis.azureus.core.peermanager.control.PeerControlInstance ""
com.aelitis.azureus.core.peermanager.control.PeerControlScheduler ", set to true to disable weighted priorities, false to use"
com.aelitis.azureus.core.peermanager.control.PeerControlSchedulerFactory ""
com.aelitis.azureus.core.peermanager.control.SpeedTokenDispenser ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZBadPiece ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZGenericMapPayload "this is a helper class for creating messages with a map'd beencode-able payload. create a new az message with the given message type id, with the given bencode-able map payload. _type of message payload (to be bencoded)"
com.aelitis.azureus.core.peermanager.messaging.azureus.AZHandshake "az handshake message. verify given port info is ok skip ourself client info available message list skip ourself random padding if crypto old handshake old handshake old handshake only 2307+ send type"
com.aelitis.azureus.core.peermanager.messaging.azureus.AZHave ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZMessage "a core az type peer message. todo"
com.aelitis.azureus.core.peermanager.messaging.azureus.AZMessageDecoder "length-prefixed message decoding. nothing int lbuff_read = 0; int pbuff_read = 0; length_buffer.limit( ss, 4 ); if( reading_length_mode ) { lbuff_read = length_buffer.position( ss ); } else { //reading payload length_buffer.position( ss, 4 ); lbuff_read = 4; pbuff_read = payload_buffer == null ? 0 : payload_buffer.position( ss ); } bytebuffer unused = bytebuffer.allocate( lbuff_read + pbuff_read ); length_buffer.flip( ss ); unused.put( length_buffer.getbuffer( ss ) ); if ( payload_buffer != null ) { payload_buffer.flip( ss ); unused.put( payload_buffer.getbuffer( ss ) ); } unused.flip(); 4 byte id length + at least 1 byte for id + 1 byte version 128k arbitrary limit destruction currently isn't thread safe so one thread can destroy the decoder (e.g. when closing a connection) while the read-controller is still actively processing the us debug.out( "az decoder already destroyed: " +transport.getdescription() ); only read into length buffer read payload buffer, and possibly next message length buffer reading payload as access to messages_last_read isn't synchronized we can get this error if we destroy the decoder in parallel with messages being removed. we don't really want to synchronize access to this so we'll take the hit here return unused; note: we don't bother returning any already-read data ensure the decode array has the latest payload pointer set buffer limits according to bytes allowed ensure no read into this next buffer is possible skip full buffer read only part of this buffer limit current buffer shrink any tail buffers full buffer is allowed to be read count this buffer toward allowed and move on to the next reading payload data mode ensure-restore proper buffer limits need to have read the message id length first 4 bytes restore need to have also read the message id bytes restore full message received! prepare for use maintain unexpected erorrs as such so they get logged later see if we've already read the next message's length reset receive percentage only partial received so far compute receive percentage ensure proper buffer limit done reading the length reset it for next length read"
com.aelitis.azureus.core.peermanager.messaging.azureus.AZMessageEncoder ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZMessageFactory "factory for handling az message creation. note: wire format: [total message length] + [id length] + [id bytes] + [version byte] + [payload bytes] initialize the factory, i.e. register the messages with the message manager. messagemanager.getsingleton().registermessagetype( new azsessionsyn( new byte[20], -1, null) ); messagemanager.getsingleton().registermessagetype( new azsessionack( new byte[20], -1, null) ); messagemanager.getsingleton().registermessagetype( new azsessionend( new byte[20], "" ) ); messagemanager.getsingleton().registermessagetype( new azsessionbitfield( -1, null ) ); messagemanager.getsingleton().registermessagetype( new azsessioncancel( -1, -1, -1, -1 ) ); messagemanager.getsingleton().registermessagetype( new azsessionhave( -1, new int[]{-1} ) ); messagemanager.getsingleton().registermessagetype( new azsessionpiece( -1, -1, -1, null ) ); messagemanager.getsingleton().registermessagetype( new azsessionrequest( -1, (byte)-1, -1, -1, -1 ) ); register a generic map payload type with the factory. _id to register @throws messageexception on registration error construct a new az message instance from the given message raw byte stream. _payload data /deserialized az message @throws messageexception if message creation failed. note: does not auto-return to buffer pool the given direct buffer on thrown exception. create the proper az raw message from the given base message. _message to create from raw message with support for fast extension we don't cancel outstanding piece data, new btpiece(-1, -1, null,(byte)0 )} ) ); if only the version came first we could save a lot of space by changing the id length + id.... in the meantime we overload the version byte to have a version number and flags flags = top 4 bits, version = bottom 4 bits create and fill header buffer determine if a legacy bt message legacy message, use pre-configured values these should really be properties of the message... handshake needs to go out first - if not high then bitfield can get in front of it... standard message, ensure that protocol messages have wire priority over data payload messages"
com.aelitis.azureus.core.peermanager.messaging.azureus.AZMetaData ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZPeerExchange "az peer exchange message. 2403 b55+ 2403 b55+ only 2307+ send types"
com.aelitis.azureus.core.peermanager.messaging.azureus.AZRequestHint ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZStatReply ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZStatRequest ""
com.aelitis.azureus.core.peermanager.messaging.azureus.AZStylePeerExchange "represents a class which supports az style peer exchange (with a list of added peers and a list of dropped peers)."
com.aelitis.azureus.core.peermanager.messaging.azureus.AZUTMetaData ""
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTAllowedFast ""
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTBitfield "bittorrent bitfield message."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTCancel "bittorrent cancel message."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTChoke "bittorrent choke message. nothing"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTDHTPort ""
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTHandshake "bittorrent handshake message. used for outgoing handshake message. _hash _id _reserve_bit no reserve bits set. set first bit of first byte to indicate advanced az messaging support. (128) set fourth bit of fifth byte to indicate lt messaging support. (16) set seventh bit (2) and eight bit (1) to force azmp over ltep. [current behaviour] set seventh bit (2) only to prefer azmp over ltep. set eighth bit (1) only to prefer ltep over azmp. bt_reserved[7] = (byte)(bt_reserved[7] | 0x01); bt_reserved[7] = (byte)(bt_reserved[7] & 0xfe); bt_reserved[7] = (byte)(bt_reserved[7] | 0x04); bt_reserved[7] = (byte)(bt_reserved[7] & 0xf3); message bad peer id decode raw message"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTHave "bittorrent have message."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTHaveAll "nothing"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTHaveNone "nothing"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTInterested "bittorrent interested message. nothing"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTKeepAlive "bittorrent keep-alive message. message raw message"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTLTMessage "this class should not be used for deserialisation!"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTMessage "a bittorrent peer protocol message."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTMessageDecoder "nothing if (identifier != null && me.getmessage() != null && me.getmessage().startswith("unknown bt message id")) { system.out.println(identifier + " " + me.getmessage()); } for type id should never be > 16kb+9b, as we never request chunks > 16kb - update, some lt extensions can be bigger (byte)19 + "bit" readint() value of header destruction currently isn't thread safe so one thread can destroy the decoder (e.g. when closing a connection) while the read-controller is still actively processing the us throw( new ioexception( "btmessagedecoder already destroyed" )); only read into length buffer read into payload buffer, and possibly next message length hack to stop a 0-byte-read after receiving a keep-alive message otherwise we won't realize there's nothing left on the line until trying to read again there's a concurrency issue with the decoder whereby it can be destroyed while will being messed with. don't have the energy to look into it properly atm so just try to ensure that it doesn't bork too badly (parg: 29/04/2012) only occasional but does have potential to generate direct buffer mem leak ;( reading payload todo convert to direct? got a buffer overflow exception here in the past - related to pex? happens if messages modified by alt thread... ensure the decode array has the latest payload pointer set buffer limits according to bytes allowed ensure no read into this next buffer is possible skip full buffer read only part of this buffer limit current buffer shrink any tail buffers full buffer is allowed to be read count this buffer toward allowed and move on to the next reading payload data mode ensure-restore proper buffer limits need to have read the message id first byte full message received! decode handshake we need to auto-pause decoding until we're told to start again externally, as we don't want to accidentally read the next message on the stream if it's an az-format handshake decode normal message maintain unexpected errors as such so they get logged later see if we've already read the next message's length reset receive percentage only partial received so far compute receive percentage ensure proper buffer limit done reading the length reset it for next length read handshake message restore 'real' length keep-alive message normal message overridden by ltmessagedecoder."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTMessageEncoder "creates legacy (i.e. traditional bittorrent wire protocol) raw messages. note: wire format: [total message length] + [message id byte] + [payload bytes] nothing"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTMessageFactory "initialize the factory, i.e. register the messages with the message manager. construct a new bt message instance from the given message raw byte stream. _payload data /deserialized bt message @throws messageexception if message creation failed note: does not auto-return given direct buffer on thrown exception. create the proper bt raw message from the given base message. _message to create from raw message most of these messages are also used by az code clients seeing our handshake reserved bit will send us the old 'extended' messaging hello message accidentally. instead of throwing an exception and dropping the peer connection, we'll just fake it as a keep-alive :) handshake message byte in position 4 invalid, return whatever used for handshake and keep-alive messages message id type not found"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTPiece "bittorrent piece message."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTRawMessage ""
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTRejectRequest "note: overrides equals() used for removing individual requests from the message queue"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTRequest "bittorrent request message. note: overrides equals() used for removing individual requests from the message queue"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTSuggestPiece ""
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTUnchoke "bittorrent unchoke message. nothing"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.BTUninterested "bittorrent uninterested message. nothing"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.LTDisabledExtensionMessage "(non-javadoc) @see com.aelitis.azureus.core.peermanager.messaging.message#getdescription() not meant to be used for outgoing messages, so raise an error if anyone tries to do it."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.LTHandshake "try decoding the data now. been seeing a bunch of java.lang.classcastexception: [b cannot be cast to java.lang.long at com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.lthandshake.isuploadonly(lthandshake.java:108) seeing string value '0' here.... should not happen"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.LTMessage "placeholder message indicating that a message was sent for an extension which has been disabled."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.LTMessageDecoder "check to see if it is a lt-extension message. if not, delegate to btmessagedecoder. here is where we decode the message."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.LTMessageEncoder "only instantiate it when we need to. what type of message is it? lt_handshake messages are always straight forward. other message types have to be matched up against the appropriate id. logger.log(new logevent(this.log_object, logid, "converting lt message to bt message, ext id is " + ext_id)); anything else means that the client doesn't support that extension. we'll drop the message instead."
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.LTMessageFactory "most of these messages are also used by az code"
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.UTMetaData ""
com.aelitis.azureus.core.peermanager.messaging.bittorrent.ltep.UTPeerExchange "arbitrary value - most clients are configured to about 100 or so... we'll allow ourselves to be informed about 200 connected peers from the initial handshake, and then cap either list to about 100. these values are plucked from the air really - although i've seen pex sizes where the added list is about 300 (sometimes), most contain a sensible number (not normally over 100). subsequent pex messages are relatively small too, so we'll stick to smaller limits - 50 would be probably fine, but watching some big swarms over a short period, the biggest "added" list i saw was one containing 38 peers, so it's quite possible list sizes above 50 get sent out. so 100 is a safe-ish figure. update: utorrent doesn't have any set limits on this, apparently it simply depends on the number of connections a peer has, which can be large for fast peers (i've seen 350 peers for example). increased limits somewhat and added code to ignore excessive peers rather than dropping the connection debug stuff public string tostring() { list adds = (this.peers_added != null) ? arrays.aslist(this.peers_added) : null; list drops = (this.peers_dropped != null) ? arrays.aslist(this.peers_dropped) : null; return "utpex: " + adds + ", " + drops; } private map bencoded_buffer = null; public static void main(string[] args) throws exception { peeritem[] p1 = new peeritem[] { peeritemfactory.createpeeritem("2001:0db8:85a3:08d3:1319:8a2e:0370:7334", 4096, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_plain, 0, peeritemfactory.crypto_level_1, 10), peeritemfactory.createpeeritem("2001:0db8:0:0:0:8a2e:0370:7334", 128, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_crypto, 0, peeritemfactory.crypto_level_current, 10), peeritemfactory.createpeeritem("1280:0:0:0:0:0:0:7334", 255, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_plain, 0, peeritemfactory.crypto_level_1, 25), }; peeritem[] p2 = new peeritem[] { peeritemfactory.createpeeritem("192.168.0.1", 6473, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_plain, 16, peeritemfactory.crypto_level_1, 10), peeritemfactory.createpeeritem("127.0.0.1", 128, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_crypto, 0, peeritemfactory.crypto_level_1, 10), peeritemfactory.createpeeritem("172.16.0.1", 255, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_plain, 0, peeritemfactory.crypto_level_1, 25), }; peeritem[] p3 = new peeritem[] { peeritemfactory.createpeeritem("2001:0db8:85a3:08d3:1319:8a2e:0370:7334", 55, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_plain, 0, peeritemfactory.crypto_level_1, 10), peeritemfactory.createpeeritem("127.0.0.1", 128, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_crypto, 0, peeritemfactory.crypto_level_1, 10), peeritemfactory.createpeeritem("1280:0:0:0:0:0:0:7334", 255, peeritemfactory.peer_source_peer_exchange, peeritemfactory.handshake_type_plain, 0, peeritemfactory.crypto_level_1, 25), }; utpeerexchange u1 = new utpeerexchange(p1, p2, (byte)0); utpeerexchange u2 = new utpeerexchange(p2, p3, (byte)0); utpeerexchange u3 = new utpeerexchange(p3, p1, (byte)0); utpeerexchange u4 = new utpeerexchange(new peeritem[0], p1, (byte)0); utpeerexchange u5 = new utpeerexchange(p1, new peeritem[0], (byte)0); u1.getdata(); u2.getdata(); u3.getdata(); u4.getdata(); u5.getdata(); utpeerexchange[] uts = new utpeerexchange[] {null, u1, u2, u3, u4, u5}; for (int i=1; i<6; i++) { java.util.iterator itr = uts[i].bencoded_buffer.keyset().iterator(); while (itr.hasnext()) { string k = (string)itr.next(); byte[] b = (byte[])uts[i].bencoded_buffer.get(k); system.out.println(k + ": " + org.gudy.azureus2.core3.util.byteformatter.encodestring(b)); } system.out.println('-'); } system.out.println(u1.deserialize(u1.getdata()[0], (byte)0)); system.out.println(u1.deserialize(u2.getdata()[0], (byte)0)); system.out.println(u1.deserialize(u3.getdata()[0], (byte)0)); system.out.println(u1.deserialize(u4.getdata()[0], (byte)0)); system.out.println(u1.deserialize(u5.getdata()[0], (byte)0)); } encrypted connection. 0x02 indicates if the peer is a seed, but that's difficult to determine so we'll leave it. end for for some reason, some peers send flags as longs. i haven't seen it myself, so i don't know how to extract data from it. so we'll just stick to byte arrays. bencoded_buffer = payload_map;"
com.aelitis.azureus.core.peermanager.messaging.Message "basic peer message. a message is uniquely identified by the combination of id and version. is a protocol-bearing message, i.e. messaging/overhead data. is a data-bearing message, i.e. file data. get message id. get the main feature set name this message belongs to. id get the static message sub-id for the feature. id get message type. get textual description of this particular message. get message payload data. data buffers create a new instance of this message by decoding the given byte serialization. to deserialize message instance @throws messageexception if the decoding process fails note: does not auto-return given direct buffer on thrown exception. destroy the message; i.e. perform cleanup actions."
com.aelitis.azureus.core.peermanager.messaging.MessageException ""
com.aelitis.azureus.core.peermanager.messaging.MessageManager "nothing perform manager initialization. register the given message type with the manager for processing. instance to use for decoding @throws messageexception if this message type has already been registered remove registration of given message type from manager. type to remove construct a new message instance from the given message information. of message _data payload /deserialized message @throws messageexception if message creation failed lookup a registered message type via id and version. to look for default registered message instance if found, otherwise returns null if this message type is not registered get a list of the registered messages. register az message types register bt message types register lt message types"
com.aelitis.azureus.core.peermanager.messaging.MessageStreamDecoder "decodes a message stream into separate messages. decode message stream from the given transport. to decode from _bytes to decode/read from the stream of bytes decoded @throws ioexception on decoding error get the messages decoded from the transport, if any, from the last decode op. messages, or null if no new complete messages were decoded get the number of protocol (overhead) bytes decoded from the transport, from the last decode op. of protocol bytes recevied get the number of (piece) data bytes decoded from the transport, from the last decode op. of data bytes received get the percentage of the current message that has already been received (read from the transport). complete (0-99), or -1 if no message is currently being received pause message decoding. resume message decoding. destroy this decoder, i.e. perform cleanup. bytes already-read and still remaining within the decoder"
com.aelitis.azureus.core.peermanager.messaging.MessageStreamEncoder "encodes messages into a raw message stream format for sending. encode the given message into the raw message output stream format. to encode messages encoding"
com.aelitis.azureus.core.peermanager.messaging.MessageStreamFactory "factory interface for creating stream encoders and decoders. create message stream encoder. create message stream decoder."
com.aelitis.azureus.core.peermanager.messaging.MessagingUtil "convert the given message payload map to a bencoded byte stream. to convert serialization convert the given bencoded byte stream into a message map. to convert _size of stream of message deserialization @throws messageexception on convertion error note: does not auto-return given direct buffer on thrown exception."
com.aelitis.azureus.core.peermanager.nat.PeerNATInitiator ""
com.aelitis.azureus.core.peermanager.nat.PeerNATTraversalAdapter ""
com.aelitis.azureus.core.peermanager.nat.PeerNATTraverser "system.out.println( msg ); system.out.println( "usage = " + usage ); todo: prioritisation based on initiator connections etc? we get here when download stopped at same time debug.out( "initiator not found" ); system.out.println( "peernat: received traversal from " + originator ); sanity check"
com.aelitis.azureus.core.peermanager.peerdb.PeerDatabase "nothing register a new peer connection with the database. _peer_item key connection add a potential peer obtained via tracker announce, dht announce, plugin, etc. to add mark the given peer as ourself. peer get the peer item that represents ourself. peer, or null if unknown //disabled for now, as getexternalipaddress() will potential run a full version check every 60s if( self_peer == null ) { //determine our 'self' info from config string ip = versioncheckclient.getsingleton().getexternalipaddress(); if( ip != null && ip.length() > 0 ) { self_peer = peeritemfactory.createpeeritem( ip, networkmanager.getsingleton().gettcplisteningportnumber(), 0 ); } } get the next potential peer for optimistic connect. to connect, or null of no optimistic peer available update connection adds go through all existing connections dont exchange seed peers to other seeds notify existing connection of new one notify new connection of existing one for initial exchange update connection drops go through all remaining connections dont skip seed2seed drop notification, as the dropped peer may not have been seeding initially notify existing connection of drop only bother with non-seed -> seed transitions, the opposite are too rate to bother with system.out.println( "seedstatuschanged: dropping: originator= " + item.getbasepeer().getaddressstring() + ",target=" + connection.getbasepeer().getaddressstring()); check to make sure we dont already know about this peer we already know about this peer via exchange, so ignore discovery add unknown peer cache twice the amount to allow for failures disabled for now, as getexternalipaddress() will potential run a full version check every 60s determine our 'self' info from config inject a few pex peers during startup as we know they're live and can help bootstrap the torrent first see if there are any unknown peers to try pick one from those obtained via peer exchange if needed to reduce the number of wasted outgoing attempts, we limit how frequently we hand out the same optimistic peer in a given time period check if it's time to rotate the bloom filters check to see if we've already given this peer out optimistically in the last 5-10min we've recently given this peer, so recursively find another peer to try we've found a better peer that this one. if the existing peer was discovered (as opposed to pex) then we'll save it for later as it might come in useful this peer's already in the bloom filters as it has been returned by a recursive call we've found a suitable peer rebuild needed clear cache only allow exchange list rebuild every few min, otherwise we'll spam attempts endlessly ensure rebuild waits min rebuild time after the cache is depleted before trying attempts again count popularity of all known peers now sort by popularity we want least popular in front todo destroy() method?"
com.aelitis.azureus.core.peermanager.peerdb.PeerDatabaseFactory ""
com.aelitis.azureus.core.peermanager.peerdb.PeerExchangerItem "add peer info obtained via peer exchange. to add remove peer info obtained via peer exchange. to remove get the list of peer connections added since this method was last called. peer connections get the list of peer connections dropped since this method was last called. peer connections clears all current peer state records and stops any future state maintenance. does this connection item represent a seed peer? if seeding, false if not assume we do until explicitly disabled register new add was dropped and then re-added pull drop and ignore add register new drop was added and then re-dropped pull add and ignore drop"
com.aelitis.azureus.core.peermanager.peerdb.PeerItem "represents a peer item, unique by ip address + port combo. note: overrides equals(). public string tostring() { return org.gudy.azureus2.core3.util.byteformatter.encodestring(this.address) + ":" + this.tcp_port; } see if we can resolve the address into a compact raw ipv4/6 byte array (4 or 16 bytes) not a standard ipv4/6 address, so just use the full string bytes extract address and port todo: serialise this... todo:... combine address and port bytes into one see if it's an ipv4/6 address (4 or 16 bytes) not a standard ipv4/6 address, so just return as full string we use an int to store the source text string as this class is supposed to be lightweight"
com.aelitis.azureus.core.peermanager.peerdb.PeerItemFactory "create a peer item using the given peer address and port information. of peer of peer this peer info was obtained from create a peer item using the given peer raw byte serialization (address and port). bytes this peer info was obtained from"
com.aelitis.azureus.core.peermanager.PeerManager "get the singleton instance of the peer manager. peer manager ensure it gets initialized compare header restore buffer structure restore buffer structure registered manually above register for incoming connection routing normally we only get a max of 1 of these. however, due to downloadmanager crazyness we can get an extra one when adding a download that already exists... can't include port as it will be a randomly allocated one in general. two people behind the same nat will have to connect to each other using lan peer finder not yet activated, queue connection for use on activation do this outside the monitor as the timeout code calls us back holding the timeout monitor and we need to grab managers_mon inside this to run timeouts make sure not already connected to the same ip address; allow loopback connects for co-located proxy-based connections and testing this is mainly to deal with seeds that incorrectly connect to us"
com.aelitis.azureus.core.peermanager.PeerManagerRegistration "used by the cdn used by the cdn xxx: doesn't appear to be used."
com.aelitis.azureus.core.peermanager.PeerManagerRegistrationAdapter ""
com.aelitis.azureus.core.peermanager.PeerManagerRoutingListener "returns true if the routing has been accepted by the listener @return"
com.aelitis.azureus.core.peermanager.piecepicker.EndGameModeChunk "file : endgamemodechunk.java  @deprecated this implementation is suitable for equals(); compare() should return int for sorting @return returns the piecenumber. returns the blocknumber. this.piece = piece;"
com.aelitis.azureus.core.peermanager.piecepicker.impl.PiecePickerImpl "min ms for recalculating availability - reducing this has serious ramifications min ms for recalculating base priorities min ms for forced availability rebuild  min # pieces in file for first/last prioritization number of pieces for first pieces prioritization  additional boost for more completed high priority priority boost due to being too old ms a block is expected to complete in ms since last write finish pieces already almost done keep working on same piece currently webseeds + other explicit priorities are around 10000 or more - at this point we ignore rarity priority at and above which pieces require real-time scheduling min number of requests sent to a peer max number of request sent to a peer default number of requests sent to a peer, (for each x b/s another request will be used) event # of asyncronously updated availability indicates availability needs to be recomputed due to detected drift periodically updated consistent view of availability for calculating the rarest availability level of pieces that we affirmatively want to try to request from others soonest ie; our prime targets for requesting rarest pieces event # of last last last availability event # when priority bases were calculated time that base priorities were last computed the priority for starting each piece/base priority for resuming a flag to indicate when we're in endgame mode the list of chunks needing to be downloaded (the mechanism change when entering end-game mode) this methd will compute the pieces' overall availability (including ourself) and the _globalminothers & _globalavail early-outs when finds a downloadable piece either way sets hasneededundonepiece and neededundonepiecechange if necessary one reason requests don't stem from the individual peers is so the connections can be sorted by best uploaders, providing some ooprtunity to download the most important (ie; rarest and/or highest priority) pieces faster and more reliably sort all peers we're currently downloading from with the most favorable for the next request one as 1st entry randomize list first to not pick the same candidates if the list of best doesn't return conclusive results pt1 comes first if we want to request data from it more than from pt2 it is "smaller", i.e. return is =fileinfo.getfirstpiecenumber() && i 1) return piecenumber; bitflags of potential candidates to choose from the piece number that was chosen to be started. note it's possible for the chosen piece to have been started already (by another thread). this method considers that potential to not be relevant. adds every block from the piece to the list of chuncks to be selected for egm requesting an instance of this listener is registered with peercontrol through this, we learn of peers joining and leaving and attach/detach listeners to them an instance of this listener is registered with each peer switch (newstate) { case pepeer.connecting: return; case pepeer.handshaking: return; case pepeer.transfering: return; case pepeer.closing: return; case pepeer.disconnected: return; } nothing to do here takes away the given pieces from global availability peer this is about bitflags of the pieces an instance of this listener is registered with peercontrol the following are added to the base private static final int first_piece_range_percent= 10; min amount added for priority file. > first/last piece priority offset to give file priority priority over f/l piece priority priority range added for prioritized files the following are only used when resuming already running pieces list of pieces started as rarest first this is a this is a class administration first now do stuff related to availability always needed ensure all periodic calculaters perform operations at least once initialize each piece; on going changes will use event driven tracking with availability charged and primed, ready for peer messages now do stuff related to starting/continuing pieces startpriorities =new long[nbpieces]; computebasepriorities(); with priorities charged and primed, ready for dm messages peer is null if called from disk-manager callback if this is an interesting piece then clear any record of "no requests" so the peer gets scheduled next loop take a snapshot of availabilityasynch most important targets for near future requests from others copy updated local variables into globals copy updated local variables into globals first our pieces for all peers get the peer connection cycle trhough the pieces they actually have this only gets called when the my torrents view is displayed final long[] uprates =new long[peerssize]; final long uprate =peer.getstats().getsmoothdatareceiverate(); unchokerutil.updatelargestvaluefirstsort(uprate, uprates, peer, bestuploaders, 0); lan peers to the front of the queue as they'll ignore request limiting try to download from the currently fastest, this is important for the request focusing we're here because we're not requesting anything from both peers which means we might have to send the next request to this peer first try to download from peers that we're uploading to, that should stabilize tit-for-tat a bit ok, we checked all downloading and uploading peers by now avoid snubbed ones for the next step here try some peer we haven't downloaded from yet (this should allow us to taste all peers) still nothing, next try peers from which we have downloaded most in the past no, we don't want to focus on what may have become a bad peer jdk1.7 introduced this exception java.lang.illegalargumentexception: comparison method violates its general contract! under contract violation. we have an unstable comparator here as it uses all sorts of data that can change during the sort. to fix this properly we would need to cache this data for the duration of the sort, which is expensive given that we don't hugely care for the accuracy of this sort. so swallow the occasional error no usable peers, bail out early to keep the ordering consistent we need to use a fixed metric unless we remove + re-add a peer, at which point we need to take account of the fact that it has a new request allocated v unlikely - inconsistent but better than losing a peer give priority pieces the first look-in we need to sort by how quickly the peer can get a block, not just its base speed ignore request number advice from peers in rta mode, we gotta do what we can might have stopped rta as this is calculated in computebasepriorities add back in to see if we can allocate a further request if we have plenty of blocks outstanding we can afford to be more generous in the minimum number of requests we allocate dispenser.refill(); only request when there are still free tokens in the bucket or when it's a lan peer (which get sorted to the front of the queue) system.out.println("#"+i+" "+pt.getstats().getsmoothdatareceiverate()); can we transfer something? if request queue is too low, enqueue another request only loop when 3/5 of the queue is empty, in order to make more consecutive requests, and improve cache efficiency if ( pt.getnbrequests()  0; can the peer continuea piece with lowest avail of all pieces we want aggregate priority of piece under inspection (start priority or resume priority for pieces to be resumed) the swarm-wide availability level of the piece under inspection how long since the pepiece first started downloading (requesting, actually) try to continue a piece already loaded, according to priority is the piece available from this peer? if we are considering realtime pieces then don't bother with non-realtime ones if this priority exceeds the priority-override threshold then we override rarity piece is: needed, not fully: requested, downloaded, written, hash-checking or done maybe we didn't know we could get it before but the peer says s/he has it is the piece active maintained for display purposes only how many requests can still be made on this piece? don't touch pieces reserved for others reserved to somebody else the peer forgot this is reserved to him; re-associate it ### piece/peer speed checks snubbed peers shouldn't stall fast pieces under any condition may lead to trouble when the snubbed peer is the only seed, needs further testing slower peers are allowed as long as there is enough free room || rarestprio; prevent non-subbed peers from resuming on snubbed-peer-pieces but still allow them to resume stalled pieces find a fallback piece in case the peer could theoretically contribute to an existing piece but is prevented by the snubbing rules etc. this will prevent unecessary piece starting adjust priority for purpose of continuing pieces how long since last written to (if written to) how long since piece was started how much is already written to disk this is only for display this piece seems like best choice for resuming verify it's still possible to get a block to request from this piece change the different variables to reflect interest in this block rarest pieces only from now on 1st rarest piece clear the non-rarest bits in favor of only rarest continuing rarest, higher priority level continuing rares, same priority level not doing rarest pieces new priority level continuing same priority level same priority, new availability level same priority level, same availability level don't start pieces when snubbed, unless it's the only peer with that piece returns -1 if no piece to resume is found can & should or must resume a piece? system.out.println("second choice resume:"+secondchoiceresume); this would allow more non-rarest pieces to be resumed so they get completed so they can be re-shared, which can make us intersting to more peers, and generally improve the speed of the swarm, however, it can sometimes be hard to get the rarest pieces, such as when a holder unchokes very infrequently 20060312[mjrtom] this can lead to too many active pieces, so do the extra check with arbitrary # of active pieces check at arbitrary figure of 32 pieces start a new piece; select piece from start candidates bitfield randomly select a bit flag to be the one figure out the piece number of that selected bitflag is piece flagged if the piece isn't even needed, or doesn't need more downloading, simply continue if the piece is being downloaded (fully requested), count it and continue https://jira.vuze.com/browse/sup-154 if we have a piece reserved to a slow peer then this can prevent end-game mode from being entered and result poopy end-of-dl speeds else, some piece is needed, not downloaded/fully requested; this isn't end game mode when doing rta we don't want egm to kick in too early as it interfers with progressive playback by increasing discard. so we use a smaller trigger value to limit the impact only flip into end-game mode if < trigger size left system.out.println("end-game mode activated"); pieces not needed or not needing more downloading are of no interest ok, we try one, if it doesn't work, we'll try another next time we're here because there are no endgame mode chunks left either the torrent is done or something unusual happened cleanup anyway and allow a proper re-entry into endgame mode if neccessary coming out of real-time mode - clear down prolly more efficient to reallocate than reset to 0 we don't want end-game mode kicking in and screwing with the rta logic at the end of the download. simplest way is to abandon it for this download. if someone gives up rt later then the download will just complete without egm remove this listener from list of listeners and from the peer starting torrent record that this is a only need to re-calc needed on file's pieces; priority is calculated seperatly if didn't have anything to do before, now only need to check if we need to dl from this file, but if had something to do before, must rescan all pieces to see if now nothing to do file done (write to read) starting to upload from the file (read to write)"
com.aelitis.azureus.core.peermanager.piecepicker.PiecePicker "value indicated serial number of current count of changes to hasneededundonepiece. a method interesting in tracking changes can compare this with a locally stored value to determine if the hasneededundonepiece status has changed since the last check. this is called periodically by the peer control scheduler. it should not normally be called by other methods. it will update the global availability if neccesary and then update the derived information adds all blocks in the piece to endgamemodechunks"
com.aelitis.azureus.core.peermanager.piecepicker.PiecePickerFactory ""
com.aelitis.azureus.core.peermanager.piecepicker.PiecePickerListener ""
com.aelitis.azureus.core.peermanager.piecepicker.PiecePriorityProvider ""
com.aelitis.azureus.core.peermanager.piecepicker.PieceRTAProvider "sets an external view of how much buffer is being maintained by an external source. this reduces piece urgency and therefore reduces discard. returns the"
com.aelitis.azureus.core.peermanager.piecepicker.util.BitFlags "index of first set bit index of last set bit how many bits are set the array of bit flags clone constructor you can read flags.length instead (but please don't modify it) number of elements in this array for setting a flag that is already known to be the first true flag for setting a flag that is not known to be the first or last, or not this is for setting a flag that is already known to be the last true flag clears the array then sets the given flag experimental. returns a new bitflags with flags set as the logical and of both bitflags. the length of both must be the same. bitflags to be anded with this bitflags. must not be null. bitflags representing the logical and of the two these are public so they can be read quickly. please don't try to modify them outside of the given methods. setup outer union bounds find the first common set bit find any remaining common bits"
com.aelitis.azureus.core.peermanager.unchoker.DownloadingUnchoker "unchoker implementation to be used while in downloading mode. nothing count all the currently unchoked peers if not enough unchokes no more new unchokes avail one optimistic unchoke for every 10 upload slots ensure we never pick more slots than allowed to unchoke get all the currently unchoked peers should be immediately choked ensure current optimistic unchokes remain unchoked add them to the front of the "best" list too many optimistics fill slots with peers who we are currently downloading the fastest from viable peer found filter out really slow peers if we havent yet picked enough slots fill the remaining slots with peers that we have downloaded from in the past viable peer found make sure we haven't already uploaded several times as much data as they've sent us make space for new optimistic unchokes if we still have remaining slots just pick one optimistically no more new unchokes avail we're here because the given optimistic peer is already "best", but is choked still, which means it will continually get picked by the getnextoptimisticpeer() method, and we'll loop forever if there are no other peers to choose from send unchoke immediately, so it won't get picked optimistically anymore all_peers.remove( peer ); //remove from all_peers list, so it won't get picked optimistically anymore //todo update chokes should be choked but there are still slots needed (no optimistics avail), so don't bother choking them update unchokes"
com.aelitis.azureus.core.peermanager.unchoker.SeedingUnchoker "unchoker implementation to be used while in seeding mode. nothing count all the currently unchoked peers if not enough unchokes one optimistic unchoke for every 5 upload slots get all the currently unchoked peers should be immediately choked if too many unchokes we only recalculate the uploads when we're forced to refresh the optimistic unchokes we need to make room for new opt unchokes by finding the "worst" peers 0-initialized 0-initialized calculate factor rankings filter out really slow peers calculate reverse order by our upload rate to them calculate order by the total number of bytes we've uploaded to them we want higher rates at the end combine factor rankings to get best "better" peers have high indexes (toward the end of each list) wasn't downloading fast enough, skip add so it will be choked automatically make space for new optimistic unchokes update choke list with drops and unchoke list with optimistic unchokes should be choked we assume that any/all chokes are to be replace by optimistics only choke if we've got a peer to replace it with add friend peers preferentially, leaving room for 1 non-friend peer for every 5 upload slots don't bother trying to replace peers in an empty list find all buddies we want to give all connected friends an equal chance if there are more than max_friends allowed go through unchoke list and replace non-buddy peers with buddy ones pop peer from front of unchoke list peer is already in the buddy list so insert confirmed buddy peer back into list at the end not a buddy, so replace get next buddy just in case add buddy to back of list"
com.aelitis.azureus.core.peermanager.unchoker.Unchoker "performs peer choke/unchoke calculations. get any unchokes that should be performed immediately. _to_unchoke maximum number of peers allowed to be unchoked _peers list of peers to choose from to unchoke perform peer choke, unchoke and optimistic calculations _to_unchoke maximum number of peers allowed to be unchoked _peers list of peers to choose from _refresh force a refresh of optimistic unchokes get the list of peers calculated to be choked. to choke get the list of peers calculated to be unchoked. to unchoke"
com.aelitis.azureus.core.peermanager.unchoker.UnchokerFactory ""
com.aelitis.azureus.core.peermanager.unchoker.UnchokerUtil "utility collection for unchokers. test whether or not the given peer is allowed to be unchoked. to test _snubbed if true, ignore snubbed state if peer is allowed to be unchoked, false if not update (if necessary) the given list with the given value while maintaining a largest-value-first (as seen so far) sort order. note: you will need to initialize the values array to long.min_value if you want to store negative values! _value to use existing values array _item to insert existing items _pos index at which to start compare choose the next peer, optimistically, that should be unchoked. _peers list of peer to choose from _reciprocated if true, factor in how much (if any) this peer has reciprocated when choosing _snubbed allow the picking of snubbed-state peers as last resort next peer to optimistically unchoke, or null if there are no peers available send choke/unchoke messages to the given peers. _to_choke _to_unchoke shift displaced values to the right throw away last item if list too large find all potential optimistic peers try again, allowing snubbed peers as last resort no unchokable peers avail factor in peer reciprocation ratio when picking optimistic peers order by upload ratio score of >0 means we've uploaded more, <0 means we've downloaded more higher value = worse score map to sorted list using a logistic curve todo: in downloading mode, we would be better off optimistically unchoking just peers we are interested in ourselves, as they could potentially reciprocate. however, new peers have no pieces to share, and are not interesting to us, and would never be unchoked, and thus would never get any data. we could use a deterministic method for new peers to get their very first piece from us do chokes do unchokes todo add unchokerutil.isunchokable() test here to be safe?"
com.aelitis.azureus.core.peermanager.unchoker.UnchokerUtilTest "todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub todo auto-generated method stub"
com.aelitis.azureus.core.peermanager.uploadslots.DownloadingRanker "unchoker implementation to be used while in downloading mode. nothing todo extract and optimize? ensure we never rank more peers than needed fill slots with peers who we are currently downloading the fastest from viable peer found filter out really slow peers if we havent yet picked enough slots fill the remaining slots with peers that we have downloaded from in the past viable peer found make sure we haven't already uploaded several times as much data as they've sent us"
com.aelitis.azureus.core.peermanager.uploadslots.SeedingRanker "unchoker implementation to be used while in seeding mode. nothing no connected peers pick a random peer to start ensure we only loop once get next potential peer filter out peers already unchoked, and unchokable found the next optimistic! try next loop 'round if necessary todo test to see if peers really are picked evenly"
com.aelitis.azureus.core.peermanager.uploadslots.UploadHelper "get download opt unchoke priority. @return get all (pepeertransport) peers for this download. -mutable list of peers is this download in seeding mode. if seeding, false if downloading"
com.aelitis.azureus.core.peermanager.uploadslots.UploadSession ""
com.aelitis.azureus.core.peermanager.uploadslots.UploadSessionPicker "nothing the higher the priority, the more optimistic unchoke chances they get new priority new priority is lower trim new priority is higher add pick a random location store into location this picks both downloading and seeding sessions take from front add back at end pre-emptive check to see if we've already tried this helper no peers from this helper to unchoke lazy alloc remember this helper in case we get it again in another loop round found an optimistic peer! no optimistic peer found filter out seeding this picks downloading sessions only todo factor download priority into best calculation?"
com.aelitis.azureus.core.peermanager.uploadslots.UploadSlot "slot is expired by default"
com.aelitis.azureus.core.peermanager.uploadslots.UploadSlotManager "removed pending rework peercontrolschedulerfactory.getsingleton().register( new peercontrolinstance() { public void schedule() { long now = systemtime.getcurrenttime(); if( now - last_process_time >= 10000 ) { //10sec process loop process(); last_process_time = now; } else if( last_process_time > now ) { debug.out( "oops, time went backwards!" ); last_process_time = now; } } public int getschedulepriority() { return 0; } }); notify of helper state change (i.e. priority changed) 1 round = 10sec upload 3 rounds = 30sec upload 6 rounds = 60sec upload change this to true and you'll have to fix the scheduleing code below todo init with empty slots, optimistic first in line todo dynamic # of slots 10sec process loop system.out.println( "auto upload slot disabled" ); get a list of the best sessions, peers who are uploading to us in download mode go through all currently expired slots and pick sessions for next round expired make sure it gets stopped clear slot optimistic pick new session for optimistic upload place first seed session in a normal slot put at front of good list to ensure it gets picked pick a new optimistic session, whatever type place the new session in the slot set the new expire time normal get the next "best" session no download mode peers, must be only seeding; or all best are already slotted just pick the next optimistic no optimistic either place the session in the slot set the new expire time start and stop sessions for the round filter out sessions allowed to continue another round, so we don't stop-start them need to do this because two session objects can represent the same peer stop discontinued sessions ensure sessions are started get next found an unslotted session oops, already been slotted, try again max number of sessions the picker will return before it loops back 'round make sure we don't loop found! no optimistic sessions"
com.aelitis.azureus.core.peermanager.utils.AZPeerIdentityManager ""
com.aelitis.azureus.core.peermanager.utils.BTPeerIDByteDecoder "used for identifying clients by their peerid. if the client reuses parts of the peer id of other peers, then try to determine this first (before we misidentify the client). see if the client uses az style identification. hack for fake ziptorrent clients - there seems to be some clients which use the same identifier, but they aren't valid ziptorrent clients. bittorrent 6.0 beta currently misidentifies itself. if it's the rakshasa libtorrent, then it's probably rtorrent. see if the client uses shadow style identification. see if the client uses mainline style identification. we haven't got a good way of detecting whether this is a mainline style version of peer id until we start decoding peer id information. so for that reason, we wait until we get client version information here - if we don't manage to determine a version number, then we assume that it has been misidentified and carry on with it. check for bitspirit / bitcomet (non possible spoof client mode). see if the client identifies itself using a particular substring. decodes the given peerid, returning an identification string. older versions of bitlord are of the form x.yy, whereas new versions (1 and onwards), are of the form x.y. bitcomet is of the form x.yy. generate the string used that we will log. we'll encode the name in byte form to help us decode it. avoid logging the same combination of things again. if this text has been recorded before, then avoid doing it again. add peer id bytes. enable this block for now - just until we get more feedback about problematic clients. i don't expect this to grow too big, and it won't grow if there's no logging going on. avoid logging the same client id multiple times. if the id has been recorded before, then avoid doing it again. enable this block for now - just until we get more feedback about unknown clients. we store the result here. shareaza check do not log any clients. the latest version at time of writing is v0.12, but i'll assume this is valid. assertdecode("wyzo 0.3.0.0", "-wy0300-6huhf5pr7vde"); shadow style clients. seen this quite a bit - not sure that it is abc, but i guess we should default to that... seen recently - is this really tribler? simple substring style clients. seen recently, have they changed peer id format? version substring style clients. seen in the wild - no idea what version that's meant to be - a pre-release? ap0.70rc30->>-{1-a--]" based on description on wiki.theory.org. bitcomet/lord/spirit mainline style clients. not currently decoded as mainline style... various specialised clients. unknown clients - may be random bytes. unknown az style clients. i made this one up. unknown shadow style clients. todo assertdecode("ktorrent 2.2", "-kt22b1-695754334315"); // we could use the b1 information... assertdecode("ktorrent 2.1.4", "-kt2140-584815613993"); // currently shows as 2.1. assertdecode("", "c8f2d9cd3a90455354426578626300362d2d2d92"); // looks like a bitlord client - estbexbc? assertdecode("", "303030302d2d0000005433585859684b59584c72"); // seen in the wild, appears to be a modified version of azureus 2.5.0.0 (that's what was in the azmp handshake)? assertdecode("", "b5546f7272656e742f3330323520202020202020");"
com.aelitis.azureus.core.peermanager.utils.BTPeerIDByteDecoderDefinitions "list of objects which describes clients with their own custom naming scheme. just use the unformatted string ok - here's where we store the definitions. good place for information about bt peer id conventions: http://wiki.theory.org/bittorrentspecification http://transmission.m0k.org/trac/browser/trunk/libtransmission/clients.c (hello transmission this is interesting - it uses mainline style, except uses two characters instead of one. and then - the particular numbering style it uses would actually break the way we decode version numbers (our code is too hardcoded to "-x-y-z--" style version numbers). this should really be declared as a mainline style peer id, but i would have to make my code more generic. not a bad thing - just something i'm not doing right now. az style two byte code identifiers to real client name. shadow's style one byte code identifiers to real client name. mainline's new style uses one byte code identifiers too. version number formats. so 'b' = 11, for example, like shadow's style of numbering. these are used to describe how the version number is extracted from the peer id as well as how that version number is formatted. is given as a block in the peer id, we show the same block is given as a dotted block in the peer id, we show the block in the same dotted format. is given a block in the peer id, but should be displayed in a dotted format. is given a block in the peer id, but should be displayed in a dotted format. used to register client information. we define ourselves first... :) ... and then do everything else alphabetically. addazstyle("ar", "arctictorrent", no_version); //based on libtorrent but same peerid for different versions ares is more likely than arctictorrent todo: format is v"abcd" the "0001" bytes found after the lw commonly refers to the version of the bt protocol implemented. documented here: http://www.limewire.org/wiki/index.php?title=bittorrentrevision apparently, the english name of the client is "thunderbolt". >= 3.6 formerly wyzo. apparently, the english name of the client is "thunderbolt". simple clients with no version number. http://forum.utorrent.com/viewtopic.php?pid=260927#p260927 used internally to denote incoming webseed connections clients with their own custom format and version number style. dna01000 becomes version 1.0 - we'll ignore the other digits for now. pre build 10000 versions. post build 10000 versions. 3 version components, 5 chars which describe it refers to bitmagnet - predecessor to rufus. top-bt - based on bittornado, but doesn't quite stick to shadow's naming conventions - so we'll use substring matching instead. seems to have a sub-version encoded in following 3 bytes, not worked out how: "folx/1.0.456.591" : 2d 464c 3130 ff862d 486263574a43585f66314d5a how to extract and put digit components together where does this appear in a client id string? how do we display that version number? how long is the version number?"
com.aelitis.azureus.core.peermanager.utils.BTPeerIDByteDecoderUtils "hack for flashget - it doesn't use the trailing dash. also, lh-abc has strayed into "forgetting about the delimiter" territory. in fact, the code to generate a peer id for lh-abc is based on bittornado's, yet tries to give an az style peer id... oh dear. bt next evolution seems to be in the same boat as well. ktorrent 3 appears to use a dash rather than a final character. checking whether a peer id is shadow style or not is a bit tricky. the bittornado peer id convention code is explained here: http://forums.degreez.net/viewtopic.php?t=7070 the main thing we are interested in is the first six characters. although the other characters are base64 characters, there's no guarantee that other clients which follow that style will follow that convention (though the fact that some of these clients use bittornado in the core does blur the lines a bit between what is "style" and what is just common across clients). so if we base it on the version number information, there's another problem - there isn't the use of absolute delimiters (no fixed dash character, for example). there are various things we can do to determine how likely the peer id is to be of that style, but for now, i'll keep it to a relatively simple check. we'll assume that no client uses the fifth version digit, so we'll expect a dash. we'll also assume that no client has reached version 10 yet, so we expect the first two characters to be "letter,digit". we've seen some clients which don't appear to contain any version information, so we need to allow for that. one of the following styles will be used: mx-y-z-- mx-yy-z- specialness for transmission - thanks to bentmywookie for the information, i'm sure he'll be reading this comment anyway... ;) look at the peer id and just grab as many readable characters to form the version substring as possible. just for transmission at the moment. find where the version number string ends. for each digit in the version string, check it is a valid version identifier. we'll strip off trailing redundant zeroes. very old client style: -tr0006- is 0.6. previous client style: -tr0072- is 0.72. current client style: -tr072z- is 0.72 (dev). either something like this: 1.2 rc 4 [where 3 == 'r') 1.2 dev [where 3 == 'd') 1.2 [where 3 doesn't equal the above] this is based on all peers peer id at the time of writing, e.g: ap0.70rc30->>... must be delimiter character."
com.aelitis.azureus.core.peermanager.utils.ClientIdentifier "hack for bittyrant - the handshake resembles this: client: azureusbittyrant clientversion: 2.5.0.0bittyrant yuck - let's format it so it resembles something pleasant. do both names seem to match? there may be some discrepancy - a different version number perhaps. if the main client name still seems to be the same, then return the one given to us in the az handshake. if both are azureus, the version numbers shouldn't differ. this is what we should have - 15 characters both the same (sometimes beta version is included in the version number but not in the peer id, but we can deal with that. "azureus a.b.c.d" official bittorrent clients should still be shown as mainline. this is to be consistent with previous azureus behaviour. libtorrent is... unsurprisingly... a torrent library. many clients use it, so cope with clients which don't identify themselves through the peer id, but do identify themselves through the handshake. and some clients do things the other way round - they don't bother with the handshake name, but do remember to change the peer id name. bitthief check. we do care if something is claiming to be azureus when it isn't. if it's a recent version of azureus, but doesn't support advanced messaging, we know it's a fake. older versions of azureus won't have support, so discount these first. must be a fake. transmission and xtorrent. there is an inconsistency. let's try figuring out what we can. shouldn't happen. we've got a peer id that says azureus, but it doesn't say azureus in the handshake. it's definitely fake. it might be xtorrent - it does use az2504 in the peer id and "transmission 0.7-svn" in the handshake. our peer id decoding can't decode it, but the client identifies itself anyway. in that case, we won't say that it is a mismatch, and we'll just use the name provided to us. log it though. we've got a general mismatch, we don't know what client it is - in most cases. ares galaxy sometimes uses the same peer id as arctic torrent, so allow it to be overridden. 1.6.0 misidentifies itself as 1.5 in the handshake. older ï¿½torrent versions will not always use the appropriate character for the first letter, so compensate here. some versions indicate they are the beta version in the peer id, but not in the handshake - we prefer to keep the beta identifier. some mainline 4.x versions identify themselves as ï¿½torrent - according to alus, this was a bug, so just identify as mainline. azureus should never be using ltep when connected to another azureus client! we allow a client to have a different version number than the one decoded from the peer id. some clients separate version and client name using a forward slash, so we split on that as well. transmission and xtorrent. like we do with azmp peers, allow the handshake to define the client even if we can't extract the name from the peer id, but log it so we can possibly identify it in future. peer id doesn't mention libtorrent (just the client name) and the handshake name doesn't mention the client name (just "libtorrent"), then combine them together. can't determine what the client is. bloody xtorrent. bloody xtorrent! transmission 0.96 still uses 0.95 in the lt handshake, so cope with that and just display 0.96. use this form as it is shorter. throw new exception("client name decoded - " + decoded_client); assertdecodeazmp("", "-bs5820-oy4la2mwgefj", "bearshare premium p2p", "5.8.2.0"); assertdecodeazmp("", "-ar6360-6ozyymwooobe", "imesh turbo", "6.3.6.0"); assertdecodeazmp("", "-ag2083-s1hif8vgaag0", "ares", "2.0.8.3029"); assertdecodeazmp("", "-ag3003-lel2mm4neo4n", "ares destiny", "3.0.0.3805"); assertdecodeltep("libtorrent 0.11.9", "2d6c7430 4239302d 11f3eb39 5d44eefd cea07e79", "libtorrent 0.11.9"); assertdecodeltep("", "b5546f72 72656e74 2f333037 36202020 20202020", "\ufdfftorrent/3.0.7.6"); assertdecodeextprotocol("", "-xx1150-dv220cotgj4d", "transmission", "0.72z");"
com.aelitis.azureus.core.peermanager.utils.OutgoingBTHaveMessageAggregator "utility class to enable write aggregation of bt have messages, in order to save bandwidth by not wasting a whole network packet on a single small 9-byte message, and instead pad them onto other messages. nothing nothing ignore ignore create a new aggregator, which will send messages out the given queue. _message_q queue a new have message for aggregated sending. _number of the have message if true, send this and any other pending haves right away destroy the aggregator, along with any pending messages. force send of any aggregated/pending have messages. are there haves messages pending? if there are any unsent haves, false otherwise if another message is going to be sent anyway, add our haves as well system.out.println("enough pending haves for a full packet!"); there's enough pending bytes to fill a packet payload single have -> use bt"
com.aelitis.azureus.core.peermanager.utils.OutgoingBTPieceMessageHandler "front-end manager for handling requested outgoing bittorrent piece messages. peers often make piece requests in batch, with multiple requests always outstanding, all of which won't necessarily be honored (i.e. we choke them), so we don't want to waste time reading in the piece data from disk ahead of time for all the requests. thus, we only want to perform read-aheads for a small subset of the requested data at any given time, which is what this handler does, before passing the messages onto the outgoing message queue for transmission. create a new handler for outbound piece messages, reading piece data from the given disk manager and transmitting the messages out the given message queue. _manager _message_q if ( peer.getip().equals( "64.71.5.2" )){ outgoing_message_queue.settrace( true ); // btpiece p = (btpiece)message; // timeformatter.millitrace( "obt sent: " + p.getpiecenumber() + "/" + p.getpieceoffset()); } nothing nothing ignore ignore register a new piece data request. _number _offset remove an outstanding piece data request. _number _offset remove all outstanding piece data requests. int num_queued = queued_messages.size(); int num_removed = 0; for( iterator i = queued_messages.keyset().iterator(); i.hasnext(); ) { btpiece msg = (btpiece)i.next(); if( outgoing_message_queue.removemessage( msg, true ) ) { i.remove(); num_removed++; } } if( num_removed  " + (to_submit==null?0:to_submit.size()) + " [lo=" + loading_messages.size() + ",qm=" + queued_messages.size() + ",re=" + requests.size() + ",rl=" + request_read_ahead + "]"); } get a list of piece numbers being requested of long values cheap hack to reduce (but not remove all) the # of duplicate entries was canceled was canceled due to timing issues we can get in here with a message already removed btpiece p = (btpiece)message; timeformatter.millitrace( "obt sent: " + p.getpiecenumber() + "/" + p.getpieceoffset()); it's already been queued do manual listener notify removed this trace as alon can't remember why the trace is here anyway and as far as i can see there's nothing to stop a piece being delivered to transport and removed from the message queue before we're notified of this and thus it is entirely possible that our view of queued messages is lagging. string before_trace = outgoing_message_queue.getqueuetrace(); this replaces stuff above allocate max size needed (we'll shrink it later)"
com.aelitis.azureus.core.peermanager.utils.OutgoingBTPieceMessageHandlerAdapter ""
com.aelitis.azureus.core.peermanager.utils.PeerClassifier "handles peer client identification and banning. get a client description (name and version) from the given peerid byte array. _id peerid sent in handshake get a printable representation of the given raw peerid byte array, i.e. filter out the first 32 non-printing ascii chars. _id peerid sent in handshake peerid check if the client type is allowed to connect. _description given by getclientdescription if allowed, false if banned this only works for ones that have been explicitly set as az ips @return sync call! @return if( client_description.startswith( "bitcomet" ) ) return false; some clients don't ever offer any fast-allow pieces so we reciprocate"
com.aelitis.azureus.core.peermanager.utils.PeerMessageLimiter "handles incoming peer message counting/timing/stats in order to catch and block abusive peers. nothing add the reception of the given message to time-limited count. _id message to count _counts max counts allowed within the given time limit _limit_ms time in ms that the count limiting applies if the added count is within acceptable time limits, false if there have been too many counts new message we've potentially reached our count limit prune out any expired counts this count is older than the limit allows drop it still within limit too many counts within the time limit return error"
com.aelitis.azureus.core.proxy.AEProxy ""
com.aelitis.azureus.core.proxy.AEProxyAddressMapper "socks 5 is limited to 255 char dns names. so for longer ones (e.g. i2p 'names') we have to replace then with somethin shorter to get through the socks layer and then remap them on the otherside. these functions are only active if a socks proxy is enabled and looping back (in process is the assumption) @return"
com.aelitis.azureus.core.proxy.AEProxyConnection "returns the non-blocking channel associated with the initiator of this proxy connection @return marks the transition between connecting and connected marks the last time that something happened on the connection for read timeout purposes indicate that the connection has failed close the connection state manipulation methods selector manipulation"
com.aelitis.azureus.core.proxy.AEProxyConnectionListener ""
com.aelitis.azureus.core.proxy.AEProxyException ""
com.aelitis.azureus.core.proxy.AEProxyFactory "0 = free port _timeout 0 = no timeout _timeout 0 = no timeout @return @throws aeproxyexception"
com.aelitis.azureus.core.proxy.AEProxyHandler ""
com.aelitis.azureus.core.proxy.AEProxySelector ""
com.aelitis.azureus.core.proxy.AEProxySelectorFactory ""
com.aelitis.azureus.core.proxy.AEProxyState ""
com.aelitis.azureus.core.proxy.impl.AEProxyAddressMapperImpl "system.out.println( "aeproxyaddressmapper: internalise " + address + " -> " + target ); system.out.println( "aeproxyaddressmapper: externalise " + address + " -> " + target );"
com.aelitis.azureus.core.proxy.impl.AEProxyConnectionImpl ""
com.aelitis.azureus.core.proxy.impl.AEProxyImpl "looks like its not going to work... some kind of socket problem only use one selector to trigger the timeouts! already been registered, just resume not yet registered"
com.aelitis.azureus.core.proxy.impl.AEProxySelectorImpl "decouple swt stuff from core new aeproxyselectorswtimpl( core, aeproxyselectorimpl.this ); we don't want to be recursing on this! bit mindless this but the easiest way to see if we should apply socks proxy to this uri is to hit the existing selector and see if it would (take a look at http://www.docjar.com/html/api/sun/net/spi/defaultproxyselector.java.html).... requires the existing one to be picking up socks details which requires a restart after enabling but this is no worse than current... only return one proxy - this avoids the java runtime from cycling through a bunch of them that fail in the same way (e.g. if the address being connected to is unreachable) and thus slugging everything filter out errors that are not associated with the socks server itself but rather then destination avoid reverse dns lookup if resolved stick the failed proxy at the end of the list avoid reverse dns lookup if resolved can happen when switching proxy config stick it at the end of the list make sure the host isn't an ip address... the proxy resolved so at least the name appears valid so we might as well try the system dns before moving onto possible others"
com.aelitis.azureus.core.proxy.impl.swt.AEProxySelectorSWTImpl ""
com.aelitis.azureus.core.proxy.socks.AESocksProxy "set the next socks proxy in a chain - i.e. this socks proxy's default plugable connection will connect onwards using this socks proxy"
com.aelitis.azureus.core.proxy.socks.AESocksProxyAddress ""
com.aelitis.azureus.core.proxy.socks.AESocksProxyConnection ""
com.aelitis.azureus.core.proxy.socks.AESocksProxyFactory ""
com.aelitis.azureus.core.proxy.socks.AESocksProxyPlugableConnection ""
com.aelitis.azureus.core.proxy.socks.AESocksProxyPlugableConnectionFactory ""
com.aelitis.azureus.core.proxy.socks.impl.AESocksProxyAddressImpl "see if we've been passed an ip address as unresolved todo: ipv6 one day? nnn.nnn.nnn.nnn"
com.aelitis.azureus.core.proxy.socks.impl.AESocksProxyConnectionImpl "+----+----+----+----+----+----+----+----+----+----+....+----+ | vn | cd | dstport | dstip | +----+----+----+----+----+----+----+----+ | vn | cd | dstport | dstip | +----+----+----+----+----+----+----+----+ # of bytes: 1 1 2 4 +----+-----+-------+------+----------+----------+ |ver | cmd | rsv | atyp | dst.addr | dst.port | +----+-----+-------+------+----------+----------+ | 1 | 1 | x'00' | 1 | variable | 2 | +----+-----+-------+------+----------+----------+ where: o ver protocol version: x'05' o cmd o connect x'01' o bind x'02' o udp associate x'03' o rsv reserved o atyp address type of following address o ip v4 address: x'01' o domainname: x'03' o ip v6 address: x'04' o dst.addr desired destination address o dst.port desired destination port in network octet order +----+-----+-------+------+----------+----------+ |ver | rep | rsv | atyp | bnd.addr | bnd.port | +----+-----+-------+------+----------+----------+ | 1 | 1 | x'00' | 1 | variable | 2 | +----+-----+-------+------+----------+----------+ where: o ver protocol version: x'05' o rep reply field: o x'00' succeeded o x'01' general socks server failure o x'02' connection not allowed by ruleset o x'03' network unreachable o x'04' host unreachable o x'05' connection refused o x'06' ttl expired o x'07' command not supported o x'08' address type not supported o x'09' to x'ff' unassigned o rsv reserved o atyp address type of following address o ip v4 address: x'01' o domainname: x'03' o ip v6 address: x'04' o bnd.addr server bound address o bnd.port server bound port in network octet order v4 end of play socks 4a drop the prepare for dns name follows, null terminated re-activate the read select suspended while resolving ready for next byte v5 we just ignore actual method values version reserved"
com.aelitis.azureus.core.proxy.socks.impl.AESocksProxyImpl ""
com.aelitis.azureus.core.proxy.socks.impl.AESocksProxyPlugableConnectionDefault "ok, we're almost ready to roll. unregister the read select until we're connected if the address is unresolved then the calculated bind address can be invalid (might pick an ipv6 address for example when this is unbindable). in this case carry on and attempt to connect as this will fail anyway if we've got a proxy chain, now's the time to negotiate the connection means that the channel has been shutdown socket sx -> sy via bx so if sx = source_channel then bx is target buffer"
com.aelitis.azureus.core.proxy.socks.impl.AESocksProxyState ""
com.aelitis.azureus.core.rssgen.RSSGeneratorPlugin "migrate from when the rss feed was tied to devices"
com.aelitis.azureus.core.security.CryptoECCUtils ""
com.aelitis.azureus.core.security.CryptoHandler "explicit unlock request @throws cryptomanagerexception puts the handler back into a state where password will be required to access private stuff 0-> infinite if an azureus restart is required @throws cryptomanagerexception"
com.aelitis.azureus.core.security.CryptoManager "don't even think about changing this!!!!"
com.aelitis.azureus.core.security.CryptoManagerException ""
com.aelitis.azureus.core.security.CryptoManagerFactory ""
com.aelitis.azureus.core.security.CryptoManagerKeyListener ""
com.aelitis.azureus.core.security.CryptoManagerPasswordException ""
com.aelitis.azureus.core.security.CryptoManagerPasswordHandler "handler_type_unknown is not for public use gets a password _type from aesecuritymanager.handler_x enum _type from above action_x enum reason for the password being sought details or null if no password available @return 0 -> don't persist, integer.max_value -> persist forever  current session; other -> seconds to persist just for clearing passwords..."
com.aelitis.azureus.core.security.CryptoSTSEngine ""
com.aelitis.azureus.core.security.impl.CryptoHandlerECC "migration away from system managed keys ensure we unlock the private key so we can then re-persist it with new password not much to do as keys not yet we can't actually verify the key size as although it should be 192 bits it can be less due to leading bits being 0 we use this class to obtain compatability with bc"
com.aelitis.azureus.core.security.impl.CryptoManagerImpl "session timeout absolute timeout try next password provider no point in going through verification if same as last transform password so we can persist if needed retry session only next provider handler1.resetkeys( null ); handler2.resetkeys( null );"
com.aelitis.azureus.core.security.impl.CryptoSTSEngineImpl "sts authentication protocol using a symmetric 4 message ecdh/ecdsa handshake keypair representing our current identity ecdh = keyagreement.getinstance("ecdh", "bc_vuze"); system.out.println( "put( " + keys + ") " + this ); system.out.println( "get( " + keys + ") " + this ); we use this class to obtain compatability with bc"
com.aelitis.azureus.core.speedmanager.impl.SpeedManagerAlgorithmProvider "reset any state to start of day values called periodically (see period above) to allow stats to be updated. called when a new source of ping times has been found _replacement one of the initial sources or a replacement for a failed one ping source has failed called whenever a new set of ping values is available for processing various getters for interesting info shown in stats view @return returns the current view of when choking occurs in bytes/sec indicates whether or not the provider is adjusting download as well as upload limits @return"
com.aelitis.azureus.core.speedmanager.impl.SpeedManagerAlgorithmProviderAdapter "creates a mapper starting from current time. must be destroyed by calling "destroy" when done with @return"
com.aelitis.azureus.core.speedmanager.impl.SpeedManagerImpl "returns the current view of when choking occurs in bytes/sec keep history for 1 hour config items start shadow of config_version for config. informative only config end if enabled the ping stream drives the stats update for the ping mappers when not enabled we do it here instead happens, no biggy debug.out( "auto-speed: source missing" ); remove worst value if we have > 1 bias towards min unfortunately we need this to run synchronously as the caller may be disabling it and then setting speed limits in which case we can't go async and restore the original values below and overwrite the new limit... single thread enable/disable (and derivative reset) ops 56 k/bit currently we don't go lower than this 1mbit 10mbit note, we start from 3 to avoid over-restricting things when we don't have a reasonable speed estimate convert to upload kbyte/sec assuming 80% achieved trivial test implementation"
com.aelitis.azureus.core.speedmanager.impl.SpeedManagerPingMapperImpl "don't make this too large as we don't start considering capacity decreases until this is full 3 min skip key intern to save cpu as there are a lot of keys and we end up ditching the map after it's processed first time with this asn - removed auto speed test in 4813 so decided to increase the initial estimated upload limit to avoid starting out too low note: add to this you will need to modify the "reset" method appropriately ping time won't refer to current x+y due to latencies, apply to average between current and previous we deliberately don't divide by num samples as this accentuates larger deviations variance is a useful measure. however, under some conditions, in particular high download speeds, we get elevated ping times with little variance factor this in discard oldest pings and reset if result is bad then we return this see if best good/last bad are relevant need to convert this into a good rating to correspond to the actual estimate type we have don't count the duplicates we naturally get when sitting here with a bad limit and nothing going on to change this situation remeber, 0 means unlimited!!! sanity check drop top bottom quarter of measurements only consider decreases! adjust if deviation within 50% of capacity remove the last 1/4 bad stats so we don't reconsider adjusting until more data collected flatten out all observations into a single munged metric a good variance applies to all speeds up to this one. this means that previously occuring bad variance will get flattened out by subsequent good variance medium values, treat at face value bad ones, treat at face value a bad variance resets totals as we have encountered this after (in time) the existing data and this is more relevant and replaces any feel good factor we might have accumulated via prior observations now average out values based on history computed above break history up into segments of same speed take smallest bad value and largest good override any estimates < 5k to be ok ones as there's little point in recording negative values lower than this value of 0 means unlimited sanitize sanitize"
com.aelitis.azureus.core.speedmanager.impl.TestPingSourceImpl ""
com.aelitis.azureus.core.speedmanager.impl.TestPingSourceRandom ""
com.aelitis.azureus.core.speedmanager.impl.v1.SpeedManagerAlgorithmProviderV1 "returns the current view of when choking occurs in bytes/sec time we'll force low upload to get baseline how long we'll wait on start up before forcing min speed at which upload is treated as "idle" any lower than this and small ping variations cause overreaction discount anything 5min reported unless min is really small, in which case round up as we're only trying to catch badly behaved ones all failed bias towards min if we're uploading slowly or the current ping rate is better than our current idle average then we count this towards establishing the baseline bump down if we happen to come across lower idle values we've had 5 secs of min up speed, clear out the ping average now to get accurate times we've been running a while but no min set, or we've got some new untested contacts - force it if we're close to the last choke-speed then decrease increments don't drop below the current protocol upload speed. this is to address the situation whereby it is downloading that is choking the line - killing protocol upspeed kills the downspeed final tidy up if we're not achieving the current limit and the advice is to increase it, don't bother round limit up to nearest k three strikes and you're out!"
com.aelitis.azureus.core.speedmanager.impl.v2.LimitControl ""
com.aelitis.azureus.core.speedmanager.impl.v2.LimitControlDropUploadFirst "here is how the limitcontrol will handle the "unlimited" case. #1) download is allowed to be unlimited. #2) upload is not allowed to be unlimited. a) only the isdownloadulimited boolean and upcurr value - used for unlimited case. b) in upload unlimited. valuedown is set to 1.0. c) upmax and upmin values keep there non-zero values. verify the limits. increase download first only increase upload if used. decrease upload first todo: remove diagnotics later. zero means to unlimit the download rate. don't show a download limit unless it is needed. only apply a limit to the download when needed. log this change."
com.aelitis.azureus.core.speedmanager.impl.v2.LimitControlPreferDownload "prefer the download, but something like 80,20 calculate rates above this limit. calculate rates below this limit. return null; //to change body of implemented methods use file | settings | file templates. to change body of implemented methods use file | settings | file templates. to change body of implemented methods use file | settings | file templates. to change body of implemented methods use file | settings | file templates."
com.aelitis.azureus.core.speedmanager.impl.v2.LimitControlSetting "between 0.0 and 1.0f"
com.aelitis.azureus.core.speedmanager.impl.v2.PingSourceManager "this class manage cycling though the pingsources. it keep track of pingsource stats and applies rules on if/when to cycle though a ping-source. #1) if the slowest ping-source is 10x the the best for a 1 min average. kick it. #2) if a ping-source is slower then two combined sources (2x) for a 5 min average. then kick it. #3) every 30 minutes kick the slowest ping source and request a new one. just to keep things fresh. also maintain logic do determine if a new source is better then the previous one. (to determine if these rules lead to good data.) determine if we should drop any ping sources. sort them, if one significantly higher then the other two. then drop it. - speedmanagerpingsource[] inputs if one ping source is twice the fastest then replace it. otherwise reset the timer. - - true is a souce has been changed. a slow source is something that is 2x the slower then the two fastest. - - true is a source has been removed. if the slowest ping in 10x the fastest then remove it. - - true is a source has been removed. after a ping-source has been removed, need to resettimer the timer.   two minutes. if the long term average of one source is 10 the lowest and twice a large as the two lowest then drop the highest at the moment. also, don't force sources to drop to frequently. no sources. if we have only two sources then don't do this test. test for a very bad ping source. i.e. slowest source is 10x slower then the fastest, test for slower then average source. i.e. slowest source is 3x media. even if everything is going well then force a change every 30 minutes. we only apply this rule if nothing has been removed in the past 30 minutes. just find the slowest ping-source and remove it. find slowest find sped of fastest. regardless of result, resettimer the timer. only replace the slowest if it is twice the fastest. we only apply this rule if nothing has been removed in the past 15 minutes. determine fastest or second fastest. determine slowest. destroy this source. it is a bit too slow. if we just recently removed a ping source then wait. is this a new highest value? is this a new lowest value? if the highest value is 8x the lowest then find another source. remove the slow source we will get a new one to replace it."
com.aelitis.azureus.core.speedmanager.impl.v2.PingSourceStats "keeps the ping time stats for a single source. should calculate averages for the data. speculative method to see if it can determine a trend. the larger the number the stronger the trend. - interger. a positive number is an increasing trend. a negative number is a decreasing trend. get the long-term average. - longterm get the average that should be used for checking ping times. - ping time of history. want to make all the values nan until it is ready to compare again. based on current ping values. compare shortterm and medium term averages. compare short-term with long term. compare medium-term with long-term. modify results based on absolute ping values. modify results based on absolute ping values that are too long."
com.aelitis.azureus.core.speedmanager.impl.v2.PingSpaceMapper "classifies the ping-times and then maps them against the a grid of upload and download rates. create a two dimensional map of upload and download rates. map onto this space ping times. the mesh size will be smaller near zero, and larger higher up. 0 - 100 kbyte/sec - 10 kbytes mesh size. 100 - 500 kbytes/sec - 50 kbytes mesh size. 500 - 5000 kbytes/sec - 100 kbytes mesh size. anything above 5 mbytes/sec is one region. create a grid and define good and bad ping times. _goodpinginmillisec - _badpinginmillisec - we have a hard coded mesh. 0-9999 = 0, 10000- - - mesh index. the reverse of bit/sec -> mesh index calculation. - value between 0 and 70 bitspersecond that meets that criteria. start accumlating data from scratch. try to determine if a chocking ping occured during this test. - set true if this is a download_search_test. set false if upload search test. - true if it appears a chocking ping occured. look at the map and find the highest index for each catagory. [2], where index 0 is goodping, index 1 is anyping make a guess at the upload capacity based on metric data. - make a guess at the download capacity based on metric data. - class to return a result. if the input index is higher then stored, update it with the new value. - - a region on the grid for accumulating counts. todo: use the speedmanagerpingmapper interface and move this up a level. here upindex,downindex create the mesh. we will have 70 by 70 grid. even though we only use 63. return max mesh index. register this grid point if it has more good then bad pings. register this grid point if it has any ping values."
com.aelitis.azureus.core.speedmanager.impl.v2.PingSpaceMon "- - true if is has a new mode, and the clock starts over. get the current estimated upload limit from the ping mapper. - true if the long-term persistent result should be used. - speedmanagerlimitestimate. get the current estimated download limit from the ping mapper. - speedmanagerlimitestimate get the estimated download capacity from the speedmanager. - speedmanagerlimitestimate don't register the same listener twice. log this event and something to return 1 and -1.0f results. log this event and something to return 1 and -1.0f results. log this event and something to return 0 and -1.0f results. log this event and something to return 0 and -1.0f results."
com.aelitis.azureus.core.speedmanager.impl.v2.PingSpaceMonitor "- - do a check and decide if the limits should be dropped. - - the transfermode - true if the limits should be dropped. true if we have a new limit. - true if there is a new limit. call after getting new limits. five min interval. ping counters saturated mode counters to get new limit back to application reset if we are changing modes. reset everything if we change modes. if the interval is up, we need to either recommend new limits or reset. prepare the package for lowering the limit. recommend a new downloading limit. on cable modems uploads can be over-estimated by 3x. download limit cannot be less the 40k only seeding mode is left recommend a new upload limit. upload limit cannot be less the 20k no need for lower limits. simple test currently. 15% bad pings. log results. reset all the counters. saturated mode counters"
com.aelitis.azureus.core.speedmanager.impl.v2.PSMonitorListener ""
com.aelitis.azureus.core.speedmanager.impl.v2.SaturatedMode "from the currentrate and limit determine the mode. - - - saturatedmode @return the saturatedmode to be compared. negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object. the object to be compared. negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object. @throws classcastexception if the specified object's type prevents it from being compared to this object. unlimited mode has this value as zero. put a value in so it will not stay in downloading mode."
com.aelitis.azureus.core.speedmanager.impl.v2.SMConfigurationAdapter ""
com.aelitis.azureus.core.speedmanager.impl.v2.SMConfigurationAdapterImpl "one of the above constants @return for estimated limits: -1 = estimate derived from bad metrics +1 = estimate derived from good metric  -1 = relative goodness of metric @return don't call this method. @return @return to change body of implemented methods use file | settings | file templates. to change body of implemented methods use file | settings | file templates. conversion routines need to be here."
com.aelitis.azureus.core.speedmanager.impl.v2.SMConst "no limit should go below 5k bytes/sec. - - "bytes/sec" rate. rule: min value is alway 10% of max, but not below 5k. - - minrate. early in the search process the ping-mapper can give estimates that are too low due to a lack of information. the starting upload and download limits is 60k/30k should not go below the starting value a slow dsl lines should. - download rate estimate. - starting upload/download value. - strictly a utility class. zero is unlimited. don't filter that value."
com.aelitis.azureus.core.speedmanager.impl.v2.SMInstance ""
com.aelitis.azureus.core.speedmanager.impl.v2.SMSearchLogger "limit search diagnostics. get the adapter values. get the coconfigurationmanager values."
com.aelitis.azureus.core.speedmanager.impl.v2.SMSearchManager "deals with when a search should start. uses the history of previous searches more effectively."
com.aelitis.azureus.core.speedmanager.impl.v2.SMUpdate ""
com.aelitis.azureus.core.speedmanager.impl.v2.SpeedLimitConfidence "turns a string into a speedlimitconfidence class. - string is expected to be one of: none, low, med, high, absolue. - class corresponding to string. if it isn't one of the know values then none is returned. get the internationalized string for ui panels and drop downs. - internationalized string. compareto to with boolean syntax. - - true if greater then, false if equal or less. comparable interface - item to compare with. - comparable interface. - - todo: move to proper class. this is to do something now."
com.aelitis.azureus.core.speedmanager.impl.v2.SpeedLimitListener ""
com.aelitis.azureus.core.speedmanager.impl.v2.SpeedLimitMonitor "this class is responsible for re-adjusting the limits used by autospeedv2. this class will keep track of the "status" (i.e. seeding, downloading)of the application. it will then re-adjust the max limits when it thinks limits are being reached. here are the rules it will use. #1) when seeding. if the upload is at_limit for a period of time it will allow that to adjust upward. #2) when downloading. if the download is at_limit for a period of time it will allow that to adjust upward. #3) when downloading, if a down-tick is detected and the upload is near a limit, it will drop the upload limit to 80% of max_upload. #4) once that limit is reached it will drop both the upload and download limits together. #5) seeding mode is triggered when - download bandwidth at low - compared to capacity for 5 minutes continously. #6) download mode is triggered when - download bandwidth reaches medium - compared to current_limit for the first time. rules #5 and #6 favor downloading over seeding. splitting the limits our from other setting for speedmanageralgorithmti. replaces - updatefromcoconfigmanager() the criteria for download being unlimited is if the configpanel has the "download == 0 " && "type==fixed" - true are both the upload and download bandwidths usages is low? otherwise false. - - true if both are at limits. - true only if both the upload and download usages are at the limits. true if the upload bandwidth usage is high or at_limit. - does the same as createnewlimit except it drops the upload rate first when in download mode. - - - - - if a progressive download is currently active. then the download limit should not be allowed to go below that limit, regardless of anything else. - - log debug info needed during beta period. - - if setting is less then 100kbytes take 1 kbyte steps. if setting is less then 500kbytes take 5 kbyte steps. if setting is larger take 10 kbytes steps. - current limit setting. - set size for next change. make a decision about unpinning either the upload or download limit. this is based on the time we are saturating the limit without a down-tick signal. if we have a down-tick signal then resettimer all the counters for increasing the limits. return true if we are confidence testing the limits. - smupdate determine if we have low confidence in this limit. - true if the confidence setting is low or none. otherwise return true. - currentuploadrate in bytes/sec - currentuploadrate in bytes/sec convert raw ping value to new metric. - new metric from the pingmapper is value between -1.0 and +1.0f. - call this method to start the limit testing. - - - smupdate ramp the upload and download rates higher, so ping-times are relevant. - - - call this method to end the limit testing. - smupdate after a test is complete determine how condifent the client should be in it based on how different it is from the previous result. if the new result is within 20% of the old result then give it a med. if it is great then give it a low. - what the new confidence interval should be. if the make some choices about how usable the limits are before passing them on. - - it is likely the this is a lot of data, but is important debug info. - - - - - - should return true if had a recent chocking ping. - true if just log this data until we decide if it is useful. use for home network. upload and download bandwidth usage modes. compare usage to current limit. compare current limit to max limit. how much confidence to we have in the current limits? these methods are used to see how high limits can go. 30 seconds. which percent of the measured upload capacity to use in download and seeding mode. pingspacemaps for the entire session.  get persistent mapper. get upload estimate. will find upload limit via slow search. get download estimate. speedmanagerlogger.log("speed-limit-conf: "+type+" rate="+orate+" conf="+oconf.getstring()+"("+oconf.asestimatetype() +") pm-rate="+nrate+" pm-conf="+nconf); for testing. for testing.   start the search in unlimited mode. speedlimitmonitorstatus either upload or download is at medium or above. either upload or download is at medium or above. this flag is set in a previous method. do we have an active download limit? if the value is zero, then the no progressive download is currently active. we seem to have an active progressive download. make sure the limit does not drop below that limit. first verify that is this is an up signal. down-tick is a signal to stop moving the files up. just verify settings to make sure everything is sane before updating. slow the upload rate the more. increase limit by calculated amount, but only if not in downloading mode. increase limit by calculated amount. apply any rules that need applied. the download limit can never be less then the upload limit. (unless zero. unlimited) verify the download is not unlimited. upload useage must be at limits for a set period of time before unpinning. start the clock over. check to see if we have been here for the time limit. alway slow search the upload limit. don't unpin the limit is we have absolute confidence in it. we have been at_limit long enough. time to un-pin the limit see if we can go higher. download usage must be at limits for a set period of time before unpinning. start the clock over. check to see if we have been here for the time limit. we have been at_limit long enough. time to un-pin the limit see if we can go higher. chocking ping needs higher limit. the exit criteria for this test is 30 seconds without an increase in the limits. set the test done flag. or 30 seconds after its first bad ping. set the test done flag. convert raw - pings into a rating. setting this time is a signal to end soon. reset the flag. get the limits before the test, we are restoring them after the test. configure the limits for this test. one will be at min and the other unlimited. test the download limit. test the upload limit. if we are using a persistent pingsource then get that here. start a new transientpingmap; determine if the new setting is different then the old setting. set that value. change back to original mode. set that value. change back to original mode. this is an "illegal state" make it in the logs, but try to recover by setting back to original state. reset the counter string settingminlimitname;  only set to medium if had both a chocking ping and two tests with similar results. update the values. downloadcapacity can never be less then upload capacity. upload capacity should never be 40x less then download. during a confidence level test, anything goes. no estimate less then 20k accepted. things looking good, this is just a new limit estimate and shouldn't affect the actual limit in force chose ping mapper. check for the case when the line-speed capacity is below the current limit. check for the case when the min setting has been moved above the current limit. another possibility is the min limits have been raised. pingmonitor = new pingspacemonitor(maxgoodping,minbadping,transfermode); todo: remove after beta-testing - just to characterize the different methods. pingmapofdownloadmode = new pingspacemapper(150,500); pingmapofseedingmode = new pingspacemapper(150,500); note: currently just getting the persistentmap for temp logging purposes. log the different ping-mappers for beta. note: currently just getting the persistentmap for temp logging purposes. log the different ping-mappers for beta. if either had a choking ping. add point to map for download mode add point to map for seeding mode. if confidence limit testing, inform of bad ping. update coconfiguration update coconfiguration"
com.aelitis.azureus.core.speedmanager.impl.v2.SpeedManagerAlgorithmProviderDHTPing "this class implements an vivaldi strategy for adjusting the upload limit. it will every 30 seconds calculate the distance to the center of the vivaldi structure and if it determines that it is going away from it will lower the limit. if it getting closer it will increase the upload limit. reset any state to start of day values called periodically (see period above) to allow stats to be updated. log "curr-data" line to the autospeed-beta file. - - - - called when a new source of ping times has been found - _replacement one of the initial sources or a replacement for a failed one ping source has failed - called whenever a new set of ping values is available for processing - log the limit status. max, min and conf. log("limits:down-max:down-min:down-conf:up-max:up-min:up-conf"); dht ping data is one of the metrics used. calculate it here. - true if should exit early from the caluculate method. just update the limits. - smupdate determined by the vivaldi value and the number of consecutive calculations with the same result. - - the longer were get the same signal the stronger it is. on upticks however we only increase the rates when if the upload or download is saturated. - want to rise much slower then drop. - number of upsignals recieved in a row - multiple factor. want to drop rate faster then increase. - - various getters for interesting info shown in stats view - returns the current view of when choking occurs in bytes/sec metric values for dht ping times and vivaldi speedlimitmonitor variables for display and vivaldi. use for dht ping.  for managing ping sources. need to log this condition if it ever happens!! update some stats used in the ui. update the bandwidth status update the limts status. (is it near a forced max or min?) update ping maps only for the ui. "curr-data" .... we might not use ping source if the vivaldi data is available. add a new ping source to the list. where does the vivaldi data for the chart come from. get new data to ping-source-manager. exclude ping-times of -1 which mess up the averages. if we are in a limit finding mode then don't even bother with this calculation. will increase the limit each cycle. use the dht ping times instead. update the metric data if are are not looking for limits and we have a signal then make an adjustment. log setting new verify the limits. it is possible for the determine if we need to drop a ping source. print out the pingmap data to compare. reset ping space map for next round. log setting new don't count this data point, if we skip the next ping times after an adjustment. have we accululated enough data to make an adjustment? get more data before making another calculation. we have enough data. find the median ping time. if we don't have any pings, then either the connection is lost or very bad network congestion. force an adjustment down. this is a high value to force an adjusment down. we have now consumed this data. reset the counters. determine if this is an up-tick (+1), down-tick (-1) or neutral (0). strong up signal. weak up signal. strong down signal weak down signal this is a neutral signal. set the consecutive upticks back to zero if the bandwidth is not being used. decrease the signal strength if bandwith usage is only in med use. return the vivaldi time. currently a fixed number to be sure of algorightm. todo auto-generated method stub"
com.aelitis.azureus.core.speedmanager.impl.v2.SpeedManagerAlgorithmProviderPingMap "reset any state to start of day values called periodically (see period above) to allow stats to be updated. log "curr-data" line to the autospeed-beta file. - - - - called when a new source of ping times has been found - _replacement one of the initial sources or a replacement for a failed one ping source has failed - called whenever a new set of ping values is available for processing - log the limit status. max, min and conf. log("limits:down-max:down-min:down-conf:up-max:up-min:up-conf"); variance pingmap data is the metrics used. calculate it here. - true if should exit early from the caluculate method. just update the limits. - smupdate - float -1.0f to +1.0f signal as float with 0.0 meaning don't make an adjustment. the longer were get the same signal the stronger it is. on upticks however we only increase the rates when if the upload or download is saturated. - want to rise much slower then drop. - number of upsignals recieved in a row - multiple factor. want to drop rate faster then increase. - - various getters for interesting info shown in stats view - returns the current view of when choking occurs in bytes/sec speedlimitmonitor variables for display and vivaldi. private int lastmetricvalue; use for ping.  //if we want to average variance pings. for managing ping sources. note:: pingsourcemanager is something that should be moved up one level!!! update some stats used in the ui. update the bandwidth status update the limts status. (is it near a forced max or min?) update ping maps only for the ui. "curr-data" .... we might not use ping source if the vivaldi data is available. add a new ping source to the list. where does the vivaldi data for the chart come from. get new data to ping-source-manager. exclude ping-times of -1 which mess up the averages. pingtimelist.add( new integer( sources[i].getpingtime() ) ); if we are in a limit finding mode then don't even bother with this calculation. will increase the limit each cycle. use the variance ping times instead. update the metric data limitmonitor.addtopingmapdata(lastmetricvalue); if are are not looking for limits and we have a signal then make an adjustment. log setting new verify the limits. it is possible for the determine if we need to drop a ping source. print out the pingmap data to compare. reset ping space map for next round. log setting new don't count this data point, if we skip the next ping times after an adjustment. have we accululated enough data to make an adjustment? get more data before making another calculation. we have enough data. find the median ping time. we have now consumed this data. reset the counters. determine if this is an up-tick (+1), down-tick (-1) or neutral (0). range should be -1.0 (bad) to +1.0 (good) this is a neutral signal. here we will map the neutral region to -0.5f to +0.5f to singal = 0; map weak up signal. map weak down signal set the consecutive upticks back to zero if the bandwidth is not being used. decrease the signal strength if bandwith usage is only in med use. return the vivaldi time. currently a fixed number to be sure of algorightm. todo auto-generated method stub"
com.aelitis.azureus.core.speedmanager.impl.v2.SpeedManagerAlgorithmProviderV2 "test algorithms below. key names are below. temporary two names for upload/download max until we sort out which. sets the input source, vivaldi, dht ping, icmp pint, etc .... dht ping settings. ping factors settings. enable this mode. strategy = new speedmanageralgorithmproviderdhtping(_adapter); todo auto-generated method stub"
com.aelitis.azureus.core.speedmanager.impl.v2.SpeedManagerLogger ""
com.aelitis.azureus.core.speedmanager.impl.v2.TransferMode "is the application in a "download" mode? or is it in a "seeding" mode? this is used to determine up we cut back on upload bandwidth limit. here is how to determine the mode. if the download rate is low compared to the capacity for five minutes continously then it will be considered in a seeding mode. if the download bandwidth ever goes into the med range then it switches to downloading mode immediately. the application will favor downloading mode to seeding mode. if the download bandwidth is ever in med or above switch immediately to downloading mode. if th download bandwidth is low or less for more then 5 min, switch to seeding mode. - current download status. are we in downloading mode? - boolean - true if in downloading mode. otherwise false. we have two types of limit testing. if we are doing a "confidence test" for the limits then return true. this mode is putting one value at the min setting and the other at unlimited. - true if doing a "conf test of the limits" java 1.4 enumeration. - seeding mode or downloading mode. this setting have no effect while testing the limits. we don't seem to be downloading at the moment. see if this state has persisted for more then five minutes. some downloading is happening. remove from seeding mode."
com.aelitis.azureus.core.speedmanager.impl.v3.SpeedManagerAlgorithmProviderV3 "returns the current view of when choking occurs in bytes/sec time we'll force low upload to get baseline how long we'll wait on start up before forcing min speed at which upload is treated as "idle" any lower than this and small ping variations cause overreaction discount anything 5min reported unless min is really small, in which case round up as we're only trying to catch badly behaved ones all failed bias towards min if we're uploading slowly or the current ping rate is better than our current idle average then we count this towards establishing the baseline bump down if we happen to come across lower idle values we've had 5 secs of min up speed, clear out the ping average now to get accurate times we've been running a while but no min set, or we've got some new untested contacts - force it three strikes and you're out!"
com.aelitis.azureus.core.speedmanager.SpeedLimitHandler "if ( tick_count % 30 == 0 ){ string str = ""; for ( ipset set: current_ip_sets.values()){ str += (str.length()==0?"":", ") + set.getstring(); } logger.log( str ); } uppercase here as category names are case sensitive.. same rule as before migration from before lan rates added just maintain the auto upload setting over a reset don't manage this properly because the speedmanager has a 'memory' of the last upload limit in force before it became active and we're not persisting this... rare use case methinks anyway handle rules that wrap across days. e.g. 23:00 to 00:00 average over 10s, update every 1000ms average over 10s, update every 1000ms no ipv6 atm system.out.println( cidr + " -> " + byteformatter.encodestring( start_bytes ) + " - " + byteformatter.encodestring( end_bytes ) + ": " + ((l_end-l_start+1))); ", addresses=" + addresses +"
com.aelitis.azureus.core.speedmanager.SpeedManager "_per_sec see constants above for help"
com.aelitis.azureus.core.speedmanager.SpeedManagerAdapter ""
com.aelitis.azureus.core.speedmanager.SpeedManagerFactory ""
com.aelitis.azureus.core.speedmanager.SpeedManagerLimitEstimate "one of the above constants @return for estimated limits: -1 = estimate derived from bad metrics +1 = estimate derived from good metric  -1 = relative goodness of metric @return"
com.aelitis.azureus.core.speedmanager.SpeedManagerListener ""
com.aelitis.azureus.core.speedmanager.SpeedManagerPingMapper "+1 : good -1 : bad >-1 <+1 : relative goodness/badness @return"
com.aelitis.azureus.core.speedmanager.SpeedManagerPingSource ""
com.aelitis.azureus.core.speedmanager.SpeedManagerPingZone ""
com.aelitis.azureus.core.stats.AzureusCoreStats "disk network tcp http peer control peer manager tracker xfer (persistent) skip initial value as 'last_value' is invalid"
com.aelitis.azureus.core.stats.AzureusCoreStatsProvider ""
com.aelitis.azureus.core.subs.impl.SubscriptionBodyImpl "key for signature is hash + version + size so we have some control over auto-update process and prevent people from injecting potentially huge bogus updates load constructor import constructor verify backwards compat for pre-az_version create constructor derived data backward compat from before az_version was introduced"
com.aelitis.azureus.core.subs.impl.SubscriptionDownloader "if ( mature != null ){ sps.add( new searchparameter( "m", mature.tostring())); }"
com.aelitis.azureus.core.subs.impl.SubscriptionHistoryImpl "first download feed -> mark all existing as read see if we can associate result with existing download we first of all insist on names uniqueness only if non-unique name do we fall back and use uid to remove duplicate entries where the name has changed see if we need to delete any old ones always save config as we have a new scan time never scanned, scan immediately migration - if we've already downloaded this feed then we default to being enabled"
com.aelitis.azureus.core.subs.impl.SubscriptionImpl "private static final byte[] generic_public_key = {(byte)0x04,(byte)0xd0,(byte)0x1a,(byte)0xd9,(byte)0xb9,(byte)0x99,(byte)0xd8,(byte)0x49,(byte)0x15,(byte)0x5f,(byte)0xe9,(byte)0x6b,(byte)0x3c,(byte)0xd8,(byte)0x18,(byte)0x81,(byte)0xf7,(byte)0x92,(byte)0x15,(byte)0x3f,(byte)0x24,(byte)0xaa,(byte)0x35,(byte)0x6f,(byte)0x52,(byte)0x01,(byte)0x79,(byte)0x2e,(byte)0x93,(byte)0xf6,(byte)0xf1,(byte)0x57,(byte)0x13,(byte)0x2a,(byte)0x3c,(byte)0x31,(byte)0x66,(byte)0xa5,(byte)0x34,(byte)0x9f,(byte)0x79,(byte)0x62,(byte)0x04,(byte)0x31,(byte)0x68,(byte)0x37,(byte)0x8f,(byte)0x77,(byte)0x5c}; private static final byte[] generic_private_key = {(byte)0x71,(byte)0xc3,(byte)0xe8,(byte)0x6c,(byte)0x56,(byte)0xbb,(byte)0x30,(byte)0x14,(byte)0x9e,(byte)0x19,(byte)0xa5,(byte)0x3d,(byte)0xcb,(byte)0x47,(byte)0xbb,(byte)0x6d,(byte)0x57,(byte)0x57,(byte)0xd3,(byte)0x59,(byte)0xce,(byte)0x8f,(byte)0x79,(byte)0xe5}; new subs constructor cache detail constructor import constructor this picks up latest values of version, name + is_public from here body data local data pick up details from the body (excluding json that is maintained in body only) write to file see if we need to embed private search templates singleton's not available for upgrade nothing to do for unsubscribed ones one off test on whether to track so we have around 100 active make a version based filename to avoid issues regarding multiple versions first set in order of most recent remaining randomised singleton versions always 1 and each instance has separate private key so verification will always fail so save to just return current version"
com.aelitis.azureus.core.subs.impl.SubscriptionManagerImpl "if ( constants.iscvsversion()){ addlistener( new subscriptionmanagerlistener() { public void subscriptionadded( subscription subscription ) { } public void subscriptionchanged( subscription subscription ) { } public void subscriptionremoved( subscription subscription ) { } public void associationschanged( byte[] hash ) { system.out.println( "subscriptions changed: " + byteformatter.encodestring( hash )); subscription[] subs = getknownsubscriptions( hash ); for (int i=0;i<subs.length;i++){ system.out.println( " " + subs[i].getstring()); } } }); } subscription subs = getsingleton(true).createsingletonrss( name, new url( url_str ), 240 ); subs.getvuzefile().write( new file( "c:\\temp\\srss.vuze" )); subs.remove(); saw deadlock here when adding core listener while synced on class - rework to avoid if ever changed to handle non-persistent then you need to fix init deadlock potential with share-hoster plugin if new download then we want to check out its subscription status hack to minimise encoded url length for our own urls only defined type is singleton rss engine name may have been modified so re-read it for subscription default best guess never update singletons use a transient key-pair as we won't have the private key in general we have a newer one, ignore to avoid development links polluting production we filter out such subscriptions check if already subscribed record zero assoc here for completeness unknown sid - if singleton try to register for popularity tracking purposes todo: fix? fall back to dht startswith as actual url may have had additional parameters added such as azid inject a random element so we can count occurrences properly (as the dht logic removes duplicates) remove any injected random seed update size is just that of signed content, torrent itself is .vuze file so take this into account testing purposes, see if local exists platformtorrentutils.setcontentthumbnail(torrent, thumbnail); vuze template but best guess sync not required (and has caused deadlock) as aediagnostics handles singleton term is made up of space separated bits - all bits must match each bit can be prefixed by + or -, a leading - means 'bit doesn't match'. + doesn't mean anything each bit (with prefix removed) can be "(" regexp ")" if bit isn't regexp but has "|" in it it is turned into a regexp so a|b means 'a or b' azureuscorefactory.create();"
com.aelitis.azureus.core.subs.impl.SubscriptionResultFilter "unused invalid filters array results need a name, or they are by default invalid if one of the text filters do not match, let's not keep testing the others and mark the result as not valid double check against reg-expr if exists if invalid after name check, let's get to the next result if one of the text filters do not match, let's not keep testing the others and mark the result as not valid if invalid after name check, let's get to the next result all filters are ok, let's add the results to the filtered results"
com.aelitis.azureus.core.subs.impl.SubscriptionResultImpl "migration - trim digits"
com.aelitis.azureus.core.subs.impl.SubscriptionRSSFeed "absolute url is borked as it doesn't set the host properly. hack"
com.aelitis.azureus.core.subs.impl.SubscriptionSchedulerImpl "platformtorrentutils.setcontenttitle(torrent, torr );"
com.aelitis.azureus.core.subs.Subscription "shortcut to help plugin interface"
com.aelitis.azureus.core.subs.SubscriptionAssociationLookup ""
com.aelitis.azureus.core.subs.SubscriptionDownloadListener ""
com.aelitis.azureus.core.subs.SubscriptionException ""
com.aelitis.azureus.core.subs.SubscriptionHistory ""
com.aelitis.azureus.core.subs.SubscriptionListener ""
com.aelitis.azureus.core.subs.SubscriptionLookupListener ""
com.aelitis.azureus.core.subs.SubscriptionManager "full lookup @return @throws subscriptionexception cached view of hash's subs @return creates a subscription that will always have the same identity for the given parameters and can't be updated"
com.aelitis.azureus.core.subs.SubscriptionManagerFactory ""
com.aelitis.azureus.core.subs.SubscriptionManagerListener ""
com.aelitis.azureus.core.subs.SubscriptionPopularityListener ""
com.aelitis.azureus.core.subs.SubscriptionResult "see searchresult properties for list @return"
com.aelitis.azureus.core.subs.SubscriptionScheduler ""
com.aelitis.azureus.core.subs.SubscriptionUtils ""
com.aelitis.azureus.core.torrent.HasBeenOpenedListener ""
com.aelitis.azureus.core.torrent.PlatformTorrentUtils "needs to be lowercase needs to be lowercase todo auto-generated catch block allow local addresses for testing check them all incase someone includes one of our trackers in a multi-tracker torrent cache true, always recheck false in case plugin installs. dm state's display name can be set by"
com.aelitis.azureus.core.tracker.TrackerPeerSource ""
com.aelitis.azureus.core.tracker.TrackerPeerSourceAdapter ""
com.aelitis.azureus.core.update.AzureusRestarter ""
com.aelitis.azureus.core.update.AzureusRestarterFactory ""
com.aelitis.azureus.core.update.impl.AzureusRestarterImpl "some results: 0: oom 2: fnf 3: path not foud 5: access denied ( private void logstream(string message,inputstream stream,printwriter log) { bufferedreader br = new bufferedreader (new inputstreamreader(stream)); string line = null; boolean first = true; try { while((line = br.readline()) != null) { if( first ) { log.println(message); first = false; } log.println(line); } } catch(exception e) { log.println(e); e.printstacktrace(log); } } private void chmod(string filename,string rights,printwriter log) { string[] execstr = new string[3]; execstr[0] = "chmod"; execstr[1] = rights; execstr[2] = filename; runexternalcommandslogged( log, execstr ); } private process runexternalcommandlogged( printwriter log, string command ) { //note: will not return until external command process has completed log.println("about to execute: r:[" +command+ "]" ); try { process runner = runtime.getruntime().exec( command ); runner.waitfor(); logstream( "runtime.exec() output:", runner.getinputstream(), log); logstream( "runtime.exec() error:", runner.geterrorstream(), log); return runner; } catch( throwable t ) { log.println( t.getmessage() != null ? t.getmessage() : "" ); log.println( t ); t.printstacktrace( log ); return null; } } private process runexternalcommandslogged( printwriter log, string[] commands ) { //note: will not return until external command process has completed string cmd = "about to execute: r:["; for( int i=0; i " ); log.println( t ); t.printstacktrace( log ); return null; } } already logged note: new 2306 osx bundle now sets azureus.nativelauncher=1, but older bundles dont this handles unicode chars by writing \\u escapes we intercept these logs and log immediately just check if any non-logged data exists we need to spawn without inheriting handles vista test: we will need to run an elevated exe updater if we can't write to the program dir. ignore vista test todo auto-generated catch block create a batch file to run the updater, then to restart azureus bceause the updater would restart azureus as administrator and confuse the this code is copied into restarter / updater so make changes there too !!! not for updater.java classic restart way using runtime.exec directly on java(w) hmm, try java method - this will inherit handles but might work :) note: no logging done here, as we need the method to return right away, before the external process completes run script after az shutdown to launch updater and then re-run az remove any quotes from the damn thing remove trailing separator chars if they exist as they stuff up the following " note: will not return until external command process has completed note: will not return until external command process has completed note: no logging done here, as we need the method to return right away, before the external process completes"
com.aelitis.azureus.core.util.AEPriorityMixin ""
com.aelitis.azureus.core.util.average.Average "standard methods for different types of long-term averages. update average and return average-so-far. return average-so-far. sets back to start-of-day"
com.aelitis.azureus.core.util.average.AverageFactory "generates different types of averages. create a simple running average. create a moving average, that moves over the given number of periods. create a moving average, that moves over the given number of periods and gives immediate results (i.e. after the first update of x the average will be x create an exponential moving average, smoothing over the given number of periods, using a default smoothing weight value of 2/(1 + periods). create an exponential moving average, with the given smoothing weight. larger weigths (closer to 1.0) will give more influence to recent data and smaller weights (closer to 0.00) will provide smoother averaging (give more influence to older data)."
com.aelitis.azureus.core.util.average.ExponentialMovingAverage "implements an exponential moving average. create a new exponential moving average which smooths over the given number of periods. create a new exponential moving average, using the given smoothing rate weight. update average and return average-so-far. return average-so-far."
com.aelitis.azureus.core.util.average.MovingAverage "implements a basic moving average. create a new moving average. update average and return average-so-far. return average-so-far."
com.aelitis.azureus.core.util.average.MovingImmediateAverage "implements a basic moving average. create a new moving average. update average and return average-so-far. return average-so-far."
com.aelitis.azureus.core.util.average.RunningAverage "implements a simple running average. create a new running average. update average and return average-so-far. return average-so-far."
com.aelitis.azureus.core.util.AZ3Functions ""
com.aelitis.azureus.core.util.bloom.BloomFilter "returns number of unique entries @return returns overall capacity @return"
com.aelitis.azureus.core.util.bloom.BloomFilterFactory "creates a new bloom filter. _entries the filter size. a size of 10 expected entries gives a false-positive of around 0.01% 17 -> 0.001 29 -> 0.0001 each entry takes 1, 4 or 8 bits depending on type so, if 0.01% is acceptable and expected max entries is 100, use a filter size of 1000. @return"
com.aelitis.azureus.core.util.bloom.impl.BloomFilterAddOnly "returns the value before increment returns the value before decrement b = (byte)(b&~(0x01" + integer.tohexstring( b&0xff ));"
com.aelitis.azureus.core.util.bloom.impl.BloomFilterAddRemove4Bit "4 bits per entry system.out.println( "setvalue[" + index + "]:" + integer.tohexstring( map[index/2]&0xff) + "->" + integer.tohexstring( b&0xff ));"
com.aelitis.azureus.core.util.bloom.impl.BloomFilterAddRemove8Bit ""
com.aelitis.azureus.core.util.bloom.impl.BloomFilterImpl "private static final boolean use_big_ints = false; private static final char[] hex_chars = "0123456789abcdef".tochararray(); private static final biginteger bi_zero = new biginteger("0"); private static final biginteger bi_a2 = new biginteger(""+a2); private static final biginteger bi_a3 = new biginteger(""+a3); private static final biginteger bi_a4 = new biginteger(""+a4); private static final biginteger bi_b2 = new biginteger(""+b2); private static final biginteger bi_b3 = new biginteger(""+b3); private static final biginteger bi_b4 = new biginteger(""+b4); protected int add( biginteger value ) { entry_count++; for (int i=0;i 0 ){ setvalue( index, (byte)(v-1)); } } } public boolean contains( biginteger value ) { for (int i=0;i" + r ); return( r % max_entries ); } protected biginteger bytestobiginteger( byte[] data ) { stringbuffer buffer = new stringbuffer(data.length2); for (int i=0;i>4)&0x0f] ); buffer.append( hex_chars[data[i]&0x0f] ); } biginteger res = new biginteger( new string(buffer), 16 ); return( res ); } bloomfilter b1 = new bloomfilteraddremove8bit(10000); for (int i=0;i" + r ); res ^= (data[i]&0xff)" + r ); combine address and port bytes into one bloomfilter b = new bloomfilteraddonly(10000); string key = "" + rand.nextint(); key = getserialization( key, 6881 );"
com.aelitis.azureus.core.util.bloom.impl.BloomFilterRotator "capacity limit system.out.println( "rot_bloom: cur=" + current_filter_index + ", upd=" + num_to_update + ",ent=" + filter_entries );"
com.aelitis.azureus.core.util.ByteCountedInputStream ""
com.aelitis.azureus.core.util.CaseSensitiveFileMap ""
com.aelitis.azureus.core.util.CopyOnWriteList "private void mutated() { mutation_count++; if ( mutation_count%10 == 0 ){ system.out.println( this + ": mut=" + mutation_count ); } } private int mutation_count = 0; smaller default initial capacity as most of our lists are small last check on 7/24/2008: 444 lists with 456 total entries mutated(); mutated(); mutated(); mutated(); todo: we need to either make this a read-only-list or obey the copy-on-write semantics correctly... don't actually remove it from the iterator. can't go backwards with this iterator so this is not a problem"
com.aelitis.azureus.core.util.CopyOnWriteMap "shouldn't return underlying map directly as open to abuse. either wrap in unmodifyable map or implement desired features explicitly public map getmap() { return this.map; }"
com.aelitis.azureus.core.util.CopyOnWriteSet "don't actually remove it from the iterator. can't go backwards with this iterator so this is not a problem"
com.aelitis.azureus.core.util.DeleteFileOnCloseInputStream "@return"
com.aelitis.azureus.core.util.dns.DNSUtilsImpl "list records = gettxtrecords( "www.ibm.com" ); for ( string record: records ){ system.out.println( record ); } system.out.println( "dnstxtquery: " + query ); e.printstacktrace(); list records = gettxtrecords( "tracker.openbittorrent.com" );"
com.aelitis.azureus.core.util.DNSUtils ""
com.aelitis.azureus.core.util.FeatureAvailability "public static final boolean enable_plus() { return( true ); }"
com.aelitis.azureus.core.util.GeneralUtils "string.replaceall does \\ and $ expansion in replacement, this doesn't, in fact it doesn't do any pattern matching at all, it is a literal replacement _str = note, no regex support @return as above but does safe replacement of multiple strings (i.e. a match in the replacement of one string won't be substituted by another) _strs _strs @return splits space separated tokens respecting quotes (either " or ' ) @return xxx: this doesn't appear to be used..."
com.aelitis.azureus.core.util.HashCodeUtils "bob jenkin's hash function"
com.aelitis.azureus.core.util.http.HTTPAuthHelper "create children for related domains relative remove domain restriction so cookie sent to localhost force to be session cookie otherwise we'll end up sending cookies from multiple sites to 'localhost' http://www.ietf.org/rfc/rfc2965.txt one or more comma separated absolute relative. actually illegal as must be absolute strip out absolute part chunking uses iso-8859-1 second time around the chunk will be prefixed with nl from end of previous so make sure we ignore this dechunked iso-8859-1 - unzip if required and then apply correct charset fileutil.writebytesasfile( "c:\\temp\\xxx" + new random().nextint(100000) + ".txt", str.getbytes()); http://a.b make sure vald url override if form action or meta"
com.aelitis.azureus.core.util.http.HTTPAuthHelperListener ""
com.aelitis.azureus.core.util.HTTPUtils "_type file extension content type string if found _type a file type like text/plain if the file_type should be compressed gzip;q=0 look to see if there's a q=0 (or 0.0) disabling gzip limit time spent trying to read debug second time around the chunk will be prefixed with nl from end of previous so make sure we ignore this"
com.aelitis.azureus.core.util.LaunchManager ""
com.aelitis.azureus.core.util.loopcontrol.impl.PIDLoopControler "proportional"
com.aelitis.azureus.core.util.loopcontrol.LoopControler "xxx: i can't find anywhere where this is used..."
com.aelitis.azureus.core.util.MultiPartDecoder "-----------------------------7d4f2a310bca content-disposition: form-data; name="upfile"; filename="c:\temp\banana_custard.torrent" content-type: application/octet-stream   -----------------------------7d4f2a310bca content-disposition: form-data; name="category"  music -----------------------------7d4f2a310bca-- if we didn't find the target and we're not in the header then any remaining data in the buffer (less current target length) is part of a body and can be written out process buffer 0 length rem nothing read and no progress made should end in -- system.out.println( "formfield:" + name ); system.out.println( " total_len = " + total_len );"
com.aelitis.azureus.core.util.NetUtils "got some major cpu issues on some machines with crap loads of nis system.out.println( "getni: elapsed=" + elapsed_millis + ", result=" + result.size()); sometimes get this when changing host name return first non-loopback one"
com.aelitis.azureus.core.util.png.Chunk ""
com.aelitis.azureus.core.util.png.CRCedChunk "table of crcs of all 8-bit messages. flag: has the table been computed? initially false. make the table for a fast crc. update a running crc with the bytes buf[0..len-1]--the crc should be initialized to all 1's, and the transmitted value is the 1's complement of the final running crc (see the crc() routine below). return the crc of the bytes buf[0..len-1]."
com.aelitis.azureus.core.util.png.IDATChunk "no filter on this line"
com.aelitis.azureus.core.util.png.IENDChunk ""
com.aelitis.azureus.core.util.png.IHDRChunk "width : 4 bytes height : 4 bytes color depth : 1 byte color type : 1 byte compression method : 1 byte filter method : 1 byte interlace method : 1 byte"
com.aelitis.azureus.core.util.png.PNG ""
com.aelitis.azureus.core.util.png.PngSignatureChunk ""
com.aelitis.azureus.core.util.QTFastStartRAF "the original code by mike melanson (melanson@pcisys.net) was placed in the public domain and so is this java port of it. system.out.println( "got " + ah.type +", size=" + ah.size ); store ftyp atom to buffer e.printstacktrace(); [header] file [body-start -> body-end] get atom size get atom type 64 bit size. read new size from body and store it skip back to atom start"
com.aelitis.azureus.core.util.RegExUtil "http://en.wikipedia.org/wiki/redos"
com.aelitis.azureus.core.util.UUIDGenerator "128 bit uuid using random method @return 129 byte random uuid formatted as standard 36 char hex and hyphen string @return"
com.aelitis.azureus.core.versioncheck.VersionCheckClient "client for checking version information from a remote server. get the singleton instance of the version check client. check client get the version check reply info. data, possibly cached, if the server was already checked within the last minute get the ip address seen by the version check server. note: this information may be cached, see getversioncheckinfo(). ip address, or empty string if no address information found is the dht plugin allowed to be enabled. if dht can be enabled, false if it should not be enabled is the dht allowed to be used by external plugins. if extended dht use is allowed, false if not allowed perform the actual version check by connecting to the version server. _to_send version message reply @throws exception if the server check connection fails construct the default version check message. to send ignore ignore ignore suppress all of these errors being displayed in the output. these things should be temporary... if we've never checked before then we go ahead even if the "only_if_cached" flag is set as its had not chance of being cached yet! internet is broken debug.out(t.getclass().getname() + ": " + t.getmessage()); dns is broken debug.out(t.getclass().getname() + ": " + t.getmessage()); if we've never checked before then we go ahead even if the "only_if_cached" flag is set as its had not chance of being cached yet! clear down any plugin-specific data that has successfully been sent to the version server installed plugin ids no internet general connection problem. currently we maintain v4 much more accurately than v6 we take the view that if the version check failed then we go ahead and enable the dht (i.e. we're being optimistic) be generous and enable extended use if check failed send our version message get the server reply connection ids for requests must always have their msb set... apart from the original darn udp tracker spec.... kick off a secondary version check to communicate the new information require crypto set ui.toolbar.uiswitcher based on instructions from tracker really shouldn't be in versioncheck client, but instead have some listener and have the code elsewhere. simply calling getversioncheckinfo from "code elsewhere" (to get the cached result) caused a deadlock at startup. only send if anonymous-check flag is not set always send see http://lopica.sourceforge.net/os.html might be needed to openjdk on osx swt stuff time since last long total_bytes_downloaded = stats.getdownloadedbytes(); long total_bytes_uploaded = stats.getuploadedbytes(); removed due to complaints about anonymous stats collection message.put( "total_bytes_downloaded", new long( total_bytes_downloaded ) ); message.put( "total_bytes_uploaded", new long( total_bytes_uploaded ) ); message.put( "dlstats", stats.getdownloadstats()); send locale, so we can determine which languages need attention we may want to reply differently if the installed plugin ids filter out built-in and core ones test connectivity. system.out.println( "udp: " + getsingleton().getexternalipaddressudp(null,0,v6)); system.out.println( "tcp: " + getsingleton().getexternalipaddresstcp(null,0,v6)); system.out.println( "http: " + getsingleton().getexternalipaddresshttp(v6)); used for debugging in main."
com.aelitis.azureus.core.versioncheck.VersionCheckClientListener ""
com.aelitis.azureus.core.versioncheck.VersionCheckClientUDPCodecs ""
com.aelitis.azureus.core.versioncheck.VersionCheckClientUDPReply ""
com.aelitis.azureus.core.versioncheck.VersionCheckClientUDPRequest ""
com.aelitis.azureus.core.vuzefile.VuzeFile ""
com.aelitis.azureus.core.vuzefile.VuzeFileComponent ""
com.aelitis.azureus.core.vuzefile.VuzeFileHandler ""
com.aelitis.azureus.core.vuzefile.VuzeFileImpl ""
com.aelitis.azureus.core.vuzefile.VuzeFileMerger ""
com.aelitis.azureus.core.vuzefile.VuzeFileProcessor ""
com.aelitis.azureus.launcher.ClassLoaderWitness "the only purpose of this class is to have a reference point for classloader checks"
com.aelitis.azureus.launcher.classloading.PeeringClassloader ""
com.aelitis.azureus.launcher.classloading.PrimaryClassloader "initialization path when loaded through bootstrapping initialization path when loaded via -djava.system.class.loader=com.aelitis.azureus.launcher.classloading.primaryclassloader instead of bootstrapping, has the advantage that this gets registered as system classloader @deprecated do not invoke manually altered class lookup  follow normal delegation, circumventing the system classloader as we bootstraped it away or delegate to the system classloader iff it is for classes from this package, this allows us to rebootstrap and discard other branches in the hierarchy check for loaded by peers try to load from peers   system.out.println(this+" loading "+name); no removal here, peerfindloadedclass should take care of that anyway"
com.aelitis.azureus.launcher.classloading.SecondaryClassLoader ""
com.aelitis.azureus.launcher.Launcher "this will (hopefully) become a unified launching pathway covering everything from the bare core over uis to launchable plugins in the future bootstraps a new {@link primaryclassloader} from the system class loader, creates a new thread, instantiates mainclass and invokes its main method with the provided arguments. class implementing the main() method which will be invoked arguments to the main method checks if the current classloader is not part of the {@link peeringclassloader} hierarchy and performs {@link #launch(class, string[])} if so  this example shows how to implement a main class that ensures that it is called through the primary class loader:  myclass { public static void main(string[] args) { if(launcher.checkandlaunch(myclass.class,args)) return; ... // normal main code } }  if bootstrapping was necessary and the main method was called through a new classloader, false otherwise if the current classloader is part of the {@link peeringclassloader} hierarchy normal main code todo unify commandline processing and gui selection code here"
com.aelitis.azureus.launcher.LauncherTest ""
com.aelitis.azureus.launcher.MainExecutor ""
com.aelitis.azureus.plugins.clientid.ClientIDPlugin "trim of any _bnn or _cvs suffix as unfortunately some trackers can't cope with this (well, apparently can't cope with b10) its not a big deal anyway"
com.aelitis.azureus.plugins.dht.DHTPlugin "data will be the dht instance every 15 mins don't check for new_port being dht_data_port here as we want to continue to pick up changes that occurred during dht init go async again as don't want to block other tasks adapter only added to first dhtpluginimpl we create pick up any port changes that occurred during init don't know yet just the cvs dht hook into cvs completion to prevent runaway cvs dht operations both dhts active. initially (at least :) v6 is going to be very sparse. we therefore don't want to be blocking the "get" operation waiting for v6 to timeout when v4 is returning hits only report if not yet complete we are guaranteed to come through here at least twice if we have reported any results then we can't report timeout! one of the two gets, see how much longer we're happy to hang around for only of interest if timeout then uninterested as the other will be about to timeout ignore a v6 completion ahead of a v4 hack - use different keys so we can distinguish which completion event we have received above lazy - just report ops on one dht first dht will do here fallback first dht will do here direct read/write support"
com.aelitis.azureus.plugins.dht.DHTPluginContact ""
com.aelitis.azureus.plugins.dht.DHTPluginKeyStats ""
com.aelitis.azureus.plugins.dht.DHTPluginListener ""
com.aelitis.azureus.plugins.dht.DHTPluginOperationListener ""
com.aelitis.azureus.plugins.dht.DHTPluginProgressListener ""
com.aelitis.azureus.plugins.dht.DHTPluginTransferHandler ""
com.aelitis.azureus.plugins.dht.DHTPluginValue ""
com.aelitis.azureus.plugins.dht.impl.DHTPluginContactImpl ""
com.aelitis.azureus.plugins.dht.impl.DHTPluginImpl "system.out.println( "frigged refresh period" ); props.put( dht.pr_cache_republish_interval, new integer( 5601000 )); start off optimistic with reachable = true udp timeout - tried less but a significant number of premature timeouts occurred reduce network usage system.out.println( "cvs dht cache republish interval modified" ); reachability has changed only try boostrapping off connected peers on the main network as it is unlikely any of them are running cvs and hence the boostrap will fail first look for peers to directly import log.log( "put: wrote " + _value.getstring() + " to " + _contact.getstring()); log.log( "put: complete, timeout = " + timeout ); log.log( "get: read " + value.getstring() + " from " + contact.getstring() + ", originator = " + value.getoriginator().getstring()); log.log( "get: wrote " + value.getstring() + " to " + contact.getstring()); log.log( "get: complete, timeout = " + _timeout ); log.log( "remove: read " + value.getstring() + " from " + contact.getstring()); log.log( "remove: wrote " + value.getstring() + " to " + contact.getstring()); log.log( "remove: complete, timeout = " + timeout ); log.log( "remove: read " + value.getstring() + " from " + contact.getstring()); log.log( "remove: wrote " + value.getstring() + " to " + contact.getstring()); log.log( "remove: complete, timeout = " + timeout ); direct read/write support"
com.aelitis.azureus.plugins.dht.impl.DHTPluginImplAdapter ""
com.aelitis.azureus.plugins.dht.impl.DHTPluginStorageManager "if ( cause == null ){ debug.out( description + ": div cause is null!" ); }else{ new aethread2("") { public void run() { dhttransportfullstats stats = cause.getstats(); if ( stats == null ){ debug.out( description + ": div stats is null!" ); }else{ debug.out( description + ": div stats: " + stats.getstring()); } } }.start(); } string indent = ""; for(int i=0;i" ); work around issue whereby puts to the cvs dht went out of control and diversified everything this is a good point to save diversifications - useful when they've expired as writing isn't triggered at expiry time remove any old crud if "next" is in the future then we live with it to try and ensure increasing values (system clock must have changed) key storage system.out.println( "dht key system.out.println( "dht key deleted" ); system.out.println( "dht value read" ); version system.out.println( network + ": dht value added: " + dhtlog.getstring2( ((storagekey)key).getkey().getbytes()) + " -> " + value.getstring()); system.out.println( "dht value updated" ); system.out.println( "dht value deleted" ); get diversifications for put operations must deterministically return the same end points but gets for gets should be randomised to load balance system.out.println( "dht get existing diversification: put = " + put_operation ); must always return a value - original if no diversification exists system.out.println( "dht create new diversification: desc=" + description + ", put=" + put_operation +", type=" + diversification_type ); for each entry, if there are no diversifications then we just return the value for those with divs we replace their entry with the diversified set (which can include the entry itself under some circumstances ) we've recursed on the key, this means that a prior diversification wanted the key included, so include it now replace this entry with the diversified keys system.out.println( indent + " unblock, 01 ->block never override a direct value with an indirect one as direct = first hand knowledge whereas indirect is hearsay don't let older instructions override newer ones only direct operations can "remove" blocks seeing as we've received this from someone there's no point in replicating it back to them later - mark them to prevent this put to all keys include original key system.out.println( "followdivs:put:freq adding original" ); put to a fixed subset. has to be fixed else over time we'll put to all the fragmented locations and nullify the point of this. gets are randomised to we don't loose out by fixing the puts include original key system.out.println( "followdivs:put:size adding original" ); get always returns a randomised selection diversification has lead to caching at all 'n' places diversification has fragmented across 'n' places select 2 to search or all if exhaustive system.out.println( "followdivs:get:size adding all" ); trigger timeouts here system.out.println( "read: " + dhtlog.getstring2( key.getbytes())); show at least some activity! just null it and drop this read, doesn't matter and means that we don't bother creating a filter for infrequently accessed data we want to hold enough ips to detect a hit rate of reads_per_minmin with a reasonable accuracy (sized to 10/3 to save space - this gives an average of 100 adds required to detect 90 unique) system.out.println( "value changed: entries = " + entries + "(" + entries_diff + "), size = " + size + "(" + size_diff + ")");"
com.aelitis.azureus.plugins.dht.impl.DHTPluginValueImpl ""
com.aelitis.azureus.plugins.extseed.ExternalSeedException ""
com.aelitis.azureus.plugins.extseed.ExternalSeedManualPeer ""
com.aelitis.azureus.plugins.extseed.ExternalSeedPeer "we can get synchronously disconnected - e.g. ip filter rules moved to the rate-limiting code for more accurate stats stats.received( request.getlength()); mindless snubbing control - if we have no outstanding requests then we drop the snubbed status :) odds are ..."
com.aelitis.azureus.plugins.extseed.ExternalSeedPlugin "bail out early if the state changed for this peer so one peer at a time gets a chance to activate we do this without holding the monitor as doing so causes potential deadlock between download_mon and the connection's connection_mon so ignore possible errors here that may be caused by concurrent modification to the download_map ans associated lists. we are only reading the data so errors will only be transient fix up newly added peers to current peer manager"
com.aelitis.azureus.plugins.extseed.ExternalSeedReader "transient peers are moved from the download on failure"
com.aelitis.azureus.plugins.extseed.ExternalSeedReaderFactory ""
com.aelitis.azureus.plugins.extseed.ExternalSeedReaderListener ""
com.aelitis.azureus.plugins.extseed.impl.ExternalSeedReaderImpl "rate handling default is avail based first respect failure count next obvious things like validity and the fact that we're complete check dnd completeness too now the more interesting stuff kick any existing peers that are running too slowly if the download appears to be stalled if we have a global limit in force and we are near it then no point in activating no point in booting ourselves! check to see if we have pending connections to the same address availability and speed based stuff needs a little time before being applied obvious stuff first more interesting stuff if the peer manager's changed then we always go inactive for a period to wait for download status to stabilise a bit overridden if needed get an advisory set to process together if this is the first request then process it, otherwise leave for the next-round we've only got the sem for the first request, catch up for subsequent permission to read a bunch of bytes we're out of step here due to multiple threads so we have to report what has already happened and prepare for what will one byte a sec to check for connection liveness"
com.aelitis.azureus.plugins.extseed.impl.ExternalSeedReaderRequest "no timeout"
com.aelitis.azureus.plugins.extseed.impl.getright.ExternalSeedReaderFactoryGetRight "resolve url-list according to specification (http://www.getright.com/seedtorrent.html) list urls = (list)map.get( "url-list" ); if ( urls == null ){ urls = new arraylist(); } urls.add( "http://127.0.0.1:888/files/%df%26%5b7w%c9%13i%88%8d%ec%e5b%2c9%0f%8d%0co%bc/" ); map.put( "url-list", urls); map params = new hashmap(); map.put( "url-list-params", params ); avoid java encoding ' ' as '+' as this is not conformant with apache (for example)"
com.aelitis.azureus.plugins.extseed.impl.getright.ExternalSeedReaderGetRight "encoding is a problem, assume iso-8859-1 force overall download order to be from end of file to start (for bt peers) force us linear from front of file to end system.out.println( "req: start=" + request_start + ", len=" + request_length ); we've got to multiplex the (possible) multiple request buffers onto (possible) multi files system.out.println( " sub_req: start=" + sub_request_start + ", len=" + sub_len + ",url=" + http_downloader.geturl()); the current buffer is full up to the declared length this request is complete. save any partial buffer for next request prepare for next buffer if needed"
com.aelitis.azureus.plugins.extseed.impl.webseed.ExternalSeedReaderFactoryWebSeed "might as well handle case where there's a single entry rather than a list avoid java encoding ' ' as '+' as this is not conformant with apache (for example)"
com.aelitis.azureus.plugins.extseed.impl.webseed.ExternalSeedReaderWebSeed "unfortunately using httpurlconnection it isn't possible to read the 503 response as per protocol - however, for az http web seeds we don't uses 503 anyway so we cna use urlcon. the main benefit here is we also get http proxying which we don't get with our direct socket support..."
com.aelitis.azureus.plugins.extseed.util.ExternalSeedHTTPDownloader ""
com.aelitis.azureus.plugins.extseed.util.ExternalSeedHTTPDownloaderLinear "system.out.println( "connecting to " + url + ": " + thread.currentthread().getid()); webseed support for temp unavail - read the retry_after should be one at least idle if no reqs assume out of space of something permanent, abandon system.out.println( "done to " + url + ": " + thread.currentthread().getid() + ", outcome=" + outcome ); on successful completion we kill the read thread but leave things 'running' so we continue to service requests. we will be de-activated when no longer required not yet initialised"
com.aelitis.azureus.plugins.extseed.util.ExternalSeedHTTPDownloaderListener ""
com.aelitis.azureus.plugins.extseed.util.ExternalSeedHTTPDownloaderRange "system.out.println( "connecting to " + url + ": " + thread.currentthread().getid()); allow for certs that contain ip addresses rather than dns names auto redirect doesn't work from http to https or vice-versa try again with original url certificate has been installed retry with new certificate don't need another ssl loop webseed support for temp unavail - read the retry_after system.out.println( "download length: " + pos ); system.out.println( "done to " + url + ": " + thread.currentthread().getid() + ", outcome=" + outcome ); if we want to support keep-alive we'll need to implement a socket cache etc. http/1.1 403 forbidden system.out.println( "download length: " + pos ); webseed support for temp unavail - read the data this gets trapped below and turned into an appropriate externalseedexception"
com.aelitis.azureus.plugins.magnet.MagnetPlugin "if ( torrent.isprivate()){ cb_data = getmessagetext( "private_torrent" ); }else if ( torrent.isdecentralised()){ relaxed this as we allow such torrents to be downloaded via magnet links (as opposed to tracked in the dht) }else if ( torrent.isdecentralisedbackupenabled()){ torrentattribute ta_peer_sources = plugin_interface.gettorrentmanager().getattribute( torrentattribute.ta_peer_sources ); string[] sources = download.getlistattribute( ta_peer_sources ); boolean ok = false; for (int i=0;i<sources.length;i++){ if ( sources[i].equalsignorecase( "dht")){ ok = true; break; } } if ( !ok ){ cb_data = getmessagetext( "decentral_disabled" ); } }else{ cb_data = getmessagetext( "decentral_backup_disabled" ); removed this as well - nothing wrong with allowing magnet copy for private torrents - they still can't be tracked if you don't have permission ok } system.out.println( "magnetplugin: export = " + url ); see if we've already got it! make sure ddb is initialised as we need it to register its transfer types single-thread per hash to avoid madness ensuing if we get multiple concurrent hits give live results a chance before kicking in explicit ones now inject any explicit sources try and place before first dead entry dead at end ignore, we just continue processing system.out.println( "rem=" + remaining + ",pot=" + potential_contacts.size() + ",out=" + outstanding[0] ); system.out.println( "magnetdownload: " + contact.getname() + ", live = " + live_contact ); let's verify the torrent nothing found"
com.aelitis.azureus.plugins.magnet.MagnetPluginException ""
com.aelitis.azureus.plugins.magnet.MagnetPluginListener ".min_value if not handled"
com.aelitis.azureus.plugins.magnet.MagnetPluginMDDownloader "system.out.println( "waiting..." );"
com.aelitis.azureus.plugins.magnet.MagnetPluginProgressListener ""
com.aelitis.azureus.plugins.magnet.swt.MagnetPluginUISWT ""
com.aelitis.azureus.plugins.net.buddy.BuddyPlugin "returns public static final int stream_crypto = messagemanager.stream_encryption_none; public static final int block_crypto = sesecuritymanager.block_encryption_none; enabled nickname online status if we add this then use proper message texts in the status_strings protocol speed chat notifications default published cats config end new buddypluginview( buddyplugin.this, swt_ui, view_id ); only toggle overall state on a real change pick up initial values before enabling try to re-establish connection to previously connectd buddies gotta clone else we're messing with stuff that ain't ours don't accept a second or subsequent connection for unauth buddies as they have a single chance to be processed no existing only execute the most recent publish ensure we have a sensible ip system.out.println( "deleted status from " + event.getcontact().getname()); buddy may be already present as un and un randomise a bit last check may be in the future as we defer checks for seemingly inactive buddies trim any non- string etag = connection.getrequestproperty( "if-none-match" ); first try and get torrent direct from the buddy second try and get via magnet update every now and then to pick up new peer/seed values ensure feed date goes up"
com.aelitis.azureus.plugins.net.buddy.BuddyPluginAdapter ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginAZ2 ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginAZ2ChatListener ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginAZ2Listener ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginAZ2TrackerListener ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginBuddy "default assume everyone now supports chat for inactive buddies we schedule their status checks so that on average we don't do more than one check every 5 minutes tell buddy of change tell buddy of change tell buddy of change not persisted currently - plugin.setconfigdirty(); if we're connected then we're online whatever logmessage( "msg " + message.getstring() + " ok" ); only try and reconcile this failure with the current message if the message has actually been sent logmessage( "msg " + message.getstring() + " retrying" ); logmessage( "msg " + message.getstring() + " added: num=" + size ); single-thread outgoing connect attempts can't perform connect op while synchronized as may deadlock on password aquisition logmessage( "con " + bc.getstring() + " added: num=" + connections.size() ); logmessage( "allocating msg " + allocated_message.getstring() + " to con " + bc.getstring()); dropped connection, kick in a keep alive delay a bit logmessage( "con " + bc.getstring() + " removed: num=" + size ); connection failed, see if we need to attempt to re-establish do we explicitly know that this sequence number denotes an offline buddy change in sequence means we're online if we are connected then we use the status sent over the connection as it is more up to date no active connections see if we should attempt a pre-emptive connect int size; size = connections.size(); logmessage( "con " + bc.getstring() + " added: num=" + size ); logmessage( "sending " + msg.getstring() + " to " + getstring()); logmessage( "received type=" + type + " from " + getstring()); don't record as active here as (1) we recorded as active above when receiving request (2) we may be replying to a 'closing' message and we don't want the reply to mark as online ignore unknown message types while in unauth state we only allow a few messages. max should be 1 for an 'accept request' but i feel generous debug.printstacktrace( e ); system.out.println( "received chunk " + chunk_num + " of " + num_chunks );"
com.aelitis.azureus.plugins.net.buddy.BuddyPluginBuddyMessage "only available for pending-success messages, so don't make public @return @throws buddypluginexception"
com.aelitis.azureus.plugins.net.buddy.BuddyPluginBuddyMessageHandler "no messages pending should never happen... inform listeners before deleting message as it gives them one last chance to do something with the message if they so desire prematurely reduce message count when informing listeners so they see the "correct" value we don't want to delete the message if failed due to password issue"
com.aelitis.azureus.plugins.net.buddy.BuddyPluginBuddyMessageListener "indicates that a message has been delivered to the buddy if message procesing complete, false if failed and require re-invocation of this listener periodically to attempt processing again"
com.aelitis.azureus.plugins.net.buddy.BuddyPluginBuddyReplyListener ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginBuddyRequestListener "request receieved for a buddy. if the buddy is not"
com.aelitis.azureus.plugins.net.buddy.BuddyPluginException ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginListener ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginPasswordException ""
com.aelitis.azureus.plugins.net.buddy.BuddyPluginTimeoutException ""
com.aelitis.azureus.plugins.net.buddy.swt.BuddyPluginView "menuitem mi = plugin.getplugininterface().getuimanager().getmenumanager().addmenuitem( status.getmenucontext(), "dweeble" ); mi.addlistener( new menuitemlistener() { public void selected( menuitem menu, object target ) { system.out.println( "whee" ); } });"
com.aelitis.azureus.plugins.net.buddy.swt.BuddyPluginViewChat "table text"
com.aelitis.azureus.plugins.net.buddy.swt.BuddyPluginViewInstance "control area table and log child1 sash child2 table fall through get public key disconnect message send message chat ping ygm encrypt decrypt sign verify cats cats - share cats - subscribe done with menus log area @see com.aelitis.azureus.plugins.net.buddy.buddypluginlistener#enabledstatechanged(boolean)"
com.aelitis.azureus.plugins.net.buddy.tracker.BuddyPluginTracker "maps to download object assumed if we already have a plugin reference, that the azureus core is available log( "isbuddy: " + peer_ip + " -> " + result + ",tested=" + tested ); not running mark as peer early so that we get optimistic disconnect if needed withdraw buddy marker if it turns out our earlier optimism was misplaced withdraw buddy marker if it turns out our earlier optimism was misplaced go over peers for active torrents to see if we've missed and. can really only happen with multi-homed lan setups where a new (and utilised) route is found after we start tracking only track private torrents if we have successfully received peers from tracker which means we have the torrent legitimately. as this rule is enforced by both ends of the tracking operation it means we will only track between peers that both have a legitimate copy of the torrent. assume we're going to fail so we avoid falling through here multiple times before actuallt failing again first check to see if completion state changed for any common downloads if not incremental then remove any downloads that no longer are in common full hashes on reply nothing interesting in reply for these both complete, nothing to do!"
com.aelitis.azureus.plugins.net.buddy.tracker.BuddyPluginTrackerListener ""
com.aelitis.azureus.plugins.net.netstatus.NetStatusPlugin ""
com.aelitis.azureus.plugins.net.netstatus.NetStatusProtocolTester ""
com.aelitis.azureus.plugins.net.netstatus.NetStatusProtocolTesterBT "regardless of the caller's desires, override with crypto if we're using it if we're a responder then we limit connections as we should only receive 1 (be generous, give them 3 in case we decide to reconnect) we use the piece at 'n' + 1 to indicate a close request by sending a have for it this helps us tidily close things"
com.aelitis.azureus.plugins.net.netstatus.NetStatusProtocolTesterListener ""
com.aelitis.azureus.plugins.net.netstatus.swt.NetStatusPluginTester "see if we're getting nowhere see if we've got far enough"
com.aelitis.azureus.plugins.net.netstatus.swt.NetStatusPluginView "control start cancel log area"
com.aelitis.azureus.plugins.removerules.DownloadRemoveRulesPlugin "10 to 1 needs to be lowercase we don't auto-remove non-persistent downloads as these are managed elsewhere (e.g. shares) auto remove low noise torrents if their data is missing completed only emergency instruction from tracker try to maintain an upper bound on seeds that isn't going to kill the tracker big is a relative term here and generally distinguishes between core updates and plugin updates we need to delay this because other actions may be being performed on the download (e.g. completion may trigger update install)"
com.aelitis.azureus.plugins.sharing.hoster.ShareHosterPlugin "file : sharehosterplugin.java  system.out.println( "sh: res -> ds: " + attribute.getname() + "/" + resource.getattribute( attribute )); the resource has gone! stop torrent if running to permit deletion ignore this as it might already be stopped"
com.aelitis.azureus.plugins.startstoprules.always.RunEverythingPlugin "remove any ignored ones execute an "initialize" on any waiting ones execute a "start" on any ready ones start downloads start seeds"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.DefaultRankCalculator "all of the first priority rules must match any of the first priority rules must match force torrent to be "actively seeding/downloading" for this many ms upon start of torrent. wait xx ms before really changing activity (dl or cding) state when state changes via speed change maximium ranking that a torrent can get using the spratio ranking type amount to shift over the rank of the seedonly ranking type, to make room in case the for loading config settings rank that complete starts at (and incomplete ends at + 1) maximimum ranking for time queue mode. 1 unit is a second ranks below this value are for torrents to be ignored (moved to bottom & queued) seeding rank value when download is marked as not queued seeding rank value when download is marked as s:p ratio met for fp seeding rank value when download is marked as p:1s ratio met seeding rank value when download is marked as # seeds met seeding rank value when download is marked as 0 peers and fp seeding rank value when download is marked as 0 peers seeding rank value when download is marked as share ratio met ranking system to use min # of peers needed before boosting the rank of downloads with no seeds min speed needed to count a incomplete download as being actively downloading min speed needed to count a complete download as being actively seeding public for tooltip to access it public for tooltip to access it public for tooltip to access it default initializer _rules _dl load config values into the static variables sort first by seedingrank descending, then by position ascending. retrieves whether the torrent is "actively" downloading : actively downloading retrieves whether the torrent is "actively" seeding : actively seeding assign seeding rank based on ranktype seeding rank value add to seedingrank based on rank type does the torrent match first priority criteria? state calculated fp state check seeders for various changes not triggered by listeners : something changed ready downloads are usually waiting for a seeding torrent to stop (the seeding torrent probably is within the "minimum seeding time" setting) the rules may go through several cycles before a ready torrent is processed  seeding rank (sr) limits and values billion 11.57 days  static config values ignore torrent if seed count is at least.. ignore even when first priority ignore first priority  class variables state-caches for sorting modified by a listener in startstoprulesdefaultplugin ignore torrent if seed count is at least.. ignore fp test fp. fp goes to top test completeness. complete go to bottom check rank. large to top test large/small swarm pref test share ratio test position in order to be active, - must be downloading (and thus incomplete) - must be above speed threshold, or started less than 30s ago activity based on dl average change start timer continue as old state until timer finishes no change, reset timer timed torrents don't use a speed threshold, since they are based on time! however, first priorities need to be checked for activity so that timed ones can start when fps are below threshold. ditto for 0 peers when bautostart0peers not active if we aren't seeding not active if we are autostarting 0 peers, and peer count == 0 change start timer continue as old state until timer finishes no change, reset timer make undownloaded sort to top so they can start first. make sure we capture fp being turned off when torrent does from complete to incomplete here we are seeding check ignore rules never apply ignore rules to first priority matches (we don't want leechers circumventing the 0.5 rule) 0 means unlimited if both bignore0peers and bfirstpriorityignore0peer are on, we won't know which one it is at this point. we have to use the normal sr_0peers in case it isn't fp if (bfirstpriorityignore0peer) { if (rules.bdebuglog) sexplainsr += " ignore 0 peers criteria for fp met\n";  dl.setseedingrank(sr_fp0peers); return sr_fp0peers; } if (numpeers != 0 && ifirstpriorityignorespratio != 0 && numseeds / numpeers >= ifirstpriorityignorespratio) { if (rules.bdebuglog) sexplainsr += " ignore rule for s:p ratio for fp met. current: (" + (numseeds / numpeers) + ") >= threshold(" + ifirstpriorityignorespratio + ")\n";  dl.setseedingrank(sr_fp_spratiomet); return sr_fp_spratiomet; } 0 means disabled ignore when p:s ratio met (more peers for each seed than specified in config) 0 means never stop never do anything with rank type of none everythink ok! force sort to top seedcount and spratio require scrape results.. shift over to make way for fallback iranktype == rank_spratio or we are falling back numseeds != 0 && numpeers != 0 every 10 minutes of not being active, subtract one sr recalcseedingrank fp only applies to completed fp doesn't apply when s:p >= set spratio (spratio = 0 means ignore) not fp if no peers //nolar, 2105 - gouss, 2203"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.SeedingRankColumnListener "file : seedingrankcolumnlistener.java a "my torrents" column for displaying seeding rank. (non-javadoc) @see org.gudy.azureus2.core3.config.coconfigurationlistener#configurationsaved() stext += "" + sr + " "; add a star if it's before mintimealive"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.StartStopRulesDefaultPlugin "file : startstoprulesdefaultplugin.java handles starting and stopping of torrents. todo: rank_timed is quite a hack and is spread all over. it needs to be redone, probably with a timer on each seeding torrent which triggers when time is up and it needs to stop. bug: when "autostart 0 peers" is on, and minspeedforactivelyseeding is enabled, the 0 peer torrents will continuously switch from seeding to queued, probably due to the connection attempt registering speed. this might be fixed by the "wait xx ms before switching active state" code. other notes: "cd" is often used to refer to "seed" or "seeding", because "c" sounds like "see" do not rank completed torrents rank completed torrents using seeds:peer ratio rank completed torrents using seed count method rank completed torrents using a timed rotation of mintimealive rank completed torrents using the peers count, weighted by the seeds to peers ratio force at least one check every period of time (in ms). used in changeflagcheckertask check for non triggerable changes ever period of time (in ms) interval in ms between checks to see if the {@link #somethingchanged} flag changed wait xx ms before starting completed torrents (so scrapes can come in) wait at least xx ms for first scrape, before starting completed torrents used only for rank_timed. recalculate ranks on a timer map to relate downloaddata to a download this is used to reduce the number of comperator invocations by keeping a mostly sorted copy around, must be nulled whenever the map is changed when rules class started. used for initial waiting logic whether debug info is written to the log and tooltip ranking system to use. one of rank_ constants maximimum # of stalled torrents that are in seeding mode not implemented  a simple timer task to recalculate all seeding ranks.  this class check if the somethingchanged flag and call process() when its set. this allows pooling of changes, thus cutting down on the number of sucessive process() calls. listen to download changes and recalc sr if needed update seedingrank when a new scrape result comes in. create/remove downloaddata object when download gets added/removed. recalcseedingrank & process if necessary. // limit _maxactive and maxdownloads based on thecolonel's specs // maxactive = max_upload_speed / (slots_per_torrent min_speed_per_peer) if (_maxactive > 0) { int islotspertorrent = plugin_config.getintparameter("max uploads"); // todo: track upload speed, storing the max upload speed over a minute // and use that for "unlimited" setting, or huge settings (like 200) if (islotspertorrent > 0) { int iminspeedperpeer = 3; // for now. todo: config value int _maxactivelimit = imaxuploadspeed / (islotspertorrent iminspeedperpeer); if (_maxactive > _maxactivelimit) { _maxactive = _maxactivelimit; plugin_config.setintparameter(pluginconfig.core_param_int_max_active, _maxactive); } } if (maxdownloads > _maxactive) { maxdownloads = _maxactive; plugin_config.setintparameter(pluginconfig.core_param_int_max_downloads, maxdownloads); } } indicate whether it's ok to start seeding.  seeding can start right away when there's no auto-ranking or we are on timed ranking. otherwise, we wait until one of the following happens:  any non-stopped/errored torrent gets a scrape result and it's after {@link #min_seeding_startup_wait} all scrape results come in for completed, non-stopped/errored torrent any completed non-stopped/errored torrent is fp any torrent has 0 seeds (which, in most cases means it's the highest rank)   if none of the above happen, then after {@link #min_first_scrape_wait}, the flag will turned on. default constructor list of download data (rank calculators) objects to base calculations on. running totals and stuff that gets used during a process run. split into class so complete/incomplete can be seperated into functions running count of torrents waiting or seeding, not including stalled store whether there's a torrent higher in the list that is queued we don't want to start a torrent lower in the list if there's a higherqueued tracks the position we should be at in the completed torrents list updates position. ignore ignore  ignore ignore ignore process completed (seeding) downloads, starting and stopping as needed all download data (rank objects) we handle current download data (rank object) we are processing running calculations summary values used in logic ignore ignore ignore ignore ignore get # of peers not including us get # of seeds, not including us, and including fake full copies download to get # of seeds for count get # of seeds, not including us, and including fake full copies download to get # of seeds for # peers we know of, required to calculate fake full copies count request that the startstop rules process. used when it's known that something has changed that will effect torrent's state/position/rank. for debugging core/plugin classes config settings count x peers as a full copy, but.. don't count x peers as a full copy if seeds below ui ui create a configmodel for startstoprules we always need to do this in order to set up configuration defaults todo: don't name it q we don't want to go off recalculating stuff when config is saved on closedown we have our own config model :) ignore rules subsection --------- for "stop peers ratio" ignore rule for "stop ratio" ignore rule auto starting --------- queue section --------- first priority subsection --------- for ignore fp rules seeding subsection check group #1: ones that always should run since they set things system.out.println("recalcallseedingranks"); force a sr recalc, so that it gets position properly next process() skip if error (which happens when listener is first added and the torrent isn't scraped yet) announces are useless to us. even if the announce contains seed/peer count, they are not stored in the downloadannounceresult. instead, they are passed off to the downloadscraperesult, and a scraperesult is triggered system.out.println("startstop: activation request: count = " + event.getactivationcount()); ok to be null quick and dirty check: keep connection if scrape peer count is 0 there's a (good?) chance we'll start in the next process cycle make sure process isn't running and stop it from running while we do stuff system.out.println(systemtime.getcurrenttime() - llastruntime); check dls for change in activeness (speed threshold) (the call sets somethingchanged it was changed) check seeders for change in activeness (speed threshold) (the call sets somethingchanged it was changed) configurationlistener @see org.gudy.azureus2.core3.util.aethread2#run() insanity :) shorten recalc for timed rank type, since the calculation is fast and we want to stop on the second limit _maxactive and maxdownloads based on thecolonel's specs maxactive = max_upload_speed / (slots_per_torrent min_speed_per_peer) todo: track upload speed, storing the max upload speed over a minute and use that for "unlimited" setting, or huge settings (like 200) for now. todo: config value force a recalc on all downloads by setting sr to 0, scheduling a recalc on next process, and requsting a process cycle xxx put in subtraction logic here danger here if we are in a position where allowing more to start when seeding allows a non-seeding download to start (looping occurs) not interesting, they can't potentially cause trouble look for incomplete files total forced seeding doesn't include stalled torrents not a total :) count the # of ok scrapes when !boktostartseeding, and flip to true if all scrapes for non-stopped/errored completes are okay. - build a seedingrank list for sorting - build count totals - do anything that doesn't need to be done in queued order no stats colllected on error or stopped torrents count forced seedings as using a slot don't count forced downloading as using a slot only used when !boktostartseeding.. set only to make compiler happy !completed if state if completionlevel for don't allow more "seeding/downloading" torrents than there is enough bandwidth for. there needs to be enough bandwidth for at least each torrent to get to its minspeedforactiveseeding (we buffer it at 2x just to be safe). even more buffering/limiting. limit to enough bandwidth for each torrent to have potentially 3kbps. allow upload speed can handle system.out.println("maxtorrents = " + maxtorrents + " = " + imaxuploadspeed + " / " + minspeedperactive); system.out.println("totaltorrents = " + (activeseedingcount + totalstalledseeders + totaldownloading)); constructor running count running count running count pull the data into a local array, so we don't have to lock/synchronize sort: seedingrank desc, position desc pre-included forced start torrents so a torrent "above" it doesn't start (since normally it would start and assume the torrent below it would stop) running count running count running count; loop 2 of 2: - start/stop torrents based on criteria initialize state_waiting torrents never do anything to stopped entries handle incomplete dls loop 2/2 (start/stopping) process() don't mess with preparing torrents. they could be in the middle of resume-data building, or file allocating. must use fresh getactivelydownloading() in case state changed to downloading stop torrent if over limit reduce counts adjust counts increase counts queuing process: 1) torrent is queued (stopped) 2) slot becomes available 3) queued torrent changes to waiting 4) waiting torrent changes to ready 5) ready torrent changes to seeding (with startdownload) 6) trigger stops seeding torrent a) queue ranking drops b) c) other 7) seeding torrent changes to queued. go to step 1. ignore rules and other auto-starting rules do not apply when bautostart0peers and peers == 0. so, handle starting 0 peers right at the beginning, and loop early set to waiting short circuit: if rank is set to ignored, we can skip everything except when: (1) torrent is force started (2) the torrent is in ready or seeding state (we have to stop the torrent) (3) we auto start 0 peers short circuit: if seed higher in the queue is marked to start, we can skip everything, except when: (1) torrent is force started (2) we auto start 0 peers (3) the torrent is in ready or seeding state (we have to stop the torrent) in debug mode, we already calculated fp is it ok to set this download to a queued state? it is if: 1) it is either ready or seeding; and 2) it is either one of the following; and a) not a first priority torrent; or b) there is a limit to the number of active torrents, and the number of waiting and seeding torrents is already higher (or equal to) the number of maximum allowed active torrents (taking away the number of minimum required downloads).  so i understand that to mean - it isn't first priority and leaving this torrent running would mean that there aren't enough slots to fulfil the minimum downloads requirement, because there are so many torrents seeding (or waiting to seed) already. or, in the case there is no minimum downloads requirement - it's just overrun the maximum active torrents count.  3) it hasn't been force started. && (!isfp || (isfp && ((vars.numwaitingorseeding >= totals.maxseeders) || (!bactivelyseeding && (vars.numwaitingorseeding + totals.totalstalledseeders) >= totals.maxseeders))) ) in rank_timed mode, we use mintimealive for rotation time, so skip check xxx do we want changes to take effect immediately ? short circuit. forced start torrents are pre-included in count note: first priority are sorted to the top, so they will always start first xxx change to waiting if queued and we have an open slot && (totals.maxactive == 0 || (activeseedingcount + activedlcount) < totals.maxactive) && set to waiting start download if ready and slot is available in between switching from state_waiting and state_ready, and ignore rule was met, so move it back to queued if there's more torrents waiting/seeding than our max, or if there's a higher ranked torrent queued, stop this one break up the logic into variables to make more readable not checking and (at limit of seeders or rank is set to ignore) and (actively seeding or startingup or seeding a non-active download) oktoqueue only allows ready and seeding state.. and in both cases we have to reduce counts force stop allows ready states in here, so adjust counts move completed timed rank types to bottom of the list move everyone up we always start by setting sr to sr_timed_queued_ends_at - position then, the torrent with the biggest starts seeding which is (sr_timed_queued_ends_at - 1), leaving a gap. when it's time to stop the torrent, move everyone up, and put us at the end if we've scraped after we started downloading remove ourselves from count fallback to the # of peers we know of if we've scraped after we started seeding remove ourselves from count fallback to the # of seeds we know of ignore class"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.StartStopRulesFPListener "this method should return true to force a download to be first priority. you can only use this listener to force downloads to be first priority - you can't force downloads not to be first priority - if you return false, then the other first priority settings and logic will be used to determine its status. listeners will not be called for all downloads - the following checks may prevent listeners being called: - non persistent downloads - stopped or error state - incomplete downloads this means that listeners don't have to do these basic checks. the stringbuffer argument is intended to output debug information about why the item is (or isn't) first priority. the item may be null if debugging is not enabled. it is not mandatory to log to the buffer."
com.aelitis.azureus.plugins.startstoprules.defaultplugin.ui.swt.ConfigSectionQueue "general queueing options create the "queue" tab in the configuration view main tab set up row row row change controllers for above items row row subrow row row row row"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.ui.swt.ConfigSectionSeeding "seeding automation specific options seeding automation setup general seeding options"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.ui.swt.ConfigSectionSeedingAutoStarting "auto starting specific options seeding automation setup begin rank type area rank type area. encompases the 4 (or more) options groups seeds:peer options seed count options timed rotation ranking type timed rotation ranking type no ranking end rank type area $non-nls-1$ $non-nls-1$"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.ui.swt.ConfigSectionSeedingFirstPriority "first priority specific options. seeding automation setup begin no touch area group fp row row row row group ignore fp ignore s:p ratio ignore 0 peers ignore idle hours row"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.ui.swt.ConfigSectionSeedingIgnore "config section for items that make us ignore torrents when seeding seeding automation setup $non-nls-1$ $non-nls-1$ $non-nls-1$ share ratio ignore 0 peers"
com.aelitis.azureus.plugins.startstoprules.defaultplugin.ui.swt.StartStopRulesDefaultPluginSWTUI ""
com.aelitis.azureus.plugins.tracker.dht.DHTTrackerPlugin "queued downloads are removed from the set to consider as we now have the "presence store" mechanism to ensure that there are a number of peers out there to provide torrent download if required. this has been done to avoid the large number of registrations that this is used by the dhtscraper plugin system.out.println( "netpos: " + loc_str ); string tr1_str = ""; for (int j=0;j num_want peers then don't bother with the main announce don't query if this download already has an active dht operation if we didn't kick off a get then we have to reschedule here as normally the get operation will do the rescheduling when it receives a result use "min" here as we're just deferring it metric between -100 and + 100. note that all -ve mean 'don't do it' they're just indicating different reasons int seeds = scrape.getseedcount(); parg - changed to just use leecher count rather than seeds+leechers don't let a put block an announce as we don't want to be waiting for this at start of day to get a torrent running increaseactive( dl ); already present, no point in updating decreaseactive( dl ); in case we get crap back (someone spamming the dht) just silently ignore scale min and max based on number of active torrents we don't want more than a few announces a minute only update next query time if none set yet or we appear to have set the existing one. if we don't do this then we'll overwrite any rescheduled announces when we are seeding ignore seeds remove ourselves try some limited direct injection only inject the scrape result if the torrent is decentralised. if we do this for "normal" torrents then it can have unwanted side-effects, such as stopping the torrent due to ignore rules if there are no downloaders in the dht - bthub backup, for example, isn't scrapable... hmm, ok, try being a bit more relaxed about this, inject the scrape if we have any peers. if the currently reported values are the same as the ones we previously injected then overwrite them note that we can't test the url to see if we originated the scrape values as this gets replaced when a normal scrape fails :( make sure that the injected scrape values are consistent with our currently connected peers unfortunately getting scrape results can acquire locks and there is a vague possibility of deadlock here, so pre-fetch the scrape results looks like we'll need the scrape below catch it next time round system.out.println( "presence query for " + f_ready_download.getname() + "-> diversified pre start" ); system.out.println( "presence query for " + ready_download.getname()); system.out.println( " presence query for " + f_ready_download.getname() + "->" + total + "/div = " + diversified ); once we're registered we don't need to process this download any more unless it goes active and then inactive again port 0, no connections included queued here for the mo to avoid lots of thrash for torrents that flip a lot force requery don't do anything if paused as we want things to just continue as they are (we would force an announce here otherwise) first remove any redundant entries now look to see if we have any new stuff system.out.println( "mixing in " + target.getdesc()); system.out.println( download.getname() + ": metric=" + metric + ", adding=" + num_to_add ); int distance = ((integer)entry[0]).intvalue(); these distances are in different dimensions - make up a combined distance weither the triangle is like /\ (true) or \/ (false) we have a / separator in the "cell" we're in the upper part we're in the lower part we have a \ separator in the "cell" we're in the upper part we're in the lower part"
com.aelitis.azureus.plugins.tracker.local.LocalTrackerPlugin "actually these parameters affect lan detection as a whole, not just the local tracker, so leave them enabled... enabled.addenabledonselection( lp1 ); enabled.addenabledonselection( subnets ); enabled.addenabledonselection( lp2 ); enabled.addenabledonselection( autoadd ); assume we have a core, since this is a plugin initial small delay to let things stabilise this code is here to deal with multiple interface machines that receive the result multiple times"
com.aelitis.azureus.plugins.tracker.peerauth.TrackerPeerAuthPlugin "plugin_interface.getdownloadmanager().addlistener( this ); simpletimer.addperiodicevent( "trackerpeerauthplugin:checker", timer_period, new timereventperformer() { private long tick_count = 0; public void perform( timerevent event ) { tick_count++; synchronized( dt_map ){ iterator it = dt_map.values().iterator(); while( it.hasnext()){ ((downloadtracker)it.next()).checkpeers( tick_count ); } } } }); todo: detect enabled state todo: get check period todo: need to drop the bad bloom here too. we rely on the ok bloom to filter false positives on the bad bloom todo: this should be logged once and checked earlier only happens on outbound connectas we don't retain the peer-id pending outbound connect shouldn't get here as we should check for ok first we just leave it here and pick up for checking periodically assume all outgoing connections are valid as we got them from the tracker"
com.aelitis.azureus.plugins.upnp.UPnPMapping ""
com.aelitis.azureus.plugins.upnp.UPnPMappingListener ""
com.aelitis.azureus.plugins.upnp.UPnPMappingManager "incoming data port zyxel routers currently seem to overwrite the tcp mapping for a given port with the udp one, leaving the tcp one non-operational. hack to try setting them in udp -> tcp order to hopefully leave the more important one working :) this is actually the udp tracker client mapping, very badly named params... note that the dht plugin registers its own mapping tracker server tcp tracker server udp doesn't support udp and tcp on same port number - patch up unfortunately some routers remember the stuffed ports and makes them unusable for either udp or tcp until a hard reset so we need to change both ports... system.out.println( "upnpmappingmanager: added '" + desc_resource + "'" + (tcp?"tcp":"udp") + "/" + port + ", enabled = " + enabled );"
com.aelitis.azureus.plugins.upnp.UPnPMappingManagerListener ""
com.aelitis.azureus.plugins.upnp.UPnPPlugin "useless stats try{ upnpwancommoninterfaceconfig config = (upnpwancommoninterfaceconfig)s.getspecificservice(); long[] speeds = config.getcommonlinkproperties(); if ( speeds[0] > 0 && speeds[1] > 0 ){ log.log( "device speed: down=" + plugin_interface.getutilities().getformatters().formatbytecounttokibetcpersec(speeds[0]/8) + ", up=" + plugin_interface.getutilities().getformatters().formatbytecounttokibetcpersec(speeds[1]/8)); } }catch( throwable e ){ log.log(e); } provided for use by other plugins. provided for use by other plugins. natpmp upnp auto-refresh mappings every minute when enabled. only try to refresh the mappings if this is the first bad nat message we've been given in the last 15 minutes - we don't want to endlessly retry performing the mappings startup() used to be called on initializationcomplete() moved to delayed task because rootdevicefound can take a lot of cpu cycle. let's hope nothing breaks only take note of this if enabled to do so already started up, must have been re-enabled problems here at end of day regarding devices that hang and cause az to hang around got ages before terminating if all exclude then we let others through discovered but never found - something went wrong with the device construction process found ok before, reset details in case now its screwed check this isn't a single device with multiple services for external use, e.g. webui"
com.aelitis.azureus.plugins.upnp.UPnPPluginService "remove one day - port name was changed as some routers use name uniqueness to manage ports, hence udp and tcp with same name failed check for change of port number and delete old value if so make sure we tie this to the mapping in case it was external to begin with we're going to grab it not found - try and establish it + add entry even if we fail so that we don't retry later mapping is disabled default - leave as is transient overrides default persistent overrides all others set effective persistency only time we take note of whether or not to release the mapping is at closedown get the name here for the deletion case so that the subsequent message makes sense (as the name is derived from the current mappings, so getting it after deleting it results in a name of ) true -> not defined by us"
com.aelitis.azureus.ui.common.RememberedDecisionsManager "remembers decisions (usually from message boxes) set a remembered value remember id value to store. -1 to remove system.out.println("getr " + id + " -> " + l); system.out.println("setr " + id + " -> " + value);"
com.aelitis.azureus.ui.common.table.impl.CoreTableColumn "file : coretablecolumn.java copyright (c) 2004, 2005, 2006 aelitis sas, all rights reserved this program is free software; you can redistribute it and/or modify it under the terms of the gnu general public license as published by the free software foundation; either version 2 of the license. this program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. see the gnu general public license for more details ( see the license file ). you should have received a copy of the gnu general public license along with this program; if not, write to the free software foundation, inc., 59 temple place, suite 330, boston, ma 02111-1307 usa aelitis, sas au capital de 46,603.30 euros, 8 allee lenotre, la grille royale, 78600 le mesnil le roi, france. this class provides constructors for setting most of the common column attributes and sets the column as a 'core' column. ok, this really shouldn't be here, we shoudl create a coretablecolumnswt that implements this listener logic, but i'm not sure exactly construct a new coretablecolumn type will be type_text, update interval will be interval_invalid_only  tablecell listeners (added, refresh, dispose, tooltip) are added based on whether the class is an instance of them. unique id for column see {@link #getalignment()} see {@link tablecolumn#setposition(int)} see {@link tablecolumn#setwidth(int)} see {@link tablemanager}_table construct a new coretablecolumn. alignment will be align_lead, type will be type_text, update interval will be interval_invalid_only  tablecell listeners (added, refresh, dispose, tooltip) are added based on whether the class is an instance of them. unique id for column see {@link tablecolumn#setposition(int)} see {@link tablecolumn#setwidth(int)} see {@link tablemanager}_table construct a new coretablecolumn. alignment will be align_lead, type will be type_text, position will be position_invisible, update interval will be interval_invalid_only  tablecell listeners (added, refresh, dispose, tooltip) are added based on whether the class is an instance of them. unique id for column see {@link tablecolumn#setwidth(int)} see {@link tablemanager}_table construct a new coretablecolumn. alignment will be align_lead, type will be type_text, position will be position_invisible, width will be 50, update interval will be interval_invalid_only  tablecell listeners (added, refresh, dispose, tooltip) are added based on whether the class is an instance of them. unique id for column see {@link tablemanager}_table"
com.aelitis.azureus.ui.common.table.impl.DataSourceCallBackUtil "push timer forward, unless we've pushed it forward for over x seconds process outside the synchronized block, otherwise we'll end up with deadlocks"
com.aelitis.azureus.ui.common.table.impl.TableColumnImpl "table column definition and modification routines. implements both the plugin api and the core api.  a column is defined in relation to a table. when one column is in multiple tables of different table ids, each table has it's own column instance internal name/id of the column key of the displayed title for the column. if null, uses default calc table the column belongs to if (bcolumnadded) { throw (new uiruntimeexception("can't set properties. column '" + sname + " already added")); } start of not plugin public api functions (non-javadoc) @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#postload() the bascending to set. the bascending. if (ipreferredwidth = 0) { setvisible(true); } if (position < 0) { setvisible(false); } format: key = [tableid].column.[columnname] value[] = { visible, width, position, autotooltip, sortorder } dont call setsordorder, since it will change llastsortvaluechange which we shouldn't do if we aren't the sorting column cleanup old config support "old style" language keys, which have a prefix of tableid + "view." also, "myseeders" is actually stored in "mytorrents".. the "all peers" view should just share the same peer columns, so reuse them. or try "peersview". try a generic one of "tablecolumn." + columnid another "old style" 99% sure this can be removed now, but why risk it.. system.out.println(skeyprefix + ";" + messagetext.getstring(skeyprefix)); @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#addcontextmenuitem(java.lang.string, int) hack.. should be using our own implementation.. system.out.println(getname() + "] setlastsortvaluechange via " + debug.getcompressedstacktrace()); if (live && !bsortvaluelive) { system.out.println("setting " + stableid + ": " + sname + " to live sort value"); } @see java.util.comparator#compare(t, t) put nulls and empty strings at bottom. @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#getminwidth() @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setminwidth(int) @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#getmaxwidth() @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setmaxwidth(int) @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setwidthlimits(int, int) @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#isvisible() @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setvisible(boolean) system.out.println("set " + stableid + "/" + sname + " to " + visible + " via " + debug.getcompressedstacktrace()); @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#ismaxwidthauto() @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setmaxwidthauto(boolean) @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#isminwidthauto() @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setminwidthauto(boolean) @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#getpreferredwidth() @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setpreferredwidthauto(boolean) @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#ispreferredwidthauto() dead dead @see org.gudy.azureus2.plugins.ui.tables.tablecolumn#setpreferredwidth(int) commented out because size hasn't changed! if (bcolumnadded && bvisible) { triggercolumnsizechange(); } guess forplugindatasourcetype based on tableid"
com.aelitis.azureus.ui.common.table.impl.TableColumnInfoImpl "@see org.gudy.azureus2.ui.swt.views.table.utils.tablecolumninfo#getcategories() @see org.gudy.azureus2.ui.swt.views.table.utils.tablecolumninfo#setcategories(java.lang.string[]) @see org.gudy.azureus2.ui.swt.views.table.utils.tablecolumninfo#getproficiency() @see org.gudy.azureus2.ui.swt.views.table.utils.tablecolumninfo#setproficiency(int)"
com.aelitis.azureus.ui.common.table.impl.TableColumnManager "file : tablecolumnmanager.java copyright (c) 2004, 2005, 2006 aelitis sas, all rights reserved this program is free software; you can redistribute it and/or modify it under the terms of the gnu general public license as published by the free software foundation; either version 2 of the license. this program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. see the gnu general public license for more details ( see the license file ). you should have received a copy of the gnu general public license along with this program; if not, write to the free software foundation, inc., 59 temple place, suite 330, boston, ma 02111-1307 usa aelitis, sas au capital de 46,603.30 euros, 8 allee lenotre, la grille royale, 78600 le mesnil le roi, france. holds a list of column definitions (tablecolumncore) for all the tables in azureus. column definitions are added via plugininterface.addcolumn(tablecolumn) see use javadoc section for more uses. holds all the tablecolumncore objects. key = table_ type (see tablecolumncore) value = map: key = column name value = tablecolumncore object holds the order in which the columns are auto-hidden key = table_ type value = list of tablecolumn, indexed in the order they should be removed key = table; value = map of columns do not access directly. use {@link #gettableconfigmap(string)} or {@link #savetableconfigs()} key = tableid; value = table column ids retrieve the static tablecolumnmanager instance static tablecolumnmanager instance adds a column definition to the list the column definition object adds a column definition to the list the column definition object retrieves tablecolumncore objects of a particular type. table_ constant. see {@link tablecolumn} for list of constants of column definition objects matching the supplied criteria. key = name value = tablecolumncore object private map getalltablecolumncore( class fordatasourcetype, string tableid) { string[] dstcolumnids = new string[0]; if (fordatasourcetype != null) { list listdst = (list) mapdatasourcetypetocolumnids.get(fordatasourcetype); if (listdst != null) { dstcolumnids = (string[]) listdst.toarray(new string[0]); } if (fordatasourcetype.equals(downloadtypecomplete.class) || fordatasourcetype.equals(downloadtypeincomplete.class)) { listdst = (list) mapdatasourcetypetocolumnids.get(download.class); if (listdst != null && listdst.size() > 0) { string[] ids1 = (string[]) listdst.toarray(new string[0]); string[] ids2 = dstcolumnids; dstcolumnids = new string[ids2.length + ids1.length]; system.arraycopy(ids2, 0, dstcolumnids, 0, ids2.length); system.arraycopy(ids1, 0, dstcolumnids, ids2.length, ids1.length); } } else if (fordatasourcetype.equals(download.class)) { listdst = (list) mapdatasourcetypetocolumnids.get(downloadtypecomplete.class); if (listdst != null && listdst.size() > 0) { string[] ids = (string[]) listdst.toarray(new string[listdst.size()]); dstcolumnids = appendlists(ids, dstcolumnids); } listdst = (list) mapdatasourcetypetocolumnids.get(downloadtypeincomplete.class); if (listdst != null && listdst.size() > 0) { string[] ids = (string[]) listdst.toarray(new string[listdst.size()]); dstcolumnids = appendlists(ids, dstcolumnids); } } } try { items_mon.enter(); map mtypes = (map) items.get(tableid); if (mtypes == null) { mtypes = new linkedhashmap(); items.put(tableid, mtypes); } for (int i = 0; i < dstcolumnids.length; i++) { string columnid = dstcolumnids[i]; if (!mtypes.containskey(columnid)) { try { tablecolumncreationlistener l = mapcolumnidstolistener.get(fordatasourcetype + "." + columnid); tablecolumncore tc = null; if (l instanceof tablecolumncorecreationlistener) { tc = ((tablecolumncorecreationlistener) l).createtablecolumncore( tableid, columnid); } if (tc == null) { tc = new tablecolumnimpl(tableid, columnid); } l.tablecolumn will create columns for tableid if needed helper for getalltablecolumncore saves all the   linkedhashmap to preserve order system.out.println("gettablecolumnsasmap(" + stableid + ")"); system.out.println("gettablecolumnsasmap(" + stableid + ") returnsize: " + mreturn.size()); dispose of tableconfigs after xxs.. saves up to 50k"
com.aelitis.azureus.ui.common.table.impl.TableRowCoreSorter ""
com.aelitis.azureus.ui.common.table.impl.TableViewImpl "helpful output when trying to debug add/removal of rows tableid (from {@link org.gudy.azureus2.plugins.ui.tables.tablemanager}) of the table this class is handling. config settings are stored with the prefix of "table.tableid" prefix for retrieving text from the properties file (messagetext) typically tableid + "view" sorting functions timestamp of when last sorted all the rows was filtered rows in the table link datasource to their row in the table. key = datasource value = tablerowswt queue added datasources and add them on refresh queue removed datasources and add them on refresh basic (pre-defined) column definitions all column definitions. the array is not necessarily in column order we need to remember the order of the columns at the time we added them in case the up to date list of selected rows, so we can access rows without being on swt thread. guaranteed to have no nulls    for each row source that the @note bincludequeue can return an invalid number, such as a negative :( remove the specified datasource from the table. data sources to be removed remove immediately, or queue and remove at next refresh (non-javadoc) @see com.aelitis.azureus.ui.common.table.tableview#gettablecolumn(java.lang.string) various selected rows functions  returns an array of all selected data sources. null data sources are ommitted. array containing the selected data sources @todo tuxpaper: virtual row not returns an array of all selected data sources. null data sources are ommitted. array containing the selected data sources @see com.aelitis.azureus.ui.common.table.tableview#getselectedrows() returns an list of all selected tablerowswt objects. null data sources are ommitted. list containing the selected tablerowswt objects returns the first selected data sources. first selected data source, or null if no data source is selected invalidate and refresh whole table system.out.print(newselectionarray.length + " selected rows: "); for (tablerowcore row : newselectionarray) { system.out.print(indexof(row)); system.out.print(", "); } system.out.println(" via " + debug.getcompressedstacktrace(4)); / shorter name for configmanager, easier to read code what type of data is stored in this table list of datasourcechangedlistener note the use of identityhashmap - we have to do this to behave reliably in the face of some datasourcetypes (downloadmanagerimpl to mention no names) redefining equals/hashcode if you quickly remove+add a download with the same hash this can cause borkage here unless we use identity maps class used to keep filter stuff in a nice readable parcel xxx adding columns only has to be done once per tableid. doing it more than once won't harm anything, but it's a waste. fixup order @see com.aelitis.azureus.ui.common.table.tableview#addtabledatasourcechangedlistener(com.aelitis.azureus.ui.common.table.tabledatasourcechangedlistener, boolean) @see com.aelitis.azureus.ui.common.table.tableview#removetabledatasourcechangedlistener(com.aelitis.azureus.ui.common.table.tabledatasourcechangedlistener) @see com.aelitis.azureus.ui.common.table.tableview#setparentdatasource(java.lang.object) @see com.aelitis.azureus.ui.common.table.tableview#addlifecyclelistener(com.aelitis.azureus.ui.common.table.tablelifecyclelistener) @see com.aelitis.azureus.ui.common.table.tableview#addrefreshlistener(com.aelitis.azureus.ui.common.table.tablerefreshlistener, boolean) @see com.aelitis.azureus.ui.common.table.tableview#addcountchangelistener(com.aelitis.azureus.ui.common.table.tablecountchangelistener) put to array instead of synchronised iterator, so that runner can remove see common.tableview put to array instead of synchronised iterator, so that runner can remove add back the ones removedatasources removed in order to save time, we cache entries to be added and process them in a refresh cycle. this is a huge benefit to tables that have many rows being added and removed in rapid succession we're adding, override any pending removal added twice.. ensure it's not in the remove list see common.tableview in order to save time, we cache entries to be added and process them in a refresh cycle. this is a huge benefit to tables that have many rows being added and removed in rapid succession may be pending removal, override @see com.aelitis.azureus.ui.common.table.tableview#datasourceexists(java.lang.object) @see com.aelitis.azureus.ui.common.table.tableview#getrows() @see com.aelitis.azureus.ui.common.table.tableview#getrow(java.lang.object) @see com.aelitis.azureus.ui.common.table.tableview#getrow(int) don't use sortedrows here, it's not always up to date @see com.aelitis.azureus.ui.common.table.tableview#getdatasources() @see com.aelitis.azureus.ui.common.table.tableview#removedatasource(java.lang.object) override any pending addition override any pending addition when processdatasourcequeuecallback is null, we are disposing /////////////////////////////////////////////////////////////////////////////// if (celleditnotifier != null) { /////////////////////////////////////////////////////////////////////////////// celleditnotifier.sourceschanged(); /////////////////////////////////////////////////////////////////////////////// } note: we assume filtercheck has already run, and the list of datasources all passed the filter create row, and add to map immediately long lstarttime = systemtime.getcurrenttime(); use sortedrows position instead of item.getindex(), because getindex may have a wrong value (unless we fillrowgaps() which is more time consuming and we do afterwards anyway) finally, delete the rows system.out.println(">>> sort.. "); @see com.aelitis.azureus.ui.common.table.tableview#getcolumncells(java.lang.string) add to sortedrows list in best position. we need to be in the swt thread because the rowsorter may end up calling swt objects. if ((row == null) || row.isrowdisposed() || sortedrows.indexof(row) >= 0) { if we are >= to the last item, then just add it to the end instead of relying on binarysearch, which may return an item in the middle that also is equal. best guess xxx don't set table item here, it will mess up selected rows handling (which is handled in fillrowgaps called later on) row.settableitem(index); row.seticonsize(pticonsize); for datasources note: if the listener tries to do something like setselected, it will fail because we aren't done adding. we should trigger after fillrowgaps() @see com.aelitis.azureus.ui.common.table.tablestructuremodificationlistener#cellinvalidate(com.aelitis.azureus.ui.common.table.tablecolumncore, java.lang.object) see common.tableview itablestructuremodificationlistener tableview we are being called from a plugin (probably), so we must refresh @see com.aelitis.azureus.ui.common.table.tableview#getpropertiesprefix() @see com.aelitis.azureus.ui.common.table.tableview#gettableid() @see com.aelitis.azureus.ui.common.table.tableview#getdatasourcetype() initializetablecolumns() some implementers re-add datasource on initialized trigger. if they do, we don't have to re-add the unfiltlered (if we do, it could case dups if the new datasources have different derps) @see com.aelitis.azureus.ui.common.table.tableview#getvisiblecolumns() see common.tableview see common.tableview @see com.aelitis.azureus.ui.common.table.tableview#getselectedrowssize() @see com.aelitis.azureus.ui.common.table.tableview#getfocusedrow() @see com.aelitis.azureus.ui.common.table.tableview#getfirstselecteddatasource() /// @see com.aelitis.azureus.ui.common.table.tableview#getheadervisible() @see com.aelitis.azureus.ui.common.table.tableview#setheadervisible(boolean) @see org.gudy.azureus2.ui.swt.views.tableviewswt#getsortcolumn() we'll remove items still selected from oldselectionleft, leaving it with a list of items that need to fire the deselection event. @see com.aelitis.azureus.ui.common.table.tableview#setselectedrows(com.aelitis.azureus.ui.common.table.tablerowcore[])"
com.aelitis.azureus.ui.common.table.TableCellCore "file : tablecellcore.java core table cell functions are those available to plugins plus some core-only functions. the core-only functions are listed here. @see org.gudy.azureus2.ui.swt.views.table.impl.tablecellimpl refresh the cell whether to update graphic cells refresh the cell, including graphic types refresh the cell. this method overide takes a browvisible paramater and a bcellvisible parameter in order to reduce the number of calls to tablerow.isvisible() and calculations of cell visibility. whether to update graphic cells assumed visibility state of row assumed visibility state of the cell refresh the cell. this method override takes a browvisible parameter in order to reduce the number of calls to tablerow.isvisible() in cases where multiple cells on the same row are being refreshed. whether to update graphic cells visibility state of row dispose of the cell retrieve whether the cell need any paint calls (graphic) the cell needs painting location of the cell has changed retrieve the row that this cell belongs to row that this cell belongs to trigger all the tooltip listeners that have been added to this cell {@link #tooltiplistener_hover}, {@link #tooltiplistener_hovercomplete} trigger all the mouse listeners that have been addded to this cell event to trigger trigger all the visibility listeners that have been added to this cell. see {@link tablecellvisibilitylistener}.visibility_ constants sets whether the cell will need updating when it's visible again returns whether the cell will need updating when it's visible again @return return the text used when generating diagnostics @return get the cursor id we are currently using xxx should not be swt.cursor_ constants! @return set the cursor id that should be used for the cell _hand  returns whether the cell has visually changed since the last refresh call. could be used to prevent a refresh, or refresh early. changed since refresh state   sets tooltip to be shown in absence of an explicit one"
com.aelitis.azureus.ui.common.table.TableColumnCore "core table column functions are those available to plugins plus some core-only functions. the core-only functions are listed here. @see tablecolumnmanager set the internal flag specifying whether the column has been added to the tablecolumnmanager. some functions can not be run after a column has been added. retrieve whether the column has been added to the tablecolumnmanager - column has been added false - column has not been added changes what {@link tablecellcore.getdatasource()} and {@link tablerowcore.getdatasource()} return. true - returns a core object false - returns a plugin object (if available) retrieve whether a core or plugin object is sent via getdatasource() - returns a core object false - returns a plugin object (if available) send a refresh trigger to all listeners stored in tablecolumn the cell is being refreshed @throws throwable retrieve all the refresh listeners for the cell refresh listeners send a celladded trigger to all listeners stored in tablecolumn the cell is being added retreive all the cell added listeners added listeners for this cell send a dispose trigger to all listeners stored in tablecolumn the cell is being disposed send a tool tip event to the tool tip listeners cell to get the tool tip event send a mouse event to the cell mouse listeners mouse event to send send a visibility event to the cell's visibility listeners visibility state sets the position of the column without adjusting the other columns. this will cause duplicate columns, and is only usefull if you are adjusting the positions of multiple columns at once. new position (0 based) @see tablecolumnmanager.ensureintegrity() load width and position settings from config. map to place settings into save width and position settings to config. map to place settings into returns the key in the properties bundle that has the title of the column. 's language key # of consecutive errors # of consecutive errors  @return @return @return   @return    @return   @return override this function to obtain edited values that is being edited the new value true if the  @return @return @return @return"
com.aelitis.azureus.ui.common.table.TableColumnCoreCreationListener ""
com.aelitis.azureus.ui.common.table.TableColumnSortObject "if cell.setsortvalue is set to an object of this type, the column will not be set to "live". live columns get invalidated every single refresh. if you use this object, it's wise to {@link tablecolumncore#setlastsortvaluechange(long)} when you know you've changed the sort value."
com.aelitis.azureus.ui.common.table.TableCountChangeAdapter "@see com.aelitis.azureus.ui.common.table.tablecountchangelistener#rowadded(com.aelitis.azureus.ui.swt.views.list.listrow) @see com.aelitis.azureus.ui.common.table.tablecountchangelistener#rowremoved(com.aelitis.azureus.ui.swt.views.list.listrow)"
com.aelitis.azureus.ui.common.table.TableCountChangeListener ""
com.aelitis.azureus.ui.common.table.TableDataSourceChangedListener ""
com.aelitis.azureus.ui.common.table.TableGroupRowRunner "used with {@link tableview#runforselectedrows} code to run tablerowcore to run code against code to run against multiple rows. return true if this object supports it, false otherwise. @return"
com.aelitis.azureus.ui.common.table.TableGroupRowVisibilityRunner "code to run tablerowcore to run code against"
com.aelitis.azureus.ui.common.table.TableLifeCycleListener ""
com.aelitis.azureus.ui.common.table.TableRefreshListener ""
com.aelitis.azureus.ui.common.table.TableRowCore "core table row functions are those available to plugins plus some core-only functions. the core-only functions are listed here. invalidates row delete the row refresh all the cells in the row refresh graphic cells to location of a column has changed cells starting at this value may need repainting xxx rename to celllocationchanged? retrieve the data source related to this row true - return a core object false - return a plugin object data source object related to the row adjust cell height. don't use if any other column/cell uses setimage() new row height. will not reduce row's height (swt) level retrieve a cell based on the supplied value column name of the cell to be returned object related to this row and the specified column retreive whether the row is visible to the link the row to a swt tableitem new position row should be - already linked to that item at that index  like refresh, except a different name to confuse us. @return  @return  @return"
com.aelitis.azureus.ui.common.table.TableSelectionAdapter "@see com.aelitis.azureus.ui.common.table.tableselectionlistener#defaultselected(com.aelitis.azureus.ui.common.table.tablerowcore[]) @see com.aelitis.azureus.ui.common.table.tableselectionlistener#deselected(com.aelitis.azureus.ui.common.table.tablerowcore) @see com.aelitis.azureus.ui.common.table.tableselectionlistener#focuschanged(com.aelitis.azureus.ui.common.table.tablerowcore) @see com.aelitis.azureus.ui.common.table.tableselectionlistener#selected(com.aelitis.azureus.ui.common.table.tablerowcore) @see com.aelitis.azureus.ui.common.table.tableselectionlistener#mouseenter(com.aelitis.azureus.ui.common.table.tablerowcore) @see com.aelitis.azureus.ui.common.table.tableselectionlistener#mouseexit(com.aelitis.azureus.ui.common.table.tablerowcore)"
com.aelitis.azureus.ui.common.table.TableSelectionListener ""
com.aelitis.azureus.ui.common.table.TableStructureEventDispatcher "file : tablestructureeventdispatcher.java"
com.aelitis.azureus.ui.common.table.TableStructureModificationListener "file : itablestructuremodificationlistener.java"
com.aelitis.azureus.ui.common.table.TableView "adds a datasource to the table as a new row. if the data source is already added, a new row will not be added. this function runs asynchronously, so the rows creation is not guaranteed directly after calling this function. you can't add datasources until the table is initialized data source to add to the table add a list of datasources to the table. the array passed in may be modified, so make sure you don't need it afterwards. you can't add datasources until the table is initialized  the data set that this table represents has been changed. this is not for listening on changes to data sources changing within the table send selected rows to the clipboard in a spreadsheet friendly format (tab/cr delimited) invalidate all the cells in a column name of column to invalidate  retrieve a list of tablecells, in the last sorted order. the order will not be of the supplied cell's sort unless the table has been sorted by that column previously.  ie. you can sort on the 5th column, and retrieve the cells for the 3rd column, but they will be in order of the 5th columns sort. which column cell's to return. this does not sort the array on the column. of cells sorted @note may not necessarily return datasourcetype if table has subrows @return get the row associated with a datasource a reference to a core datasource object (not a plugin datasource object) row, or null get all the rows for this table, in the order they are displayed list of tablerowswt objects in the order the returns an array of all selected data sources. null data sources are ommitted. array containing the selected data sources @note may not necessarily return datasourcetype if table has subrows returns an array of all selected data sources. null data sources are ommitted. array containing the selected data sources returns an array of all selected tablerowswt. null data sources are ommitted. array containing the selected data sources @return @return process the queue of datasources to be added and removed  remove all the data sources (table rows) from the table.   for every row source, run the code provided by the specified parameter. code to run for each row/datasource for every row source, run the code provided by the specified parameter. code to run for each row/datasource  does not fire off selection events    @return @return @return @return @return @return  @return @return @return @return retrieves the row that has the cursor over it if mouse isn't over a row @return @return @return @return @return  @return"
com.aelitis.azureus.ui.common.table.TableViewFilterCheck ""
com.aelitis.azureus.ui.common.ToolBarEnabler "change with caution: some internal plugins use this directly"
com.aelitis.azureus.ui.common.ToolBarItem ""
com.aelitis.azureus.ui.common.updater.UIUpdatable "{@link uiupdater} update your ui! a name for this uiupdatable so we can track who's being bad name"
com.aelitis.azureus.ui.common.updater.UIUpdatableAlways "sometimes we don't update the ui (such as when nothing is visible). this class overrides this ability."
com.aelitis.azureus.ui.common.updater.UIUpdater ""
com.aelitis.azureus.ui.common.viewtitleinfo.ViewTitleInfo "title_ , or null if you don't want to set it -> long: 0 - not supported; 1 - active; 2 - inactive"
com.aelitis.azureus.ui.common.viewtitleinfo.ViewTitleInfo2 ""
com.aelitis.azureus.ui.common.viewtitleinfo.ViewTitleInfoListener ""
com.aelitis.azureus.ui.common.viewtitleinfo.ViewTitleInfoManager ""
com.aelitis.azureus.ui.console.MakeTorrent ""
com.aelitis.azureus.ui.InitializerListener ""
com.aelitis.azureus.ui.IUIIntializer "add a listener that gets triggered on progress changes (tasks, percent) remove listener that gets triggered on progress changes (tasks, percent)    code that will be executed in the swt thread before {@link #run()} is invoked"
com.aelitis.azureus.ui.Main "this is the main of all mains! for now, fire off the old org.gudy.azureus2.ui.swt.main class  in the future, this class may do some logic to find out available startup classes and pick one (like the uis one does) todo auto-generated catch block"
com.aelitis.azureus.ui.mdi.MdiChildCloseListener ""
com.aelitis.azureus.ui.mdi.MdiCloseListener "triggered when a {@link mdientry} is closed {@link mdientry} that is closed true if the"
com.aelitis.azureus.ui.mdi.MdiEntry "@deprecated for azburn"
com.aelitis.azureus.ui.mdi.MdiEntryCreationListener ""
com.aelitis.azureus.ui.mdi.MdiEntryDropListener "if you handled it, false if you didn't"
com.aelitis.azureus.ui.mdi.MdiEntryLoadedListener ""
com.aelitis.azureus.ui.mdi.MdiEntryLogIdListener ""
com.aelitis.azureus.ui.mdi.MdiEntryOpenListener ""
com.aelitis.azureus.ui.mdi.MdiEntryVitalityImage "should really be id"
com.aelitis.azureus.ui.mdi.MdiEntryVitalityImageListener ""
com.aelitis.azureus.ui.mdi.MdiListener ""
com.aelitis.azureus.ui.mdi.MultipleDocumentInterface ""
com.aelitis.azureus.ui.selectedcontent.DownloadUrlInfo "@return the requestproperties to set the additionalproperties to set additionalproperties add more fields here -> amend sameas below"
com.aelitis.azureus.ui.selectedcontent.DownloadUrlInfoContentNetwork ""
com.aelitis.azureus.ui.selectedcontent.ISelectedContent ""
com.aelitis.azureus.ui.selectedcontent.ISelectedVuzeFileContent ""
com.aelitis.azureus.ui.selectedcontent.SelectedContent "represents a piece of content (torrent) that is selected 2 @throws exception  add more fields and you need to amend sameas below @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#gethash() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#sethash(java.lang.string) @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#getdm() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#setdm(org.gudy.azureus2.core3.download.downloadmanager) @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#getdisplayname() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#setdisplayname(java.lang.string) @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#getdownloadinfo() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#setdownloadinfo(com.aelitis.azureus.ui.selectedcontent.selectedcontentdownloadinfo)"
com.aelitis.azureus.ui.selectedcontent.SelectedContentListener ""
com.aelitis.azureus.ui.selectedcontent.SelectedContentManager "manages the currently selected content in the visible display system.out.println("change cursel for '" + viewid + "' to " + currentlyselectedcontent.length + ";" + (currentlyselectedcontent.length > 0 ? currentlyselectedcontent[0] : "") + debug.getcompressedstacktrace()); always trigger selected content listeners since toolbar relies it them to reset the toolbaritems if something that didn't use selectedcontentmanager modified the toolbaritems states don't allow clearing if someone else set the currently selected system.out.println("-->abort because it's not " + selectedcontentmanager.viewid);"
com.aelitis.azureus.ui.selectedcontent.SelectedContentV3 "the imagebytes to set imagebytes if you add more fields here be sure to amend 'sameas' logic below @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#getdisplayname() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#getdm() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#gethash() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#setdisplayname(java.lang.string) @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#setdm(org.gudy.azureus2.core3.download.downloadmanager) @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#sethash(java.lang.string) @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#getdownloadinfo() @see com.aelitis.azureus.ui.selectedcontent.iselectedcontent#setdownloadinfo(com.aelitis.azureus.ui.selectedcontent.selectedcontentdownloadinfo)"
com.aelitis.azureus.ui.skin.SkinConstants ""
com.aelitis.azureus.ui.skin.SkinProperties "interface for reading skin properties (might be better) retrieve all the properties the properties add a property key/value pair to the list name of property value of property retrieve a property's int value name of property default value if property not found retrieve a string value name of property string value, or null if not found  @return @return todo  @return properties getproperties();"
com.aelitis.azureus.ui.skin.SkinPropertiesImpl "implementation of skinproperties using a integratedresourcebundle loaded from hard coded paths.  three level lookup of keys: (plugin) skin property file defaults property file azureus messagetext class  additionally, checks each for platform specific keys.  values containing "{}" are replaced with a lookup of get this if skin.include not defined, which is entirely possible public properties getproperties() { return properties; } can't use containskey on integratedresourcebundle :( ignore error.. it might be valid to store a non-numeric.. e.printstacktrace(); hex color string e.printstacktrace(); @see com.aelitis.azureus.ui.skin.skinproperties#getbooleanvalue(java.lang.string, boolean)"
com.aelitis.azureus.ui.swt.browser.BrowserContext "manages the context for a single swt {@link browser} component, including listeners and messages. creates a context and registers the given browser. unique identifier of this context the browser to be registered the browser might have been disposed already by the time this method is called the browser might have been disposed already by the time this method is called the browser might have been disposed already by the time this method is called  accesses the context associated with the given browser. holds the context in its application data map browser's context or null if there is none system.out.println( "registered browser context: id=" + getid()); int pct = event.total == 0 ? 0 : 100 event.current / event.total; system.out.println(pct + "%/" + event.current + "/" + event.total); todo auto-generated method stub parg 2012/10/2 increase delay in case causing crashes event.top is only filled on changed event (not changing!) we don't get a changed state on non urls (mailto, javascript, etc) system.out.println("cd" + event.location); event.top is always false. changed event has it set though.. utils.openmessagebox(utils.findanyshell(), swt.ok, "location changing", "navigating to " + event_location ); we don't get a changed state on non urls (mailto, javascript, etc) backup in case changed(..) is never called try to catch .torrent files urls ending in "?torrent" on amazon s3's simple storage service return an auto-generated a torrent based on the url, but only on get. head will fail, so we have to trap and assume if it's not obviously a web page system.out.println( "test for t/v: " + event_location + " -> " + istorrent + "/" + isvuzefile ); enable right-click context menu only if system property is set @see org.eclipse.swt.widgets.listener#handleevent(org.eclipse.swt.widgets.event) see what the content type is we're only trying to get the content type so just use head there's a bug in the ":3" server where a head followed by a get on the same content results in a corrupt get reply system.out.println("spl: " + b + ";" + url + ";" + debug.getcompressedstacktrace()); we may get multiple "load done"s (from each frame) which we don't want to skip system.out.println( "unregistered browser context: id=" + getid()); swallow errors silently"
com.aelitis.azureus.ui.swt.browser.BrowserWrapper "system.out.println( "execute: " + str );"
com.aelitis.azureus.ui.swt.browser.ClientMessageContextSWT "attaches this context and its message dispatcher to the browser. the browser to be attached widget to be shown when browser is loading detaches everything from this context's browser. deregisters the browser before it's disposed. used to verify it's the correct context @see org.eclipse.swt.events.disposelistener#widgetdisposed(org.eclipse.swt.events.disposeevent)"
com.aelitis.azureus.ui.swt.browser.CookiesListener ""
com.aelitis.azureus.ui.swt.browser.listener.ConfigListener "@see com.aelitis.azureus.ui.swt.browser.msg.abstractmessagelistener#handlemessage(com.aelitis.azureus.ui.swt.browser.msg.browsermessage)"
com.aelitis.azureus.ui.swt.browser.listener.DisplayListener "refreshes all except the currently active tab make a copy of the torrent remove any non-standard stuff (e.g. resume data) pass decodemap down to torrentuiutilsv3.loadtorrent in case is needs some other params 3.2 todo: need to fix this up check if not active view and refresh (personally, sounds dangerous) mdientryswt entry = mdi.getentryswt(browserid); // use when uis merged if only there was a way to tell if it was our irc"
com.aelitis.azureus.ui.swt.browser.listener.DownloadUrlInfoSWT "context callback"
com.aelitis.azureus.ui.swt.browser.listener.ExternalLoginCookieListener ""{" + "}";"
com.aelitis.azureus.ui.swt.browser.listener.ExternalLoginListener ""
com.aelitis.azureus.ui.swt.browser.listener.ExternalLoginWindow "if we sniffed more cookies that we grabbed through js then use proxy"
com.aelitis.azureus.ui.swt.browser.listener.MetaSearchListener "gouss doesn't wan't anything on cancel if ( !outcome_informed ){ outcome_informed = true; map params = getparams( webengine ); params.put( "error", "operation cancelled" ); sendbrowsermessage("metasearch", "enginefailed", params ); } if ( !outcome_informed ){ outcome_informed = true; map params = getparams( webengine ); sendbrowsermessage("metasearch", "enginecompleted", params ); } protected map getparams( engine engine ) { map params = new hashmap(); params.put("id", new long(engine.getid())); params.put("name", engine.getname()); params.put("favicon", engine.geticon()); params.put("dl_link_css", engine.getdownloadlinkcss()); params.put("shareable", new boolean( engine.isshareable())); if ( sid != null ){ params.put( "sid", sid ); } return( params ); } ok, we now have the torrent associated with the url shouldn't happen, but.. search operation will report outcome no parameters don't skip if this is an explicit get there's some code that attempts to switch to 'auto=true' on first use as when 3110 defaults to false and the decision was made to switch this disable the behaviour if we are customised listener handles don't change this message as the ui uses it! change this you need to change update too below override public flag if not shareable change this you need to change create too above search operation will report outcome override engine selection for subscriptions"
com.aelitis.azureus.ui.swt.browser.listener.TorrentListener "security: only allow torrents from whitelisted trackers"
com.aelitis.azureus.ui.swt.browser.listener.VuzeListener ""
com.aelitis.azureus.ui.swt.browser.msg.MessageDispatcherSWT "dispatches messages to listeners registered with unique ids. registers itself as a listener to receive sequence number reset message. detaches this dispatcher from the given {@link browser}. this dispatcher listens for dispose events from the browser and calls this method in response. {@link browser} which will no longer send messages registers the given listener for the given id. unique identifier used when dispatching messages receives messages targetted at the given id @throws illegalstateexception if another listener is already registered under the same id deregisters the listener with the given id. unique identifier of the listener to be removed deregisters the listener with the given id. unique identifier of the listener to be removed throw new illegalstateexception("no listener is registered for id " + id); @see com.aelitis.azureus.ui.swt.browser.msg.messagedispatcher#getlistener(java.lang.string) @see com.aelitis.azureus.ui.swt.browser.msg.messagedispatcher#dispatch(com.aelitis.azureus.core.messenger.browser.browsermessage) handle messages for dispatcher and context regardless of sequence number"
com.aelitis.azureus.ui.swt.browser.OpenCloseSearchDetailsListener ""
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionAutoDownload "default constructor"
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionCategory "default constructor"
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionLastChecked ""
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionName "default constructor gc.drawtext(cell.gettext(), bounds.x,bounds.y);"
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionNbNewResults "default constructor"
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionNbResults "default constructor"
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionNew "enough to fit title @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.columns.subscriptions.ColumnSubscriptionSubscribers "default constructor"
com.aelitis.azureus.ui.swt.columns.torrent.ColumnControls "@see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) if (fonttext == null) { fonttext = utils.getfontwithheight(gcimage.getfont(), gcimage, 15); } gcimage.setfont(fonttext); @see org.gudy.azureus2.plugins.ui.tables.tablecellmouselistener#cellmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablecellmouseevent) log(cell, "whoo, save"); log(cell, "whoo, draw"); log(cell, oldgraphic); log(cell, "dispose");"
com.aelitis.azureus.ui.swt.columns.torrent.ColumnProgressETA "if (buttonbounds != null && buttonbounds.contains(event.x, event.y)) { int st = fileinfo.getstoragetype(); if ((st == diskmanagerfileinfo.st_compact || st == diskmanagerfileinfo.st_reorder_compact) && fileinfo.isskipped()) { // deleted: move to normal fileinfo.setpriority(0); fileinfo.setskipped(false); } else if (fileinfo.isskipped()) { // skipped: move to normal fileinfo.setpriority(0); fileinfo.setskipped(false); } else if (fileinfo.getpriority() > 0) { // high: move to skipped fileinfo.setskipped(true); } else { // normal: move to high fileinfo.setpriority(1); } //((tablerowcore) event.row).invalidate(); ((tablerowcore) event.row).redraw(); } system.out.println("refresh " + sortvalue + ";" + ds); @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.ui.swt.views.table.tablecellswt) compute bounds ... draw progress bar fgfirst = colors.colorerror; pftt, no colours allowed apparently setaline = displayformatters.formatbytecounttokibetc(dm.getsize()); " " + character.tolowercase(s.charat(0))+s.substring(1); gc.setforeground(colorcache.getrandomcolor()); tmp = messagetext.getstring("fileitem.normal"); system.out.println(cellarea + s + ";" + debug.getcompressedstacktrace()); make relative to row, because mouse events are deleted: move to normal skipped: move to normal high: move to skipped normal: move to high ((tablerowcore) event.row).invalidate();"
com.aelitis.azureus.ui.swt.columns.torrent.ColumnStream "enough to fit title @see com.aelitis.azureus.ui.common.table.impl.tablecolumnimpl#preadd() @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) first call may take forever @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellmouselistener#cellmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablecellmouseevent) @see org.gudy.azureus2.plugins.ui.tables.tablecelltooltiplistener#cellhover(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelltooltiplistener#cellhovercomplete(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.columns.torrent.ColumnThumbAndName "file : nameitem.java torrent name cell for my torrents.  the showicon to set showicon this may be triggered many times, so only invalidate and don't force a refresh() rectangle dst = new rectangle(x, y, dstwidth, dstheight); system.out.println(cellarea); mousedown"
com.aelitis.azureus.ui.swt.columns.torrent.ColumnThumbnail "file : healthitem.java a non-interactive (no click no hover) thumbnail column each cell is mapped to a torrent default constructor  for sorting we only create 2 buckets... vuze content and non-vuze content get the torrent for this cell if the cell is not shown or nothing has changed then skip since there's nothing to update cheat. todo: either auto-add (in above method), or provide access via tablecolumn instead of type casting system.out.println("ref"); tablecellimpl c1 = ((tablecellimpl) cell); tablerowswt tablerowswt = c1.gettablerowswt(); tableviewswtimpl view = (tableviewswtimpl) tablerowswt.getview(); system.out.println(view.getcomposite()); view.gettablecomposite().redraw(0, 0, 5000, 5000, true); view.gettablecomposite().update(); @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.ui.swt.views.table.tablecellswt) this may be triggered many times, so only invalidate and don't force a refresh() don't need to release a null image @see org.gudy.azureus2.plugins.ui.tables.tablecelltooltiplistener#cellhover(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelltooltiplistener#cellhovercomplete(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.columns.torrent.ColumnUnopened "enough to fit title @see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellmouselistener#cellmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablecellmouseevent)"
com.aelitis.azureus.ui.swt.columns.utils.ColumnImageClickArea "2 area the area to set id image the image to set  system.out.println("setimage scale " + scale + ";" + area + ";" + imagearea); image = imgonrow; @see org.gudy.azureus2.plugins.ui.tables.tablecellmouselistener#cellmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablecellmouseevent) system.out.println(event.cell + ": " + event.eventtype + ";" + event.x + "x" + event.y + "; b" + event.button + "; " + event.keyboardstate); event_mouseclick would be nice.. todo: convert to coord relative to image? @see org.gudy.azureus2.plugins.ui.tables.tablerowmouselistener#rowmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablerowmouseevent) system.out.println("d=" + image); system.out.println("e=" + image);"
com.aelitis.azureus.ui.swt.columns.utils.TableColumnCreatorV3 "a utility class for creating some common column sets; this is a virtual clone of tablecolumncreator with slight modifications @return    datecompleteditem.column_id, special changes special changes special changes special changes short variable names to reduce wrapping /////// core columns are implementors of tablecolumn to save one class creation otherwise, we'd have to create a generic tablecolumnimpl class, pass it to another class so that it could manipulate it and act upon changes. @see org.gudy.azureus2.ui.swt.views.table.tablecolumncorecreationlistener#createtablecolumncore(java.lang.class, java.lang.string, java.lang.string)"
com.aelitis.azureus.ui.swt.columns.vuzeactivity.ColumnActivityActions "@see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.ui.swt.views.table.tablecellswt) handle fake row when showing in column editor @see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellmouselistener#cellmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablecellmouseevent) run via play or stream so we get the security warning"
com.aelitis.azureus.ui.swt.columns.vuzeactivity.ColumnActivityDate "@see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.ui.swt.views.tableitems.columndatesizer#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell, long)"
com.aelitis.azureus.ui.swt.columns.vuzeactivity.ColumnActivityNew "enough to fit title @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellmouselistener#cellmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablecellmouseevent)"
com.aelitis.azureus.ui.swt.columns.vuzeactivity.ColumnActivityText "@see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellmouselistener#cellmousetrigger(org.gudy.azureus2.plugins.ui.tables.tablecellmouseevent) rectangle bounds = getdrawbounds((tablecellswt) event.cell); @see org.gudy.azureus2.plugins.ui.tables.tablecelltooltiplistener#cellhover(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelltooltiplistener#cellhovercomplete(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.columns.vuzeactivity.ColumnActivityType "enough to fit title in most cases @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.devices.add.DeviceTemplateChooser "devicetemplate.geticonurl();"
com.aelitis.azureus.ui.swt.devices.add.ManufacturerChooser ""
com.aelitis.azureus.ui.swt.devices.columns.ColumnOD_Completion "cheat. todo: either auto-add (in above method), or provide access via tablecolumn instead of type casting @see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelldisposelistener#dispose(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.ui.swt.views.table.tablecellswt) draw begining and end"
com.aelitis.azureus.ui.swt.devices.columns.ColumnOD_Name ""
com.aelitis.azureus.ui.swt.devices.columns.ColumnOD_Remaining ""
com.aelitis.azureus.ui.swt.devices.columns.ColumnOD_Status ""
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Category "@see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Completion "cheat. todo: either auto-add (in above method), or provide access via tablecolumn instead of type casting @see org.gudy.azureus2.plugins.ui.tables.tablecelladdedlistener#celladded(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecelldisposelistener#dispose(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) @see org.gudy.azureus2.ui.swt.views.table.tablecellswtpaintlistener#cellpaint(org.eclipse.swt.graphics.gc, org.gudy.azureus2.ui.swt.views.table.tablecellswt) draw begining and end displayformatters.formatpercentfromthousands(perthoudone);"
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_CopiedToDevice ""
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Device "@see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Duration "@see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Name ""
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Profile "@see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Rank "@see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Resolution "@see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell)"
com.aelitis.azureus.ui.swt.devices.columns.ColumnTJ_Status "5 7 8 9 10 11 @see org.gudy.azureus2.plugins.ui.tables.tablecellrefreshlistener#refresh(org.gudy.azureus2.plugins.ui.tables.tablecell) error message can be very large and technical as it includes output from ffmpeg error etc. so trim it back for currently we try to ensure that tech info appears after second comma"
com.aelitis.azureus.ui.swt.devices.DeviceInfoArea "@see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjectinitialshow(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjectshown(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjecthidden(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) control browse to local dir deviceinfoarea isn't used, but if it were we'd want to do a check to see if core is available yet.. buildbetaarea(main, top); both - installer test"
com.aelitis.azureus.ui.swt.devices.DeviceInternetView "control start cancel log area swtview = (uiswtview)event.getdata();"
com.aelitis.azureus.ui.swt.devices.DeviceManagerUI "and away you go! sidebarvitalityimage adddevice = entryheader.addvitalityimage("image.sidebar.subs.add"); adddevice.settooltip("add device"); adddevice.addlistener( new sidebarvitalityimagelistener() { public void sbvitalityimage_clicked( int x, int y) { addnewdevice(); //new deviceswizard( devicemanagerui.this ); } }); if ( last_indicator > 0 ){ if ( show_vitality ){ return( to_copy_indicator_colors ); } }  don't really need to choose a profile now.. transcodeprofile[] profiles = null;// devicetemplate.gettranscodeprofiles(); new transcodechooser(profiles) { public void closed() { utils.openmessagebox(null, 0, "chose", "you chose " + (selectedprofile == null ? "null" : selectedprofile.getname())); } }; if ( last_indicator > 0 ){ if ( show_vitality ){ return( to_copy_indicator_colors ); } } if ( last_indicator > 0 ){ if ( show_vitality ){ return( to_copy_indicator_colors ); } } not supported for unix and osx ppc private static final string[] to_copy_indicator_colors = { "#000000", "#000000", "#168866", "#1c5620" }; benefit of the doubt? not yet init, safe to close the assumption here is that if the device is hidden either the about the loss of active transcode as either it is something that they don't know about or alternative canclose listeners have been registered to handle the situation (e.g. burn-in-progress) buildsidebar( false ); mdientry not required mdientry not required auto search send qos config - simple view generic devices transcoding default dir max xcode disable sleep itunes media servers max lines rss offline downloaders enable auto manage private torrents play now shouldn't get here really as its hidden :) renderers media servers routers offline downloaders internet new deviceswizard( devicemanagerui.this ); rollup spinner/warning/info /////// turn on provider plugin not installed yet only triggers when vuzexcode is avail provider plugin installed, but we had a bug in older versions, so fixup ////// beta /////// menu show generic show tagged show hidden simple on complete do options top level menus tux todo: make a table_manager.addcontentmenuitem(class fordatasourcetype, string resourcekey) instead of forcing a loop like this tux todo: make a table_manager.addcontentmenuitem(class fordatasourcetype, string resourcekey) instead of forcing a loop like this filter view show cats cache files publish to rss feed rename export hide tag remove sep props update skin3_constants.properties! we could use the primary file int index = downloadmanagerenhancer.getsingleton().getenhanceddownload(hash).getprimaryfile().getindex(); diskmanagerfileinfo dm_file = plugin_interface.getshortcuts().getdownload(hash).getdiskmanagerfileinfo()[index]; but lets just grab all files limit number of files we can add to avoid crazyness could be smarter here and check extension or whatever swtview = (uiswtview)event.getdata(); this code is to allow a click+drag operation to work as styledtext needs a selection before it will initiate a drag these parameters are used to identify this as a content-data relative url as opposed to a torrent download one, and also provide the content name possible during initialisation, status will be shown again on complete"
com.aelitis.azureus.ui.swt.devices.DevicesFTUX "@return    @return this is a simple dialog box, so instead of using skinneddialog, we'll just built it old school utils.openmessagebox(utils.findanyshell(), swt.ok, "error", e.tostring()); catch any devices we found before installing additional plugins fixup bug where item was turned on via non-dialog way dialog always set the parameter"
com.aelitis.azureus.ui.swt.devices.DevicesODFTUX "data.height = 50;"
com.aelitis.azureus.ui.swt.devices.DevicesWizard "data.height = 50;"
com.aelitis.azureus.ui.swt.devices.SBC_DevicesODView "(non-javadoc) @see org.gudy.azureus2.plugins.ui.uipluginviewtoolbarlistener#refreshtoolbaritems(java.util.map) for now, all columns are default enabled disabled"
com.aelitis.azureus.ui.swt.devices.SBC_DevicesView "(non-javadoc) @see org.gudy.azureus2.plugins.ui.uipluginviewtoolbarlistener#refreshtoolbaritems(java.util.map) (non-javadoc) @see org.gudy.azureus2.plugins.ui.toolbar.uitoolbaractivationlistener#toolbaritemactivated(com.aelitis.azureus.ui.common.toolbaritem, long, java.lang.object) @see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjectinitialshow(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.swt.views.skin.infobarutil# @see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjectshown(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) this is bad. example: 1) do a search 2) sidebar entry opens under devices 3) close search sidebar 4) device entry gets auto-selected 5) 6) 7) go to 1 devicesftux.ensureinstalled(); @see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjecthidden(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) just add all jobs' files open file show in explorer category pause resume separator retry separator copy stream uri remove separator logic to disable items ensure name is up to date @see com.aelitis.azureus.core.devices.transcodequeuelistener#jobadded(com.aelitis.azureus.core.devices.transcodejob) @see com.aelitis.azureus.core.devices.transcodequeuelistener#jobchanged(com.aelitis.azureus.core.devices.transcodejob) @see com.aelitis.azureus.core.devices.transcodequeuelistener#jobremoved(com.aelitis.azureus.core.devices.transcodejob) since table-views were moved to using identity hash maps to manage rows (which is good!) this has broken removal of files as due to the caching optimisations employed by the device manager muliple file-facades can be assumed to be on swt thread, so it's safe to use tvfiles without a sync @see com.aelitis.azureus.ui.common.updater.uiupdatable#getupdateuiname() @see com.aelitis.azureus.ui.common.updater.uiupdatable#updateui() @see com.aelitis.azureus.core.devices.transcodetargetlistener#fileadded(com.aelitis.azureus.core.devices.transcodefile) @see com.aelitis.azureus.core.devices.transcodetargetlistener#filechanged(com.aelitis.azureus.core.devices.transcodefile, int, java.lang.object) @see com.aelitis.azureus.core.devices.transcodetargetlistener#fileremoved(com.aelitis.azureus.core.devices.transcodefile) system.out.println("dragstart"); build eventdata here because on osx, selection gets cleared by the time dragsetdata occurs system.out.println("dragsetdata"); no event.data on dragover, use drag_drop_line_start to determine if ours todo: support drag and drop reordering of xcode queue?"
com.aelitis.azureus.ui.swt.devices.TranscodeChooser "2    paintlistener paintlistener = new paintlistener() { public void paintcontrol(paintevent e) { rectangle ca = ((composite) e.widget).getclientarea(); e.gc.setforeground(e.display.getsystemcolor(swt.color_widget_normal_shadow)); e.gc.setbackground(e.display.getsystemcolor(swt.color_widget_light_shadow)); e.gc.setantialias(swt.on); e.gc.fillroundrectangle(ca.x, ca.y, ca.width - 1, ca.height - 1, 10, 10); e.gc.drawroundrectangle(ca.x, ca.y, ca.width - 1, ca.height - 1, 10, 10); } };  check if plugin is installed we may have disposed of shell during device/profile list building (ex. no devices avail)  take note of never-xcode override once we get to 13 icons (e.g. for itunes now we have ipad4/ipad mini we increase the width) to ensure the dialog doesn't get too long if (event.display.getcursorcontrol() == lblimage) { s += " (via " + profile.getprovider().getname() + ")"; shell.pack(); buttons are center when they have an image.. fill with a bunch of spaces so it left aligns char[] c = new char[100]; arrays.fill(c, ' '); sb.append(c);"
com.aelitis.azureus.ui.swt.extlistener.StimulusRPC "hooks some listeners this code block was moved here from being in-line in mainwindow azmsg;x;listener-id;op-id;params implicit bring to front this is actually sync.. so we could add a completion listener and return the boolean result if we wanted/needed security: only allow torrents from whitelisted urls content network of context is invalid because it's the internal one used for anythin. get network id from params instead the platform needs to know when it can call open-url, and it determines this by the is-ready function 3.2 todo: should we be checking for partial matches?"
com.aelitis.azureus.ui.swt.feature.FeatureManagerInstallWindow "never reach 100%!"
com.aelitis.azureus.ui.swt.feature.FeatureManagerUI "needs to stay here as used by burn plugin && featureavailability.enable_plus() abandon all hope, something already added dvd stuff open installing window open validating window"
com.aelitis.azureus.ui.swt.feature.FeatureManagerUIListener "licencechanged gets fired for all licences after listener is added (via code in featuremanagerui) skip case where licence is already cancelled else assumed install process is taking place if no days left but our display days > 0, that means we ran out of offline time and daysdisplayleft is when the real license expires"
com.aelitis.azureus.ui.swt.imageloader.ImageLoader "loads images from a skinproperty object.  will look for special suffixes (over, down, disabled) and try to load resources using base key and suffix. ie. loadimage("foo-over") when foo=image.png, will load image-over.png  will also create own disabled images if base image found and no disabled image found. disabled opacity can be set via imageloader.disabled-opacity key classloader classloader, private image loadimage(display display, string key) { for (skinproperties sp : skinproperties) { string value = sp.getstringvalue(key); if (value != null) { return loadimage(display, sp.getclassloader(), value, key); } } return loadimage(display, null, null, key); } image[] images = getimages(sparentname); if (images != null && images.length > 0 && isrealimage(images[0])) { return images; }   @return adds image to repository. refcount will be 1, or if key already exists, refcount will increase. public void removeimage(string key) { // eep! mapimages.remove(key); } if ( key != null ){ system.out.println( "missing image: " + key ); }    private final classloader classloader; always add az2 icons to instance this.classloader = classloader; system.out.println("yay " + ssuffix + " for " + skey); maybe there's another suffix.. all if image in repository then this will be non-null not in repo not in repo check the cache to see if full image is in there system.out.println("loadimage " + skey + " - " + res); system.out.println("yay " + ssuffix + " for " + skey); system.out.println("loaded image from " + res + " via " + debug.getcompressedstacktrace()); don't do on skey.endswith("-disabled") because caller parsevaluestring requires a failure so it can retry with _disabled. if that fails, we'll get here (stupid, i know) system.err.println("imagerepository:loadimage:: resource not found: " + res); decrease alpha system.out.println("getimages " + skey); ugly hack to show sidebar items that are disabled note this messes up refcount (increments but doesn't decrement) system.out.println(skey + "=" + properties.getstringvalue(skey) + ";" + ((locations == null) ? "null" : "" + locations.length)); todo: cleanup? should probably fail if refcount > 0 should probably fail if refcount > 0 eep! should probably fail if refcount > 0 if (exists) { // getimage prety much always adds a ref for the 'name' so make sure we do the corresponding unref here } no synchronization here - might have already been downloaded @see org.gudy.azureus2.core3.util.aediagnosticsevidencegenerator#generate(org.gudy.azureus2.core3.util.indentwriter) no one can addref in between candispose and dispose because all our addrefs are in swt threads. system.out.println("imageloader: gc'd " + numremoved);"
com.aelitis.azureus.ui.swt.imageloader.ImageLoaderRefInfo "-2: non-disposable; -1: someone over unref'd; 0: no refs (dispose!)"
com.aelitis.azureus.ui.swt.Initializer "main initializer. usually called by reflection via org.gudy.azureus2.ui.swt.main(string[])  if (op.getoperationtype() != azureuscoreoperation.op_initialisation) { return; } if (percent == 100) { long now = systemtime.getcurrenttime(); long diff = now - starttime; if (diff > 10 && diff < 1000 60 5) { system.out.println(" core: " + diff + "ms for " + slasttask); } } try { string capturesnapshot = new controller().capturesnapshot(profilingmodes.snapshot_with_heap); system.out.println(capturesnapshot); } catch (exception e) { // todo auto-generated catch block e.printstacktrace(); }  whether to initialize the ui before the core has been started used in debug to find out how long initialization took ensure colors initialized initialise the swt locale util ensure colors initialized system.out.println(currenttask); todo auto-generated method stub @see com.aelitis.azureus.core.azureuscorelifecycleadapter#started(com.aelitis.azureus.core.azureuscore) ensure colors initialized setup the update monitor tell listeners that all is initialized : finally, open torrents if any. ensure colors initialized cursors.dispose(); no unix as it will dispose before isterminated is set, causing a ' don't let any failure here cause the stop operation to fail do this after closing core to minimise window when the we aren't listening and therefore another azureus start can potentially get in and screw things up @see com.aelitis.azureus.ui.iuiintializer#addlistener(org.gudy.azureus2.ui.swt.mainwindow.initializerlistener) @see com.aelitis.azureus.ui.iuiintializer#removelistener(org.gudy.azureus2.ui.swt.mainwindow.initializerlistener) ignore @see com.aelitis.azureus.ui.iuiintializer#abortprogress() ignore old initializer would delay 8500 todo auto-generated catch block system.out.println("release init. task");"
com.aelitis.azureus.ui.swt.mdi.BaseMDI "sync changes to entry maps on mapidentry might be a very rare thread issue here if entry gets loaded while we are walking through entries mdientryswt entry = (mdientryswt) sidebarentries[i]; when a new plugin view is added, check out auto-open list to see if the @see org.gudy.azureus2.ui.swt.mainwindow.pluginsmenuhelper.pluginaddedviewlistener#pluginviewadded(org.gudy.azureus2.ui.swt.mainwindow.pluginsmenuhelper.iviewinfo) system.out.println("pluginview added: " + viewinfo.viewid); update title system.out.println("processautoopenmap " + id + " via " + debug.getcompressedstacktrace());"
com.aelitis.azureus.ui.swt.mdi.BaseMdiEntry "(non-javadoc) @see com.aelitis.azureus.ui.mdi.mdientry#close() : handled; false: not handled (non-javadoc) @see com.aelitis.azureus.ui.mdi.mdientry#getviewtitleinfo() (non-javadoc) @see com.aelitis.azureus.ui.mdi.mdientry#setviewtitleinfo(com.aelitis.azureus.ui.common.viewtitleinfo.viewtitleinfo) @deprecated for azburn the imageleft to set ensure parent gets todo: need to listen for viewtitleinfo triggers so we can refresh items below @see com.aelitis.azureus.ui.mdi.mdientry#addtoolbarenabler(com.aelitis.azureus.ui.common.toolbarenabler) uif.refreshiconbar(); // needed? composite.setfocus(); container.getparent().relayout(); this causes double show because createskinobject already calls show container.triggerlisteners(swtskinobjectlistener.event_show); @see com.aelitis.azureus.ui.mdi.mdientry#setdefaultexpanded(boolean) system.out.println("real" + gettitle() + "/" + img.getbounds() + debug.getcompressedstacktrace());"
com.aelitis.azureus.ui.swt.mdi.MdiEntrySWT "public swtskinobject getskinobject();"
com.aelitis.azureus.ui.swt.mdi.MdiSWTMenuHackListener ""
com.aelitis.azureus.ui.swt.mdi.MultipleDocumentInterfaceSWT ""
com.aelitis.azureus.ui.swt.mdi.TabbedEntry "(non-javadoc) @note sidebarentryswt is neary identical to this one. please keep them in sync until commonalities are placed in basemdientry (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#show() tabs don't have vitality image support (yet) (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#iscloseable() (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#settitle(java.lang.string) (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#getvitalityimages() (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#close() swtskinobjectcontainer socontents = (swtskinobjectcontainer) skin.createskinobject( "mdicontents." + uniquenumber++, "mdi.content.item", soparent, getskinrefparams()); skin.addskinobject(socontents); swtitem.setcontrol will set the control's visibility based on whether the control is selected. to ensure it doesn't set our control invisible, set selection now viewcomposite.setbackground(parent.getdisplay().getsystemcolor(swt.color_widget_background)); viewcomposite.setforeground(parent.getdisplay().getsystemcolor(swt.color_widget_foreground)); socontents is invisible, so of course iviwcomposite is invisible we should do the one time layout on the first show.. if (iviewcomposite.isvisible()) { parent.layout(true, true); } now that we have an view, go through show one more time ensure show order by fixes case where two showentries are called, the first from a non swt thread, and the 2nd from a swt thread. the first one will run last showing itself new sidebarvitalityimageswt(this, imageid); override.. we don't support non-closeable triggercloselistener delay saving of removing of auto-open flag. if after the delay, we are still alive, it's assumed the remove the auto-open flag even though execthreadlater will not run on close of app because the display is disposed, do a double check of tree disposal just in case. we don't want to trigger close listeners or remove autoopen parameters if the opposed to closing the sidebar) @see com.aelitis.azureus.ui.mdi.mdientry#isselectable() @see com.aelitis.azureus.ui.mdi.mdientry#setselectable(boolean) @see com.aelitis.azureus.ui.swt.mdi.mdientryswt#addlistener(com.aelitis.azureus.ui.swt.mdi.mdiswtmenuhacklistener) todo auto-generated method stub @see com.aelitis.azureus.ui.swt.mdi.mdientryswt#removelistener(com.aelitis.azureus.ui.swt.mdi.mdiswtmenuhacklistener) todo auto-generated method stub"
com.aelitis.azureus.ui.swt.mdi.TabbedMDI "@see com.aelitis.azureus.ui.swt.mdi.basemdi#skinobjectinitialshow(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) another window has control, skip filter esc or ctrl+f4 closes current tab f6 or ctrl-tab selects next tab on windows the tab key will not reach this filter, as it is processed by the traversal traverse_tab_next. it's unknown what other oses do, so the code is here in case we get tab shift+f6 or ctrl+shift+tab selects previous tab instead of .setselection, use showentry which will ensure view de/activations @see com.aelitis.azureus.ui.mdi.multipledocumentinterface#loadentrybyid(java.lang.string, boolean) assumed mdientryswt @see com.aelitis.azureus.ui.swt.mdi.basemdi#createentryfromskinref(java.lang.string, java.lang.string, java.lang.string, java.lang.string, com.aelitis.azureus.ui.common.viewtitleinfo.viewtitleinfo, java.lang.object, boolean, java.lang.string) afterid not fully supported yet @see com.aelitis.azureus.ui.swt.mdi.multipledocumentinterfaceswt#getentryfromskinobject(org.gudy.azureus2.ui.swt.plugins.pluginuiswtskinobject)"
com.aelitis.azureus.ui.swt.player.PlayerInstaller ""
com.aelitis.azureus.ui.swt.player.PlayerInstallerListener ""
com.aelitis.azureus.ui.swt.player.PlayerInstallWindow "never reach 100%!"
com.aelitis.azureus.ui.swt.plugininstall.SimplePluginInstaller ""
com.aelitis.azureus.ui.swt.plugininstall.SimplePluginInstallerListener ""
com.aelitis.azureus.ui.swt.plugininstall.SimplePluginInstallWindow "never reach 100%!"
com.aelitis.azureus.ui.swt.shells.BrowserWindow "for some reason disposing the shell / browser in the same thread makes ieframe.dll crash on windows. shell.setsize(w, h);"
com.aelitis.azureus.ui.swt.shells.main.DebugMenuHelper "a convenience class for creating the debug menu  this has been extracted out into its own class since it does not really belong to production code creates the debug menu and its children note: this is a development only menu and so it's not modularized into separate menu items because this menu is always rendered in its entirety @return item = new menuitem(menudebug, swt.cascade); item.settext("subscriptions"); menu menusubscriptions = new menu(menudebug.getparent(), swt.drop_down); item.setmenu(menusubscriptions); item = new menuitem(menusubscriptions, swt.none); item.settext("create rss feed"); item.addselectionlistener(new selectionadapter() { public void widgetselected(selectionevent e) { final shell shell = new shell(uifunctions.getmainshell()); shell.setlayout(new formlayout()); label label = new label(shell,swt.none); label.settext("rss feed url :"); final text urltext = new text(shell,swt.border); urltext.settext(utils.getlinkfromclipboard(shell.getdisplay(),false)); label separator = new label(shell,swt.separator | swt.horizontal); button cancel = new button(shell,swt.push); cancel.settext("cancel"); button ok = new button(shell,swt.push); ok.settext("ok"); formdata data; data = new formdata(); data.left = new formattachment(0,5); data.right = new formattachment(100,-5); data.top = new formattachment(0,5); label.setlayoutdata(data); data = new formdata(); data.left = new formattachment(0,5); data.right = new formattachment(100,-5); data.top = new formattachment(label); data.width = 400; urltext.setlayoutdata(data); data = new formdata(); data.left = new formattachment(0,5); data.right = new formattachment(100,-5); data.top = new formattachment(urltext); separator.setlayoutdata(data); data = new formdata(); data.right = new formattachment(ok); data.width = 100; data.top = new formattachment(separator); cancel.setlayoutdata(data); data = new formdata(); data.right = new formattachment(100,-5); data.width = 100; data.top = new formattachment(separator); ok.setlayoutdata(data); cancel.addlistener(swt.selection, new listener() { public void handleevent(event arg0) { shell.dispose(); } }); ok.addlistener(swt.selection, new listener() { public void handleevent(event arg0) { string url_str = urltext.gettext(); shell.dispose(); try{ url url = new url( url_str ); subscriptionmanagerfactory.getsingleton().createsingletonrss( url_str, url, 120, true ); }catch( throwable e ){ debug.printstacktrace(e); } } }); shell.pack(); utils.centerwindowrelativeto(shell, uifunctions.getmainshell()); shell.open(); shell.setfocus(); urltext.setfocus(); } });"
com.aelitis.azureus.ui.swt.shells.main.MainHelpers ""
com.aelitis.azureus.ui.swt.shells.main.MainMDISetup "contentnetworkmanager cnm = contentnetworkmanagerfactory.getsingleton(); if (cnm != null) { contentnetwork[] contentnetworks = cnm.getcontentnetworks(); for (contentnetwork cn : contentnetworks) { if (cn == null) { continue; } if (cn.getid() == constantsvuze.getdefaultcontentnetwork().getid()) { cn.setpersistentproperty(contentnetwork.pp_active, boolean.true); continue; } object oisactive = cn.getpersistentproperty(contentnetwork.pp_active); boolean isactive = (oisactive instanceof boolean) ? ((boolean) oisactive).booleanvalue() : false; if (isactive) { mdi.createcontentnetworksidebarentry(cn); } } } system.out.println("activate sidebar " + starttab + " took " + (systemtime.getcurrenttime() - starttime) + "ms"); starttime = systemtime.getcurrenttime(); loadentrybyid(multipledocumentinterface.sidebar_section_aboutplugins, true, false); building plugin views needs uiswtinstance, which needs core. blah, can't add until plugin initialization is done"
com.aelitis.azureus.ui.swt.shells.main.MainMenu "creates the main menu on the supplied shell the torrents menu is a enabled/disable menus based on what ui mode we're in; this method call controls which menus are enabled when we're in vuze vs. vuze advanced creates the file menu and all its children builds the file menu dynamically no need for restart and exit on os x since it's already handled on the application menu  creates the tools menu and all its children creates the help menu and all its children the 'about' menu is on the application menu on osx creates the window menu and all its children creates the torrent menu and all its children @deprecated this method has been replaced with {@link #getmenu(string)}; use {@link #getmenu(imenuconstants.menu_id_menu_bar)} instead menubar 2 the main menu addviewmenu(); ===== debug menu (development only)==== ==================================== backward compat.. backward compat.."
com.aelitis.azureus.ui.swt.shells.main.MainWindow ""
com.aelitis.azureus.ui.swt.shells.main.MainWindowDelayStub "barp toot osx vuze->preferences menu auto-update restart prompt (for example) osx vuze->about menu"
com.aelitis.azureus.ui.swt.shells.main.MainWindowFactory ""
com.aelitis.azureus.ui.swt.shells.main.MainWindowImpl "old initializer. azureuscore is required to be started new initializer. azureuscore does not need to be started. use {@link #init(azureuscore)} when core is available. called for startup_uifirst 1) constructor 2) createwindow 3) init(core) called only on startup_uifirst called only on startup_uifirst called in both delayedcore and !delayedcore kn: passing the skin to the uifunctions so it can be used by uifunctionsswt.createmenu() todo if (utils.isthisthreadswt()) { // clean the dispatch loop so the splash screen gets updated int i = 1000; while (display.readanddispatch() && i > 0) { i--; } //if (i < 999) { // system.out.println("dispatched " + (1000 - i)); //} } explicitly force the transfer bar location to be saved (if appropriate and open). we can't rely that the normal mechanism for doing this won't fail (which it usually does) when the gui is being disposed of.  associates every view id that we use to a class, and creates the class on first event_show.    when a download is added, check for new meta data and un-"wait state" the rating system.out.println("mainwindow: constructor"); system.out.println("createwindow"); system.out.println("mainwindow: _init(core)"); system.out.println("_init"); system.out.println("mainwindow: init(core)"); when a download is added, check for new meta data and un-"wait state" the rating we pass core in just as reminder that this function needs core share manager init is async so we need to deal with this share progress window must be in a new thread because we don't want to block initilization or any other add listeners iscontent system.out.println("mainwindow: createwindow)"); shell activeshell = display.getactiveshell(); shell.setvisible(true); shell.movebelow(activeshell); 0ms system.out.println("skinlisteners init took " + (systemtime.getcurrenttime() - starttime) + "ms"); starttime = systemtime.getcurrenttime(); 0ms system.out.println("skin layout took " + (systemtime.getcurrenttime() - starttime) + "ms"); starttime = systemtime.getcurrenttime(); another window has control, skip filter ctrl-l: open url 0ms system.out.println("hooks init took " + (systemtime.getcurrenttime() - starttime) + "ms"); starttime = systemtime.getcurrenttime(); attach the ui to plugins must be done before initializing views, since plugins may register table columns and other objects uiswtinstanceimpl.addview(uiswtinstance.view_mytorrents, "piecegraphview", new piecegraphview()); ================ xxx disabled because plugin update window will pop up and take control of the dispatch loop.. clean the dispatch loop so the splash screen gets updated if (i < 999) { system.out.println("dispatched " + (1000 - i)); } so when we become active again, we count a few seconds (if the mouse moves) no use keeping old usage stats if we are told no one wants them max 5 seconds of dispatching. we don't display.sleep here because we only want to clear the backlog of swt events, and sleep would add new ones only show the password if not started minimized correct bug #878227 invokes password do this before other checks as these are blocking dialogs to force order 4813 - removed auto-speedtest on new install run_speed_test = true; donation stuff sesecuritymanagerimpl.getsingleton().exitvm(0); config used to store int, such as 2500. now, it stores a string getintparameter will return default value if parameter is string ( downgraded) getstringparameter will bork if parameter isn't really a string check if we have an old style version setting parameter to a different value type makes az unhappy temp disabled trytricks && visible && constants.iswindows && display.getactiveshell() != shell; we don't want the window to just flash and not open, so: -minimize main shell -set all shells invisible xxx hack for release.. should not access param outside utils.linkshellmetrics added this test so that we can call this method with null parameter. xxx hack for release.. should not access param outside utils.linkshellmetrics @see org.eclipse.swt.events.selectionadapter#widgetselected(org.eclipse.swt.events.selectionevent) todo auto-generated method stub must be done after layout text.selectall(); system.out.println("update: " + stabid); system.out.println("update: " + stabid + ";" + newlength); todo: todo: todo: todo: 3.2 todo: obfusticate! (esp advanced view) @see com.aelitis.azureus.ui.swt.views.skin.sidebar.sidebarlistener#sidebaritemselected(com.aelitis.azureus.ui.swt.views.skin.sidebar.sidebarinfoswt, com.aelitis.azureus.ui.swt.views.skin.sidebar.sidebarinfoswt) @see com.aelitis.azureus.ui.swt.views.skin.sidebar.mdilogidlistener#sidebarlogidchanged(com.aelitis.azureus.ui.swt.views.skin.sidebar.sidebarentryswt, java.lang.string, java.lang.string) @see org.gudy.azureus2.core3.util.aediagnosticsevidencegenerator#generate(org.gudy.azureus2.core3.util.indentwriter) download basket if (shell != null) { utils.setshellicon(shell); }"
com.aelitis.azureus.ui.swt.shells.main.UIFunctionsImpl "stores the current swtskin so it can be used by {@link #createmenu(shell)}  (non-javadoc) @see com.aelitis.azureus.ui.swt.uifunctionsswt#closepluginview(org.gudy.azureus2.ui.swt.pluginsimpl.uiswtviewcore) (non-javadoc) @see com.aelitis.azureus.ui.swt.uifunctionsswt#openpluginview(org.gudy.azureus2.ui.swt.pluginsimpl.uiswtviewcore, java.lang.string)   @see com.aelitis.azureus.ui.swt.uifunctionsswt#addpluginview(java.lang.string, org.gudy.azureus2.ui.swt.plugins.uiswtvieweventlistener) @see com.aelitis.azureus.ui.uifunctions#bringtofront() @see com.aelitis.azureus.ui.uifunctions#bringtofront(boolean) this will force active and set !minimized after pw test @see com.aelitis.azureus.ui.swt.uifunctionsswt#closedownloadbars() @see com.aelitis.azureus.ui.swt.uifunctionsswt#closepluginviews(java.lang.string) @see com.aelitis.azureus.ui.uifunctions#dispose(boolean, boolean) @see com.aelitis.azureus.ui.swt.uifunctionsswt#getmainshell() @see com.aelitis.azureus.ui.swt.uifunctionsswt#getpluginviews() @see com.aelitis.azureus.ui.swt.uifunctionsswt#getswtplugininstanceimpl() @see com.aelitis.azureus.ui.swt.uifunctionsswt#openpluginview(java.lang.string, java.lang.string, org.gudy.azureus2.ui.swt.plugins.uiswtvieweventlistener, java.lang.object, boolean) some plugins (cvs updater) want their view's composite initialized on openpluginview, otherwise they won't do logic (like check for new snapshots). so, enforce loading entry. @see com.aelitis.azureus.ui.uifunctions#refreshiconbar() @see com.aelitis.azureus.ui.uifunctions#refreshlanguage() @see com.aelitis.azureus.ui.swt.uifunctionsswt#removepluginview(java.lang.string) @see com.aelitis.azureus.ui.uifunctions#setstatustext(java.lang.string) @see com.aelitis.azureus.ui.uifunctions#setstatustext(int, java.lang.string, com.aelitis.azureus.ui.uistatustextclicklistener) @see com.aelitis.azureus.ui.swt.uifunctionsswt#getmainstatusbar() @see com.aelitis.azureus.ui.uifunctions#showconfig(java.lang.string) @see com.aelitis.azureus.ui.uifunctions#viewurl(java.lang.string, java.lang.string, java.lang.string) note; we don't setsourceref on contentnetwork here like we do everywhere else because the source ref should already be set by the caller ((swtskinobjectbrowser) skinobject).getbrowser().setvisible(false); 4010 tux: this shouldn't be.. either determine contentnetwork from url or target, or do something.. @see com.aelitis.azureus.ui.uifunctions#prompt @see com.aelitis.azureus.ui.uifunctions#get @see com.aelitis.azureus.ui.uifunctions#getuiupdater() @see com.aelitis.azureus.ui.swt.uifunctionsswt#closealldetails() @see com.aelitis.azureus.ui.swt.uifunctionsswt#hasdetailviews() @see com.aelitis.azureus.ui.swt.uifunctionsswt#showcorewaitdlg() todo : tux move to utils? could you also add a "mode" or something that would be added to the url eg: &subscribe_mode=true"
com.aelitis.azureus.ui.swt.shells.RemotePairingWindow "@see com.aelitis.azureus.ui.swt.skin.swtskinbuttonutility.buttonlisteneradapter#pressed(com.aelitis.azureus.ui.swt.skin.swtskinbuttonutility, com.aelitis.azureus.ui.swt.skin.swtskinobject, int) enabling will automatically get access code and trigger somethingchanged fire something changed ourselves, so that accesscode gets picked up ignore.. if error, lasterrorupdates will trigger use somethingchanged to trigger testpairing if needed utils.openmessagebox(utils.findanyshell(), swt.ok, "error", e.tostring()); @see com.aelitis.azureus.core.pairing.pairingmanagerlistener#somethingchanged(com.aelitis.azureus.core.pairing.pairingmanager) pause while registering.."
com.aelitis.azureus.ui.swt.shells.uiswitcher.UISwitcherWindow "full az3ui @see org.eclipse.swt.widgets.listener#handleevent(org.eclipse.swt.widgets.event)"
com.aelitis.azureus.ui.swt.skin.SWTBGImagePainter "constants.isosx; system.out.println("bb: " + control.getdata("configid")); todo: can also exit early if size shrunk but position same and imgbgbounds same. control.setredraw(false); + "\n" + debug.getcompressedstacktrace()); system.out.println(size); size.x = 10; size.y = 10; gc gc = new gc(newimage); gc.setbackground(shell.getdisplay().getsystemcolor( (int) (math.random() 16))); gc.fillrectangle(0, 0, size.x, size.y); gc.dispose(); system.out.println("comparea=" + comparea); system.out.println("locc="+ loccontrol + ";locs=" + locshell); todo: tile down todo: tile down control.setredraw(true); control.update(); control.getshell().update(); if (control instanceof composite) { control[] children = ((composite)control).getchildren(); ((composite)control).layout(true, true); for (int i = 0; i < children.length; i++) { control control2 = children[i]; control2.redraw(); control2.update(); } }"
com.aelitis.azureus.ui.swt.skin.SWTColorWithAlpha ""
com.aelitis.azureus.ui.swt.skin.SWTSkin "ontoppaintlistener = new listener() { public void handleevent(event event) { for (iterator iter = ontopimages.iterator(); iter.hasnext();) { swtskinobject skinobject = (swtskinobject) iter.next(); control control = skinobject.getcontrol(); if (control == null) { continue; } rectangle bounds = control.getbounds(); point point = control.todisplay(0, 0); bounds.x = point.x; bounds.y = point.y; rectangle eventbounds = event.getbounds(); point = ((control) event.widget).todisplay(0, 0); eventbounds.x += point.x; eventbounds.y += point.y; //system.out.println(eventbounds + ";" + bounds); if (eventbounds.intersects(bounds)) { point dst = new point(bounds.x - point.x, bounds.y - point.y); //system.out.println("painting on " + event.widget + " at " + dst); image image = (image) control.getdata("image"); // todo: clipping otherwise alpha will multiply //event.gc.setclipping(eventbounds); event.gc.drawimage(image, dst.x, dst.y); } } } }; public void dumpobjects() { system.out.println("====="); formdata formdata; for (iterator iter = mapidstocontrols.keyset().iterator(); iter.hasnext();) { string sid = (string) iter.next(); control control = getskinobjectbyid(sid).getcontrol(); formdata = (formdata) control.getlayoutdata(); system.out.println(sid); sid += ".attach."; if (formdata.left != null) { system.out.println(sid + "left=" + getattachline(formdata.left)); } if (formdata.right != null) { system.out.println(sid + "right=" + getattachline(formdata.right)); } if (formdata.top != null) { system.out.println(sid + "top=" + getattachline(formdata.top)); } if (formdata.bottom != null) { system.out.println(sid + "bottom=" + getattachline(formdata.bottom)); } } } @return 2 replaced by mousemove // when shell activates or deactivates, send a mouseenter or mouseexit // this fixes the problem where we are hovering over a skinobject with // a "-over" state, we tab away, move the mouse, and tab back again. // without this code, the skinobject would still be in "-over" state shell.addlistener(swt.deactivate, new listener() { public void handleevent(event event) { control cursorcontrol = shell.getdisplay().getcursorcontrol(); if (cursorcontrol != null) { while (cursorcontrol != null) { event mouseexitevent = new event(); mouseexitevent.type = swt.mouseexit; mouseexitevent.widget = cursorcontrol; shell.getdisplay().post(mouseexitevent); system.out.println(cursorcontrol.getdata("skinobject")); cursorcontrol = cursorcontrol.getparent(); } } } }); shell.addlistener(swt.activate, new listener() { public void handleevent(event event) { control cursorcontrol = shell.getdisplay().getcursorcontrol(); if (cursorcontrol != null) { while (cursorcontrol != null) { event mouseexitevent = new event(); mouseexitevent.type = swt.mouseenter; mouseexitevent.widget = cursorcontrol; shell.getdisplay().post(mouseexitevent); cursorcontrol = cursorcontrol.getparent(); } } } });  private void addpaintlistenertoall(control control) { // xxx: bug: when paint listener is set to shell, browser widget will flicker on osx when resizing if (!(control instanceof shell)) { control.addlistener(swt.paint, ontoppaintlistener); } if (control instanceof composite) { composite composite = (composite) control; control[] children = composite.getchildren(); for (int i = 0; i  (objectid).view.template.(stemplatekey)=(reference to template skin object) id to give the new tab template key to read to get the tab's template skin object where to read the template key from new tab, or null if tab could not be  used for dumpobjectsonly private string getattachline(formattachment attach) { string s = ""; if (attach.control != null) { s += attach.control.getdata("configid"); if (attach.offset != 0 || attach.alignment != swt.default) { s += "," + attach.offset; } if (attach.alignment != swt.default) { if (attach.alignment == swt.left) { s += ",left"; } else if (attach.alignment == swt.right) { s += ",right"; } else if (attach.alignment == swt.top) { s += ",top"; } else if (attach.alignment == swt.bottom) { s += ",bottom"; } else if (attach.alignment == swt.center) { s += ",center"; } } } else { s += (int) (100.0 attach.numerator / attach.denominator) + "," + attach.offset; } return s; } create a skin object based off an existing config "template" id of new skin object config id to use to create new skin object location to place new skin object in skin object  @return @return  private listener ontoppaintlistener; key = skin object id; value = array of swtskinobject key = tabset id; value = swtskintabset key = widget id; value = array of swtskinobject the default skin gets the default image loader. we don't add it to the mapimageloaders because non-skin objects may use the image loader mapimageloaders is used to dispose of images when the skin is disposed system.out.println(eventbounds + ";" + bounds); system.out.println("painting on " + event.widget + " at " + dst); todo: clipping otherwise alpha will multiply event.gc.setclipping(eventbounds); for swt layout -- add a reverse lookup system.out.println(".." + existingobjects[i]); system.out.println("addtoarraymap: " + key + " : " + object + " #" + (length + 1)); xxx search for parent is shell directly xxx search for parent is shell directly system.out.println("check " + tab + ";" + object + " for " + skinobjectintab); system.out.println("found"); system.out.println(" check " + object + " in " + start + " for " + skinobject); system.out.println(" check " + start + " for " + skinobject); system.out.println("move from " + (lastcontrol == null ? null : lastcontrol.handle) + " to " + (cursorcontrol == null ? "null" : cursorcontrol.handle)); can't just add a mouseexit listener to the shell, because it doesn't get fired when a control is on the edge when shell activates or deactivates, send a mouseenter or mouseexit this fixes the problem where we are hovering over a skinobject with a "-over" state, we tab away, move the mouse, and tab back again. without this code, the skinobject would still be in "-over" state we handle cases where width || height < 0 later in layout() xxx: bug: when paint listener is set to shell, browser widget will flicker on osx when resizing apply layout data from skin disabled due to browser flickering addpaintlistenertoall(shell); because layout data is cached, we can't just set the data's properties we need to create a brand new formdata. templateid = skinobject.getskinobjectid(); grab any defaults from existing attachment xxx assumed: denominator is 100 parse skin config percent, offset object, offset, alignment create new attachment if (newformdata.width != 0 && newformdata.height != 0) { controltolayout.setdata("oldsize", new point(newformdata.width, newformdata.height)); } properties.getstringarray(sconfigid + ".widgets-onshow"); cloning is only for one level. children get the original properties object propogate any parameter values. xxx this could get ugly, we should could the # of swtskinpropertiesparam to determine if this needs optimizing ie. if a top container has paramvalues, every child will get a new object. how would this affect memory/performace? attach only after all children are added no widgets, so it's a sash widgets exist, so use a sashform to split them swtskinobjecttab skinobject = _createtab(skinproperties, sid, stemplateid, tabholder); system.out.println("createtab " + sid + ", " + sconfigid + ", " + sparentid); private void createtextwidget(final string sconfigid) { swtskinobject parent = getparent(sconfigid); if (parent == null) { return; }  swttextpaintlistener listener = new swttextpaintlistener(this, parent.getcontrol(), sconfigid); parent.getcontrol().addpaintlistener(listener);  //addtocontrolmap(listener, sconfigid);  return; } no type, use best guess system.out.println("create clone " + sid + " == " + sconfigid + " for " + parentskinobject); system.out.println(scloneparams[0]); skinobject = new swtskinobjectimagecontainer(this, properties, sid, sconfigid, parentskinobject); cloning is only for one level. children get the original properties object propogate any parameter values. xxx this could get ugly, we should could the # of swtskinpropertiesparam to determine if this needs optimizing ie. if a top container has paramvalues, every child will get a new object. how would this affect memory/performace?"
com.aelitis.azureus.ui.swt.skin.SWTSkinButtonUtility "simple encapsulation of swtskinobjectcontainer that provides typical button funtionality todo implement todo implement"
com.aelitis.azureus.ui.swt.skin.SWTSkinCheckboxListener ""
com.aelitis.azureus.ui.swt.skin.SWTSkinFactory ""
com.aelitis.azureus.ui.swt.skin.SWTSkinImageChanger "default constructor system.out.println("event " + event.type + ";" + control.handle); check if exiting and going into child mouse exit and enter happens on client area (not full widget area) check if exiting and going into parent xxx disabled for now as it doesn't always work return;"
com.aelitis.azureus.ui.swt.skin.SWTSkinLayoutCompleteListener ""
com.aelitis.azureus.ui.swt.skin.SWTSkinObject "retrieve the associated swt control used by the skin object control retrieve the type of widget. @return todo move widget types to swtskinobject retrieve the skin object id that represents this object. typically the same as {@link #getconfigid()}, however, may be different if a config id is used to make independant copies unique skin object id retrieve the config id which is id in the skin config file. id  todo convenience method for switching suffix using defaults @return @return  @return @return     @return @return @return @return  void gettemplate(string property)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectAdapter "converts {@link swtskinobjectlistener} events to method calls (non-javadoc) @see com.aelitis.azureus.ui.swt.skin.swtskinobjectlistener#eventoccured(com.aelitis.azureus.ui.swt.skin.swtskinobject, int, java.lang.object)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectBasic "todo  switch the suffix using the default of 1 for level and false for walkup properties (non-javadoc) @see com.aelitis.azureus.ui.swt.skin.swtskinobject#addlistener(com.aelitis.azureus.ui.swt.skin.swtskinobjectlistener) (non-javadoc) @see com.aelitis.azureus.ui.swt.skin.swtskinobject#removelistener(com.aelitis.azureus.ui.swt.skin.swtskinobjectlistener) the debug to set debug xxx might be wise to force this to swtskinobjectcontainer use tile_both because a gradient should never set the size of the control rather, the gradient image is made based on the control size. if we used tile_x, swtbgimagepainter would force the height to the current image, thus preventing any auto-resizing setvisible is one time only system.out.println(swtskinobjectbasic.this + "> swt.show/hide " + ((event.widget).getdata("skinobject")) + ";" + ((control)event.widget).isvisible() + ";" + debug.getcompressedstacktrace()); wait until show or hide event is processed to guarantee isvisible will be correct for listener triggers system.out.println(">>swt.show/hide " + ((event.widget).getdata("skinobject")) + ";" + ((control) event.widget).isvisible()); only walkup when visible.. yes? @see com.aelitis.azureus.ui.swt.skin.swtskinobject#getparent() if (ssuffix.length() > 0) { setbackground(sconfigid, ""); } if (ssuffix.length() > 0) { setbackground(sconfigid, ""); } control.setbackgroundimage doesn't handle transparency! control.setbackgroundimage(image); workaround: create our own image with shell's background for "transparent" area. doesn't allow control's image to show through. to do that, we'd have to walk up the tree until we found a composite with an image control.setbackgroundmode(swt.inherit_none); control.setbackgroundimage(imagebg); painter = new swtbgimagepainter(control, imagebgleft, imagebgright, imagebg, tilemode); system.out.println("setimage " + sconfigid + " " + ssuffix); xxx is this needed? it causes flicker and slows things down. maybe a redraw instead (if anything at all)? control.update(); @see java.lang.object#tostring() @see com.aelitis.azureus.ui.swt.skin.swtskinobject#getskin() @see java.lang.object#equals(java.lang.object) @see com.aelitis.azureus.ui.swt.skin.swtskinobject#setvisible(boolean) for some reason this is required even when !changed (on windows) still need to call setisvisible to walk up/down @see com.aelitis.azureus.ui.swt.skin.swtskinobject#setdefaultvisibility() move up the tree until propogation stops system.out.println(sconfigid + suffix + "; walkup"); system.out.println(systemtime.getcurrenttime() + ": " + this + suffix + "; switchy"); ignore color,[width] delay show and hide events while not initialized system.out.println("not initialized! " + swtskinobjectbasic.this + ";;;" + debug.getcompressedstacktrace()); system.out.println("initialized! " + swtskinobjectbasic.this + ";;;" + debug.getcompressedstacktrace()); process listeners added locally don't use iterator as triggering code may try to remove itself process listeners added to skin this will fire @see com.aelitis.azureus.ui.swt.skin.swtskinobject#dispose() @see com.aelitis.azureus.ui.swt.skin.swtskinobject#settooltipid(java.lang.string) @see com.aelitis.azureus.ui.swt.skin.swtskinobject#gettooltipid(boolean) @see org.eclipse.swt.events.paintlistener#paintcontrol(org.eclipse.swt.events.paintevent) if (colorfilltype == border_gradient) { color oldfg = e.gc.getforeground(); e.gc.setforeground(bgcolor2); e.gc.fillgradientrectangle(0, 0, bounds.width - 1, bounds.height - 1, true); e.gc.setforeground(oldfg); } @see com.aelitis.azureus.ui.swt.skin.swtskinobject#getdata(java.lang.string) @see com.aelitis.azureus.ui.swt.skin.swtskinobject#setdata(java.lang.string, java.lang.object) @see org.gudy.azureus2.ui.swt.debug.obfusticateimage#obfusticatedimage(org.eclipse.swt.graphics.image, org.eclipse.swt.graphics.point) @see com.aelitis.azureus.ui.swt.skin.swtskinobject#relayout()"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectBrowser "hack so search results page doesn't clear cur selected system.out.println(systemtime.getcurrenttime() + "] set url: " + url); @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#setvisible(boolean) notify browser after we've fully processed visibility"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectButton "native button. for non-native, use swtskinbuttonutility on any swtskinobject  windows swt bug: button bg won't draw properly without inherit force create a intermediate composite with forced inherit and put button in that. @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#switchsuffix(java.lang.string, int, boolean)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectCheckbox "native checkbox stored so we can access it after button is disposed, and so we can retrieve without being on swt thread winxp classic theme will not bring though parent's background image without forceing the background mode @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#switchsuffix(java.lang.string, int, boolean)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectContainer "a swtskinobject that contains other swtskinobjects lovely swt has a default size of 64x64 if no children have sizes. let's fix that.. setting inherit_force here would make the bg of a text box be this parent's bg (on win7 at least) parentcomposite.setbackgroundmode(swt.inherit_force); @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#setcontrol(org.eclipse.swt.widgets.control) todo: need find child(view id) @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#switchsuffix(java.lang.string) @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#setisvisible(boolean) currently we ignore "changed" and set visibility on children to ensure things display child.setisvisible(visible, false); system.out.println("child control " + child + " is " + (childcontrol.isvisible() ? "visible" : "invisible")); @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#obfusticatedimage(org.eclipse.swt.graphics.image, org.eclipse.swt.graphics.point)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectImage "swtskinobject so = (swtskinobject) e.widget.getdata("skinobject"); todo: real scale.. todo: tile down todo: tile down { public point computesize(int whint, int hhint) { object image = canvas.getdata("image"); object imageid = canvas.getdata("imageid"); if (image == null && (imageid == null || ((string) imageid).length() == 0)) { return new point(0, 0); } return super.computesize(whint, hhint); };  public point computesize(int whint, int hhint, boolean changed) { object image = canvas.getdata("image"); object imageid = canvas.getdata("imageid"); if (image == null && (imageid == null || ((string) imageid).length() == 0)) { return new point(0, 0); } return super.computesize(whint, hhint, changed); }; }; swtbgimagepainter painter = (swtbgimagepainter) parent.getdata("bgpainter"); if (painter != null) { canvas.addlistener(swt.paint, painter); } needed to set paint listener and canvas size protected void setcanvasimage(string sconfigid, aecallback callback) { private void setcanvasimage(final string sconfigid, final string simageid, aecallback callback) { allowimagedimming = sdrawmode.equalsignorecase("dim"); canvas.setdata("image", image); xxx huh? a tile of one? :) remove in case already added @see com.aelitis.azureus.ui.swt.skin.swtskinobject#setbackground(java.lang.string, java.lang.string) no background for images? @see com.aelitis.azureus.ui.swt.skin.swtskinobject#switchsuffix(java.lang.string)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectListener "allows monitoring of {@link swtskinobject}'s events skin object was shown skin object was hidden skin object was selected (activated) skin object was destroyed skin object was skinobject will be null, params will be an array { view id, config id } function who creates the object should return a swtskinobject skinobject needs to update any text friendly names of events, useful for debug called when an event occurs skin object the event occurred on event_ constant any parameters the event needs to send you"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectSash " parameters:  .startpos position in pixels of where to start the sash by default    fast drag disables resizing left and right sides on each mouse move (when mouse is down) two problems with disabling fastdrag: 1) the places we use the sash currently have very slow re-rendering 2) when the    fall through delay so soabove's show gets triggered need to figure out if we have to use border width elsewhere in calculations (probably) d = (double) (abovedata.width + sash.getsize().x) / parentwidth;"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectSeparator ""
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectSlider "@return null if you do not wish to change the value @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#paintcontrol(org.eclipse.swt.graphics.gc) @see org.eclipse.swt.events.mouselistener#mousedoubleclick(org.eclipse.swt.events.mouseevent) @see org.eclipse.swt.events.mouselistener#mousedown(org.eclipse.swt.events.mouseevent) @see org.eclipse.swt.events.mouselistener#mouseup(org.eclipse.swt.events.mouseevent) @see org.eclipse.swt.events.mousemovelistener#mousemove(org.eclipse.swt.events.mouseevent)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectTab "retrieve the parent skin object to which the active widgets belong to. skin object, or null if it doesn't matter sets the parent skin object to which the active widgets belong to.  this is usefull when there are multiple widgets with the same id system.out.println("looking for " + sids[i] + " w/parent " + activewidgetsparent);"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectTabFolder "todo: ensure correct tabfolder child comp is visible super.childadded(sochild); ctabitem tabitem = new ctabitem(tabfolder, swt.none); tabitem.settext("wow"); tabitem.setcontrol(sochild.getcontrol());"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectText "@return  @return"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectText1 "text skin object. this one uses a label widget.  default constructor system.out.println(this + "; " + sprefix + ";" + suffix + "; " + color + "; " + text); todo auto-generated catch block i know what i'm doing. maybe ;) @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#getstyle() @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#setstyle(int) @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#gettext() @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#addurlclickedlistener(com.aelitis.azureus.ui.swt.skin.swtskinobjecttext_urlclickedlistener) todo auto-generated method stub @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#removeurlclickedlistener(com.aelitis.azureus.ui.swt.skin.swtskinobjecttext_urlclickedlistener) todo auto-generated method stub @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#settextcolor(org.eclipse.swt.graphics.color) todo auto-generated method stub"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectText2 "text skin object. this one paints text on parent.  kn: disabling the caching of the key since this is parameterized it may be called multiple times with different parameters @see org.eclipse.swt.widgets.composite#computesize(int, int, boolean) ignore error_no_graphics_library error or any others ignore int curpx = fontutils.getfontheightinpx(tempfontdata); fontsize = fontutils.getfontheightfrompx(canvas.getdisplay(), tempfontdata, null, (int) (curpx + dsize)); ifontsize = utils.getfontheightfrompx(canvas.getfont(), null, (int) dsize); ifontsize = utils.pixelstopoint(dsize, canvas.getdisplay().getdpi().y); todo auto-generated catch block doing execswtthreadlater delays the relayout for too long at skin startup since there are a lot of async execs at skin startup, we generally see the window a second or two before this async call would get called (if it were async) laststringprinter must be set while in swt thread otherwise sync issues happen @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#paintcontrol(org.eclipse.swt.graphics.gc) ignore error_no_graphics_library error or any others else if (key.equals(skey)) { return; } @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#triggerlisteners(int, java.lang.object) @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#gettext() @see com.aelitis.azureus.ui.swt.skin.swtskinobjecttext#settextcolor(org.eclipse.swt.graphics.color)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectText_UrlClickedListener "= url processed; false = do default"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectTextbox "native checkbox e.gc.setlinewidth(1); pick up changes in the text control's bg color and propagate to the bubble @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#switchsuffix(java.lang.string, int, boolean)"
com.aelitis.azureus.ui.swt.skin.SWTSkinObjectToggle "native toggle stored so we can access it after button is disposed, and so we can retrieve without being on swt thread winxp classic theme will not bring though parent's background image without forceing the background mode @see com.aelitis.azureus.ui.swt.skin.swtskinobjectbasic#switchsuffix(java.lang.string, int, boolean)"
com.aelitis.azureus.ui.swt.skin.SWTSkinProperties "extends skinproperties with swt specific methods retrieve a color property in as a swt color property name color, or null @return"
com.aelitis.azureus.ui.swt.skin.SWTSkinPropertiesClone "simple extension of swtskinproperties that first checks the original cloning id before checking the keys that it's cloning.  cloned skin objects will be calling this class with a config id of "" plus whatever property name string they add on. initialize where to read properties from the config key that told us to clone something  @see com.aelitis.azureus.ui.swt.skin.swtskinproperties#getcolorwithalpha(java.lang.string) @see com.aelitis.azureus.ui.skin.skinproperties#getbooleanvalue(java.lang.string, boolean) @see com.aelitis.azureus.ui.skin.skinproperties#clearcache() @see com.aelitis.azureus.ui.skin.skinproperties#contains(java.lang.string) @see com.aelitis.azureus.ui.skin.skinproperties#getreferenceid(java.lang.string) @see com.aelitis.azureus.ui.skin.skinproperties#addresourcebundle(java.util.resourcebundle) @see com.aelitis.azureus.ui.skin.skinproperties#getclassloader()"
com.aelitis.azureus.ui.swt.skin.SWTSkinPropertiesImpl "@see com.aelitis.azureus.ui.swt.skin.swtskinproperties#getcolor(java.lang.string) imp.getlogger().log(loggerchannel.lt_error, "failed loading color : color." + colornames[i]); @see com.aelitis.azureus.ui.swt.skin.swtskinproperties#getcolor(java.lang.string, org.eclipse.swt.graphics.color)"
com.aelitis.azureus.ui.swt.skin.SWTSkinPropertiesParam "@return"
com.aelitis.azureus.ui.swt.skin.SWTSkinPropertiesParamImpl "swtskinproperties delegator that always passes a set of parameters to string requests.  @see com.aelitis.azureus.ui.skin.skinproperties#getbooleanvalue(java.lang.string, boolean) @see com.aelitis.azureus.ui.skin.skinproperties#contains(java.lang.string) @see com.aelitis.azureus.ui.skin.skinproperties#getreferenceid(java.lang.string) @see com.aelitis.azureus.ui.skin.skinproperties#addresourcebundle(java.util.resourcebundle) @see com.aelitis.azureus.ui.skin.skinproperties#getclassloader()"
com.aelitis.azureus.ui.swt.skin.SWTSkinTabSet "list of swtskinobjecttab @deprecated use {@link #setactivetab(string)}  xxx do we need to pass in skinproperties in case of cloning? system.out.println("addtab for " + sid + ": " + tab.getskinobjectid()); don't exit early if we are already on tab. we want to be notified if the system.out.println(((visible ? "show" : "hide") + " " + skinobject) + debug.getcompressedstacktrace());"
com.aelitis.azureus.ui.swt.skin.SWTSkinTabSetListener ""
com.aelitis.azureus.ui.swt.skin.SWTSkinToggleListener ""
com.aelitis.azureus.ui.swt.skin.SWTSkinUtils "@return private static listener imageoverlistener; imageoverlistener = new swtskinimagechanger("-over", swt.mouseenter, swt.mouseexit); widget.addlistener(swt.mouseenter, imageoverlistener); widget.addlistener(swt.mouseexit, imageoverlistener); new mouseenterexitlistener(widget); system.out.println(control.getdata("skinid") + " oldheight = " + size + ";v=" + control.getvisible() + ";s=" + control.getsize()); formdata should now be 0,0, but setvisible may have explicitly changed it prevent calling again prevent calling again system.out.println("slide to " + size + " via "+ debug.getcompressedstacktrace()); system.out.println(control + "] newh=" + newheight + "/" + newwidth + " to " + size.y); system.out.println(control + "] side to " + size.y + " done" + size.x); utils.relayout(control, false);"
com.aelitis.azureus.ui.swt.skin.SWTTextPaintListener ""
com.aelitis.azureus.ui.swt.subscriptions.SubscriptionListWindow "private void populatesubscription(final subscription subscription) { final tableitem item = new tableitem(subscriptionslist,swt.none); item.setdata("subscription",subscription); item.settext(0,subscription.getname()); try { item.settext(1,messagetext.getstring("subscriptions.listwindow.popularity.reading")); action.setenabled(true); } determine new sort column and direction shell.setsize(400,300); thread.sleep(100); todo auto-generated method stub"
com.aelitis.azureus.ui.swt.subscriptions.SubscriptionManagerUI "for (int i=0;i " + subscription.getname()); } public void complete( byte[] hash, subscription[] subscriptions ) { log( " lookup: complete " + byteformatter.encodestring( hash ) + " -> " +subscriptions.length ); } public void failed( byte[] hash, subscriptionexception error ) { log( " lookup: failed", error ); } }); }catch( throwable e ){ log( "lookup failed", e ); } } } check assoc make assoc - cvs only as for testing purposes closing down search download subs enable auto int param fires intermediate events so we have to rely on the save :( rss new subscriptionlistwindow(plugincoreutils.unwrap(dl),true); coconfigurationmanager.setparameter( "subscriptions.wizard.shown", false ); hack to hide useless entries closing down closing down this sets up the entry (menu, etc)"
com.aelitis.azureus.ui.swt.subscriptions.SubscriptionMDIEntry "sep category sep possible during initialisation, status will be shown again on complete don't report problem until its happened a few times, but not for auth fails as this is a perm error"
com.aelitis.azureus.ui.swt.subscriptions.SubscriptionSelectedContent "if you add more fields here be sure to amend 'sameas' logic below hack alert - we embed the vuze-file into a torrent to allow it to go through the normal share route, then pick it out again when the recipient 'downloads' it if not corrupt...."
com.aelitis.azureus.ui.swt.subscriptions.SubscriptionsView "(non-javadoc) @see com.aelitis.azureus.core.subs.subscriptionmanagerlistener#associationschanged(byte[]) (non-javadoc) @see com.aelitis.azureus.core.subs.subscriptionmanagerlistener#subscriptionselected(com.aelitis.azureus.core.subs.subscription) (non-javadoc) @see com.aelitis.azureus.core.subs.subscriptionmanagerlistener#subscriptionadded(com.aelitis.azureus.core.subs.subscription) (non-javadoc) @see com.aelitis.azureus.core.subs.subscriptionmanagerlistener#subscriptionremoved(com.aelitis.azureus.core.subs.subscription) (non-javadoc) @see com.aelitis.azureus.core.subs.subscriptionmanagerlistener#subscriptionchanged(com.aelitis.azureus.core.subs.subscription) (non-javadoc) @see org.gudy.azureus2.plugins.ui.uipluginviewtoolbarlistener#refreshtoolbaritems(java.util.map) (non-javadoc) @see org.gudy.azureus2.plugins.ui.toolbar.uitoolbaractivationlistener#toolbaritemactivated(com.aelitis.azureus.ui.common.toolbaritem, long, java.lang.object) todo auto-generated method stub can happen on first selection it seems data.top = new formattachment(pretext,5); datasourcechanged(event.getdata());"
com.aelitis.azureus.ui.swt.subscriptions.SubscriptionView "controls = new composite(composite, swt.none); gridlayout layout = new gridlayout(); layout.numcolumns = 1; layout.marginheight = 0; layout.marginwidth = 0; controls.setlayout(layout); data = new formdata(); data.left = new formattachment(0,0); data.right = new formattachment(100,0); data.top = new formattachment(0,0); controls.setlayoutdata(data); griddata grid_data; info_lab = new label( controls, swt.null ); grid_data = new griddata(griddata.fill_horizontal); info_lab.setlayoutdata(grid_data); info_lab2 = new label( controls, swt.null ); grid_data = new griddata(griddata.fill_horizontal); info_lab2.setlayoutdata(grid_data); json_area = new styledtext(controls,swt.border); grid_data = new griddata(griddata.fill_horizontal); grid_data.heighthint = 50; json_area.setlayoutdata(grid_data); json_area.setwordwrap(true); (non-javadoc) @see org.gudy.azureus2.plugins.ui.uipluginviewtoolbarlistener#refreshtoolbaritems(java.util.map) (non-javadoc) @see org.gudy.azureus2.plugins.ui.toolbar.uitoolbaractivationlistener#toolbaritemactivated(com.aelitis.azureus.ui.common.toolbaritem, long, java.lang.object) string engine_str = ""; try{ engine engine = subs.getengine(); engine_str = engine.getstring(); }catch( throwable e ){ engine_str = debug.getnestedexceptionmessage(e); debug.out(e); } info_lab.settext( "id=" + subs.getid() + ", version=" + subs.getversion() + ", subscribed=" + subs.issubscribed() + ", public=" + subs.ispublic() + ", mine=" + subs.ismine() + ", popularity=" + subs.getcachedpopularity() + ", associations=" + subs.getassociationcount() + ", engine=" + engine_str ); subscriptionhistory history = subs.gethistory(); info_lab2.settext( "history: " + "enabled=" + history.isenabled() + ", auto=" + history.isautodownload() + ", last_scan=" + new simpledateformat().format(new date( history.getlastscantime())) + ", next_scan=" + new simpledateformat().format(new date( history.getnextscantime())) + ", last_new=" + new simpledateformat().format(new date( history.getlastnewresulttime())) + ", read=" + history.getnumread() + " ,unread=" + history.getnumunread() + ", error=" + history.getlasterror() + " [af=" + history.isauthfail() + "]" ); try{ json_area.settext( subs.getjson()); }catch( throwable e ){ e.printstacktrace(); } private label info_lab; private label info_lab2; private styledtext json_area; private composite controls; griddata grid_data = new griddata(griddata.fill_both ); composite.setlayoutdata(grid_data); formdata data; control area contentnetwork won't be null because a new browser context has the default content network detailsbrowser.set mainbrowser.seturl( (string)mainbrowser.getdata( "starturl" )); gudy, not tux, listener added erase it, so that it's only used once after the page loads string execafterloaddisplay = execafterload.replaceall("'","\\\\'"); search.execute("alert('injecting script : " + execafterloaddisplay + "');"); system.out.println("injection : " + execafterload + " (" + result + ")"); store the "css" match string in the search cdp browser object mainbrowser.setlayoutdata(data); see if end of edit process indicated by the subscription being re-downloaded on auto-mode todo auto-generated method stub"
com.aelitis.azureus.ui.swt.subscriptions.SubscriptionWizard "subscriptiondownloaddetails[] allsubscriptions = subscriptionutils.getallcacheddownloaddetails(); list notyetsubscribed = new arraylist(allsubscriptions.length); for(int i = 0 ; i  no subscriber 80 -> 1000 subscribers cancelbutton.settext(messagetext.getstring("button.cancel"));"
com.aelitis.azureus.ui.swt.toolbar.ToolBarItemSO "(non-javadoc) @see com.aelitis.azureus.ui.common.toolbaritem#triggertoolbaritem(long, java.lang.object) the textid to set textid toolbarview will dispose of skinobjects"
com.aelitis.azureus.ui.swt.UIConfigDefaultsSWTv3 "todo hack for demo purposes only! up to az > 3.0.0.2, we did not store the original version the on. another hack to fix up some 3.x versions thinking their first version was 2.5.0.0.. boolean virginswitch = config.getbooleanparameter("az3.virgin.switch", false); we can guess first version based on the default save path. in 3.0.0.0 to 3.0.0.3, we set it to else is 2.x. we don't want to change the defaults for 2.x people guess first version was 3.0.0.0 - 3.0.0.3, which used remove save path, which will default it to azureus' doc dir === defaults used by mainwindow by default, turn off some slidey warning since they are plugin configs, we need to set the default after the plugin sets the default "v3.starttab" didn't exist before 4209_b49 and is written at startup. use it as indicator to reset columns so beta ("big view" only). as a backup (in addition to), reset on first 4210 run reset 'big' columns, remove some tables that no longer exist az3 doesn't have a view->toolbar, so force enable"
com.aelitis.azureus.ui.swt.UIFunctionsManagerSWT ""
com.aelitis.azureus.ui.swt.UIFunctionsSWT "@return    creates the main application menu and attach it to the given shell; this is only used for osx so that we can attach the global menu to popup dialogs which is the expected behavior on osx. windows and linux do not require this since they do not have a global menu and because their main menu is already attached to the main application window. @return  @return"
com.aelitis.azureus.ui.swt.UISkinnableManagerSWT "key: id; value: arraylist of uiskinnableswtlistener"
com.aelitis.azureus.ui.swt.UISkinnableSWTListener ""
com.aelitis.azureus.ui.swt.uiupdater.UIUpdaterSWT "calculate timer statistics for gui update @see org.gudy.azureus2.core3.util.aethread2#run() system.out.println("nothing visible!"); inactive used to mean "active shell is not mainwindow", but now that this is more generic (can be used with any shell), inactive means "active shell is not one of our app's" @see org.gudy.azureus2.core3.config.parameterlistener#parameterchanged(java.lang.string) @see com.aelitis.azureus.ui.swt.utils.uiupdater#addupdater(com.aelitis.azureus.ui.swt.utils.uiupdatable) @see com.aelitis.azureus.ui.swt.utils.uiupdater#removeupdater(com.aelitis.azureus.ui.swt.utils.uiupdatable) @see com.aelitis.azureus.ui.swt.utils.uiupdater#stopit() limit to 20. gives slightly scewed averages, but doesn't require storing all 20 values and averaging them each time system.out.println(systemtime.getcurrenttime() + "] refresh " + ttl + "ms");"
com.aelitis.azureus.ui.swt.utils.ColorCache "@return  @return hsb[2] = colors.difflumpct; hex color string"
com.aelitis.azureus.ui.swt.utils.ColorCache2 "system.out.println( "cc ++: color=" + color + ", refs=" + ref_count ); system.out.println( "cc --: color=" + color + ", refs=" + ref_count );"
com.aelitis.azureus.ui.swt.utils.FontUtils "can be null @return change the height of the installed font and takes care of disposing the new font when the control is disposed one or both of swt.bold, swt.italic, or swt.normal hack.. this isn't accurate, but gets us close hack.. this isn't accurate, but gets us close hack.. this isn't accurate, but gets us close system.out.println("yay " + size + " = " + gc.textextent(utils.good_string).y + " (want " + heightinpixels + ")");"
com.aelitis.azureus.ui.swt.utils.TorrentUIUtilsV3 "no clue if we have a easy way to add a totorrent to the gm, so here it is @return retrieves the thumbnail for the content, pulling it from the web if it can when the thumbnail is available, this listener is triggered the image is immediately available, the image will be returned as well as the trigger being fired. if the image isn't available null will be returned and the listener will trigger when avail if ((image == null || image.isdisposed()) && thumbnailurl != null) { //system.out.println("get image from " + thumbnailurl); image = imageloader.geturlimage(thumbnailurl, new imagedownloaderlistener() { public void imagedownloaded(image image, boolean returnedimmediately) { l.contentimageloaded(image, returnedimmediately); //system.out.println("got image from thumburl"); } }); //system.out.println("returning " + image + " (url loading)"); return image == null ? null : new image[] { image }; } try to get an image from the os image was also returned from getcontentimage catches http://www.vuze.com/download/chjw43pls277rc7u3s5xrs2pz4uug7rs.torrent open player as for open player but don't actually open it open player as for open player but don't actually open it if it's going to our urls, add some extra authenication do a quick check to see if it's a torrent security: only allow torrents from whitelisted trackers system.out.println("thumburl= " + thumbnailurl); system.out.println("return thumburl"); add torrent size here to differentiate meta-data downloads from actuals system.out.println("image = " + image); system.out.println("imagebytes = " + imagebytes); system.out.println("get image from " + thumbnailurl); system.out.println("got image from thumburl"); system.out.println("returning " + image + " (url loading)"); system.out.println("build image from files"); system.out.println("has mystery image");"
com.aelitis.azureus.ui.swt.utils.UIMagnetHandler "_core"
com.aelitis.azureus.ui.swt.views.PieceGraphView "_pieces @return  todo: buffer @see org.gudy.azureus2.ui.swt.views.abstractiview#initialize(org.eclipse.swt.widgets.composite) @see org.gudy.azureus2.ui.swt.views.abstractiview#datasourcechanged(java.lang.object) @see org.gudy.azureus2.ui.swt.views.abstractiview#refresh() since calc above doesn't account for not splitting squares across rows, make sure we can fit. if we can't, we have to shrink system.out.println((float)(bounds.width bounds.height) / numpieces); system.out.println(block_size + ";" + (bounds.width / block_size)); canvas.setbackground(colorcache.getcolor(canvas.getdisplay(), "#1b1b1b")); ignore ignore system.out.println("clear " + img); find upload pieces we'll have duplicates system.out.println("numpiecespersquare=" + numpiecespersquare); ignore if (count > 1) system.out.println("!!! " + startno); system.out.println(startno + ";" + count); system.out.print(pctdone + ";"); !done gc.fillroundrectangle(ixpos + q, iypos + q, size, size, size, size); system.out.println("redraws " + numredraws); canvas.redraw(); !done todo auto-generated method stub"
com.aelitis.azureus.ui.swt.views.skin.Browse "(non-javadoc) @see com.aelitis.azureus.ui.swt.views.skinview#showsupport(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.swt.skin.swtskinobjectadapter#skinobject browserskinobject.restart();"
com.aelitis.azureus.ui.swt.views.skin.InfoBarUtil "migrate existing state config to remembered decision manager so can get them back @see com.aelitis.azureus.ui.swt.skin.swtskinbuttonutility.buttonlisteneradapter#pressed(com.aelitis.azureus.ui.swt.skin.swtskinbuttonutility, com.aelitis.azureus.ui.swt.skin.swtskinobject, int)"
com.aelitis.azureus.ui.swt.views.skin.MyTorrentsView_Big "setforceheadervisible(true); @see org.gudy.azureus2.ui.swt.views.mytorrentsview#defaultselected(com.aelitis.azureus.ui.common.table.tablerowcore[])"
com.aelitis.azureus.ui.swt.views.skin.SB_Transfers "@return  numseeding + " of " + numcomplete; + " of " + numincomplete;"
com.aelitis.azureus.ui.swt.views.skin.SB_Vuze ""
com.aelitis.azureus.ui.swt.views.skin.SBC_ActivityTableView "@see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjectinitialshow(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.common.table.tableselectionadapter#selected(com.aelitis.azureus.ui.common.table.tablerowcore[]) @see com.aelitis.azureus.ui.swt.skin.swtskinobjectadapter#skinobjectdestroyed(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.common.updater.uiupdatable#getupdateuiname() @see com.aelitis.azureus.ui.common.updater.uiupdatable#updateui() @see com.aelitis.azureus.util.vuzenewslistener#vuzenewsentriesadded(com.aelitis.azureus.util.vuzenewsentry[]) @see com.aelitis.azureus.util.vuzenewslistener#vuzenewsentriesremoved(com.aelitis.azureus.util.vuzenewsentry[]) @see com.aelitis.azureus.util.vuzeactivitieslistener#vuzenewsentrychanged(com.aelitis.azureus.util.vuzeactivitiesentry) put titleinfo in another class"
com.aelitis.azureus.ui.swt.views.skin.SBC_ActivityView "skinconstants.viewid_sidebar_activity_big, "activity.table.big", @see com.aelitis.azureus.ui.swt.views.skin.skinview#showsupport(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object)"
com.aelitis.azureus.ui.swt.views.skin.SBC_BurnFTUX ""
com.aelitis.azureus.ui.swt.views.skin.SBC_GenericBrowsePage ""
com.aelitis.azureus.ui.swt.views.skin.SBC_LibraryTableView "classic my torrents view wrapped in a skinview return either mode_smalltable or mode_bigtable subclasses may override @return returns whether the big version of the tables should be used subclasses may override @return returns the appropriate set of columns for the completed or incomplete torrents views subclasses may override to return different sets of columns @return columns not needed for small mode, all torrents omg! show details! i <3 you! show in explorer launch fallback @see com.aelitis.azureus.ui.swt.utils.uiupdatable#getupdateuiname() @see com.aelitis.azureus.ui.swt.utils.uiupdatable#updateui() @see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjectshown(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.swt.views.skin.skinview#skinobjecthidden(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) currently stream and play are handled by toolbarview.. @see com.aelitis.azureus.ui.swt.skin.swtskinobjectadapter#skinobjectdestroyed(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see org.gudy.azureus2.ui.swt.debug.obfusticateimage#obfusticatedimage(org.eclipse.swt.graphics.image, org.eclipse.swt.graphics.point)"
com.aelitis.azureus.ui.swt.views.skin.SBC_LibraryTableView_Big ""
com.aelitis.azureus.ui.swt.views.skin.SBC_LibraryView "(non-javadoc) @see org.gudy.azureus2.plugins.ui.uipluginviewtoolbarlistener#refreshtoolbaritems(java.util.map) (non-javadoc) @see org.gudy.azureus2.plugins.ui.toolbar.uitoolbaractivationlistener#toolbaritemactivated(com.aelitis.azureus.ui.common.toolbaritem, long, java.lang.object) swtskinobject sooldviewarea = skin.getskinobjectbyid(modeids[oldviewmode]); @see com.aelitis.azureus.ui.swt.views.skin.skinview#showsupport(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.swt.views.skin.sbc_libraryview.countrefreshlistener#countrefreshed(com.aelitis.azureus.ui.swt.views.skin.sbc_libraryview.stats, com.aelitis.azureus.ui.swt.views.skin.sbc_libraryview.stats) azureuscore core = azureuscorefactory.getsingleton(); sowait.getcontrol().getparent().getparent().getparent().layout(true, true); @see com.aelitis.azureus.ui.swt.skin.swtskinobjectadapter#skinobjecthidden(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object)"
com.aelitis.azureus.ui.swt.views.skin.SBC_PlusFTUX ""
com.aelitis.azureus.ui.swt.views.skin.SearchResultsTabArea "(non-javadoc) @see com.aelitis.azureus.ui.swt.views.skinview#showsupport(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) final swtskintabset tabsetmain = skin.gettabset(skinconstants.tabset_main); if (tabsetmain != null) { final swtskinobjecttab tab = tabsetmain.gettab(skinconstants.viewid_searchresults_tab); if (tab != null) { swtskinobjectlistener l = new swtskinobjectlistener() { public object eventoccured(swtskinobject skinobject, int eventtype, object params) { if (eventtype == swtskinobjectlistener.event_select) { tab.setvisible(tabsetmain.getactivetab() == tab); } return null; } }; tab.addlistener(l); } }  gudy, not tux, listener added erase it, so that it's only used once after the page loads string execafterloaddisplay = execafterload.replaceall("'","\\\\'"); search.execute("alert('injecting script : " + execafterloaddisplay + "');"); system.out.println("injection : " + execafterload + " (" + result + ")"); store the "css" match string in the search cdp browser object todo auto-generated method stub todo auto-generated method stub"
com.aelitis.azureus.ui.swt.views.skin.sidebar.SideBar "private void addtestmenus() { // add some test menus pluginmanager pm = azureuscorefactory.getsingleton().getpluginmanager(); plugininterface pi = pm.getdefaultplugininterface(); uimanager uim = pi.getuimanager(); menumanager menumanager = uim.getmenumanager(); menuitem menuitem = menumanager.addmenuitem("sidebar", "test menu"); menuitem.addlistener(new menuitemlistener() { public void selected(menuitem menu, object target) { toolbarview tb = (toolbarview) skinviewmanager.getbyclass(toolbarview.class); if (tb != null) { system.out.println("found download toolbar"); toolbaritem dlitem = tb.gettoolbaritem("download"); system.out.println("download toolbar item is " + dlitem); if (dlitem != null) { system.out.println(dlitem.getskinbutton().getskinobject()); } dlitem.setenabled(!dlitem.isenabled()); } if (target instanceof sidebarentry) { sidebarentry info = (sidebarentry) target; system.err.println(info.getid() + " of " + info.getparentid() + ";ds=" + info.getdatasource()); } } }); menuitem = menumanager.addmenuitem("sidebar." + sidebar_section_activities, "activity only menu"); menuitem.addlistener(new menuitemlistener() { public void selected(menuitem menu, object target) { if (target instanceof sidebarentry) { sidebarentry info = (sidebarentry) target; system.err.println(info.getid() + " of " + info.getparentid() + ";ds=" + info.getdatasource()); } } }); }      @see com.aelitis.azureus.ui.swt.mdi.basemdi#createentryfromeventlistener(java.lang.string, org.gudy.azureus2.ui.swt.plugins.uiswtvieweventlistener, java.lang.string, boolean, java.lang.object) (non-javadoc) @see com.aelitis.azureus.ui.mdi.multipledocumentinterface#loadentrybyid(java.lang.string, boolean, boolean)  need to use paint even on cocoa, because there's cases where an area will become invalidated and we don't get a paintitem :( @see com.aelitis.azureus.ui.swt.skin.swtskinobjectadapter#skinobject addtestmenus(); don't think this is required as the sidebar constructor (well skinview) registers it f9 is standard seamonkey, but doesn't work on osx command option t is standard on osx f7 works on both add some test menus @see com.aelitis.azureus.ui.swt.views.skin.skinview#showsupport(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) after a scroll we need to recalculate the hit areas as they will have moved! tree.getclientarea().width; system.out.println("paintitem: " + event.item + ";" + event.index + ";" + event.detail + ";" + id + ";" + event.getbounds() + ";" + event.gc.getclipping()); system.out.println("paint: " + event.getbounds() + ";" + event.detail + ";" + event.index + ";" + event.gc.getclipping() + " " + debug.getcompressedstacktrace()); null itembounds is weird, the entry must be disposed. it happened once, so let's check.. system.out.println("paint " + id + " @ " + newclip); event.detail &= ~swt.foreground; event.detail &= ~(swt.foreground | swt.background); prevent "jumping" in the case where selection is off screen setselection would jump the item on screen, and then showitem would jump back to where the sethitarea needs it relative to entry note: on windows, for icons for cursor to disable collapsing @see org.eclipse.swt.dnd.droptargetadapter#dragover(org.eclipse.swt.dnd.droptargetevent) @see org.eclipse.swt.dnd.droptargetadapter#dragleave(org.eclipse.swt.dnd.droptargetevent) must dispose in an asyncexec, otherwise swt.selection doesn't get fired (async workaround provided by eclipse bug #87678) must dispose in an asyncexec, otherwise swt.selection doesn't get fired (async workaround provided by eclipse bug #87678) ind = "\t" + o; system.out.println("createitem " + id + ";" + debug.getcompressedstacktrace()); system.out.println("entry " + id + " is going to go below " + entryabove.getid() + " at " + index); system.out.println("showentry " + newentry.getid() + "; was " + (oldentry == null ? "null" : oldentry.getid()) + " via " + debug.getcompressedstacktrace()); show new hide old hack: seteventlistner will create the uiswtview. we need to have the entry available for the view to use if it wants @see com.aelitis.azureus.ui.swt.mdi.basemdi#createentryfromskinref(java.lang.string, java.lang.string, java.lang.string, java.lang.string, com.aelitis.azureus.ui.common.viewtitleinfo.viewtitleinfo, java.lang.object, boolean, java.lang.string) @see com.aelitis.azureus.ui.swt.utils.uiupdatable#updateui() @see com.aelitis.azureus.ui.swt.mdi.multipledocumentinterfaceswt#getentryfromskinobject(org.gudy.azureus2.ui.swt.plugins.pluginuiswtskinobject) @see org.gudy.azureus2.ui.swt.debug.obfusticateimage#obfusticatedimage(org.eclipse.swt.graphics.image)"
com.aelitis.azureus.ui.swt.views.skin.sidebar.SideBarEntrySWT "(non-javadoc) @see com.aelitis.azureus.ui.mdi.mdientry#redraw() (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#setexpanded(boolean) (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#close() (non-javadoc) @see com.aelitis.azureus.ui.swt.mdi.basemdientry#show() if (selected) { color1 = colorcache.getcolor(gc.getdevice(), colors[0]); color2 = colorcache.getcolor(gc.getdevice(), colors[1]); pattern = new pattern(gc.getdevice(), 0, starty, 0, starty + height, color1, 127, color2, 4); } else { color1 = colorcache.getcolor(gc.getdevice(), colors[2]); color2 = colorcache.getcolor(gc.getdevice(), colors[3]); pattern = new pattern(gc.getdevice(), 0, starty, 0, starty + height, color1, color2); } gc.setbackgroundpattern(pattern); private color colorfocus; colorfocus = skinproperties.getcolor("color.sidebar.focus"); some/all oses will auto-set treeitem's expanded flag to false if there is no children. to workaround, we store expanded state internally and set parent to expanded when a child is added @see org.gudy.azureus2.plugins.ui.sidebar.sidebarentry#addvitalityimage(java.lang.string) system.out.println("redraw " + thread.currentthread().getname() + ":" + getid() + " via " + debug.getcompressedstacktrace()); ignore npe. osx seems to be spewing this when the tree size is 0 or is invisible or something like that tree.update(); on osx, we get erroneous npe here: at org.eclipse.swt.widgets.tree.sendmeasureitem(tree.java:2443) at org.eclipse.swt.widgets.tree.cellsize(tree.java:274) at org.eclipse.swt.widgets.display.windowproc(display.java:4750) at org.eclipse.swt.internal.cocoa.os.objc_msgsend_stret(native method) at org.eclipse.swt.internal.cocoa.nscell.cellsize(nscell.java:34) at org.eclipse.swt.widgets.treeitem.getbounds(treeitem.java:467) walk up and make sure parents are expanded remove immediately from mdi because disposal is on a delay dispose will trigger dispose listener, which removed it from basemdi on osx, swt does some misguided exceptions on disposal of treeitem we occasionally get swtexception of "widget is disposed" or "argument not valid", as well as npes wrap skinref with a container that we control visibility of (invisible by default) viewcomposite.setbackground(parent.getdisplay().getsystemcolor( swt.color_widget_background)); viewcomposite.setforeground(parent.getdisplay().getsystemcolor( swt.color_widget_foreground)); force layout data of iview's composite to griddata, since we set the parent to gridlayout (most plugins use grid, so we stick with that instead of form) now that we have an iview, go through show one more time control == null ensure show order by fixes case where two showentries are called, the first from a non swt thread, and the 2nd from a swt thread. the first one will run last showing itself system.out.println(system.currenttimemillis() + "] paint " + getid() + ";sel? " + ((event.detail & swt.selected) > 0)); point size = event.gc.textextent(text); rectangle treebounds = tree.getbounds(); gc.setclipping((rectangle) null); system.out.println(system.currenttimemillis() + "] refresh " + getid() + "; " + itembounds + ";clip=" + event.gc.getclipping() + ";eb=" + event.getbounds()); point mintextsize = gc.textextent("99"); if (textsize.x  0; system.out.println("gmmm" + drawbounds + ": " + debug.getcompressedstacktrace()); always need to start gradient at the same y position +3 is to start gradient off 3 pixels lower in theory, the disposal of swtitem will trigger the disposal of the children. let's force it just in case on osx this will cause disposal confusion in swt, and possibly result in a sigsegv crash. swtitem can get set to null between the above test and here... delay saving of removing of auto-open flag. if after the delay, we are still alive, it's assumed the remove the auto-open flag even though execthreadlater will not run on close of app because the display is disposed, do a double check of tree disposal just in case. we don't want to trigger close listeners or remove autoopen parameters if the opposed to closing the sidebar) osx doesn't select a treeitem after closing an existing one force selection todo: bounds check @see org.gudy.azureus2.ui.swt.debug.obfusticateimage#obfusticatedimage(org.eclipse.swt.graphics.image)"
com.aelitis.azureus.ui.swt.views.skin.sidebar.SideBarToolTips "initialize  ignore for pre 3.0 swt.. ignore for pre 3.0 swt.. @return fall through switch handlevent() we don't get mouse down notifications on trim or borders.. compute size on label instead of shell because label calculates wrap, while shell doesn't @see com.aelitis.azureus.ui.common.updater.uiupdatable#getupdateuiname() @see com.aelitis.azureus.ui.common.updater.uiupdatable#updateui()"
com.aelitis.azureus.ui.swt.views.skin.sidebar.SideBarVitalityImageSWT "sidebarentry relative to entry  currentanimationindex @return the delaytime to set delaytime @see org.gudy.azureus2.plugins.ui.sidebar.sidebarvitalityimage#getimageid() @see org.gudy.azureus2.plugins.ui.sidebar.sidebarvitalityimage#addlistener(org.gudy.azureus2.plugins.ui.sidebar.sidebarvitalityimagelistener) @see org.gudy.azureus2.plugins.ui.sidebar.sidebarvitalityimage#settooltip(java.lang.string) @see org.gudy.azureus2.plugins.ui.sidebar.sidebarvitalityimage#getvisible() @see org.gudy.azureus2.plugins.ui.sidebar.sidebarvitalityimage#setvisible(boolean) system.out.println("gonna redraw because of " + mdientry.getid() + " set to " + this.visible + " via " + debug.getcompressedstacktrace() );"
com.aelitis.azureus.ui.swt.views.skin.SkinnedDialog "creates a dialog (shell) and fills it with a skinned layout   shell skin.destroy;"
com.aelitis.azureus.ui.swt.views.skin.SkinView "converts {@link swtskinobjectlistener} events to method calls, and ensures we only "show" (initialize) once.  available skinviews are managed by {@link skinviewmanager}  visible @return @see com.aelitis.azureus.ui.swt.skin.swtskinobjectadapter#skinobjecthidden(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) @see com.aelitis.azureus.ui.swt.skin.swtskinobjectadapter#skinobjectdestroyed(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object)"
com.aelitis.azureus.ui.swt.views.skin.SkinViewManager "manages a list of skinviews currently in use by the app map skinobjectid to skin view  gets the first skinview return all added skinviews of a certain class @return get the skinview related to a skinobjectid @return get the skinview related to a view id @return listen in on skinview adds"
com.aelitis.azureus.ui.swt.views.skin.ToolBarView "// ==start item = new toolbaritemso(this, "start", "image.toolbar.start", "iconbar.start"); item.setdefaultactivation(new uitoolbaractivationlistener() { public boolean toolbaritemactivated(toolbaritem item, long activationtype) { if (activationtype != activationtype_normal) { return false; } downloadmanager[] dms = selectedcontentmanager.getdmsfromselectedcontent(); if (dms != null) { torrentutil.queuedatasources(dms, true); return true; } return false; } }); addtoolbaritem(item, "toolbar.area.sitem", so2nd); //swtskinobjectcontainer so = (swtskinobjectcontainer) item.getskinbutton().getskinobject(); //so.setdebugandchildren(true); addseperator(so2nd); // ==stop item = new toolbaritemso(this, "stop", "image.toolbar.stop", "iconbar.stop"); item.setdefaultactivation(new uitoolbaractivationlistener() { public boolean toolbaritemactivated(toolbaritem item, long activationtype) { if (activationtype != activationtype_normal) { return false; } iselectedcontent[] currentcontent = selectedcontentmanager.getcurrentlyselectedcontent(); torrentutil.stopdatasources(currentcontent); return true; } }); addtoolbaritem(item, "toolbar.area.sitem", so2nd); addseperator(so2nd);  (non-javadoc) @see org.gudy.azureus2.plugins.ui.toolbar.uitoolbarmanager#createtoolbaritem(java.lang.string) the showtext to set showtext private globalmanager gm; @see com.aelitis.azureus.ui.swt.views.skin.skinview#showsupport(com.aelitis.azureus.ui.swt.skin.swtskinobject, java.lang.object) ==open ==download this is for our cdp pages ==play ==open ==search ==run addtoolbaritem(item, "toolbar.area.sitem", so2nd); ==top ==up ==down ==bottom ==start swtskinobjectcontainer so = (swtskinobjectcontainer) item.getskinbutton().getskinobject(); so.setdebugandchildren(true); ==stop ==startstop ==remove ///////////////////// == mode big == mode small addseperator(so2nd); updatecoreitems(currentcontent, viewid); don't trust them plugins so in files view we can have multiple selections that map onto the same download manager - ensure that we only add the listener once! system.out.println( "added " + dm.getdisplayname() + " - size=" + dm_listener_map.size()); system.out.println( "removed " + dm.getdisplayname() + " - size=" + dm_listener_map.size()); allow a tool-bar enabler to manually handle play/stream events fallback to handle start/stop settings when no explicit selected content (e.g. for devices transcode view) addnontoolbar("toolbar.area.sitem.left2", so2nd); take the last item and change it to right utils.relayout(control);"
com.aelitis.azureus.ui.swt.views.skin.TorrentListViewsUtils "0=good, 1 = fail, 2 = abandon new version accepts map with asx parameters. if the params are null then is uses the old version to start the player. if the - downloadmanager - int: 0 = ok, 1 = fail, 2 = abandon, installation in progress -  plays or streams a download _index index of file in torrent to play. -1 to auto-pick "best" play file. handle encapsulated vuze file we want to re-download the torrent if it's ours, since the existing one is likely stale vuzeplayer (2011) calls this if (!canplay(dm)) { return false; } fallback to normal we used to pop up a dialog saying we didn't know how to play the file but now the play toolbar and play 'default' action aren't even enabled if we can't use emp. so there's no point. assumed we have a core, since we are passed a downloadmanager fall through here if old emp"
com.aelitis.azureus.ui.swt.views.skin.VuzeMessageBox "(non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.uifunctions (non-javadoc) @see com.aelitis.azureus.ui.swt.views.skin.skinneddialog.skinneddialogclosedlistener#skindialogclosed(com.aelitis.azureus.ui.swt.views.skin.skinneddialog) @deprecated since buttons can swap around, you should use {@link #closewithbuttonval(int)} else if (cancelpos != buttons.length - 1) { // todo: move to end catch someone calling close() while we are opening skin.debuglayout = true; need to defer setting the default button to here as otherwise it doesn't work (on windows at least...) fix button bg not right on win7 on swt thread, so execswtthread just ran and we should have a shell"
com.aelitis.azureus.ui.swt.views.skin.VuzeMessageBoxListener ""
com.aelitis.azureus.ui.swt.views.skin.WelcomeView ""
com.aelitis.azureus.ui.swt.views.TopBarView "building needs uiswtinstance, which needs core. trigger autobuild todo actually use plugins.. system.out.println("prev click " + activetopbar + " ; " + topbarviews.size()); system.out.println("next click"); font font = new font(e.gc.getdevice(), "sans", 8, swt.normal); e.gc.setfont(font); ignore error_no_graphics_library error or any others e.gc.drawtext(s, 0,0, true); settransform can trhow a error_no_graphics_library error no use trying to draw.. it would look weird font.dispose(); skip, plugin probably specifically asked to not be added system.out.println("hello" + sotitle);"
com.aelitis.azureus.ui.swt.views.ViewDownSpeedGraph "downspeedgraphic.setautoalpha(true);"
com.aelitis.azureus.ui.swt.views.ViewTitleInfoBetaP ""
com.aelitis.azureus.ui.swt.views.ViewUpSpeedGraph "upspeedgraphic.setautoalpha(true);"
com.aelitis.azureus.ui.swt.widgets.AnimatedImage "wait time in ms"
com.aelitis.azureus.ui.UIFunctions "bring main window to the front bring main window to the front : try tricks to force it to the top change/refresh the language of the ui   request the ui be shut down. - request granted, ui is being shut down false - request denied (example: password entry failed) retrieves the class that handles periodically updating the ui @return  _id retrieve the mdi (sidebar, tabbedmdi) @return might launch the old-school mr slidey arg: string - url; response boolean - ok arg: boolean - true->no auto-select response boolean - ok"
com.aelitis.azureus.ui.UIFunctionsManager ""
com.aelitis.azureus.ui.UIFunctionsUserPrompter "returns the number milliseconds the prompt will wait around until it auto closes. timer starts after the returns the html that will be displayed along with the prompt  todo: create a boolean canhandlehtml() @return retrieves the remember id associated with this prompt id retrieves the text to be displayed by the "remember this action" checkbox text opens the prompt. returns when sets the # of milliseconds before auto closing. timer starts after the     determines if the prompt was auto closed after {@link #open()} was called, or if the image repository resource name (e.g. "error", "warning", "info")"
com.aelitis.azureus.ui.UIStatusTextClickListener ""
com.aelitis.azureus.ui.UserPrompterResultListener ""
com.aelitis.azureus.ui.utils.ImageBytesDownloader "system.out.println("download " + url);"
com.aelitis.azureus.util.ConstantsVuze "default_content_network"
com.aelitis.azureus.util.ContentNetworkUtils "get content network url based on service id. if service is not supported"
com.aelitis.azureus.util.DataSourceUtils "@return  getdm will check hash as well ignore"
com.aelitis.azureus.util.DLReferals ""
com.aelitis.azureus.util.DownloadUtils "if already exists then bail if prefix exists then remove existing value"
com.aelitis.azureus.util.ExternalStimulusHandler "debug.outnostack( "externalstimulus debug" ); system.out.println( "externalstimulus: " + name ); system.out.println(" " + (values == null ? -1 : values.size()) + " values: " + values);"
com.aelitis.azureus.util.ExternalStimulusListener "if not handled .min_value if not handled"
com.aelitis.azureus.util.FeatureUtils "@return debug.out("featman null"); if any of the feature details are still valid, we have a full if any of the featuredetails is a trial, return true maybe not added yet.. use featman"
com.aelitis.azureus.util.ImportExportUtils "note: there's a similarly defined map processing utility class called {@link maputils}. since there are differences in implementation, both have been kept until someone goes through each callee and check if it can be switched to use just one of them."
com.aelitis.azureus.util.InitialisationFunctions "need to get in early to ensure property present on initial announce unfortunately the has-been-opened state is updated by azureus when a but is also preserved across torrent export/import (e.g. when downloaded via magnet url. so reset it here if it is found to be set only add the azid to platform content allow the tracker to manipulate peer sources for dead/un"
com.aelitis.azureus.util.JSONUtils "decodes json formatted text into a map. parsed from a json formatted string  if the json text is not a map, a map with the key "value" will be returned. the value of "value" will either be an list, string, number, boolean, or null  if the string is formatted badly, null is returned encodes a map into a jsonobject.  it's recommended that you use {@link #encodetojson(map)} instead @return encodes a map into a json formatted string.  handles multiple layers of maps and lists. handls string, number, boolean, and null values. map to change into a json formatted string formatted string @return could be : arraylist, string, number, boolean"
com.aelitis.azureus.util.LocalResourceHTTPServer ""
com.aelitis.azureus.util.MapUtils "note: the above returns def when map doesn't contain the key, which suggests below we would return the null when o is null. but we don't! and now, some callers rely on this :("
com.aelitis.azureus.util.NavigationHelper "guarantee delivery to at least one listener by queueing if none possible duplicate delivery - assumed not a problem home tab l_args1.add( skinconstants.viewid_home_tab ); activity tab l_args2.add( skinconstants.viewid_activity_tab ); check plugin available"
com.aelitis.azureus.util.PlayUtils "access to this static is deprecated - use get/setplayablefileextensions. for legacy emp we need to keep it public for the moment... @return public static string getcontenturl(downloadmanager dmcontent) { string contentpath; if (dmcontent.isdownloadcomplete(false)) { //use the file path if download is complete. org.gudy.azureus2.core3.disk.diskmanagerfileinfo primaryfile = dmcontent.getdownloadstate().getprimaryfile(); if (primaryfile == null) { return null; } file file = primaryfile.getfile(true); try { contentpath = file.tourl().tostring(); } catch (malformedurlexception e) { contentpath = file.getabsolutepath(); } } else { //use the stream path if download is not complete. contentpath = playutils.getmediaservercontenturl(dmcontent); } return contentpath; } public static string getmediaservercontenturl(downloadmanager dm) { try { return playutils.getmediaservercontenturl(downloadmanagerimpl.getdownloadstatic(dm)); } catch (downloadexception e) { } return null; }  public static string getmediaservercontenturl(download dl) { //torrentlistviewsutils.debugdcad("enter - getmediaservercontenturl"); pluginmanager pm = azureuscorefactory.getsingleton().getpluginmanager(); plugininterface pi = pm.getplugininterfacebyid("azupnpav", false); if (pi == null) { logger.log(new logevent(logids.ui3, "media server plugin not found")); return null; } if (!pi.getpluginstate().isoperational()) { logger.log(new logevent(logids.ui3, "media server plugin not operational")); return null; } try { program program = program.findprogram(".qtl"); boolean hasquicktime = program == null ? false : (program.getname().tolowercase().indexof("quicktime") != -1); pi.getipc().invoke("setquicktimeavailable", new object[] { new boolean(hasquicktime) }); object url = pi.getipc().invoke("getcontenturl", new object[] { dl }); if (url instanceof string) { return (string) url; } } catch (throwable e) { logger.log(new logevent(logids.ui3, logevent.lt_warning, "ipc to media server plugin failed", e)); } return null; } private static final boolean isexternalempinstalled() { if(!loademppluginclass()) { return false; } if (methodisexternalplayerinstalled == null) { return false; } try { object retobj = methodisexternalplayerinstalled.invoke(null, new object[] {}); if (retobj instanceof boolean) { return ((boolean) retobj).booleanvalue(); } } catch (throwable e) { e.printstacktrace(); if (e.getmessage() == null || !e.getmessage().tolowercase().endswith("only")) { debug.out(e); } } return false; } this method available for player plugins to extend playable set if needed @deprecated but still used by emp deprecated but here for older versions of vzemp that still refer to it private static method methodisexternalplayerinstalled; not complete stream stuff use the file path if download is complete. use the stream path if download is not complete. torrentlistviewsutils.debugdcad("enter - getmediaservercontenturl"); torrentlistviewsutils.debugdcad("enter - getmediaservercontenturl");"
com.aelitis.azureus.util.StringCompareUtils ""
com.aelitis.azureus.util.UrlFilter "ensure whitelist has important network urls private string rpc_whitelist = "azmsg%3b[0-9]+%3b."; for +1 button"
com.aelitis.net.magneturi.impl.MagnetURIHandlerClient "limit the subset here as we're looping waiting for something to be alive and we can't afford to take ages getting back to the start 40 x 40 image is encoded as 134 bytes..."
com.aelitis.net.magneturi.impl.MagnetURIHandlerImpl "see http://magnet-uri.sourceforge.net/magnet-draft-overview.txt no free sockets, not much we can do leave client to close socket if not requested system.out.println( "get = " + get ); magnet:?xt=urn:sha1:ynckhtqcwbtrnjiv4wnae52sjuqczo5c hack: don't change the "error:" message below, it is used by torrentdownloader to detect this condition pause on error don't remove the "error:" (see above) debug.printstacktrace(e); pause on error no idea why we copy, but let's keep doing so no value, see if we have a default have a value, see if we have a max need to trim if too large see if we need to div/mod for clients that don't support huge images e.g. http://localhost:45100/getinfo?name=plugin.azupnpav.content_port&mod=8 divmod -> encode div+1 as width, mod+1 as height no idea why we copy, but let's keep on doing so don't change any of this without (at least) changing the jws launcher code as it manually decodes the bmp to determine its size!!!! data pos header size 1 plane and 1 bpp color"
com.aelitis.net.magneturi.MagnetURIHandler ""
com.aelitis.net.magneturi.MagnetURIHandlerException ""
com.aelitis.net.magneturi.MagnetURIHandlerListener ""
com.aelitis.net.magneturi.MagnetURIHandlerProgressListener ""
com.aelitis.net.natpmp.impl.NatPMPDeviceImpl "main class length of requests in bytes length of replies in bytes current result codes not network failure (e.g. nat box itself has not obtained a dhcp lease) instance specific globals singleton creation send a request and wait for reply this class should be threaded!!! this sends to the default natpmp_port. destination address (should be the private nat address) packet to send byte buffer big enough to hold received try to connect with a nat-pmp device. this could take sometime. if it found one datagrampacket recpkt = set the global nat public address set the global nat epoch time (in seconds) to do: set up listner for announcements from the device for address changes (public address changes) asks for a public port to be mapped to a private port from this host. nap-pmp allows the device to assign another public port if the requested one is taken. so, you should check the returned port. true tcp, false udp returned publicport. -1 if error occured @todo either take a class (like upnpmapping) or return a class delete a mapped public port true tcp, false udp port the public port to close the private port that it is mapped to @warn untested if the request was successful, a zero lifetime will delete the mapping and return a public port of 0 int result = general port mapping protocol datagrampacket recpkt = should save the epoch. this can be used to determine the time the mapping will be deleted. bunch of conversion functions convert the byte array containing 32-bit to an int starting from the given offset. the byte array the array offset integer convert the byte array containing 16-bits to an int starting from the given offset. the byte array the array offset integer convert the byte array containing 8-bits to an int starting from the given offset. the byte array the array offset integer convert a 16-bit short into a 2 byte array byte array convert a 32-bit int into a 4 byte array byte array takes the host address the address as (xxx.xxx.xxx.1) ms gives us three tries lease life in seconds 24 hours link-local multicast address - for address changes not implemented opcodes used for .. ask for a nat-pmp device map a udp port map a tcp port 4 bytes by 3 out of resources unsupported opcode our address nat's private (interal) address nat's public address natpriinet network interface this gets updated each request how do we know we hit something? we have several tries at this (like 3) log("timed out!"); log( ste.getmessage() ); sleep before trying again this.sleep(retryinterval); not sleeping?!? increase retry interval check recrep for true!!! send nat request to find out if it is pmp happy int recver = unsigned8bytearraytoint( recbuf, 0 ); int recop = unsigned8bytearraytoint( recbuf, 1 ); check for actual connection! check for actual connection should check for errors - only using lower 2 bytes generate port map request packet ver op reserved - 2 bytes private port - 2 bytes requested public port - 2 bytes unpack this and check codes int recvers = unsigned8bytearraytoint( recbuf, 0 ); int recpriport = unsigned16bytearraytoint( recbuf, 8 ); assume router is at xxx.xxx.xxx.1 is there no printf in java?"
com.aelitis.net.natpmp.NatPMPDevice ""
com.aelitis.net.natpmp.NATPMPDeviceAdapter ""
com.aelitis.net.natpmp.NatPMPDeviceFactory ""
com.aelitis.net.natpmp.upnp.impl.NatPMPImpl "natpmp stuff azureus stuff azureus framework methods @see: ...service/upnpsswanconnectionimpl.java use public port for internal port find and remove old mapping, if any add new port to list delete from the mappings check upnpsswanconnectionimpl.java for hints can we ping the nat for this info? end protected class portmapping end public class natpmpimpl not synchronized! false -> udp"
com.aelitis.net.natpmp.upnp.impl.NatPMPUPnPImpl ""
com.aelitis.net.natpmp.upnp.impl.NatPMPUPnPRootDeviceImpl "gets a specific service if such is supported @return pretend to be an ip connection ??"
com.aelitis.net.natpmp.upnp.NatPMPUPnP ""
com.aelitis.net.natpmp.upnp.NatPMPUPnPFactory ""
com.aelitis.net.udp.mc.impl.MCGroupImpl "remove these diagnostic reports on win98 remove these diagnostic reports on win98 already established turn on loopback to see if it helps for local host upnp devices nah, turn it off again, it didn;t set up group windows 98 doesn't support settimetolive note that false enables loopback mode which is what we want now do the incoming control listener system.out.println( "local port = " + control_port ); have debugs showing the send-to-group operation hanging and blocking az close, make async system.out.println( "sendtogroup: ni = " + network_interface.getname() + ", data = " + new string(data)); have debugs showing the send-to-group operation hanging and blocking az close, make async system.out.println( "sendtogroup: ni = " + network_interface.getname() + ", data = " + new string(data)); introduce a timeout so that when a network interface changes we don't sit here blocking forever and thus never realise that we should shutdown system.out.println( "receive: add = " + local_address + ", data = " + new string( data, 0, len )); system.out.println( "sendtomember: add = " + address + ", data = " +new string( data ));"
com.aelitis.net.udp.mc.MCGroup "sends to the group but will replace any occurrence of %azinterface% in the string with the interface being used for the send _data"
com.aelitis.net.udp.mc.MCGroupAdapter ""
com.aelitis.net.udp.mc.MCGroupException ""
com.aelitis.net.udp.mc.MCGroupFactory ""
com.aelitis.net.udp.uc.impl.PRUDPPacketHandlerFactoryImpl "file : prudppacketreceiverfactoryimpl.java  only set the incoming request handler if one has been specified. this is important when the port is shared (e.g. default udp tracker and dht) and only one usage has need to handle unsolicited inbound requests as we don't want the tracker null handler to erase the dht's one only set the incoming request handler if one has been specified. this is important when the port is shared (e.g. default udp tracker and dht) and only one usage has need to handle unsolicited inbound requests as we don't want the tracker null handler to erase the dht's one"
com.aelitis.net.udp.uc.impl.PRUDPPacketHandlerImpl "file : prudppacketreceiverimpl.java  [2:01:55] debug::tue dec 07 02:01:55 est 2004 [2:01:55] java.net.socketexception: socket operation on nonsocket: timeout in datagram socket peek [2:01:55] at java.net.plaindatagramsocketimpl.peekdata(native method) [2:01:55] at java.net.datagramsocket.receive(unknown source) [2:01:55] at org.gudy.azureus2.core3.tracker.server.impl.udp.trtrackerserverudp.recvloop(trtrackerserverudp.java:118) [2:01:55] at org.gudy.azureus2.core3.tracker.server.impl.udp.trtrackerserverudp$1.runsupport(trtrackerserverudp.java:90) [2:01:55] at org.gudy.azureus2.core3.util.aethread.run(aethread.java:45) if we have an altprotocoldelegate then this shares the list of handlers so no need to add if we have an altprotocoldelegate then this shares the list of handlers so no need to remove if we need to support this then the handler will have to be associated with a message type map, or we chain together and give each handler a bite at processing the message outter loop picks up bind-ip changes one off attempt to recover by selecting an explicit one. on vista (at least) we sometimes fail with wildcard but succeeed with explicit (see http://forum.vuze.com/thread.jspa?threadid=77574&tstart=0) short timeout on receive so that we can interrupt a receive fairly quickly only make the socket public once fully configured primordial handlers get their own buffer as we can't guarantee that they don't need to hang onto the data on vista we get periodic socket closures can't guarantee there aren't situations where we get into a screaming closed loop so guard against this somewhat break, sometimes get a screaming loop. e.g. make sure we destroy the delegate too if something happend don't change the text of this message, it's used elsewhere hack alert. due to the form of the tracker udp protocol (no common header for requests and replies) we enforce a rule. all connection ids must have their msb set. as requests always start with the action, which always has the msb clear, we can use this to differentiate. system.out.println( "received:" + packet_len ); system.out.println( "incoming from " + dg_packet.getaddress()); we take the processing offline so that these incoming requests don't interfere with replies to outgoing requests by the time this request gets processed it'll have timed out in the caller anyway, so discard it don't remove the request if there are more replies to come if someone's sending us junk we just log and continue generally uninteresting e.printstacktrace();  so  =  +  where  = first 8 bytes of sha1( +  yes system.out.println("prudphandler - auth = " + auth.get system.out.println( "outgoing to " + dg_packet.getaddress()); synchronous write holding lock to block senders invariant: at least one queue must have an entry too many consecutive or too imbalanced, see if there are lower priority queues with entries mark as sent before sending in case send fails and we then rely on timeout to pick this up get occasional send fails, not very interesting system.out.println( "sent:" + buffer.length ); if the send is ok then the request will be removed from the queue either when a reply comes back or when it gets timed-out never got sent, remove it immediately amc: i've seen this in debug logs - just wonder where it's coming from. system.out.println( "outgoing to " + dg_packet.getaddress()); this is a reply to a request, no time delays considered here trim a bit off this limit to include processing time system.out.println( "outgoing to " + dg_packet.getaddress());"
com.aelitis.net.udp.uc.impl.PRUDPPacketHandlerRequestImpl "file : prudppackethandlerrequest.java  do something sensible with 0 time! don't override existing reply for synchronous callers as they can do what they want with it still report errors to asyn clients (even when a reply has been received) as they need something to indicate that a continuation packet wasn't received and that the request has timed-out. ie. a multi-packet reply must terminate either with the reception of a non-continuation (i.e. last) packet or a timeout/error"
com.aelitis.net.udp.uc.impl.PRUDPPacketHandlerSocks "socks 5 2 methods no auth  version byte auth   password length version byte deal with long "hostnames" that we get for, e.g., i2p destinations version udp associate reserved port ver special hack for internal socks servers just being used for plumbing connections for other protocols - 0x45 means 'go transparent' relay is null here - this drives other direct behaviour reserved use the maped ip for dns resolution so we don't leak the actual address if this is a secure one (e.g. i2p one) resv resv frag (none) address type = domain name address type = domain name port res res assume no frag port"
com.aelitis.net.udp.uc.impl.PRUDPPacketHandlerStatsImpl ""
com.aelitis.net.udp.uc.PRUDPPacket "file : prudppacket.java"
com.aelitis.net.udp.uc.PRUDPPacketHandler "file : prudppacketreceiver.java  asynchronous send and receive _packet _address @throws prudppackethandlerexception synchronous send and receive _packet _address @return @throws prudppackethandlerexception send only _packet _address @throws prudppackethandlerexception"
com.aelitis.net.udp.uc.PRUDPPacketHandlerException "file : prudppackethandlerexception.java"
com.aelitis.net.udp.uc.PRUDPPacketHandlerFactory "file : prudppacketreceiverfactory.java"
com.aelitis.net.udp.uc.PRUDPPacketHandlerRequest ""
com.aelitis.net.udp.uc.PRUDPPacketHandlerStats ""
com.aelitis.net.udp.uc.PRUDPPacketReceiver "receive failed - timeout"
com.aelitis.net.udp.uc.PRUDPPacketReply "file : prudppacketreply.java  add to this and you need to adjust header_size above"
com.aelitis.net.udp.uc.PRUDPPacketReplyDecoder ""
com.aelitis.net.udp.uc.PRUDPPacketRequest "file : prudppacketrequest.java  add to this and you need to adjust header_size above"
com.aelitis.net.udp.uc.PRUDPPacketRequestDecoder ""
com.aelitis.net.udp.uc.PRUDPPrimordialHandler ""
com.aelitis.net.udp.uc.PRUDPReleasablePacketHandler ""
com.aelitis.net.udp.uc.PRUDPRequestHandler ""
com.aelitis.net.upnp.impl.device.UPnPDeviceImageImpl ""
com.aelitis.net.upnp.impl.device.UPnPDeviceImpl "3com adsl 11g 1.07 http://www.3com.com/"
com.aelitis.net.upnp.impl.device.UPnPRootDeviceImpl "public static void main( string[] args ) { string[] test_current = { "1.11.2" }; string[] test_bad = { "1.11" }; for (int i=0;i " + isbadversion( test_current[i], test_bad[i] )); system.out.println( test_bad[i] + " / " + test_current[i] + " -> " + isbadversion( test_bad[i], test_current[i] )); } } "wrt54g", "any", report on fail true, // report always removed, apparently it works ok now according to manufacturer url_str is sometimes blank don't warn here, only warn on failures already absolute relative url base on the root document location comparator does a10 -vs- a9 correctly (i.e. 111 is > 20 ) comparator comp = getalphanumericcomparator( true ); look for a delimiter (non alpha/numeric)"
com.aelitis.net.upnp.impl.services.UPnPActionArgumentImpl ""
com.aelitis.net.upnp.impl.services.UPnPActionImpl ""
com.aelitis.net.upnp.impl.services.UPnPActionInvocationImpl "try standard post"
com.aelitis.net.upnp.impl.services.UPnPServiceImpl "absolute relative"
com.aelitis.net.upnp.impl.services.UPnPSSOfflineDownloaderImpl ""
com.aelitis.net.upnp.impl.services.UPnPSSWANCommonInterfaceConfigImpl ""
com.aelitis.net.upnp.impl.services.UPnPSSWANConnectionImpl "debug.printstacktrace(e); start off true to avoid logging first of repetitive failures some routers appear to continually fail to report the mappings - avoid reporting this false -> udp "" = wildcard for hosts, 0 = wildcard for ports 0 -> infinite (?) some routers won't add properly if the mapping's already there "" = wildcard for hosts, 0 = wildcard for ports only bitch about the failure if we believed we mapped it in the first place upnpstatevariable noe = service.getstatevariable("portmappingnumberofentries"); system.out.println( "noe = " + noe.getvalue()); integer.parseint( noe.getvalue()); some routers (e.g. gudy's) return 0 here whatever! in this case take mindless approach hmm, even for my router the state variable isn't accurate... i've also seen some routers loop here rather than failing when the index gets too large (they seem to keep returning the last entry) - check for a duplicate entry and exit if found repeat, get out"
com.aelitis.net.upnp.impl.services.UPnPSSWANIPConnectionImpl ""
com.aelitis.net.upnp.impl.services.UPnPSSWANPPPConnectionImpl ""
com.aelitis.net.upnp.impl.services.UPnPStateVariableImpl ""
com.aelitis.net.upnp.impl.ssdp.SSDPCore "notify http/1.1 host: 239.255.255.250:1900 cache-control: max-age=3600 location: http://192.168.0.1:49152/gateway.xml nt: urn:schemas-upnp-org:service:wanipconnection:1 nts: ssdp:byebye server: linux/2.4.17_mvl21-malta-mips_fp_le, upnp/1.0, intel sdk for upnp devices /1.2 usn: uuid:ab5d9077-0710-4373-a4ea-5192c8781666::urn:schemas-upnp-org:service:wanipconnection:1 notify http/1.1 host: 239.255.255.250:1900 cache-control: max-age=3600 location: http://192.168.0.1:49152/gateway.xml nt: urn:schemas-upnp-org:service:wanipconnection:1 nts: ssdp:byebye server: linux/2.4.17_mvl21-malta-mips_fp_le, upnp/1.0, intel sdk for upnp devices /1.2 usn: uuid:ab5d9077-0710-4373-a4ea-5192c8781666::urn:schemas-upnp-org:service:wanipconnection:1 http/1.1 200 ok cache-control: max-age=600 date: tue, 20 dec 2005 13:07:31 gmt ext: location: http://192.168.1.1:2869/gatedesc.xml server: linux/2.4.17_mvl21-malta-mips_fp_le upnp/1.0 st: upnp:rootdevice usn: uuid:uuid-internetgatewaydevice-1234::upnp:rootdevice example notify event if ( originator.getaddress().gethostaddress().equals( "192.168.0.135" )){ system.out.println( originator + ":" + str ); } gudy's root: http://192.168.0.1:5678/igd.xml, uuid:upnp-internetgatewaydevice-1_0-12345678900001::upnp:rootdevice, upnp:rootdevice parg's root: http://192.168.0.1:49152/gateway.xml, uuid:824ff22b-8c7d-41c5-a131-44f534e12555::upnp:rootdevice, upnp:rootdevice xbox throws us a '' on bootup if ( location != null && location.gethost().equals( "192.168.0.135")){ system.out.println( str ); } xbox doesn't play well with us doing mx properly, seems like the delay causes it not to pick up the response, grrrrr! server must be in this alpha-case for xbox to work (server doesn't)... only actually ever run of these at a time as they have been seen to back up and flood the timer pool location is null for byebye"
com.aelitis.net.upnp.impl.ssdp.SSDPIGDImpl "alive can be reported on any interface try and work out what bind address this location corresponds to not interested, loopback or other search"
com.aelitis.net.upnp.impl.SSDPIGD ""
com.aelitis.net.upnp.impl.SSDPIGDFactory ""
com.aelitis.net.upnp.impl.SSDPIGDListener ""
com.aelitis.net.upnp.impl.UPnPImpl "the use_http_connection flag is set to false sometimes to avoid using the urlconnection library for some dopey upnp routers. public static void main( string[] args ) { try{ upnp upnp = upnpfactory.getsingleton(null,null); // won't work with null .... upnp.addrootdevicelistener( new upnplistener() { public boolean devicediscovered( string usn, url location ) { return( true ); } public void rootdevicefound( upnprootdevice device ) { try{ processdevice( device.getdevice() ); }catch( throwable e ){ e.printstacktrace(); } } }); upnp.addloglistener( new upnploglistener() { public void log( string str ) { system.out.println( str ); } public void logalert( string str, boolean error, int type ) { system.out.println( str ); } }); thread.sleep(20000); }catch( throwable e ){ e.printstacktrace(); } } protected static void processdevice( upnpdevice device ) throws upnpexception { if ( device.getdevicetype().equalsignorecase("urn:schemas-upnp-org:device:wanconnectiondevice:1")){ system.out.println( "got device"); upnpservice[] services = device.getservices(); for (int i=0;i<services.length;i++){ upnpservice s = services[i]; if ( s.getservicetype().equalsignorecase( "urn:schemas-upnp-org:service:wanipconnection:1")){ system.out.println( "got service" ); upnpaction[] actions = s.getactions(); for (int j=0;j<actions.length;j++){ system.out.println( actions[j].getname()); } upnpstatevariable[] vars = s.getstatevariables(); for (int j=0;j<vars.length;j++){ system.out.println( vars[j].getname()); } upnpstatevariable noe = s.getstatevariable("portmappingnumberofentries"); system.out.println( "noe = " + noe.getvalue()); upnpwanipconnection wan_ip = (upnpwanipconnection)s.getspecificservice(); upnpwanconnectionportmapping[] ports = wan_ip.getportmappings(); wan_ip.addportmapping( true, 7007, "moo!" ); upnpaction act = s.getaction( "getgenericportmappingentry" ); upnpactioninvocation inv = act.getinvocation(); inv.addargument( "newportmappingindex", "0" ); upnpactionargument[] outs = inv.invoke(); for (int j=0;j<outs.length;j++){ system.out.println( outs[j].getname() + " = " + outs[j].getvalue()); } } } }else{ upnpdevice[] kids = device.getsubdevices(); for (int i=0;i<kids.length;i++){ processdevice( kids[i] ); } } } we need to take this operation off the main thread as it can take some time. this is a single concurrency queued thread pool so things get done serially in the right order system.out.println( "upnp: skipping discovery of " + usn + " as already pending (queue=" + device_dispatcher_pending.size() + ")" ); we remember one route to the device - if the network interfaces change we do a full reset so we don't need to deal with that here check that the device's location is the same something changed, resetablish everything not the best "atomic" code here but it'll do as the code that adds roots (this) is single threaded via the dispatcher we need to take this operation off the main thread as it can take some time assume utf-8 gudy's router was returning trailing nulls which then stuffed up the xml parser. hence this code to try and strip them remove any obviously invalid characters - i've seen some routers generate stuff like 0x18 which stuffs the xml parser with "invalid unicode character" some devices have borked relative urls, work around extremely unlikely we want to proxy upnp requests long start = systemtime.getmonotonoustime(); gotta retry with m-post method system.out.println( "upnp: invocation of " + control + "/" + soap_action + " took " + ( systemtime.getmonotonoustime() - start )); won't work with null ...."
com.aelitis.net.upnp.services.UPnPOfflineDownloader ""
com.aelitis.net.upnp.services.UPnPSpecificService ""
com.aelitis.net.upnp.services.UPnPWANCommonInterfaceConfig "returns link down/up speed in bits/sec"
com.aelitis.net.upnp.services.UPnPWANConnection "adda new port mapping from external port x to port x on local host false -> udp"
com.aelitis.net.upnp.services.UPnPWANConnectionListener ""
com.aelitis.net.upnp.services.UPnPWANConnectionPortMapping ""
com.aelitis.net.upnp.services.UPnPWANIPConnection ""
com.aelitis.net.upnp.services.UPnPWANPPPConnection ""
com.aelitis.net.upnp.UPnP "resets by removing all root devices and then rediscovering them scan for new logs a message to all registered log listeners"
com.aelitis.net.upnp.UPnPAction ""
com.aelitis.net.upnp.UPnPActionArgument ""
com.aelitis.net.upnp.UPnPActionInvocation "returns the out/parameters and response @return @throws upnpexception"
com.aelitis.net.upnp.UPnPAdapter ""
com.aelitis.net.upnp.UPnPDevice ""
com.aelitis.net.upnp.UPnPDeviceImage ""
com.aelitis.net.upnp.UPnPException ""
com.aelitis.net.upnp.UPnPFactory ""
com.aelitis.net.upnp.UPnPListener ""
com.aelitis.net.upnp.UPnPLogListener ""
com.aelitis.net.upnp.UPnPRootDevice ""
com.aelitis.net.upnp.UPnPRootDeviceListener ""
com.aelitis.net.upnp.UPnPService "gets a specific service if such is supported @return"
com.aelitis.net.upnp.UPnPSSDP ""
com.aelitis.net.upnp.UPnPSSDPAdapter "logs are generally more informative for the"
com.aelitis.net.upnp.UPnPSSDPListener ""
com.aelitis.net.upnp.UPnPStateVariable ""
com.aelitis.net.upnpms.impl.Test ""
com.aelitis.net.upnpms.impl.UPNPMSBrowserImpl "doc.print();"
com.aelitis.net.upnpms.impl.UPNPMSContainerImpl "result.print();"
com.aelitis.net.upnpms.impl.UPNPMSItemImpl ""
com.aelitis.net.upnpms.UPNPMSBrowser ""
com.aelitis.net.upnpms.UPNPMSBrowserFactory ""
com.aelitis.net.upnpms.UPNPMSBrowserListener ""
com.aelitis.net.upnpms.UPNPMSContainer ""
com.aelitis.net.upnpms.UPnPMSException ""
com.aelitis.net.upnpms.UPNPMSItem ""
com.aelitis.net.upnpms.UPNPMSNode ""
